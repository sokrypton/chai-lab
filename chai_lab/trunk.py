def forward(self, token_single_trunk_initial_repr, token_pair_trunk_initial_repr, token_single_trunk_repr, token_pair_trunk_repr, msa_input_feats, msa_mask, template_input_feats, template_input_masks, token_single_mask, token_pair_mask):
    arg1398_1, arg1399_1, arg1400_1, arg1401_1, arg1402_1, arg1403_1, arg1404_1, arg1405_1, arg1406_1, arg1407_1, = fx_pytree.tree_flatten_spec(([], {'token_single_trunk_initial_repr':token_single_trunk_initial_repr, 'token_pair_trunk_initial_repr':token_pair_trunk_initial_repr, 'token_single_trunk_repr':token_single_trunk_repr, 'token_pair_trunk_repr':token_pair_trunk_repr, 'msa_input_feats':msa_input_feats, 'msa_mask':msa_mask, 'template_input_feats':template_input_feats, 'template_input_masks':template_input_masks, 'token_single_mask':token_single_mask, 'token_pair_mask':token_pair_mask}), self._in_spec)
    template_embedder_pairformer_blocks_0_transition_pair_layer_norm_weight = getattr(self.template_embedder.pairformer.blocks, "0").transition_pair.layer_norm.weight
    template_embedder_pairformer_blocks_0_transition_pair_layer_norm_bias = getattr(self.template_embedder.pairformer.blocks, "0").transition_pair.layer_norm.bias
    template_embedder_pairformer_blocks_0_transition_pair_linear_no_bias_ab_weight = getattr(self.template_embedder.pairformer.blocks, "0").transition_pair.linear_no_bias_ab.weight
    template_embedder_pairformer_blocks_0_transition_pair_linear_out_weight = getattr(self.template_embedder.pairformer.blocks, "0").transition_pair.linear_out.weight
    template_embedder_pairformer_blocks_0_triangle_multiplication_layernorm_z_in_weight = getattr(self.template_embedder.pairformer.blocks, "0").triangle_multiplication.layernorm_z_in.weight
    template_embedder_pairformer_blocks_0_triangle_multiplication_layernorm_z_in_bias = getattr(self.template_embedder.pairformer.blocks, "0").triangle_multiplication.layernorm_z_in.bias
    template_embedder_pairformer_blocks_0_triangle_multiplication_linear_z_out_weight = getattr(self.template_embedder.pairformer.blocks, "0").triangle_multiplication.linear_z_out.weight
    template_embedder_pairformer_blocks_0_triangle_multiplication_merged_linear_p_weight = getattr(self.template_embedder.pairformer.blocks, "0").triangle_multiplication.merged_linear_p.weight
    template_embedder_pairformer_blocks_0_triangle_multiplication_merged_linear_g_weight = getattr(self.template_embedder.pairformer.blocks, "0").triangle_multiplication.merged_linear_g.weight
    template_embedder_pairformer_blocks_0_triangle_attention_out_scalers = getattr(self.template_embedder.pairformer.blocks, "0").triangle_attention.out_scalers
    template_embedder_pairformer_blocks_0_triangle_attention_pair2b_weight = getattr(self.template_embedder.pairformer.blocks, "0").triangle_attention.pair2b.weight
    template_embedder_pairformer_blocks_0_triangle_attention_pair2qkvg1_weight = getattr(self.template_embedder.pairformer.blocks, "0").triangle_attention.pair2qkvg1.weight
    template_embedder_pairformer_blocks_0_triangle_attention_pair2qkvg2_weight = getattr(self.template_embedder.pairformer.blocks, "0").triangle_attention.pair2qkvg2.weight
    template_embedder_pairformer_blocks_0_triangle_attention_linear_out_weight = getattr(self.template_embedder.pairformer.blocks, "0").triangle_attention.linear_out.weight
    template_embedder_pairformer_blocks_1_transition_pair_layer_norm_weight = getattr(self.template_embedder.pairformer.blocks, "1").transition_pair.layer_norm.weight
    template_embedder_pairformer_blocks_1_transition_pair_layer_norm_bias = getattr(self.template_embedder.pairformer.blocks, "1").transition_pair.layer_norm.bias
    template_embedder_pairformer_blocks_1_transition_pair_linear_no_bias_ab_weight = getattr(self.template_embedder.pairformer.blocks, "1").transition_pair.linear_no_bias_ab.weight
    template_embedder_pairformer_blocks_1_transition_pair_linear_out_weight = getattr(self.template_embedder.pairformer.blocks, "1").transition_pair.linear_out.weight
    template_embedder_pairformer_blocks_1_triangle_multiplication_layernorm_z_in_weight = getattr(self.template_embedder.pairformer.blocks, "1").triangle_multiplication.layernorm_z_in.weight
    template_embedder_pairformer_blocks_1_triangle_multiplication_layernorm_z_in_bias = getattr(self.template_embedder.pairformer.blocks, "1").triangle_multiplication.layernorm_z_in.bias
    template_embedder_pairformer_blocks_1_triangle_multiplication_linear_z_out_weight = getattr(self.template_embedder.pairformer.blocks, "1").triangle_multiplication.linear_z_out.weight
    template_embedder_pairformer_blocks_1_triangle_multiplication_merged_linear_p_weight = getattr(self.template_embedder.pairformer.blocks, "1").triangle_multiplication.merged_linear_p.weight
    template_embedder_pairformer_blocks_1_triangle_multiplication_merged_linear_g_weight = getattr(self.template_embedder.pairformer.blocks, "1").triangle_multiplication.merged_linear_g.weight
    template_embedder_pairformer_blocks_1_triangle_attention_out_scalers = getattr(self.template_embedder.pairformer.blocks, "1").triangle_attention.out_scalers
    template_embedder_pairformer_blocks_1_triangle_attention_pair2b_weight = getattr(self.template_embedder.pairformer.blocks, "1").triangle_attention.pair2b.weight
    template_embedder_pairformer_blocks_1_triangle_attention_pair2qkvg1_weight = getattr(self.template_embedder.pairformer.blocks, "1").triangle_attention.pair2qkvg1.weight
    template_embedder_pairformer_blocks_1_triangle_attention_pair2qkvg2_weight = getattr(self.template_embedder.pairformer.blocks, "1").triangle_attention.pair2qkvg2.weight
    template_embedder_pairformer_blocks_1_triangle_attention_linear_out_weight = getattr(self.template_embedder.pairformer.blocks, "1").triangle_attention.linear_out.weight
    template_embedder_proj_in_0_weight = getattr(self.template_embedder.proj_in, "0").weight
    template_embedder_proj_in_0_bias = getattr(self.template_embedder.proj_in, "0").bias
    template_embedder_proj_in_1_weight = getattr(self.template_embedder.proj_in, "1").weight
    template_embedder_template_layernorm_weight = self.template_embedder.template_layernorm.weight
    template_embedder_template_layernorm_bias = self.template_embedder.template_layernorm.bias
    template_embedder_proj_out_1_weight = getattr(self.template_embedder.proj_out, "1").weight
    msa_module_linear_s2m_weight = self.msa_module.linear_s2m.weight
    msa_module_outer_product_mean_0_weight_ab = getattr(self.msa_module.outer_product_mean, "0").weight_ab
    msa_module_outer_product_mean_0_ln_out_weight = getattr(self.msa_module.outer_product_mean, "0").ln_out.weight
    msa_module_outer_product_mean_0_ln_out_bias = getattr(self.msa_module.outer_product_mean, "0").ln_out.bias
    msa_module_outer_product_mean_0_linear_out_weight = getattr(self.msa_module.outer_product_mean, "0").linear_out.weight
    msa_module_outer_product_mean_0_linear_out_bias = getattr(self.msa_module.outer_product_mean, "0").linear_out.bias
    msa_module_outer_product_mean_1_weight_ab = getattr(self.msa_module.outer_product_mean, "1").weight_ab
    msa_module_outer_product_mean_1_ln_out_weight = getattr(self.msa_module.outer_product_mean, "1").ln_out.weight
    msa_module_outer_product_mean_1_ln_out_bias = getattr(self.msa_module.outer_product_mean, "1").ln_out.bias
    msa_module_outer_product_mean_1_linear_out_weight = getattr(self.msa_module.outer_product_mean, "1").linear_out.weight
    msa_module_outer_product_mean_1_linear_out_bias = getattr(self.msa_module.outer_product_mean, "1").linear_out.bias
    msa_module_outer_product_mean_2_weight_ab = getattr(self.msa_module.outer_product_mean, "2").weight_ab
    msa_module_outer_product_mean_2_ln_out_weight = getattr(self.msa_module.outer_product_mean, "2").ln_out.weight
    msa_module_outer_product_mean_2_ln_out_bias = getattr(self.msa_module.outer_product_mean, "2").ln_out.bias
    msa_module_outer_product_mean_2_linear_out_weight = getattr(self.msa_module.outer_product_mean, "2").linear_out.weight
    msa_module_outer_product_mean_2_linear_out_bias = getattr(self.msa_module.outer_product_mean, "2").linear_out.bias
    msa_module_outer_product_mean_3_weight_ab = getattr(self.msa_module.outer_product_mean, "3").weight_ab
    msa_module_outer_product_mean_3_ln_out_weight = getattr(self.msa_module.outer_product_mean, "3").ln_out.weight
    msa_module_outer_product_mean_3_ln_out_bias = getattr(self.msa_module.outer_product_mean, "3").ln_out.bias
    msa_module_outer_product_mean_3_linear_out_weight = getattr(self.msa_module.outer_product_mean, "3").linear_out.weight
    msa_module_outer_product_mean_3_linear_out_bias = getattr(self.msa_module.outer_product_mean, "3").linear_out.bias
    msa_module_msa_pair_weighted_averaging_0_layernorm_msa_weight = getattr(self.msa_module.msa_pair_weighted_averaging, "0").layernorm_msa.weight
    msa_module_msa_pair_weighted_averaging_0_layernorm_msa_bias = getattr(self.msa_module.msa_pair_weighted_averaging, "0").layernorm_msa.bias
    msa_module_msa_pair_weighted_averaging_0_linear_msa2vg_weight = getattr(self.msa_module.msa_pair_weighted_averaging, "0").linear_msa2vg.weight
    msa_module_msa_pair_weighted_averaging_0_layernorm_pair_weight = getattr(self.msa_module.msa_pair_weighted_averaging, "0").layernorm_pair.weight
    msa_module_msa_pair_weighted_averaging_0_layernorm_pair_bias = getattr(self.msa_module.msa_pair_weighted_averaging, "0").layernorm_pair.bias
    msa_module_msa_pair_weighted_averaging_0_linear_pair_weight = getattr(self.msa_module.msa_pair_weighted_averaging, "0").linear_pair.weight
    msa_module_msa_pair_weighted_averaging_0_linear_out_no_bias_weight = getattr(self.msa_module.msa_pair_weighted_averaging, "0").linear_out_no_bias.weight
    msa_module_msa_pair_weighted_averaging_1_layernorm_msa_weight = getattr(self.msa_module.msa_pair_weighted_averaging, "1").layernorm_msa.weight
    msa_module_msa_pair_weighted_averaging_1_layernorm_msa_bias = getattr(self.msa_module.msa_pair_weighted_averaging, "1").layernorm_msa.bias
    msa_module_msa_pair_weighted_averaging_1_linear_msa2vg_weight = getattr(self.msa_module.msa_pair_weighted_averaging, "1").linear_msa2vg.weight
    msa_module_msa_pair_weighted_averaging_1_layernorm_pair_weight = getattr(self.msa_module.msa_pair_weighted_averaging, "1").layernorm_pair.weight
    msa_module_msa_pair_weighted_averaging_1_layernorm_pair_bias = getattr(self.msa_module.msa_pair_weighted_averaging, "1").layernorm_pair.bias
    msa_module_msa_pair_weighted_averaging_1_linear_pair_weight = getattr(self.msa_module.msa_pair_weighted_averaging, "1").linear_pair.weight
    msa_module_msa_pair_weighted_averaging_1_linear_out_no_bias_weight = getattr(self.msa_module.msa_pair_weighted_averaging, "1").linear_out_no_bias.weight
    msa_module_msa_pair_weighted_averaging_2_layernorm_msa_weight = getattr(self.msa_module.msa_pair_weighted_averaging, "2").layernorm_msa.weight
    msa_module_msa_pair_weighted_averaging_2_layernorm_msa_bias = getattr(self.msa_module.msa_pair_weighted_averaging, "2").layernorm_msa.bias
    msa_module_msa_pair_weighted_averaging_2_linear_msa2vg_weight = getattr(self.msa_module.msa_pair_weighted_averaging, "2").linear_msa2vg.weight
    msa_module_msa_pair_weighted_averaging_2_layernorm_pair_weight = getattr(self.msa_module.msa_pair_weighted_averaging, "2").layernorm_pair.weight
    msa_module_msa_pair_weighted_averaging_2_layernorm_pair_bias = getattr(self.msa_module.msa_pair_weighted_averaging, "2").layernorm_pair.bias
    msa_module_msa_pair_weighted_averaging_2_linear_pair_weight = getattr(self.msa_module.msa_pair_weighted_averaging, "2").linear_pair.weight
    msa_module_msa_pair_weighted_averaging_2_linear_out_no_bias_weight = getattr(self.msa_module.msa_pair_weighted_averaging, "2").linear_out_no_bias.weight
    msa_module_msa_transition_0_layer_norm_weight = getattr(self.msa_module.msa_transition, "0").layer_norm.weight
    msa_module_msa_transition_0_layer_norm_bias = getattr(self.msa_module.msa_transition, "0").layer_norm.bias
    msa_module_msa_transition_0_linear_no_bias_ab_weight = getattr(self.msa_module.msa_transition, "0").linear_no_bias_ab.weight
    msa_module_msa_transition_0_linear_out_weight = getattr(self.msa_module.msa_transition, "0").linear_out.weight
    msa_module_msa_transition_1_layer_norm_weight = getattr(self.msa_module.msa_transition, "1").layer_norm.weight
    msa_module_msa_transition_1_layer_norm_bias = getattr(self.msa_module.msa_transition, "1").layer_norm.bias
    msa_module_msa_transition_1_linear_no_bias_ab_weight = getattr(self.msa_module.msa_transition, "1").linear_no_bias_ab.weight
    msa_module_msa_transition_1_linear_out_weight = getattr(self.msa_module.msa_transition, "1").linear_out.weight
    msa_module_msa_transition_2_layer_norm_weight = getattr(self.msa_module.msa_transition, "2").layer_norm.weight
    msa_module_msa_transition_2_layer_norm_bias = getattr(self.msa_module.msa_transition, "2").layer_norm.bias
    msa_module_msa_transition_2_linear_no_bias_ab_weight = getattr(self.msa_module.msa_transition, "2").linear_no_bias_ab.weight
    msa_module_msa_transition_2_linear_out_weight = getattr(self.msa_module.msa_transition, "2").linear_out.weight
    msa_module_pair_transition_0_layer_norm_weight = getattr(self.msa_module.pair_transition, "0").layer_norm.weight
    msa_module_pair_transition_0_layer_norm_bias = getattr(self.msa_module.pair_transition, "0").layer_norm.bias
    msa_module_pair_transition_0_linear_no_bias_ab_weight = getattr(self.msa_module.pair_transition, "0").linear_no_bias_ab.weight
    msa_module_pair_transition_0_linear_out_weight = getattr(self.msa_module.pair_transition, "0").linear_out.weight
    msa_module_pair_transition_1_layer_norm_weight = getattr(self.msa_module.pair_transition, "1").layer_norm.weight
    msa_module_pair_transition_1_layer_norm_bias = getattr(self.msa_module.pair_transition, "1").layer_norm.bias
    msa_module_pair_transition_1_linear_no_bias_ab_weight = getattr(self.msa_module.pair_transition, "1").linear_no_bias_ab.weight
    msa_module_pair_transition_1_linear_out_weight = getattr(self.msa_module.pair_transition, "1").linear_out.weight
    msa_module_pair_transition_2_layer_norm_weight = getattr(self.msa_module.pair_transition, "2").layer_norm.weight
    msa_module_pair_transition_2_layer_norm_bias = getattr(self.msa_module.pair_transition, "2").layer_norm.bias
    msa_module_pair_transition_2_linear_no_bias_ab_weight = getattr(self.msa_module.pair_transition, "2").linear_no_bias_ab.weight
    msa_module_pair_transition_2_linear_out_weight = getattr(self.msa_module.pair_transition, "2").linear_out.weight
    msa_module_pair_transition_3_layer_norm_weight = getattr(self.msa_module.pair_transition, "3").layer_norm.weight
    msa_module_pair_transition_3_layer_norm_bias = getattr(self.msa_module.pair_transition, "3").layer_norm.bias
    msa_module_pair_transition_3_linear_no_bias_ab_weight = getattr(self.msa_module.pair_transition, "3").linear_no_bias_ab.weight
    msa_module_pair_transition_3_linear_out_weight = getattr(self.msa_module.pair_transition, "3").linear_out.weight
    msa_module_triangular_multiplication_0_layernorm_z_in_weight = getattr(self.msa_module.triangular_multiplication, "0").layernorm_z_in.weight
    msa_module_triangular_multiplication_0_layernorm_z_in_bias = getattr(self.msa_module.triangular_multiplication, "0").layernorm_z_in.bias
    msa_module_triangular_multiplication_0_linear_z_out_weight = getattr(self.msa_module.triangular_multiplication, "0").linear_z_out.weight
    msa_module_triangular_multiplication_0_merged_linear_p_weight = getattr(self.msa_module.triangular_multiplication, "0").merged_linear_p.weight
    msa_module_triangular_multiplication_0_merged_linear_g_weight = getattr(self.msa_module.triangular_multiplication, "0").merged_linear_g.weight
    msa_module_triangular_multiplication_1_layernorm_z_in_weight = getattr(self.msa_module.triangular_multiplication, "1").layernorm_z_in.weight
    msa_module_triangular_multiplication_1_layernorm_z_in_bias = getattr(self.msa_module.triangular_multiplication, "1").layernorm_z_in.bias
    msa_module_triangular_multiplication_1_linear_z_out_weight = getattr(self.msa_module.triangular_multiplication, "1").linear_z_out.weight
    msa_module_triangular_multiplication_1_merged_linear_p_weight = getattr(self.msa_module.triangular_multiplication, "1").merged_linear_p.weight
    msa_module_triangular_multiplication_1_merged_linear_g_weight = getattr(self.msa_module.triangular_multiplication, "1").merged_linear_g.weight
    msa_module_triangular_multiplication_2_layernorm_z_in_weight = getattr(self.msa_module.triangular_multiplication, "2").layernorm_z_in.weight
    msa_module_triangular_multiplication_2_layernorm_z_in_bias = getattr(self.msa_module.triangular_multiplication, "2").layernorm_z_in.bias
    msa_module_triangular_multiplication_2_linear_z_out_weight = getattr(self.msa_module.triangular_multiplication, "2").linear_z_out.weight
    msa_module_triangular_multiplication_2_merged_linear_p_weight = getattr(self.msa_module.triangular_multiplication, "2").merged_linear_p.weight
    msa_module_triangular_multiplication_2_merged_linear_g_weight = getattr(self.msa_module.triangular_multiplication, "2").merged_linear_g.weight
    msa_module_triangular_multiplication_3_layernorm_z_in_weight = getattr(self.msa_module.triangular_multiplication, "3").layernorm_z_in.weight
    msa_module_triangular_multiplication_3_layernorm_z_in_bias = getattr(self.msa_module.triangular_multiplication, "3").layernorm_z_in.bias
    msa_module_triangular_multiplication_3_linear_z_out_weight = getattr(self.msa_module.triangular_multiplication, "3").linear_z_out.weight
    msa_module_triangular_multiplication_3_merged_linear_p_weight = getattr(self.msa_module.triangular_multiplication, "3").merged_linear_p.weight
    msa_module_triangular_multiplication_3_merged_linear_g_weight = getattr(self.msa_module.triangular_multiplication, "3").merged_linear_g.weight
    msa_module_triangular_attention_0_out_scalers = getattr(self.msa_module.triangular_attention, "0").out_scalers
    msa_module_triangular_attention_0_pair2b_weight = getattr(self.msa_module.triangular_attention, "0").pair2b.weight
    msa_module_triangular_attention_0_pair2qkvg1_weight = getattr(self.msa_module.triangular_attention, "0").pair2qkvg1.weight
    msa_module_triangular_attention_0_pair2qkvg2_weight = getattr(self.msa_module.triangular_attention, "0").pair2qkvg2.weight
    msa_module_triangular_attention_0_linear_out_weight = getattr(self.msa_module.triangular_attention, "0").linear_out.weight
    msa_module_triangular_attention_1_out_scalers = getattr(self.msa_module.triangular_attention, "1").out_scalers
    msa_module_triangular_attention_1_pair2b_weight = getattr(self.msa_module.triangular_attention, "1").pair2b.weight
    msa_module_triangular_attention_1_pair2qkvg1_weight = getattr(self.msa_module.triangular_attention, "1").pair2qkvg1.weight
    msa_module_triangular_attention_1_pair2qkvg2_weight = getattr(self.msa_module.triangular_attention, "1").pair2qkvg2.weight
    msa_module_triangular_attention_1_linear_out_weight = getattr(self.msa_module.triangular_attention, "1").linear_out.weight
    msa_module_triangular_attention_2_out_scalers = getattr(self.msa_module.triangular_attention, "2").out_scalers
    msa_module_triangular_attention_2_pair2b_weight = getattr(self.msa_module.triangular_attention, "2").pair2b.weight
    msa_module_triangular_attention_2_pair2qkvg1_weight = getattr(self.msa_module.triangular_attention, "2").pair2qkvg1.weight
    msa_module_triangular_attention_2_pair2qkvg2_weight = getattr(self.msa_module.triangular_attention, "2").pair2qkvg2.weight
    msa_module_triangular_attention_2_linear_out_weight = getattr(self.msa_module.triangular_attention, "2").linear_out.weight
    msa_module_triangular_attention_3_out_scalers = getattr(self.msa_module.triangular_attention, "3").out_scalers
    msa_module_triangular_attention_3_pair2b_weight = getattr(self.msa_module.triangular_attention, "3").pair2b.weight
    msa_module_triangular_attention_3_pair2qkvg1_weight = getattr(self.msa_module.triangular_attention, "3").pair2qkvg1.weight
    msa_module_triangular_attention_3_pair2qkvg2_weight = getattr(self.msa_module.triangular_attention, "3").pair2qkvg2.weight
    msa_module_triangular_attention_3_linear_out_weight = getattr(self.msa_module.triangular_attention, "3").linear_out.weight
    pairformer_stack_blocks_0_transition_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "0").transition_pair.layer_norm.weight
    pairformer_stack_blocks_0_transition_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "0").transition_pair.layer_norm.bias
    pairformer_stack_blocks_0_transition_pair_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "0").transition_pair.linear_no_bias_ab.weight
    pairformer_stack_blocks_0_transition_pair_linear_out_weight = getattr(self.pairformer_stack.blocks, "0").transition_pair.linear_out.weight
    pairformer_stack_blocks_0_triangle_multiplication_layernorm_z_in_weight = getattr(self.pairformer_stack.blocks, "0").triangle_multiplication.layernorm_z_in.weight
    pairformer_stack_blocks_0_triangle_multiplication_layernorm_z_in_bias = getattr(self.pairformer_stack.blocks, "0").triangle_multiplication.layernorm_z_in.bias
    pairformer_stack_blocks_0_triangle_multiplication_linear_z_out_weight = getattr(self.pairformer_stack.blocks, "0").triangle_multiplication.linear_z_out.weight
    pairformer_stack_blocks_0_triangle_multiplication_merged_linear_p_weight = getattr(self.pairformer_stack.blocks, "0").triangle_multiplication.merged_linear_p.weight
    pairformer_stack_blocks_0_triangle_multiplication_merged_linear_g_weight = getattr(self.pairformer_stack.blocks, "0").triangle_multiplication.merged_linear_g.weight
    pairformer_stack_blocks_0_triangle_attention_out_scalers = getattr(self.pairformer_stack.blocks, "0").triangle_attention.out_scalers
    pairformer_stack_blocks_0_triangle_attention_pair2b_weight = getattr(self.pairformer_stack.blocks, "0").triangle_attention.pair2b.weight
    pairformer_stack_blocks_0_triangle_attention_pair2qkvg1_weight = getattr(self.pairformer_stack.blocks, "0").triangle_attention.pair2qkvg1.weight
    pairformer_stack_blocks_0_triangle_attention_pair2qkvg2_weight = getattr(self.pairformer_stack.blocks, "0").triangle_attention.pair2qkvg2.weight
    pairformer_stack_blocks_0_triangle_attention_linear_out_weight = getattr(self.pairformer_stack.blocks, "0").triangle_attention.linear_out.weight
    pairformer_stack_blocks_0_transition_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "0").transition_single.layer_norm.weight
    pairformer_stack_blocks_0_transition_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "0").transition_single.layer_norm.bias
    pairformer_stack_blocks_0_transition_single_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "0").transition_single.linear_no_bias_ab.weight
    pairformer_stack_blocks_0_transition_single_linear_out_weight = getattr(self.pairformer_stack.blocks, "0").transition_single.linear_out.weight
    pairformer_stack_blocks_0_attention_pair_bias_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "0").attention_pair_bias.single_layer_norm.weight
    pairformer_stack_blocks_0_attention_pair_bias_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "0").attention_pair_bias.single_layer_norm.bias
    pairformer_stack_blocks_0_attention_pair_bias_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "0").attention_pair_bias.pair_layer_norm.weight
    pairformer_stack_blocks_0_attention_pair_bias_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "0").attention_pair_bias.pair_layer_norm.bias
    pairformer_stack_blocks_0_attention_pair_bias_pair_linear_weight = getattr(self.pairformer_stack.blocks, "0").attention_pair_bias.pair_linear.weight
    pairformer_stack_blocks_0_attention_pair_bias_attention_query_bias = getattr(self.pairformer_stack.blocks, "0").attention_pair_bias.attention.query_bias
    pairformer_stack_blocks_0_attention_pair_bias_attention_input2qkvg_weight = getattr(self.pairformer_stack.blocks, "0").attention_pair_bias.attention.input2qkvg.weight
    pairformer_stack_blocks_0_attention_pair_bias_attention_output_proj_weight = getattr(self.pairformer_stack.blocks, "0").attention_pair_bias.attention.output_proj.weight
    pairformer_stack_blocks_1_transition_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "1").transition_pair.layer_norm.weight
    pairformer_stack_blocks_1_transition_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "1").transition_pair.layer_norm.bias
    pairformer_stack_blocks_1_transition_pair_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "1").transition_pair.linear_no_bias_ab.weight
    pairformer_stack_blocks_1_transition_pair_linear_out_weight = getattr(self.pairformer_stack.blocks, "1").transition_pair.linear_out.weight
    pairformer_stack_blocks_1_triangle_multiplication_layernorm_z_in_weight = getattr(self.pairformer_stack.blocks, "1").triangle_multiplication.layernorm_z_in.weight
    pairformer_stack_blocks_1_triangle_multiplication_layernorm_z_in_bias = getattr(self.pairformer_stack.blocks, "1").triangle_multiplication.layernorm_z_in.bias
    pairformer_stack_blocks_1_triangle_multiplication_linear_z_out_weight = getattr(self.pairformer_stack.blocks, "1").triangle_multiplication.linear_z_out.weight
    pairformer_stack_blocks_1_triangle_multiplication_merged_linear_p_weight = getattr(self.pairformer_stack.blocks, "1").triangle_multiplication.merged_linear_p.weight
    pairformer_stack_blocks_1_triangle_multiplication_merged_linear_g_weight = getattr(self.pairformer_stack.blocks, "1").triangle_multiplication.merged_linear_g.weight
    pairformer_stack_blocks_1_triangle_attention_out_scalers = getattr(self.pairformer_stack.blocks, "1").triangle_attention.out_scalers
    pairformer_stack_blocks_1_triangle_attention_pair2b_weight = getattr(self.pairformer_stack.blocks, "1").triangle_attention.pair2b.weight
    pairformer_stack_blocks_1_triangle_attention_pair2qkvg1_weight = getattr(self.pairformer_stack.blocks, "1").triangle_attention.pair2qkvg1.weight
    pairformer_stack_blocks_1_triangle_attention_pair2qkvg2_weight = getattr(self.pairformer_stack.blocks, "1").triangle_attention.pair2qkvg2.weight
    pairformer_stack_blocks_1_triangle_attention_linear_out_weight = getattr(self.pairformer_stack.blocks, "1").triangle_attention.linear_out.weight
    pairformer_stack_blocks_1_transition_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "1").transition_single.layer_norm.weight
    pairformer_stack_blocks_1_transition_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "1").transition_single.layer_norm.bias
    pairformer_stack_blocks_1_transition_single_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "1").transition_single.linear_no_bias_ab.weight
    pairformer_stack_blocks_1_transition_single_linear_out_weight = getattr(self.pairformer_stack.blocks, "1").transition_single.linear_out.weight
    pairformer_stack_blocks_1_attention_pair_bias_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "1").attention_pair_bias.single_layer_norm.weight
    pairformer_stack_blocks_1_attention_pair_bias_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "1").attention_pair_bias.single_layer_norm.bias
    pairformer_stack_blocks_1_attention_pair_bias_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "1").attention_pair_bias.pair_layer_norm.weight
    pairformer_stack_blocks_1_attention_pair_bias_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "1").attention_pair_bias.pair_layer_norm.bias
    pairformer_stack_blocks_1_attention_pair_bias_pair_linear_weight = getattr(self.pairformer_stack.blocks, "1").attention_pair_bias.pair_linear.weight
    pairformer_stack_blocks_1_attention_pair_bias_attention_query_bias = getattr(self.pairformer_stack.blocks, "1").attention_pair_bias.attention.query_bias
    pairformer_stack_blocks_1_attention_pair_bias_attention_input2qkvg_weight = getattr(self.pairformer_stack.blocks, "1").attention_pair_bias.attention.input2qkvg.weight
    pairformer_stack_blocks_1_attention_pair_bias_attention_output_proj_weight = getattr(self.pairformer_stack.blocks, "1").attention_pair_bias.attention.output_proj.weight
    pairformer_stack_blocks_2_transition_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "2").transition_pair.layer_norm.weight
    pairformer_stack_blocks_2_transition_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "2").transition_pair.layer_norm.bias
    pairformer_stack_blocks_2_transition_pair_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "2").transition_pair.linear_no_bias_ab.weight
    pairformer_stack_blocks_2_transition_pair_linear_out_weight = getattr(self.pairformer_stack.blocks, "2").transition_pair.linear_out.weight
    pairformer_stack_blocks_2_triangle_multiplication_layernorm_z_in_weight = getattr(self.pairformer_stack.blocks, "2").triangle_multiplication.layernorm_z_in.weight
    pairformer_stack_blocks_2_triangle_multiplication_layernorm_z_in_bias = getattr(self.pairformer_stack.blocks, "2").triangle_multiplication.layernorm_z_in.bias
    pairformer_stack_blocks_2_triangle_multiplication_linear_z_out_weight = getattr(self.pairformer_stack.blocks, "2").triangle_multiplication.linear_z_out.weight
    pairformer_stack_blocks_2_triangle_multiplication_merged_linear_p_weight = getattr(self.pairformer_stack.blocks, "2").triangle_multiplication.merged_linear_p.weight
    pairformer_stack_blocks_2_triangle_multiplication_merged_linear_g_weight = getattr(self.pairformer_stack.blocks, "2").triangle_multiplication.merged_linear_g.weight
    pairformer_stack_blocks_2_triangle_attention_out_scalers = getattr(self.pairformer_stack.blocks, "2").triangle_attention.out_scalers
    pairformer_stack_blocks_2_triangle_attention_pair2b_weight = getattr(self.pairformer_stack.blocks, "2").triangle_attention.pair2b.weight
    pairformer_stack_blocks_2_triangle_attention_pair2qkvg1_weight = getattr(self.pairformer_stack.blocks, "2").triangle_attention.pair2qkvg1.weight
    pairformer_stack_blocks_2_triangle_attention_pair2qkvg2_weight = getattr(self.pairformer_stack.blocks, "2").triangle_attention.pair2qkvg2.weight
    pairformer_stack_blocks_2_triangle_attention_linear_out_weight = getattr(self.pairformer_stack.blocks, "2").triangle_attention.linear_out.weight
    pairformer_stack_blocks_2_transition_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "2").transition_single.layer_norm.weight
    pairformer_stack_blocks_2_transition_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "2").transition_single.layer_norm.bias
    pairformer_stack_blocks_2_transition_single_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "2").transition_single.linear_no_bias_ab.weight
    pairformer_stack_blocks_2_transition_single_linear_out_weight = getattr(self.pairformer_stack.blocks, "2").transition_single.linear_out.weight
    pairformer_stack_blocks_2_attention_pair_bias_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "2").attention_pair_bias.single_layer_norm.weight
    pairformer_stack_blocks_2_attention_pair_bias_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "2").attention_pair_bias.single_layer_norm.bias
    pairformer_stack_blocks_2_attention_pair_bias_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "2").attention_pair_bias.pair_layer_norm.weight
    pairformer_stack_blocks_2_attention_pair_bias_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "2").attention_pair_bias.pair_layer_norm.bias
    pairformer_stack_blocks_2_attention_pair_bias_pair_linear_weight = getattr(self.pairformer_stack.blocks, "2").attention_pair_bias.pair_linear.weight
    pairformer_stack_blocks_2_attention_pair_bias_attention_query_bias = getattr(self.pairformer_stack.blocks, "2").attention_pair_bias.attention.query_bias
    pairformer_stack_blocks_2_attention_pair_bias_attention_input2qkvg_weight = getattr(self.pairformer_stack.blocks, "2").attention_pair_bias.attention.input2qkvg.weight
    pairformer_stack_blocks_2_attention_pair_bias_attention_output_proj_weight = getattr(self.pairformer_stack.blocks, "2").attention_pair_bias.attention.output_proj.weight
    pairformer_stack_blocks_3_transition_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "3").transition_pair.layer_norm.weight
    pairformer_stack_blocks_3_transition_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "3").transition_pair.layer_norm.bias
    pairformer_stack_blocks_3_transition_pair_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "3").transition_pair.linear_no_bias_ab.weight
    pairformer_stack_blocks_3_transition_pair_linear_out_weight = getattr(self.pairformer_stack.blocks, "3").transition_pair.linear_out.weight
    pairformer_stack_blocks_3_triangle_multiplication_layernorm_z_in_weight = getattr(self.pairformer_stack.blocks, "3").triangle_multiplication.layernorm_z_in.weight
    pairformer_stack_blocks_3_triangle_multiplication_layernorm_z_in_bias = getattr(self.pairformer_stack.blocks, "3").triangle_multiplication.layernorm_z_in.bias
    pairformer_stack_blocks_3_triangle_multiplication_linear_z_out_weight = getattr(self.pairformer_stack.blocks, "3").triangle_multiplication.linear_z_out.weight
    pairformer_stack_blocks_3_triangle_multiplication_merged_linear_p_weight = getattr(self.pairformer_stack.blocks, "3").triangle_multiplication.merged_linear_p.weight
    pairformer_stack_blocks_3_triangle_multiplication_merged_linear_g_weight = getattr(self.pairformer_stack.blocks, "3").triangle_multiplication.merged_linear_g.weight
    pairformer_stack_blocks_3_triangle_attention_out_scalers = getattr(self.pairformer_stack.blocks, "3").triangle_attention.out_scalers
    pairformer_stack_blocks_3_triangle_attention_pair2b_weight = getattr(self.pairformer_stack.blocks, "3").triangle_attention.pair2b.weight
    pairformer_stack_blocks_3_triangle_attention_pair2qkvg1_weight = getattr(self.pairformer_stack.blocks, "3").triangle_attention.pair2qkvg1.weight
    pairformer_stack_blocks_3_triangle_attention_pair2qkvg2_weight = getattr(self.pairformer_stack.blocks, "3").triangle_attention.pair2qkvg2.weight
    pairformer_stack_blocks_3_triangle_attention_linear_out_weight = getattr(self.pairformer_stack.blocks, "3").triangle_attention.linear_out.weight
    pairformer_stack_blocks_3_transition_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "3").transition_single.layer_norm.weight
    pairformer_stack_blocks_3_transition_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "3").transition_single.layer_norm.bias
    pairformer_stack_blocks_3_transition_single_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "3").transition_single.linear_no_bias_ab.weight
    pairformer_stack_blocks_3_transition_single_linear_out_weight = getattr(self.pairformer_stack.blocks, "3").transition_single.linear_out.weight
    pairformer_stack_blocks_3_attention_pair_bias_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "3").attention_pair_bias.single_layer_norm.weight
    pairformer_stack_blocks_3_attention_pair_bias_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "3").attention_pair_bias.single_layer_norm.bias
    pairformer_stack_blocks_3_attention_pair_bias_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "3").attention_pair_bias.pair_layer_norm.weight
    pairformer_stack_blocks_3_attention_pair_bias_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "3").attention_pair_bias.pair_layer_norm.bias
    pairformer_stack_blocks_3_attention_pair_bias_pair_linear_weight = getattr(self.pairformer_stack.blocks, "3").attention_pair_bias.pair_linear.weight
    pairformer_stack_blocks_3_attention_pair_bias_attention_query_bias = getattr(self.pairformer_stack.blocks, "3").attention_pair_bias.attention.query_bias
    pairformer_stack_blocks_3_attention_pair_bias_attention_input2qkvg_weight = getattr(self.pairformer_stack.blocks, "3").attention_pair_bias.attention.input2qkvg.weight
    pairformer_stack_blocks_3_attention_pair_bias_attention_output_proj_weight = getattr(self.pairformer_stack.blocks, "3").attention_pair_bias.attention.output_proj.weight
    pairformer_stack_blocks_4_transition_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "4").transition_pair.layer_norm.weight
    pairformer_stack_blocks_4_transition_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "4").transition_pair.layer_norm.bias
    pairformer_stack_blocks_4_transition_pair_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "4").transition_pair.linear_no_bias_ab.weight
    pairformer_stack_blocks_4_transition_pair_linear_out_weight = getattr(self.pairformer_stack.blocks, "4").transition_pair.linear_out.weight
    pairformer_stack_blocks_4_triangle_multiplication_layernorm_z_in_weight = getattr(self.pairformer_stack.blocks, "4").triangle_multiplication.layernorm_z_in.weight
    pairformer_stack_blocks_4_triangle_multiplication_layernorm_z_in_bias = getattr(self.pairformer_stack.blocks, "4").triangle_multiplication.layernorm_z_in.bias
    pairformer_stack_blocks_4_triangle_multiplication_linear_z_out_weight = getattr(self.pairformer_stack.blocks, "4").triangle_multiplication.linear_z_out.weight
    pairformer_stack_blocks_4_triangle_multiplication_merged_linear_p_weight = getattr(self.pairformer_stack.blocks, "4").triangle_multiplication.merged_linear_p.weight
    pairformer_stack_blocks_4_triangle_multiplication_merged_linear_g_weight = getattr(self.pairformer_stack.blocks, "4").triangle_multiplication.merged_linear_g.weight
    pairformer_stack_blocks_4_triangle_attention_out_scalers = getattr(self.pairformer_stack.blocks, "4").triangle_attention.out_scalers
    pairformer_stack_blocks_4_triangle_attention_pair2b_weight = getattr(self.pairformer_stack.blocks, "4").triangle_attention.pair2b.weight
    pairformer_stack_blocks_4_triangle_attention_pair2qkvg1_weight = getattr(self.pairformer_stack.blocks, "4").triangle_attention.pair2qkvg1.weight
    pairformer_stack_blocks_4_triangle_attention_pair2qkvg2_weight = getattr(self.pairformer_stack.blocks, "4").triangle_attention.pair2qkvg2.weight
    pairformer_stack_blocks_4_triangle_attention_linear_out_weight = getattr(self.pairformer_stack.blocks, "4").triangle_attention.linear_out.weight
    pairformer_stack_blocks_4_transition_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "4").transition_single.layer_norm.weight
    pairformer_stack_blocks_4_transition_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "4").transition_single.layer_norm.bias
    pairformer_stack_blocks_4_transition_single_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "4").transition_single.linear_no_bias_ab.weight
    pairformer_stack_blocks_4_transition_single_linear_out_weight = getattr(self.pairformer_stack.blocks, "4").transition_single.linear_out.weight
    pairformer_stack_blocks_4_attention_pair_bias_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "4").attention_pair_bias.single_layer_norm.weight
    pairformer_stack_blocks_4_attention_pair_bias_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "4").attention_pair_bias.single_layer_norm.bias
    pairformer_stack_blocks_4_attention_pair_bias_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "4").attention_pair_bias.pair_layer_norm.weight
    pairformer_stack_blocks_4_attention_pair_bias_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "4").attention_pair_bias.pair_layer_norm.bias
    pairformer_stack_blocks_4_attention_pair_bias_pair_linear_weight = getattr(self.pairformer_stack.blocks, "4").attention_pair_bias.pair_linear.weight
    pairformer_stack_blocks_4_attention_pair_bias_attention_query_bias = getattr(self.pairformer_stack.blocks, "4").attention_pair_bias.attention.query_bias
    pairformer_stack_blocks_4_attention_pair_bias_attention_input2qkvg_weight = getattr(self.pairformer_stack.blocks, "4").attention_pair_bias.attention.input2qkvg.weight
    pairformer_stack_blocks_4_attention_pair_bias_attention_output_proj_weight = getattr(self.pairformer_stack.blocks, "4").attention_pair_bias.attention.output_proj.weight
    pairformer_stack_blocks_5_transition_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "5").transition_pair.layer_norm.weight
    pairformer_stack_blocks_5_transition_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "5").transition_pair.layer_norm.bias
    pairformer_stack_blocks_5_transition_pair_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "5").transition_pair.linear_no_bias_ab.weight
    pairformer_stack_blocks_5_transition_pair_linear_out_weight = getattr(self.pairformer_stack.blocks, "5").transition_pair.linear_out.weight
    pairformer_stack_blocks_5_triangle_multiplication_layernorm_z_in_weight = getattr(self.pairformer_stack.blocks, "5").triangle_multiplication.layernorm_z_in.weight
    pairformer_stack_blocks_5_triangle_multiplication_layernorm_z_in_bias = getattr(self.pairformer_stack.blocks, "5").triangle_multiplication.layernorm_z_in.bias
    pairformer_stack_blocks_5_triangle_multiplication_linear_z_out_weight = getattr(self.pairformer_stack.blocks, "5").triangle_multiplication.linear_z_out.weight
    pairformer_stack_blocks_5_triangle_multiplication_merged_linear_p_weight = getattr(self.pairformer_stack.blocks, "5").triangle_multiplication.merged_linear_p.weight
    pairformer_stack_blocks_5_triangle_multiplication_merged_linear_g_weight = getattr(self.pairformer_stack.blocks, "5").triangle_multiplication.merged_linear_g.weight
    pairformer_stack_blocks_5_triangle_attention_out_scalers = getattr(self.pairformer_stack.blocks, "5").triangle_attention.out_scalers
    pairformer_stack_blocks_5_triangle_attention_pair2b_weight = getattr(self.pairformer_stack.blocks, "5").triangle_attention.pair2b.weight
    pairformer_stack_blocks_5_triangle_attention_pair2qkvg1_weight = getattr(self.pairformer_stack.blocks, "5").triangle_attention.pair2qkvg1.weight
    pairformer_stack_blocks_5_triangle_attention_pair2qkvg2_weight = getattr(self.pairformer_stack.blocks, "5").triangle_attention.pair2qkvg2.weight
    pairformer_stack_blocks_5_triangle_attention_linear_out_weight = getattr(self.pairformer_stack.blocks, "5").triangle_attention.linear_out.weight
    pairformer_stack_blocks_5_transition_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "5").transition_single.layer_norm.weight
    pairformer_stack_blocks_5_transition_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "5").transition_single.layer_norm.bias
    pairformer_stack_blocks_5_transition_single_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "5").transition_single.linear_no_bias_ab.weight
    pairformer_stack_blocks_5_transition_single_linear_out_weight = getattr(self.pairformer_stack.blocks, "5").transition_single.linear_out.weight
    pairformer_stack_blocks_5_attention_pair_bias_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "5").attention_pair_bias.single_layer_norm.weight
    pairformer_stack_blocks_5_attention_pair_bias_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "5").attention_pair_bias.single_layer_norm.bias
    pairformer_stack_blocks_5_attention_pair_bias_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "5").attention_pair_bias.pair_layer_norm.weight
    pairformer_stack_blocks_5_attention_pair_bias_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "5").attention_pair_bias.pair_layer_norm.bias
    pairformer_stack_blocks_5_attention_pair_bias_pair_linear_weight = getattr(self.pairformer_stack.blocks, "5").attention_pair_bias.pair_linear.weight
    pairformer_stack_blocks_5_attention_pair_bias_attention_query_bias = getattr(self.pairformer_stack.blocks, "5").attention_pair_bias.attention.query_bias
    pairformer_stack_blocks_5_attention_pair_bias_attention_input2qkvg_weight = getattr(self.pairformer_stack.blocks, "5").attention_pair_bias.attention.input2qkvg.weight
    pairformer_stack_blocks_5_attention_pair_bias_attention_output_proj_weight = getattr(self.pairformer_stack.blocks, "5").attention_pair_bias.attention.output_proj.weight
    pairformer_stack_blocks_6_transition_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "6").transition_pair.layer_norm.weight
    pairformer_stack_blocks_6_transition_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "6").transition_pair.layer_norm.bias
    pairformer_stack_blocks_6_transition_pair_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "6").transition_pair.linear_no_bias_ab.weight
    pairformer_stack_blocks_6_transition_pair_linear_out_weight = getattr(self.pairformer_stack.blocks, "6").transition_pair.linear_out.weight
    pairformer_stack_blocks_6_triangle_multiplication_layernorm_z_in_weight = getattr(self.pairformer_stack.blocks, "6").triangle_multiplication.layernorm_z_in.weight
    pairformer_stack_blocks_6_triangle_multiplication_layernorm_z_in_bias = getattr(self.pairformer_stack.blocks, "6").triangle_multiplication.layernorm_z_in.bias
    pairformer_stack_blocks_6_triangle_multiplication_linear_z_out_weight = getattr(self.pairformer_stack.blocks, "6").triangle_multiplication.linear_z_out.weight
    pairformer_stack_blocks_6_triangle_multiplication_merged_linear_p_weight = getattr(self.pairformer_stack.blocks, "6").triangle_multiplication.merged_linear_p.weight
    pairformer_stack_blocks_6_triangle_multiplication_merged_linear_g_weight = getattr(self.pairformer_stack.blocks, "6").triangle_multiplication.merged_linear_g.weight
    pairformer_stack_blocks_6_triangle_attention_out_scalers = getattr(self.pairformer_stack.blocks, "6").triangle_attention.out_scalers
    pairformer_stack_blocks_6_triangle_attention_pair2b_weight = getattr(self.pairformer_stack.blocks, "6").triangle_attention.pair2b.weight
    pairformer_stack_blocks_6_triangle_attention_pair2qkvg1_weight = getattr(self.pairformer_stack.blocks, "6").triangle_attention.pair2qkvg1.weight
    pairformer_stack_blocks_6_triangle_attention_pair2qkvg2_weight = getattr(self.pairformer_stack.blocks, "6").triangle_attention.pair2qkvg2.weight
    pairformer_stack_blocks_6_triangle_attention_linear_out_weight = getattr(self.pairformer_stack.blocks, "6").triangle_attention.linear_out.weight
    pairformer_stack_blocks_6_transition_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "6").transition_single.layer_norm.weight
    pairformer_stack_blocks_6_transition_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "6").transition_single.layer_norm.bias
    pairformer_stack_blocks_6_transition_single_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "6").transition_single.linear_no_bias_ab.weight
    pairformer_stack_blocks_6_transition_single_linear_out_weight = getattr(self.pairformer_stack.blocks, "6").transition_single.linear_out.weight
    pairformer_stack_blocks_6_attention_pair_bias_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "6").attention_pair_bias.single_layer_norm.weight
    pairformer_stack_blocks_6_attention_pair_bias_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "6").attention_pair_bias.single_layer_norm.bias
    pairformer_stack_blocks_6_attention_pair_bias_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "6").attention_pair_bias.pair_layer_norm.weight
    pairformer_stack_blocks_6_attention_pair_bias_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "6").attention_pair_bias.pair_layer_norm.bias
    pairformer_stack_blocks_6_attention_pair_bias_pair_linear_weight = getattr(self.pairformer_stack.blocks, "6").attention_pair_bias.pair_linear.weight
    pairformer_stack_blocks_6_attention_pair_bias_attention_query_bias = getattr(self.pairformer_stack.blocks, "6").attention_pair_bias.attention.query_bias
    pairformer_stack_blocks_6_attention_pair_bias_attention_input2qkvg_weight = getattr(self.pairformer_stack.blocks, "6").attention_pair_bias.attention.input2qkvg.weight
    pairformer_stack_blocks_6_attention_pair_bias_attention_output_proj_weight = getattr(self.pairformer_stack.blocks, "6").attention_pair_bias.attention.output_proj.weight
    pairformer_stack_blocks_7_transition_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "7").transition_pair.layer_norm.weight
    pairformer_stack_blocks_7_transition_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "7").transition_pair.layer_norm.bias
    pairformer_stack_blocks_7_transition_pair_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "7").transition_pair.linear_no_bias_ab.weight
    pairformer_stack_blocks_7_transition_pair_linear_out_weight = getattr(self.pairformer_stack.blocks, "7").transition_pair.linear_out.weight
    pairformer_stack_blocks_7_triangle_multiplication_layernorm_z_in_weight = getattr(self.pairformer_stack.blocks, "7").triangle_multiplication.layernorm_z_in.weight
    pairformer_stack_blocks_7_triangle_multiplication_layernorm_z_in_bias = getattr(self.pairformer_stack.blocks, "7").triangle_multiplication.layernorm_z_in.bias
    pairformer_stack_blocks_7_triangle_multiplication_linear_z_out_weight = getattr(self.pairformer_stack.blocks, "7").triangle_multiplication.linear_z_out.weight
    pairformer_stack_blocks_7_triangle_multiplication_merged_linear_p_weight = getattr(self.pairformer_stack.blocks, "7").triangle_multiplication.merged_linear_p.weight
    pairformer_stack_blocks_7_triangle_multiplication_merged_linear_g_weight = getattr(self.pairformer_stack.blocks, "7").triangle_multiplication.merged_linear_g.weight
    pairformer_stack_blocks_7_triangle_attention_out_scalers = getattr(self.pairformer_stack.blocks, "7").triangle_attention.out_scalers
    pairformer_stack_blocks_7_triangle_attention_pair2b_weight = getattr(self.pairformer_stack.blocks, "7").triangle_attention.pair2b.weight
    pairformer_stack_blocks_7_triangle_attention_pair2qkvg1_weight = getattr(self.pairformer_stack.blocks, "7").triangle_attention.pair2qkvg1.weight
    pairformer_stack_blocks_7_triangle_attention_pair2qkvg2_weight = getattr(self.pairformer_stack.blocks, "7").triangle_attention.pair2qkvg2.weight
    pairformer_stack_blocks_7_triangle_attention_linear_out_weight = getattr(self.pairformer_stack.blocks, "7").triangle_attention.linear_out.weight
    pairformer_stack_blocks_7_transition_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "7").transition_single.layer_norm.weight
    pairformer_stack_blocks_7_transition_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "7").transition_single.layer_norm.bias
    pairformer_stack_blocks_7_transition_single_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "7").transition_single.linear_no_bias_ab.weight
    pairformer_stack_blocks_7_transition_single_linear_out_weight = getattr(self.pairformer_stack.blocks, "7").transition_single.linear_out.weight
    pairformer_stack_blocks_7_attention_pair_bias_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "7").attention_pair_bias.single_layer_norm.weight
    pairformer_stack_blocks_7_attention_pair_bias_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "7").attention_pair_bias.single_layer_norm.bias
    pairformer_stack_blocks_7_attention_pair_bias_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "7").attention_pair_bias.pair_layer_norm.weight
    pairformer_stack_blocks_7_attention_pair_bias_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "7").attention_pair_bias.pair_layer_norm.bias
    pairformer_stack_blocks_7_attention_pair_bias_pair_linear_weight = getattr(self.pairformer_stack.blocks, "7").attention_pair_bias.pair_linear.weight
    pairformer_stack_blocks_7_attention_pair_bias_attention_query_bias = getattr(self.pairformer_stack.blocks, "7").attention_pair_bias.attention.query_bias
    pairformer_stack_blocks_7_attention_pair_bias_attention_input2qkvg_weight = getattr(self.pairformer_stack.blocks, "7").attention_pair_bias.attention.input2qkvg.weight
    pairformer_stack_blocks_7_attention_pair_bias_attention_output_proj_weight = getattr(self.pairformer_stack.blocks, "7").attention_pair_bias.attention.output_proj.weight
    pairformer_stack_blocks_8_transition_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "8").transition_pair.layer_norm.weight
    pairformer_stack_blocks_8_transition_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "8").transition_pair.layer_norm.bias
    pairformer_stack_blocks_8_transition_pair_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "8").transition_pair.linear_no_bias_ab.weight
    pairformer_stack_blocks_8_transition_pair_linear_out_weight = getattr(self.pairformer_stack.blocks, "8").transition_pair.linear_out.weight
    pairformer_stack_blocks_8_triangle_multiplication_layernorm_z_in_weight = getattr(self.pairformer_stack.blocks, "8").triangle_multiplication.layernorm_z_in.weight
    pairformer_stack_blocks_8_triangle_multiplication_layernorm_z_in_bias = getattr(self.pairformer_stack.blocks, "8").triangle_multiplication.layernorm_z_in.bias
    pairformer_stack_blocks_8_triangle_multiplication_linear_z_out_weight = getattr(self.pairformer_stack.blocks, "8").triangle_multiplication.linear_z_out.weight
    pairformer_stack_blocks_8_triangle_multiplication_merged_linear_p_weight = getattr(self.pairformer_stack.blocks, "8").triangle_multiplication.merged_linear_p.weight
    pairformer_stack_blocks_8_triangle_multiplication_merged_linear_g_weight = getattr(self.pairformer_stack.blocks, "8").triangle_multiplication.merged_linear_g.weight
    pairformer_stack_blocks_8_triangle_attention_out_scalers = getattr(self.pairformer_stack.blocks, "8").triangle_attention.out_scalers
    pairformer_stack_blocks_8_triangle_attention_pair2b_weight = getattr(self.pairformer_stack.blocks, "8").triangle_attention.pair2b.weight
    pairformer_stack_blocks_8_triangle_attention_pair2qkvg1_weight = getattr(self.pairformer_stack.blocks, "8").triangle_attention.pair2qkvg1.weight
    pairformer_stack_blocks_8_triangle_attention_pair2qkvg2_weight = getattr(self.pairformer_stack.blocks, "8").triangle_attention.pair2qkvg2.weight
    pairformer_stack_blocks_8_triangle_attention_linear_out_weight = getattr(self.pairformer_stack.blocks, "8").triangle_attention.linear_out.weight
    pairformer_stack_blocks_8_transition_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "8").transition_single.layer_norm.weight
    pairformer_stack_blocks_8_transition_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "8").transition_single.layer_norm.bias
    pairformer_stack_blocks_8_transition_single_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "8").transition_single.linear_no_bias_ab.weight
    pairformer_stack_blocks_8_transition_single_linear_out_weight = getattr(self.pairformer_stack.blocks, "8").transition_single.linear_out.weight
    pairformer_stack_blocks_8_attention_pair_bias_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "8").attention_pair_bias.single_layer_norm.weight
    pairformer_stack_blocks_8_attention_pair_bias_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "8").attention_pair_bias.single_layer_norm.bias
    pairformer_stack_blocks_8_attention_pair_bias_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "8").attention_pair_bias.pair_layer_norm.weight
    pairformer_stack_blocks_8_attention_pair_bias_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "8").attention_pair_bias.pair_layer_norm.bias
    pairformer_stack_blocks_8_attention_pair_bias_pair_linear_weight = getattr(self.pairformer_stack.blocks, "8").attention_pair_bias.pair_linear.weight
    pairformer_stack_blocks_8_attention_pair_bias_attention_query_bias = getattr(self.pairformer_stack.blocks, "8").attention_pair_bias.attention.query_bias
    pairformer_stack_blocks_8_attention_pair_bias_attention_input2qkvg_weight = getattr(self.pairformer_stack.blocks, "8").attention_pair_bias.attention.input2qkvg.weight
    pairformer_stack_blocks_8_attention_pair_bias_attention_output_proj_weight = getattr(self.pairformer_stack.blocks, "8").attention_pair_bias.attention.output_proj.weight
    pairformer_stack_blocks_9_transition_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "9").transition_pair.layer_norm.weight
    pairformer_stack_blocks_9_transition_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "9").transition_pair.layer_norm.bias
    pairformer_stack_blocks_9_transition_pair_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "9").transition_pair.linear_no_bias_ab.weight
    pairformer_stack_blocks_9_transition_pair_linear_out_weight = getattr(self.pairformer_stack.blocks, "9").transition_pair.linear_out.weight
    pairformer_stack_blocks_9_triangle_multiplication_layernorm_z_in_weight = getattr(self.pairformer_stack.blocks, "9").triangle_multiplication.layernorm_z_in.weight
    pairformer_stack_blocks_9_triangle_multiplication_layernorm_z_in_bias = getattr(self.pairformer_stack.blocks, "9").triangle_multiplication.layernorm_z_in.bias
    pairformer_stack_blocks_9_triangle_multiplication_linear_z_out_weight = getattr(self.pairformer_stack.blocks, "9").triangle_multiplication.linear_z_out.weight
    pairformer_stack_blocks_9_triangle_multiplication_merged_linear_p_weight = getattr(self.pairformer_stack.blocks, "9").triangle_multiplication.merged_linear_p.weight
    pairformer_stack_blocks_9_triangle_multiplication_merged_linear_g_weight = getattr(self.pairformer_stack.blocks, "9").triangle_multiplication.merged_linear_g.weight
    pairformer_stack_blocks_9_triangle_attention_out_scalers = getattr(self.pairformer_stack.blocks, "9").triangle_attention.out_scalers
    pairformer_stack_blocks_9_triangle_attention_pair2b_weight = getattr(self.pairformer_stack.blocks, "9").triangle_attention.pair2b.weight
    pairformer_stack_blocks_9_triangle_attention_pair2qkvg1_weight = getattr(self.pairformer_stack.blocks, "9").triangle_attention.pair2qkvg1.weight
    pairformer_stack_blocks_9_triangle_attention_pair2qkvg2_weight = getattr(self.pairformer_stack.blocks, "9").triangle_attention.pair2qkvg2.weight
    pairformer_stack_blocks_9_triangle_attention_linear_out_weight = getattr(self.pairformer_stack.blocks, "9").triangle_attention.linear_out.weight
    pairformer_stack_blocks_9_transition_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "9").transition_single.layer_norm.weight
    pairformer_stack_blocks_9_transition_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "9").transition_single.layer_norm.bias
    pairformer_stack_blocks_9_transition_single_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "9").transition_single.linear_no_bias_ab.weight
    pairformer_stack_blocks_9_transition_single_linear_out_weight = getattr(self.pairformer_stack.blocks, "9").transition_single.linear_out.weight
    pairformer_stack_blocks_9_attention_pair_bias_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "9").attention_pair_bias.single_layer_norm.weight
    pairformer_stack_blocks_9_attention_pair_bias_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "9").attention_pair_bias.single_layer_norm.bias
    pairformer_stack_blocks_9_attention_pair_bias_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "9").attention_pair_bias.pair_layer_norm.weight
    pairformer_stack_blocks_9_attention_pair_bias_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "9").attention_pair_bias.pair_layer_norm.bias
    pairformer_stack_blocks_9_attention_pair_bias_pair_linear_weight = getattr(self.pairformer_stack.blocks, "9").attention_pair_bias.pair_linear.weight
    pairformer_stack_blocks_9_attention_pair_bias_attention_query_bias = getattr(self.pairformer_stack.blocks, "9").attention_pair_bias.attention.query_bias
    pairformer_stack_blocks_9_attention_pair_bias_attention_input2qkvg_weight = getattr(self.pairformer_stack.blocks, "9").attention_pair_bias.attention.input2qkvg.weight
    pairformer_stack_blocks_9_attention_pair_bias_attention_output_proj_weight = getattr(self.pairformer_stack.blocks, "9").attention_pair_bias.attention.output_proj.weight
    pairformer_stack_blocks_10_transition_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "10").transition_pair.layer_norm.weight
    pairformer_stack_blocks_10_transition_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "10").transition_pair.layer_norm.bias
    pairformer_stack_blocks_10_transition_pair_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "10").transition_pair.linear_no_bias_ab.weight
    pairformer_stack_blocks_10_transition_pair_linear_out_weight = getattr(self.pairformer_stack.blocks, "10").transition_pair.linear_out.weight
    pairformer_stack_blocks_10_triangle_multiplication_layernorm_z_in_weight = getattr(self.pairformer_stack.blocks, "10").triangle_multiplication.layernorm_z_in.weight
    pairformer_stack_blocks_10_triangle_multiplication_layernorm_z_in_bias = getattr(self.pairformer_stack.blocks, "10").triangle_multiplication.layernorm_z_in.bias
    pairformer_stack_blocks_10_triangle_multiplication_linear_z_out_weight = getattr(self.pairformer_stack.blocks, "10").triangle_multiplication.linear_z_out.weight
    pairformer_stack_blocks_10_triangle_multiplication_merged_linear_p_weight = getattr(self.pairformer_stack.blocks, "10").triangle_multiplication.merged_linear_p.weight
    pairformer_stack_blocks_10_triangle_multiplication_merged_linear_g_weight = getattr(self.pairformer_stack.blocks, "10").triangle_multiplication.merged_linear_g.weight
    pairformer_stack_blocks_10_triangle_attention_out_scalers = getattr(self.pairformer_stack.blocks, "10").triangle_attention.out_scalers
    pairformer_stack_blocks_10_triangle_attention_pair2b_weight = getattr(self.pairformer_stack.blocks, "10").triangle_attention.pair2b.weight
    pairformer_stack_blocks_10_triangle_attention_pair2qkvg1_weight = getattr(self.pairformer_stack.blocks, "10").triangle_attention.pair2qkvg1.weight
    pairformer_stack_blocks_10_triangle_attention_pair2qkvg2_weight = getattr(self.pairformer_stack.blocks, "10").triangle_attention.pair2qkvg2.weight
    pairformer_stack_blocks_10_triangle_attention_linear_out_weight = getattr(self.pairformer_stack.blocks, "10").triangle_attention.linear_out.weight
    pairformer_stack_blocks_10_transition_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "10").transition_single.layer_norm.weight
    pairformer_stack_blocks_10_transition_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "10").transition_single.layer_norm.bias
    pairformer_stack_blocks_10_transition_single_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "10").transition_single.linear_no_bias_ab.weight
    pairformer_stack_blocks_10_transition_single_linear_out_weight = getattr(self.pairformer_stack.blocks, "10").transition_single.linear_out.weight
    pairformer_stack_blocks_10_attention_pair_bias_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "10").attention_pair_bias.single_layer_norm.weight
    pairformer_stack_blocks_10_attention_pair_bias_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "10").attention_pair_bias.single_layer_norm.bias
    pairformer_stack_blocks_10_attention_pair_bias_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "10").attention_pair_bias.pair_layer_norm.weight
    pairformer_stack_blocks_10_attention_pair_bias_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "10").attention_pair_bias.pair_layer_norm.bias
    pairformer_stack_blocks_10_attention_pair_bias_pair_linear_weight = getattr(self.pairformer_stack.blocks, "10").attention_pair_bias.pair_linear.weight
    pairformer_stack_blocks_10_attention_pair_bias_attention_query_bias = getattr(self.pairformer_stack.blocks, "10").attention_pair_bias.attention.query_bias
    pairformer_stack_blocks_10_attention_pair_bias_attention_input2qkvg_weight = getattr(self.pairformer_stack.blocks, "10").attention_pair_bias.attention.input2qkvg.weight
    pairformer_stack_blocks_10_attention_pair_bias_attention_output_proj_weight = getattr(self.pairformer_stack.blocks, "10").attention_pair_bias.attention.output_proj.weight
    pairformer_stack_blocks_11_transition_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "11").transition_pair.layer_norm.weight
    pairformer_stack_blocks_11_transition_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "11").transition_pair.layer_norm.bias
    pairformer_stack_blocks_11_transition_pair_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "11").transition_pair.linear_no_bias_ab.weight
    pairformer_stack_blocks_11_transition_pair_linear_out_weight = getattr(self.pairformer_stack.blocks, "11").transition_pair.linear_out.weight
    pairformer_stack_blocks_11_triangle_multiplication_layernorm_z_in_weight = getattr(self.pairformer_stack.blocks, "11").triangle_multiplication.layernorm_z_in.weight
    pairformer_stack_blocks_11_triangle_multiplication_layernorm_z_in_bias = getattr(self.pairformer_stack.blocks, "11").triangle_multiplication.layernorm_z_in.bias
    pairformer_stack_blocks_11_triangle_multiplication_linear_z_out_weight = getattr(self.pairformer_stack.blocks, "11").triangle_multiplication.linear_z_out.weight
    pairformer_stack_blocks_11_triangle_multiplication_merged_linear_p_weight = getattr(self.pairformer_stack.blocks, "11").triangle_multiplication.merged_linear_p.weight
    pairformer_stack_blocks_11_triangle_multiplication_merged_linear_g_weight = getattr(self.pairformer_stack.blocks, "11").triangle_multiplication.merged_linear_g.weight
    pairformer_stack_blocks_11_triangle_attention_out_scalers = getattr(self.pairformer_stack.blocks, "11").triangle_attention.out_scalers
    pairformer_stack_blocks_11_triangle_attention_pair2b_weight = getattr(self.pairformer_stack.blocks, "11").triangle_attention.pair2b.weight
    pairformer_stack_blocks_11_triangle_attention_pair2qkvg1_weight = getattr(self.pairformer_stack.blocks, "11").triangle_attention.pair2qkvg1.weight
    pairformer_stack_blocks_11_triangle_attention_pair2qkvg2_weight = getattr(self.pairformer_stack.blocks, "11").triangle_attention.pair2qkvg2.weight
    pairformer_stack_blocks_11_triangle_attention_linear_out_weight = getattr(self.pairformer_stack.blocks, "11").triangle_attention.linear_out.weight
    pairformer_stack_blocks_11_transition_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "11").transition_single.layer_norm.weight
    pairformer_stack_blocks_11_transition_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "11").transition_single.layer_norm.bias
    pairformer_stack_blocks_11_transition_single_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "11").transition_single.linear_no_bias_ab.weight
    pairformer_stack_blocks_11_transition_single_linear_out_weight = getattr(self.pairformer_stack.blocks, "11").transition_single.linear_out.weight
    pairformer_stack_blocks_11_attention_pair_bias_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "11").attention_pair_bias.single_layer_norm.weight
    pairformer_stack_blocks_11_attention_pair_bias_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "11").attention_pair_bias.single_layer_norm.bias
    pairformer_stack_blocks_11_attention_pair_bias_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "11").attention_pair_bias.pair_layer_norm.weight
    pairformer_stack_blocks_11_attention_pair_bias_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "11").attention_pair_bias.pair_layer_norm.bias
    pairformer_stack_blocks_11_attention_pair_bias_pair_linear_weight = getattr(self.pairformer_stack.blocks, "11").attention_pair_bias.pair_linear.weight
    pairformer_stack_blocks_11_attention_pair_bias_attention_query_bias = getattr(self.pairformer_stack.blocks, "11").attention_pair_bias.attention.query_bias
    pairformer_stack_blocks_11_attention_pair_bias_attention_input2qkvg_weight = getattr(self.pairformer_stack.blocks, "11").attention_pair_bias.attention.input2qkvg.weight
    pairformer_stack_blocks_11_attention_pair_bias_attention_output_proj_weight = getattr(self.pairformer_stack.blocks, "11").attention_pair_bias.attention.output_proj.weight
    pairformer_stack_blocks_12_transition_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "12").transition_pair.layer_norm.weight
    pairformer_stack_blocks_12_transition_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "12").transition_pair.layer_norm.bias
    pairformer_stack_blocks_12_transition_pair_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "12").transition_pair.linear_no_bias_ab.weight
    pairformer_stack_blocks_12_transition_pair_linear_out_weight = getattr(self.pairformer_stack.blocks, "12").transition_pair.linear_out.weight
    pairformer_stack_blocks_12_triangle_multiplication_layernorm_z_in_weight = getattr(self.pairformer_stack.blocks, "12").triangle_multiplication.layernorm_z_in.weight
    pairformer_stack_blocks_12_triangle_multiplication_layernorm_z_in_bias = getattr(self.pairformer_stack.blocks, "12").triangle_multiplication.layernorm_z_in.bias
    pairformer_stack_blocks_12_triangle_multiplication_linear_z_out_weight = getattr(self.pairformer_stack.blocks, "12").triangle_multiplication.linear_z_out.weight
    pairformer_stack_blocks_12_triangle_multiplication_merged_linear_p_weight = getattr(self.pairformer_stack.blocks, "12").triangle_multiplication.merged_linear_p.weight
    pairformer_stack_blocks_12_triangle_multiplication_merged_linear_g_weight = getattr(self.pairformer_stack.blocks, "12").triangle_multiplication.merged_linear_g.weight
    pairformer_stack_blocks_12_triangle_attention_out_scalers = getattr(self.pairformer_stack.blocks, "12").triangle_attention.out_scalers
    pairformer_stack_blocks_12_triangle_attention_pair2b_weight = getattr(self.pairformer_stack.blocks, "12").triangle_attention.pair2b.weight
    pairformer_stack_blocks_12_triangle_attention_pair2qkvg1_weight = getattr(self.pairformer_stack.blocks, "12").triangle_attention.pair2qkvg1.weight
    pairformer_stack_blocks_12_triangle_attention_pair2qkvg2_weight = getattr(self.pairformer_stack.blocks, "12").triangle_attention.pair2qkvg2.weight
    pairformer_stack_blocks_12_triangle_attention_linear_out_weight = getattr(self.pairformer_stack.blocks, "12").triangle_attention.linear_out.weight
    pairformer_stack_blocks_12_transition_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "12").transition_single.layer_norm.weight
    pairformer_stack_blocks_12_transition_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "12").transition_single.layer_norm.bias
    pairformer_stack_blocks_12_transition_single_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "12").transition_single.linear_no_bias_ab.weight
    pairformer_stack_blocks_12_transition_single_linear_out_weight = getattr(self.pairformer_stack.blocks, "12").transition_single.linear_out.weight
    pairformer_stack_blocks_12_attention_pair_bias_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "12").attention_pair_bias.single_layer_norm.weight
    pairformer_stack_blocks_12_attention_pair_bias_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "12").attention_pair_bias.single_layer_norm.bias
    pairformer_stack_blocks_12_attention_pair_bias_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "12").attention_pair_bias.pair_layer_norm.weight
    pairformer_stack_blocks_12_attention_pair_bias_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "12").attention_pair_bias.pair_layer_norm.bias
    pairformer_stack_blocks_12_attention_pair_bias_pair_linear_weight = getattr(self.pairformer_stack.blocks, "12").attention_pair_bias.pair_linear.weight
    pairformer_stack_blocks_12_attention_pair_bias_attention_query_bias = getattr(self.pairformer_stack.blocks, "12").attention_pair_bias.attention.query_bias
    pairformer_stack_blocks_12_attention_pair_bias_attention_input2qkvg_weight = getattr(self.pairformer_stack.blocks, "12").attention_pair_bias.attention.input2qkvg.weight
    pairformer_stack_blocks_12_attention_pair_bias_attention_output_proj_weight = getattr(self.pairformer_stack.blocks, "12").attention_pair_bias.attention.output_proj.weight
    pairformer_stack_blocks_13_transition_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "13").transition_pair.layer_norm.weight
    pairformer_stack_blocks_13_transition_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "13").transition_pair.layer_norm.bias
    pairformer_stack_blocks_13_transition_pair_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "13").transition_pair.linear_no_bias_ab.weight
    pairformer_stack_blocks_13_transition_pair_linear_out_weight = getattr(self.pairformer_stack.blocks, "13").transition_pair.linear_out.weight
    pairformer_stack_blocks_13_triangle_multiplication_layernorm_z_in_weight = getattr(self.pairformer_stack.blocks, "13").triangle_multiplication.layernorm_z_in.weight
    pairformer_stack_blocks_13_triangle_multiplication_layernorm_z_in_bias = getattr(self.pairformer_stack.blocks, "13").triangle_multiplication.layernorm_z_in.bias
    pairformer_stack_blocks_13_triangle_multiplication_linear_z_out_weight = getattr(self.pairformer_stack.blocks, "13").triangle_multiplication.linear_z_out.weight
    pairformer_stack_blocks_13_triangle_multiplication_merged_linear_p_weight = getattr(self.pairformer_stack.blocks, "13").triangle_multiplication.merged_linear_p.weight
    pairformer_stack_blocks_13_triangle_multiplication_merged_linear_g_weight = getattr(self.pairformer_stack.blocks, "13").triangle_multiplication.merged_linear_g.weight
    pairformer_stack_blocks_13_triangle_attention_out_scalers = getattr(self.pairformer_stack.blocks, "13").triangle_attention.out_scalers
    pairformer_stack_blocks_13_triangle_attention_pair2b_weight = getattr(self.pairformer_stack.blocks, "13").triangle_attention.pair2b.weight
    pairformer_stack_blocks_13_triangle_attention_pair2qkvg1_weight = getattr(self.pairformer_stack.blocks, "13").triangle_attention.pair2qkvg1.weight
    pairformer_stack_blocks_13_triangle_attention_pair2qkvg2_weight = getattr(self.pairformer_stack.blocks, "13").triangle_attention.pair2qkvg2.weight
    pairformer_stack_blocks_13_triangle_attention_linear_out_weight = getattr(self.pairformer_stack.blocks, "13").triangle_attention.linear_out.weight
    pairformer_stack_blocks_13_transition_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "13").transition_single.layer_norm.weight
    pairformer_stack_blocks_13_transition_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "13").transition_single.layer_norm.bias
    pairformer_stack_blocks_13_transition_single_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "13").transition_single.linear_no_bias_ab.weight
    pairformer_stack_blocks_13_transition_single_linear_out_weight = getattr(self.pairformer_stack.blocks, "13").transition_single.linear_out.weight
    pairformer_stack_blocks_13_attention_pair_bias_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "13").attention_pair_bias.single_layer_norm.weight
    pairformer_stack_blocks_13_attention_pair_bias_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "13").attention_pair_bias.single_layer_norm.bias
    pairformer_stack_blocks_13_attention_pair_bias_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "13").attention_pair_bias.pair_layer_norm.weight
    pairformer_stack_blocks_13_attention_pair_bias_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "13").attention_pair_bias.pair_layer_norm.bias
    pairformer_stack_blocks_13_attention_pair_bias_pair_linear_weight = getattr(self.pairformer_stack.blocks, "13").attention_pair_bias.pair_linear.weight
    pairformer_stack_blocks_13_attention_pair_bias_attention_query_bias = getattr(self.pairformer_stack.blocks, "13").attention_pair_bias.attention.query_bias
    pairformer_stack_blocks_13_attention_pair_bias_attention_input2qkvg_weight = getattr(self.pairformer_stack.blocks, "13").attention_pair_bias.attention.input2qkvg.weight
    pairformer_stack_blocks_13_attention_pair_bias_attention_output_proj_weight = getattr(self.pairformer_stack.blocks, "13").attention_pair_bias.attention.output_proj.weight
    pairformer_stack_blocks_14_transition_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "14").transition_pair.layer_norm.weight
    pairformer_stack_blocks_14_transition_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "14").transition_pair.layer_norm.bias
    pairformer_stack_blocks_14_transition_pair_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "14").transition_pair.linear_no_bias_ab.weight
    pairformer_stack_blocks_14_transition_pair_linear_out_weight = getattr(self.pairformer_stack.blocks, "14").transition_pair.linear_out.weight
    pairformer_stack_blocks_14_triangle_multiplication_layernorm_z_in_weight = getattr(self.pairformer_stack.blocks, "14").triangle_multiplication.layernorm_z_in.weight
    pairformer_stack_blocks_14_triangle_multiplication_layernorm_z_in_bias = getattr(self.pairformer_stack.blocks, "14").triangle_multiplication.layernorm_z_in.bias
    pairformer_stack_blocks_14_triangle_multiplication_linear_z_out_weight = getattr(self.pairformer_stack.blocks, "14").triangle_multiplication.linear_z_out.weight
    pairformer_stack_blocks_14_triangle_multiplication_merged_linear_p_weight = getattr(self.pairformer_stack.blocks, "14").triangle_multiplication.merged_linear_p.weight
    pairformer_stack_blocks_14_triangle_multiplication_merged_linear_g_weight = getattr(self.pairformer_stack.blocks, "14").triangle_multiplication.merged_linear_g.weight
    pairformer_stack_blocks_14_triangle_attention_out_scalers = getattr(self.pairformer_stack.blocks, "14").triangle_attention.out_scalers
    pairformer_stack_blocks_14_triangle_attention_pair2b_weight = getattr(self.pairformer_stack.blocks, "14").triangle_attention.pair2b.weight
    pairformer_stack_blocks_14_triangle_attention_pair2qkvg1_weight = getattr(self.pairformer_stack.blocks, "14").triangle_attention.pair2qkvg1.weight
    pairformer_stack_blocks_14_triangle_attention_pair2qkvg2_weight = getattr(self.pairformer_stack.blocks, "14").triangle_attention.pair2qkvg2.weight
    pairformer_stack_blocks_14_triangle_attention_linear_out_weight = getattr(self.pairformer_stack.blocks, "14").triangle_attention.linear_out.weight
    pairformer_stack_blocks_14_transition_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "14").transition_single.layer_norm.weight
    pairformer_stack_blocks_14_transition_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "14").transition_single.layer_norm.bias
    pairformer_stack_blocks_14_transition_single_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "14").transition_single.linear_no_bias_ab.weight
    pairformer_stack_blocks_14_transition_single_linear_out_weight = getattr(self.pairformer_stack.blocks, "14").transition_single.linear_out.weight
    pairformer_stack_blocks_14_attention_pair_bias_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "14").attention_pair_bias.single_layer_norm.weight
    pairformer_stack_blocks_14_attention_pair_bias_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "14").attention_pair_bias.single_layer_norm.bias
    pairformer_stack_blocks_14_attention_pair_bias_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "14").attention_pair_bias.pair_layer_norm.weight
    pairformer_stack_blocks_14_attention_pair_bias_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "14").attention_pair_bias.pair_layer_norm.bias
    pairformer_stack_blocks_14_attention_pair_bias_pair_linear_weight = getattr(self.pairformer_stack.blocks, "14").attention_pair_bias.pair_linear.weight
    pairformer_stack_blocks_14_attention_pair_bias_attention_query_bias = getattr(self.pairformer_stack.blocks, "14").attention_pair_bias.attention.query_bias
    pairformer_stack_blocks_14_attention_pair_bias_attention_input2qkvg_weight = getattr(self.pairformer_stack.blocks, "14").attention_pair_bias.attention.input2qkvg.weight
    pairformer_stack_blocks_14_attention_pair_bias_attention_output_proj_weight = getattr(self.pairformer_stack.blocks, "14").attention_pair_bias.attention.output_proj.weight
    pairformer_stack_blocks_15_transition_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "15").transition_pair.layer_norm.weight
    pairformer_stack_blocks_15_transition_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "15").transition_pair.layer_norm.bias
    pairformer_stack_blocks_15_transition_pair_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "15").transition_pair.linear_no_bias_ab.weight
    pairformer_stack_blocks_15_transition_pair_linear_out_weight = getattr(self.pairformer_stack.blocks, "15").transition_pair.linear_out.weight
    pairformer_stack_blocks_15_triangle_multiplication_layernorm_z_in_weight = getattr(self.pairformer_stack.blocks, "15").triangle_multiplication.layernorm_z_in.weight
    pairformer_stack_blocks_15_triangle_multiplication_layernorm_z_in_bias = getattr(self.pairformer_stack.blocks, "15").triangle_multiplication.layernorm_z_in.bias
    pairformer_stack_blocks_15_triangle_multiplication_linear_z_out_weight = getattr(self.pairformer_stack.blocks, "15").triangle_multiplication.linear_z_out.weight
    pairformer_stack_blocks_15_triangle_multiplication_merged_linear_p_weight = getattr(self.pairformer_stack.blocks, "15").triangle_multiplication.merged_linear_p.weight
    pairformer_stack_blocks_15_triangle_multiplication_merged_linear_g_weight = getattr(self.pairformer_stack.blocks, "15").triangle_multiplication.merged_linear_g.weight
    pairformer_stack_blocks_15_triangle_attention_out_scalers = getattr(self.pairformer_stack.blocks, "15").triangle_attention.out_scalers
    pairformer_stack_blocks_15_triangle_attention_pair2b_weight = getattr(self.pairformer_stack.blocks, "15").triangle_attention.pair2b.weight
    pairformer_stack_blocks_15_triangle_attention_pair2qkvg1_weight = getattr(self.pairformer_stack.blocks, "15").triangle_attention.pair2qkvg1.weight
    pairformer_stack_blocks_15_triangle_attention_pair2qkvg2_weight = getattr(self.pairformer_stack.blocks, "15").triangle_attention.pair2qkvg2.weight
    pairformer_stack_blocks_15_triangle_attention_linear_out_weight = getattr(self.pairformer_stack.blocks, "15").triangle_attention.linear_out.weight
    pairformer_stack_blocks_15_transition_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "15").transition_single.layer_norm.weight
    pairformer_stack_blocks_15_transition_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "15").transition_single.layer_norm.bias
    pairformer_stack_blocks_15_transition_single_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "15").transition_single.linear_no_bias_ab.weight
    pairformer_stack_blocks_15_transition_single_linear_out_weight = getattr(self.pairformer_stack.blocks, "15").transition_single.linear_out.weight
    pairformer_stack_blocks_15_attention_pair_bias_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "15").attention_pair_bias.single_layer_norm.weight
    pairformer_stack_blocks_15_attention_pair_bias_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "15").attention_pair_bias.single_layer_norm.bias
    pairformer_stack_blocks_15_attention_pair_bias_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "15").attention_pair_bias.pair_layer_norm.weight
    pairformer_stack_blocks_15_attention_pair_bias_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "15").attention_pair_bias.pair_layer_norm.bias
    pairformer_stack_blocks_15_attention_pair_bias_pair_linear_weight = getattr(self.pairformer_stack.blocks, "15").attention_pair_bias.pair_linear.weight
    pairformer_stack_blocks_15_attention_pair_bias_attention_query_bias = getattr(self.pairformer_stack.blocks, "15").attention_pair_bias.attention.query_bias
    pairformer_stack_blocks_15_attention_pair_bias_attention_input2qkvg_weight = getattr(self.pairformer_stack.blocks, "15").attention_pair_bias.attention.input2qkvg.weight
    pairformer_stack_blocks_15_attention_pair_bias_attention_output_proj_weight = getattr(self.pairformer_stack.blocks, "15").attention_pair_bias.attention.output_proj.weight
    pairformer_stack_blocks_16_transition_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "16").transition_pair.layer_norm.weight
    pairformer_stack_blocks_16_transition_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "16").transition_pair.layer_norm.bias
    pairformer_stack_blocks_16_transition_pair_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "16").transition_pair.linear_no_bias_ab.weight
    pairformer_stack_blocks_16_transition_pair_linear_out_weight = getattr(self.pairformer_stack.blocks, "16").transition_pair.linear_out.weight
    pairformer_stack_blocks_16_triangle_multiplication_layernorm_z_in_weight = getattr(self.pairformer_stack.blocks, "16").triangle_multiplication.layernorm_z_in.weight
    pairformer_stack_blocks_16_triangle_multiplication_layernorm_z_in_bias = getattr(self.pairformer_stack.blocks, "16").triangle_multiplication.layernorm_z_in.bias
    pairformer_stack_blocks_16_triangle_multiplication_linear_z_out_weight = getattr(self.pairformer_stack.blocks, "16").triangle_multiplication.linear_z_out.weight
    pairformer_stack_blocks_16_triangle_multiplication_merged_linear_p_weight = getattr(self.pairformer_stack.blocks, "16").triangle_multiplication.merged_linear_p.weight
    pairformer_stack_blocks_16_triangle_multiplication_merged_linear_g_weight = getattr(self.pairformer_stack.blocks, "16").triangle_multiplication.merged_linear_g.weight
    pairformer_stack_blocks_16_triangle_attention_out_scalers = getattr(self.pairformer_stack.blocks, "16").triangle_attention.out_scalers
    pairformer_stack_blocks_16_triangle_attention_pair2b_weight = getattr(self.pairformer_stack.blocks, "16").triangle_attention.pair2b.weight
    pairformer_stack_blocks_16_triangle_attention_pair2qkvg1_weight = getattr(self.pairformer_stack.blocks, "16").triangle_attention.pair2qkvg1.weight
    pairformer_stack_blocks_16_triangle_attention_pair2qkvg2_weight = getattr(self.pairformer_stack.blocks, "16").triangle_attention.pair2qkvg2.weight
    pairformer_stack_blocks_16_triangle_attention_linear_out_weight = getattr(self.pairformer_stack.blocks, "16").triangle_attention.linear_out.weight
    pairformer_stack_blocks_16_transition_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "16").transition_single.layer_norm.weight
    pairformer_stack_blocks_16_transition_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "16").transition_single.layer_norm.bias
    pairformer_stack_blocks_16_transition_single_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "16").transition_single.linear_no_bias_ab.weight
    pairformer_stack_blocks_16_transition_single_linear_out_weight = getattr(self.pairformer_stack.blocks, "16").transition_single.linear_out.weight
    pairformer_stack_blocks_16_attention_pair_bias_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "16").attention_pair_bias.single_layer_norm.weight
    pairformer_stack_blocks_16_attention_pair_bias_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "16").attention_pair_bias.single_layer_norm.bias
    pairformer_stack_blocks_16_attention_pair_bias_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "16").attention_pair_bias.pair_layer_norm.weight
    pairformer_stack_blocks_16_attention_pair_bias_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "16").attention_pair_bias.pair_layer_norm.bias
    pairformer_stack_blocks_16_attention_pair_bias_pair_linear_weight = getattr(self.pairformer_stack.blocks, "16").attention_pair_bias.pair_linear.weight
    pairformer_stack_blocks_16_attention_pair_bias_attention_query_bias = getattr(self.pairformer_stack.blocks, "16").attention_pair_bias.attention.query_bias
    pairformer_stack_blocks_16_attention_pair_bias_attention_input2qkvg_weight = getattr(self.pairformer_stack.blocks, "16").attention_pair_bias.attention.input2qkvg.weight
    pairformer_stack_blocks_16_attention_pair_bias_attention_output_proj_weight = getattr(self.pairformer_stack.blocks, "16").attention_pair_bias.attention.output_proj.weight
    pairformer_stack_blocks_17_transition_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "17").transition_pair.layer_norm.weight
    pairformer_stack_blocks_17_transition_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "17").transition_pair.layer_norm.bias
    pairformer_stack_blocks_17_transition_pair_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "17").transition_pair.linear_no_bias_ab.weight
    pairformer_stack_blocks_17_transition_pair_linear_out_weight = getattr(self.pairformer_stack.blocks, "17").transition_pair.linear_out.weight
    pairformer_stack_blocks_17_triangle_multiplication_layernorm_z_in_weight = getattr(self.pairformer_stack.blocks, "17").triangle_multiplication.layernorm_z_in.weight
    pairformer_stack_blocks_17_triangle_multiplication_layernorm_z_in_bias = getattr(self.pairformer_stack.blocks, "17").triangle_multiplication.layernorm_z_in.bias
    pairformer_stack_blocks_17_triangle_multiplication_linear_z_out_weight = getattr(self.pairformer_stack.blocks, "17").triangle_multiplication.linear_z_out.weight
    pairformer_stack_blocks_17_triangle_multiplication_merged_linear_p_weight = getattr(self.pairformer_stack.blocks, "17").triangle_multiplication.merged_linear_p.weight
    pairformer_stack_blocks_17_triangle_multiplication_merged_linear_g_weight = getattr(self.pairformer_stack.blocks, "17").triangle_multiplication.merged_linear_g.weight
    pairformer_stack_blocks_17_triangle_attention_out_scalers = getattr(self.pairformer_stack.blocks, "17").triangle_attention.out_scalers
    pairformer_stack_blocks_17_triangle_attention_pair2b_weight = getattr(self.pairformer_stack.blocks, "17").triangle_attention.pair2b.weight
    pairformer_stack_blocks_17_triangle_attention_pair2qkvg1_weight = getattr(self.pairformer_stack.blocks, "17").triangle_attention.pair2qkvg1.weight
    pairformer_stack_blocks_17_triangle_attention_pair2qkvg2_weight = getattr(self.pairformer_stack.blocks, "17").triangle_attention.pair2qkvg2.weight
    pairformer_stack_blocks_17_triangle_attention_linear_out_weight = getattr(self.pairformer_stack.blocks, "17").triangle_attention.linear_out.weight
    pairformer_stack_blocks_17_transition_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "17").transition_single.layer_norm.weight
    pairformer_stack_blocks_17_transition_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "17").transition_single.layer_norm.bias
    pairformer_stack_blocks_17_transition_single_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "17").transition_single.linear_no_bias_ab.weight
    pairformer_stack_blocks_17_transition_single_linear_out_weight = getattr(self.pairformer_stack.blocks, "17").transition_single.linear_out.weight
    pairformer_stack_blocks_17_attention_pair_bias_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "17").attention_pair_bias.single_layer_norm.weight
    pairformer_stack_blocks_17_attention_pair_bias_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "17").attention_pair_bias.single_layer_norm.bias
    pairformer_stack_blocks_17_attention_pair_bias_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "17").attention_pair_bias.pair_layer_norm.weight
    pairformer_stack_blocks_17_attention_pair_bias_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "17").attention_pair_bias.pair_layer_norm.bias
    pairformer_stack_blocks_17_attention_pair_bias_pair_linear_weight = getattr(self.pairformer_stack.blocks, "17").attention_pair_bias.pair_linear.weight
    pairformer_stack_blocks_17_attention_pair_bias_attention_query_bias = getattr(self.pairformer_stack.blocks, "17").attention_pair_bias.attention.query_bias
    pairformer_stack_blocks_17_attention_pair_bias_attention_input2qkvg_weight = getattr(self.pairformer_stack.blocks, "17").attention_pair_bias.attention.input2qkvg.weight
    pairformer_stack_blocks_17_attention_pair_bias_attention_output_proj_weight = getattr(self.pairformer_stack.blocks, "17").attention_pair_bias.attention.output_proj.weight
    pairformer_stack_blocks_18_transition_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "18").transition_pair.layer_norm.weight
    pairformer_stack_blocks_18_transition_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "18").transition_pair.layer_norm.bias
    pairformer_stack_blocks_18_transition_pair_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "18").transition_pair.linear_no_bias_ab.weight
    pairformer_stack_blocks_18_transition_pair_linear_out_weight = getattr(self.pairformer_stack.blocks, "18").transition_pair.linear_out.weight
    pairformer_stack_blocks_18_triangle_multiplication_layernorm_z_in_weight = getattr(self.pairformer_stack.blocks, "18").triangle_multiplication.layernorm_z_in.weight
    pairformer_stack_blocks_18_triangle_multiplication_layernorm_z_in_bias = getattr(self.pairformer_stack.blocks, "18").triangle_multiplication.layernorm_z_in.bias
    pairformer_stack_blocks_18_triangle_multiplication_linear_z_out_weight = getattr(self.pairformer_stack.blocks, "18").triangle_multiplication.linear_z_out.weight
    pairformer_stack_blocks_18_triangle_multiplication_merged_linear_p_weight = getattr(self.pairformer_stack.blocks, "18").triangle_multiplication.merged_linear_p.weight
    pairformer_stack_blocks_18_triangle_multiplication_merged_linear_g_weight = getattr(self.pairformer_stack.blocks, "18").triangle_multiplication.merged_linear_g.weight
    pairformer_stack_blocks_18_triangle_attention_out_scalers = getattr(self.pairformer_stack.blocks, "18").triangle_attention.out_scalers
    pairformer_stack_blocks_18_triangle_attention_pair2b_weight = getattr(self.pairformer_stack.blocks, "18").triangle_attention.pair2b.weight
    pairformer_stack_blocks_18_triangle_attention_pair2qkvg1_weight = getattr(self.pairformer_stack.blocks, "18").triangle_attention.pair2qkvg1.weight
    pairformer_stack_blocks_18_triangle_attention_pair2qkvg2_weight = getattr(self.pairformer_stack.blocks, "18").triangle_attention.pair2qkvg2.weight
    pairformer_stack_blocks_18_triangle_attention_linear_out_weight = getattr(self.pairformer_stack.blocks, "18").triangle_attention.linear_out.weight
    pairformer_stack_blocks_18_transition_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "18").transition_single.layer_norm.weight
    pairformer_stack_blocks_18_transition_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "18").transition_single.layer_norm.bias
    pairformer_stack_blocks_18_transition_single_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "18").transition_single.linear_no_bias_ab.weight
    pairformer_stack_blocks_18_transition_single_linear_out_weight = getattr(self.pairformer_stack.blocks, "18").transition_single.linear_out.weight
    pairformer_stack_blocks_18_attention_pair_bias_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "18").attention_pair_bias.single_layer_norm.weight
    pairformer_stack_blocks_18_attention_pair_bias_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "18").attention_pair_bias.single_layer_norm.bias
    pairformer_stack_blocks_18_attention_pair_bias_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "18").attention_pair_bias.pair_layer_norm.weight
    pairformer_stack_blocks_18_attention_pair_bias_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "18").attention_pair_bias.pair_layer_norm.bias
    pairformer_stack_blocks_18_attention_pair_bias_pair_linear_weight = getattr(self.pairformer_stack.blocks, "18").attention_pair_bias.pair_linear.weight
    pairformer_stack_blocks_18_attention_pair_bias_attention_query_bias = getattr(self.pairformer_stack.blocks, "18").attention_pair_bias.attention.query_bias
    pairformer_stack_blocks_18_attention_pair_bias_attention_input2qkvg_weight = getattr(self.pairformer_stack.blocks, "18").attention_pair_bias.attention.input2qkvg.weight
    pairformer_stack_blocks_18_attention_pair_bias_attention_output_proj_weight = getattr(self.pairformer_stack.blocks, "18").attention_pair_bias.attention.output_proj.weight
    pairformer_stack_blocks_19_transition_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "19").transition_pair.layer_norm.weight
    pairformer_stack_blocks_19_transition_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "19").transition_pair.layer_norm.bias
    pairformer_stack_blocks_19_transition_pair_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "19").transition_pair.linear_no_bias_ab.weight
    pairformer_stack_blocks_19_transition_pair_linear_out_weight = getattr(self.pairformer_stack.blocks, "19").transition_pair.linear_out.weight
    pairformer_stack_blocks_19_triangle_multiplication_layernorm_z_in_weight = getattr(self.pairformer_stack.blocks, "19").triangle_multiplication.layernorm_z_in.weight
    pairformer_stack_blocks_19_triangle_multiplication_layernorm_z_in_bias = getattr(self.pairformer_stack.blocks, "19").triangle_multiplication.layernorm_z_in.bias
    pairformer_stack_blocks_19_triangle_multiplication_linear_z_out_weight = getattr(self.pairformer_stack.blocks, "19").triangle_multiplication.linear_z_out.weight
    pairformer_stack_blocks_19_triangle_multiplication_merged_linear_p_weight = getattr(self.pairformer_stack.blocks, "19").triangle_multiplication.merged_linear_p.weight
    pairformer_stack_blocks_19_triangle_multiplication_merged_linear_g_weight = getattr(self.pairformer_stack.blocks, "19").triangle_multiplication.merged_linear_g.weight
    pairformer_stack_blocks_19_triangle_attention_out_scalers = getattr(self.pairformer_stack.blocks, "19").triangle_attention.out_scalers
    pairformer_stack_blocks_19_triangle_attention_pair2b_weight = getattr(self.pairformer_stack.blocks, "19").triangle_attention.pair2b.weight
    pairformer_stack_blocks_19_triangle_attention_pair2qkvg1_weight = getattr(self.pairformer_stack.blocks, "19").triangle_attention.pair2qkvg1.weight
    pairformer_stack_blocks_19_triangle_attention_pair2qkvg2_weight = getattr(self.pairformer_stack.blocks, "19").triangle_attention.pair2qkvg2.weight
    pairformer_stack_blocks_19_triangle_attention_linear_out_weight = getattr(self.pairformer_stack.blocks, "19").triangle_attention.linear_out.weight
    pairformer_stack_blocks_19_transition_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "19").transition_single.layer_norm.weight
    pairformer_stack_blocks_19_transition_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "19").transition_single.layer_norm.bias
    pairformer_stack_blocks_19_transition_single_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "19").transition_single.linear_no_bias_ab.weight
    pairformer_stack_blocks_19_transition_single_linear_out_weight = getattr(self.pairformer_stack.blocks, "19").transition_single.linear_out.weight
    pairformer_stack_blocks_19_attention_pair_bias_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "19").attention_pair_bias.single_layer_norm.weight
    pairformer_stack_blocks_19_attention_pair_bias_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "19").attention_pair_bias.single_layer_norm.bias
    pairformer_stack_blocks_19_attention_pair_bias_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "19").attention_pair_bias.pair_layer_norm.weight
    pairformer_stack_blocks_19_attention_pair_bias_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "19").attention_pair_bias.pair_layer_norm.bias
    pairformer_stack_blocks_19_attention_pair_bias_pair_linear_weight = getattr(self.pairformer_stack.blocks, "19").attention_pair_bias.pair_linear.weight
    pairformer_stack_blocks_19_attention_pair_bias_attention_query_bias = getattr(self.pairformer_stack.blocks, "19").attention_pair_bias.attention.query_bias
    pairformer_stack_blocks_19_attention_pair_bias_attention_input2qkvg_weight = getattr(self.pairformer_stack.blocks, "19").attention_pair_bias.attention.input2qkvg.weight
    pairformer_stack_blocks_19_attention_pair_bias_attention_output_proj_weight = getattr(self.pairformer_stack.blocks, "19").attention_pair_bias.attention.output_proj.weight
    pairformer_stack_blocks_20_transition_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "20").transition_pair.layer_norm.weight
    pairformer_stack_blocks_20_transition_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "20").transition_pair.layer_norm.bias
    pairformer_stack_blocks_20_transition_pair_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "20").transition_pair.linear_no_bias_ab.weight
    pairformer_stack_blocks_20_transition_pair_linear_out_weight = getattr(self.pairformer_stack.blocks, "20").transition_pair.linear_out.weight
    pairformer_stack_blocks_20_triangle_multiplication_layernorm_z_in_weight = getattr(self.pairformer_stack.blocks, "20").triangle_multiplication.layernorm_z_in.weight
    pairformer_stack_blocks_20_triangle_multiplication_layernorm_z_in_bias = getattr(self.pairformer_stack.blocks, "20").triangle_multiplication.layernorm_z_in.bias
    pairformer_stack_blocks_20_triangle_multiplication_linear_z_out_weight = getattr(self.pairformer_stack.blocks, "20").triangle_multiplication.linear_z_out.weight
    pairformer_stack_blocks_20_triangle_multiplication_merged_linear_p_weight = getattr(self.pairformer_stack.blocks, "20").triangle_multiplication.merged_linear_p.weight
    pairformer_stack_blocks_20_triangle_multiplication_merged_linear_g_weight = getattr(self.pairformer_stack.blocks, "20").triangle_multiplication.merged_linear_g.weight
    pairformer_stack_blocks_20_triangle_attention_out_scalers = getattr(self.pairformer_stack.blocks, "20").triangle_attention.out_scalers
    pairformer_stack_blocks_20_triangle_attention_pair2b_weight = getattr(self.pairformer_stack.blocks, "20").triangle_attention.pair2b.weight
    pairformer_stack_blocks_20_triangle_attention_pair2qkvg1_weight = getattr(self.pairformer_stack.blocks, "20").triangle_attention.pair2qkvg1.weight
    pairformer_stack_blocks_20_triangle_attention_pair2qkvg2_weight = getattr(self.pairformer_stack.blocks, "20").triangle_attention.pair2qkvg2.weight
    pairformer_stack_blocks_20_triangle_attention_linear_out_weight = getattr(self.pairformer_stack.blocks, "20").triangle_attention.linear_out.weight
    pairformer_stack_blocks_20_transition_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "20").transition_single.layer_norm.weight
    pairformer_stack_blocks_20_transition_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "20").transition_single.layer_norm.bias
    pairformer_stack_blocks_20_transition_single_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "20").transition_single.linear_no_bias_ab.weight
    pairformer_stack_blocks_20_transition_single_linear_out_weight = getattr(self.pairformer_stack.blocks, "20").transition_single.linear_out.weight
    pairformer_stack_blocks_20_attention_pair_bias_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "20").attention_pair_bias.single_layer_norm.weight
    pairformer_stack_blocks_20_attention_pair_bias_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "20").attention_pair_bias.single_layer_norm.bias
    pairformer_stack_blocks_20_attention_pair_bias_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "20").attention_pair_bias.pair_layer_norm.weight
    pairformer_stack_blocks_20_attention_pair_bias_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "20").attention_pair_bias.pair_layer_norm.bias
    pairformer_stack_blocks_20_attention_pair_bias_pair_linear_weight = getattr(self.pairformer_stack.blocks, "20").attention_pair_bias.pair_linear.weight
    pairformer_stack_blocks_20_attention_pair_bias_attention_query_bias = getattr(self.pairformer_stack.blocks, "20").attention_pair_bias.attention.query_bias
    pairformer_stack_blocks_20_attention_pair_bias_attention_input2qkvg_weight = getattr(self.pairformer_stack.blocks, "20").attention_pair_bias.attention.input2qkvg.weight
    pairformer_stack_blocks_20_attention_pair_bias_attention_output_proj_weight = getattr(self.pairformer_stack.blocks, "20").attention_pair_bias.attention.output_proj.weight
    pairformer_stack_blocks_21_transition_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "21").transition_pair.layer_norm.weight
    pairformer_stack_blocks_21_transition_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "21").transition_pair.layer_norm.bias
    pairformer_stack_blocks_21_transition_pair_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "21").transition_pair.linear_no_bias_ab.weight
    pairformer_stack_blocks_21_transition_pair_linear_out_weight = getattr(self.pairformer_stack.blocks, "21").transition_pair.linear_out.weight
    pairformer_stack_blocks_21_triangle_multiplication_layernorm_z_in_weight = getattr(self.pairformer_stack.blocks, "21").triangle_multiplication.layernorm_z_in.weight
    pairformer_stack_blocks_21_triangle_multiplication_layernorm_z_in_bias = getattr(self.pairformer_stack.blocks, "21").triangle_multiplication.layernorm_z_in.bias
    pairformer_stack_blocks_21_triangle_multiplication_linear_z_out_weight = getattr(self.pairformer_stack.blocks, "21").triangle_multiplication.linear_z_out.weight
    pairformer_stack_blocks_21_triangle_multiplication_merged_linear_p_weight = getattr(self.pairformer_stack.blocks, "21").triangle_multiplication.merged_linear_p.weight
    pairformer_stack_blocks_21_triangle_multiplication_merged_linear_g_weight = getattr(self.pairformer_stack.blocks, "21").triangle_multiplication.merged_linear_g.weight
    pairformer_stack_blocks_21_triangle_attention_out_scalers = getattr(self.pairformer_stack.blocks, "21").triangle_attention.out_scalers
    pairformer_stack_blocks_21_triangle_attention_pair2b_weight = getattr(self.pairformer_stack.blocks, "21").triangle_attention.pair2b.weight
    pairformer_stack_blocks_21_triangle_attention_pair2qkvg1_weight = getattr(self.pairformer_stack.blocks, "21").triangle_attention.pair2qkvg1.weight
    pairformer_stack_blocks_21_triangle_attention_pair2qkvg2_weight = getattr(self.pairformer_stack.blocks, "21").triangle_attention.pair2qkvg2.weight
    pairformer_stack_blocks_21_triangle_attention_linear_out_weight = getattr(self.pairformer_stack.blocks, "21").triangle_attention.linear_out.weight
    pairformer_stack_blocks_21_transition_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "21").transition_single.layer_norm.weight
    pairformer_stack_blocks_21_transition_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "21").transition_single.layer_norm.bias
    pairformer_stack_blocks_21_transition_single_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "21").transition_single.linear_no_bias_ab.weight
    pairformer_stack_blocks_21_transition_single_linear_out_weight = getattr(self.pairformer_stack.blocks, "21").transition_single.linear_out.weight
    pairformer_stack_blocks_21_attention_pair_bias_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "21").attention_pair_bias.single_layer_norm.weight
    pairformer_stack_blocks_21_attention_pair_bias_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "21").attention_pair_bias.single_layer_norm.bias
    pairformer_stack_blocks_21_attention_pair_bias_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "21").attention_pair_bias.pair_layer_norm.weight
    pairformer_stack_blocks_21_attention_pair_bias_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "21").attention_pair_bias.pair_layer_norm.bias
    pairformer_stack_blocks_21_attention_pair_bias_pair_linear_weight = getattr(self.pairformer_stack.blocks, "21").attention_pair_bias.pair_linear.weight
    pairformer_stack_blocks_21_attention_pair_bias_attention_query_bias = getattr(self.pairformer_stack.blocks, "21").attention_pair_bias.attention.query_bias
    pairformer_stack_blocks_21_attention_pair_bias_attention_input2qkvg_weight = getattr(self.pairformer_stack.blocks, "21").attention_pair_bias.attention.input2qkvg.weight
    pairformer_stack_blocks_21_attention_pair_bias_attention_output_proj_weight = getattr(self.pairformer_stack.blocks, "21").attention_pair_bias.attention.output_proj.weight
    pairformer_stack_blocks_22_transition_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "22").transition_pair.layer_norm.weight
    pairformer_stack_blocks_22_transition_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "22").transition_pair.layer_norm.bias
    pairformer_stack_blocks_22_transition_pair_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "22").transition_pair.linear_no_bias_ab.weight
    pairformer_stack_blocks_22_transition_pair_linear_out_weight = getattr(self.pairformer_stack.blocks, "22").transition_pair.linear_out.weight
    pairformer_stack_blocks_22_triangle_multiplication_layernorm_z_in_weight = getattr(self.pairformer_stack.blocks, "22").triangle_multiplication.layernorm_z_in.weight
    pairformer_stack_blocks_22_triangle_multiplication_layernorm_z_in_bias = getattr(self.pairformer_stack.blocks, "22").triangle_multiplication.layernorm_z_in.bias
    pairformer_stack_blocks_22_triangle_multiplication_linear_z_out_weight = getattr(self.pairformer_stack.blocks, "22").triangle_multiplication.linear_z_out.weight
    pairformer_stack_blocks_22_triangle_multiplication_merged_linear_p_weight = getattr(self.pairformer_stack.blocks, "22").triangle_multiplication.merged_linear_p.weight
    pairformer_stack_blocks_22_triangle_multiplication_merged_linear_g_weight = getattr(self.pairformer_stack.blocks, "22").triangle_multiplication.merged_linear_g.weight
    pairformer_stack_blocks_22_triangle_attention_out_scalers = getattr(self.pairformer_stack.blocks, "22").triangle_attention.out_scalers
    pairformer_stack_blocks_22_triangle_attention_pair2b_weight = getattr(self.pairformer_stack.blocks, "22").triangle_attention.pair2b.weight
    pairformer_stack_blocks_22_triangle_attention_pair2qkvg1_weight = getattr(self.pairformer_stack.blocks, "22").triangle_attention.pair2qkvg1.weight
    pairformer_stack_blocks_22_triangle_attention_pair2qkvg2_weight = getattr(self.pairformer_stack.blocks, "22").triangle_attention.pair2qkvg2.weight
    pairformer_stack_blocks_22_triangle_attention_linear_out_weight = getattr(self.pairformer_stack.blocks, "22").triangle_attention.linear_out.weight
    pairformer_stack_blocks_22_transition_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "22").transition_single.layer_norm.weight
    pairformer_stack_blocks_22_transition_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "22").transition_single.layer_norm.bias
    pairformer_stack_blocks_22_transition_single_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "22").transition_single.linear_no_bias_ab.weight
    pairformer_stack_blocks_22_transition_single_linear_out_weight = getattr(self.pairformer_stack.blocks, "22").transition_single.linear_out.weight
    pairformer_stack_blocks_22_attention_pair_bias_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "22").attention_pair_bias.single_layer_norm.weight
    pairformer_stack_blocks_22_attention_pair_bias_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "22").attention_pair_bias.single_layer_norm.bias
    pairformer_stack_blocks_22_attention_pair_bias_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "22").attention_pair_bias.pair_layer_norm.weight
    pairformer_stack_blocks_22_attention_pair_bias_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "22").attention_pair_bias.pair_layer_norm.bias
    pairformer_stack_blocks_22_attention_pair_bias_pair_linear_weight = getattr(self.pairformer_stack.blocks, "22").attention_pair_bias.pair_linear.weight
    pairformer_stack_blocks_22_attention_pair_bias_attention_query_bias = getattr(self.pairformer_stack.blocks, "22").attention_pair_bias.attention.query_bias
    pairformer_stack_blocks_22_attention_pair_bias_attention_input2qkvg_weight = getattr(self.pairformer_stack.blocks, "22").attention_pair_bias.attention.input2qkvg.weight
    pairformer_stack_blocks_22_attention_pair_bias_attention_output_proj_weight = getattr(self.pairformer_stack.blocks, "22").attention_pair_bias.attention.output_proj.weight
    pairformer_stack_blocks_23_transition_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "23").transition_pair.layer_norm.weight
    pairformer_stack_blocks_23_transition_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "23").transition_pair.layer_norm.bias
    pairformer_stack_blocks_23_transition_pair_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "23").transition_pair.linear_no_bias_ab.weight
    pairformer_stack_blocks_23_transition_pair_linear_out_weight = getattr(self.pairformer_stack.blocks, "23").transition_pair.linear_out.weight
    pairformer_stack_blocks_23_triangle_multiplication_layernorm_z_in_weight = getattr(self.pairformer_stack.blocks, "23").triangle_multiplication.layernorm_z_in.weight
    pairformer_stack_blocks_23_triangle_multiplication_layernorm_z_in_bias = getattr(self.pairformer_stack.blocks, "23").triangle_multiplication.layernorm_z_in.bias
    pairformer_stack_blocks_23_triangle_multiplication_linear_z_out_weight = getattr(self.pairformer_stack.blocks, "23").triangle_multiplication.linear_z_out.weight
    pairformer_stack_blocks_23_triangle_multiplication_merged_linear_p_weight = getattr(self.pairformer_stack.blocks, "23").triangle_multiplication.merged_linear_p.weight
    pairformer_stack_blocks_23_triangle_multiplication_merged_linear_g_weight = getattr(self.pairformer_stack.blocks, "23").triangle_multiplication.merged_linear_g.weight
    pairformer_stack_blocks_23_triangle_attention_out_scalers = getattr(self.pairformer_stack.blocks, "23").triangle_attention.out_scalers
    pairformer_stack_blocks_23_triangle_attention_pair2b_weight = getattr(self.pairformer_stack.blocks, "23").triangle_attention.pair2b.weight
    pairformer_stack_blocks_23_triangle_attention_pair2qkvg1_weight = getattr(self.pairformer_stack.blocks, "23").triangle_attention.pair2qkvg1.weight
    pairformer_stack_blocks_23_triangle_attention_pair2qkvg2_weight = getattr(self.pairformer_stack.blocks, "23").triangle_attention.pair2qkvg2.weight
    pairformer_stack_blocks_23_triangle_attention_linear_out_weight = getattr(self.pairformer_stack.blocks, "23").triangle_attention.linear_out.weight
    pairformer_stack_blocks_23_transition_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "23").transition_single.layer_norm.weight
    pairformer_stack_blocks_23_transition_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "23").transition_single.layer_norm.bias
    pairformer_stack_blocks_23_transition_single_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "23").transition_single.linear_no_bias_ab.weight
    pairformer_stack_blocks_23_transition_single_linear_out_weight = getattr(self.pairformer_stack.blocks, "23").transition_single.linear_out.weight
    pairformer_stack_blocks_23_attention_pair_bias_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "23").attention_pair_bias.single_layer_norm.weight
    pairformer_stack_blocks_23_attention_pair_bias_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "23").attention_pair_bias.single_layer_norm.bias
    pairformer_stack_blocks_23_attention_pair_bias_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "23").attention_pair_bias.pair_layer_norm.weight
    pairformer_stack_blocks_23_attention_pair_bias_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "23").attention_pair_bias.pair_layer_norm.bias
    pairformer_stack_blocks_23_attention_pair_bias_pair_linear_weight = getattr(self.pairformer_stack.blocks, "23").attention_pair_bias.pair_linear.weight
    pairformer_stack_blocks_23_attention_pair_bias_attention_query_bias = getattr(self.pairformer_stack.blocks, "23").attention_pair_bias.attention.query_bias
    pairformer_stack_blocks_23_attention_pair_bias_attention_input2qkvg_weight = getattr(self.pairformer_stack.blocks, "23").attention_pair_bias.attention.input2qkvg.weight
    pairformer_stack_blocks_23_attention_pair_bias_attention_output_proj_weight = getattr(self.pairformer_stack.blocks, "23").attention_pair_bias.attention.output_proj.weight
    pairformer_stack_blocks_24_transition_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "24").transition_pair.layer_norm.weight
    pairformer_stack_blocks_24_transition_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "24").transition_pair.layer_norm.bias
    pairformer_stack_blocks_24_transition_pair_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "24").transition_pair.linear_no_bias_ab.weight
    pairformer_stack_blocks_24_transition_pair_linear_out_weight = getattr(self.pairformer_stack.blocks, "24").transition_pair.linear_out.weight
    pairformer_stack_blocks_24_triangle_multiplication_layernorm_z_in_weight = getattr(self.pairformer_stack.blocks, "24").triangle_multiplication.layernorm_z_in.weight
    pairformer_stack_blocks_24_triangle_multiplication_layernorm_z_in_bias = getattr(self.pairformer_stack.blocks, "24").triangle_multiplication.layernorm_z_in.bias
    pairformer_stack_blocks_24_triangle_multiplication_linear_z_out_weight = getattr(self.pairformer_stack.blocks, "24").triangle_multiplication.linear_z_out.weight
    pairformer_stack_blocks_24_triangle_multiplication_merged_linear_p_weight = getattr(self.pairformer_stack.blocks, "24").triangle_multiplication.merged_linear_p.weight
    pairformer_stack_blocks_24_triangle_multiplication_merged_linear_g_weight = getattr(self.pairformer_stack.blocks, "24").triangle_multiplication.merged_linear_g.weight
    pairformer_stack_blocks_24_triangle_attention_out_scalers = getattr(self.pairformer_stack.blocks, "24").triangle_attention.out_scalers
    pairformer_stack_blocks_24_triangle_attention_pair2b_weight = getattr(self.pairformer_stack.blocks, "24").triangle_attention.pair2b.weight
    pairformer_stack_blocks_24_triangle_attention_pair2qkvg1_weight = getattr(self.pairformer_stack.blocks, "24").triangle_attention.pair2qkvg1.weight
    pairformer_stack_blocks_24_triangle_attention_pair2qkvg2_weight = getattr(self.pairformer_stack.blocks, "24").triangle_attention.pair2qkvg2.weight
    pairformer_stack_blocks_24_triangle_attention_linear_out_weight = getattr(self.pairformer_stack.blocks, "24").triangle_attention.linear_out.weight
    pairformer_stack_blocks_24_transition_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "24").transition_single.layer_norm.weight
    pairformer_stack_blocks_24_transition_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "24").transition_single.layer_norm.bias
    pairformer_stack_blocks_24_transition_single_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "24").transition_single.linear_no_bias_ab.weight
    pairformer_stack_blocks_24_transition_single_linear_out_weight = getattr(self.pairformer_stack.blocks, "24").transition_single.linear_out.weight
    pairformer_stack_blocks_24_attention_pair_bias_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "24").attention_pair_bias.single_layer_norm.weight
    pairformer_stack_blocks_24_attention_pair_bias_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "24").attention_pair_bias.single_layer_norm.bias
    pairformer_stack_blocks_24_attention_pair_bias_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "24").attention_pair_bias.pair_layer_norm.weight
    pairformer_stack_blocks_24_attention_pair_bias_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "24").attention_pair_bias.pair_layer_norm.bias
    pairformer_stack_blocks_24_attention_pair_bias_pair_linear_weight = getattr(self.pairformer_stack.blocks, "24").attention_pair_bias.pair_linear.weight
    pairformer_stack_blocks_24_attention_pair_bias_attention_query_bias = getattr(self.pairformer_stack.blocks, "24").attention_pair_bias.attention.query_bias
    pairformer_stack_blocks_24_attention_pair_bias_attention_input2qkvg_weight = getattr(self.pairformer_stack.blocks, "24").attention_pair_bias.attention.input2qkvg.weight
    pairformer_stack_blocks_24_attention_pair_bias_attention_output_proj_weight = getattr(self.pairformer_stack.blocks, "24").attention_pair_bias.attention.output_proj.weight
    pairformer_stack_blocks_25_transition_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "25").transition_pair.layer_norm.weight
    pairformer_stack_blocks_25_transition_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "25").transition_pair.layer_norm.bias
    pairformer_stack_blocks_25_transition_pair_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "25").transition_pair.linear_no_bias_ab.weight
    pairformer_stack_blocks_25_transition_pair_linear_out_weight = getattr(self.pairformer_stack.blocks, "25").transition_pair.linear_out.weight
    pairformer_stack_blocks_25_triangle_multiplication_layernorm_z_in_weight = getattr(self.pairformer_stack.blocks, "25").triangle_multiplication.layernorm_z_in.weight
    pairformer_stack_blocks_25_triangle_multiplication_layernorm_z_in_bias = getattr(self.pairformer_stack.blocks, "25").triangle_multiplication.layernorm_z_in.bias
    pairformer_stack_blocks_25_triangle_multiplication_linear_z_out_weight = getattr(self.pairformer_stack.blocks, "25").triangle_multiplication.linear_z_out.weight
    pairformer_stack_blocks_25_triangle_multiplication_merged_linear_p_weight = getattr(self.pairformer_stack.blocks, "25").triangle_multiplication.merged_linear_p.weight
    pairformer_stack_blocks_25_triangle_multiplication_merged_linear_g_weight = getattr(self.pairformer_stack.blocks, "25").triangle_multiplication.merged_linear_g.weight
    pairformer_stack_blocks_25_triangle_attention_out_scalers = getattr(self.pairformer_stack.blocks, "25").triangle_attention.out_scalers
    pairformer_stack_blocks_25_triangle_attention_pair2b_weight = getattr(self.pairformer_stack.blocks, "25").triangle_attention.pair2b.weight
    pairformer_stack_blocks_25_triangle_attention_pair2qkvg1_weight = getattr(self.pairformer_stack.blocks, "25").triangle_attention.pair2qkvg1.weight
    pairformer_stack_blocks_25_triangle_attention_pair2qkvg2_weight = getattr(self.pairformer_stack.blocks, "25").triangle_attention.pair2qkvg2.weight
    pairformer_stack_blocks_25_triangle_attention_linear_out_weight = getattr(self.pairformer_stack.blocks, "25").triangle_attention.linear_out.weight
    pairformer_stack_blocks_25_transition_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "25").transition_single.layer_norm.weight
    pairformer_stack_blocks_25_transition_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "25").transition_single.layer_norm.bias
    pairformer_stack_blocks_25_transition_single_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "25").transition_single.linear_no_bias_ab.weight
    pairformer_stack_blocks_25_transition_single_linear_out_weight = getattr(self.pairformer_stack.blocks, "25").transition_single.linear_out.weight
    pairformer_stack_blocks_25_attention_pair_bias_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "25").attention_pair_bias.single_layer_norm.weight
    pairformer_stack_blocks_25_attention_pair_bias_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "25").attention_pair_bias.single_layer_norm.bias
    pairformer_stack_blocks_25_attention_pair_bias_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "25").attention_pair_bias.pair_layer_norm.weight
    pairformer_stack_blocks_25_attention_pair_bias_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "25").attention_pair_bias.pair_layer_norm.bias
    pairformer_stack_blocks_25_attention_pair_bias_pair_linear_weight = getattr(self.pairformer_stack.blocks, "25").attention_pair_bias.pair_linear.weight
    pairformer_stack_blocks_25_attention_pair_bias_attention_query_bias = getattr(self.pairformer_stack.blocks, "25").attention_pair_bias.attention.query_bias
    pairformer_stack_blocks_25_attention_pair_bias_attention_input2qkvg_weight = getattr(self.pairformer_stack.blocks, "25").attention_pair_bias.attention.input2qkvg.weight
    pairformer_stack_blocks_25_attention_pair_bias_attention_output_proj_weight = getattr(self.pairformer_stack.blocks, "25").attention_pair_bias.attention.output_proj.weight
    pairformer_stack_blocks_26_transition_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "26").transition_pair.layer_norm.weight
    pairformer_stack_blocks_26_transition_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "26").transition_pair.layer_norm.bias
    pairformer_stack_blocks_26_transition_pair_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "26").transition_pair.linear_no_bias_ab.weight
    pairformer_stack_blocks_26_transition_pair_linear_out_weight = getattr(self.pairformer_stack.blocks, "26").transition_pair.linear_out.weight
    pairformer_stack_blocks_26_triangle_multiplication_layernorm_z_in_weight = getattr(self.pairformer_stack.blocks, "26").triangle_multiplication.layernorm_z_in.weight
    pairformer_stack_blocks_26_triangle_multiplication_layernorm_z_in_bias = getattr(self.pairformer_stack.blocks, "26").triangle_multiplication.layernorm_z_in.bias
    pairformer_stack_blocks_26_triangle_multiplication_linear_z_out_weight = getattr(self.pairformer_stack.blocks, "26").triangle_multiplication.linear_z_out.weight
    pairformer_stack_blocks_26_triangle_multiplication_merged_linear_p_weight = getattr(self.pairformer_stack.blocks, "26").triangle_multiplication.merged_linear_p.weight
    pairformer_stack_blocks_26_triangle_multiplication_merged_linear_g_weight = getattr(self.pairformer_stack.blocks, "26").triangle_multiplication.merged_linear_g.weight
    pairformer_stack_blocks_26_triangle_attention_out_scalers = getattr(self.pairformer_stack.blocks, "26").triangle_attention.out_scalers
    pairformer_stack_blocks_26_triangle_attention_pair2b_weight = getattr(self.pairformer_stack.blocks, "26").triangle_attention.pair2b.weight
    pairformer_stack_blocks_26_triangle_attention_pair2qkvg1_weight = getattr(self.pairformer_stack.blocks, "26").triangle_attention.pair2qkvg1.weight
    pairformer_stack_blocks_26_triangle_attention_pair2qkvg2_weight = getattr(self.pairformer_stack.blocks, "26").triangle_attention.pair2qkvg2.weight
    pairformer_stack_blocks_26_triangle_attention_linear_out_weight = getattr(self.pairformer_stack.blocks, "26").triangle_attention.linear_out.weight
    pairformer_stack_blocks_26_transition_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "26").transition_single.layer_norm.weight
    pairformer_stack_blocks_26_transition_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "26").transition_single.layer_norm.bias
    pairformer_stack_blocks_26_transition_single_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "26").transition_single.linear_no_bias_ab.weight
    pairformer_stack_blocks_26_transition_single_linear_out_weight = getattr(self.pairformer_stack.blocks, "26").transition_single.linear_out.weight
    pairformer_stack_blocks_26_attention_pair_bias_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "26").attention_pair_bias.single_layer_norm.weight
    pairformer_stack_blocks_26_attention_pair_bias_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "26").attention_pair_bias.single_layer_norm.bias
    pairformer_stack_blocks_26_attention_pair_bias_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "26").attention_pair_bias.pair_layer_norm.weight
    pairformer_stack_blocks_26_attention_pair_bias_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "26").attention_pair_bias.pair_layer_norm.bias
    pairformer_stack_blocks_26_attention_pair_bias_pair_linear_weight = getattr(self.pairformer_stack.blocks, "26").attention_pair_bias.pair_linear.weight
    pairformer_stack_blocks_26_attention_pair_bias_attention_query_bias = getattr(self.pairformer_stack.blocks, "26").attention_pair_bias.attention.query_bias
    pairformer_stack_blocks_26_attention_pair_bias_attention_input2qkvg_weight = getattr(self.pairformer_stack.blocks, "26").attention_pair_bias.attention.input2qkvg.weight
    pairformer_stack_blocks_26_attention_pair_bias_attention_output_proj_weight = getattr(self.pairformer_stack.blocks, "26").attention_pair_bias.attention.output_proj.weight
    pairformer_stack_blocks_27_transition_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "27").transition_pair.layer_norm.weight
    pairformer_stack_blocks_27_transition_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "27").transition_pair.layer_norm.bias
    pairformer_stack_blocks_27_transition_pair_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "27").transition_pair.linear_no_bias_ab.weight
    pairformer_stack_blocks_27_transition_pair_linear_out_weight = getattr(self.pairformer_stack.blocks, "27").transition_pair.linear_out.weight
    pairformer_stack_blocks_27_triangle_multiplication_layernorm_z_in_weight = getattr(self.pairformer_stack.blocks, "27").triangle_multiplication.layernorm_z_in.weight
    pairformer_stack_blocks_27_triangle_multiplication_layernorm_z_in_bias = getattr(self.pairformer_stack.blocks, "27").triangle_multiplication.layernorm_z_in.bias
    pairformer_stack_blocks_27_triangle_multiplication_linear_z_out_weight = getattr(self.pairformer_stack.blocks, "27").triangle_multiplication.linear_z_out.weight
    pairformer_stack_blocks_27_triangle_multiplication_merged_linear_p_weight = getattr(self.pairformer_stack.blocks, "27").triangle_multiplication.merged_linear_p.weight
    pairformer_stack_blocks_27_triangle_multiplication_merged_linear_g_weight = getattr(self.pairformer_stack.blocks, "27").triangle_multiplication.merged_linear_g.weight
    pairformer_stack_blocks_27_triangle_attention_out_scalers = getattr(self.pairformer_stack.blocks, "27").triangle_attention.out_scalers
    pairformer_stack_blocks_27_triangle_attention_pair2b_weight = getattr(self.pairformer_stack.blocks, "27").triangle_attention.pair2b.weight
    pairformer_stack_blocks_27_triangle_attention_pair2qkvg1_weight = getattr(self.pairformer_stack.blocks, "27").triangle_attention.pair2qkvg1.weight
    pairformer_stack_blocks_27_triangle_attention_pair2qkvg2_weight = getattr(self.pairformer_stack.blocks, "27").triangle_attention.pair2qkvg2.weight
    pairformer_stack_blocks_27_triangle_attention_linear_out_weight = getattr(self.pairformer_stack.blocks, "27").triangle_attention.linear_out.weight
    pairformer_stack_blocks_27_transition_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "27").transition_single.layer_norm.weight
    pairformer_stack_blocks_27_transition_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "27").transition_single.layer_norm.bias
    pairformer_stack_blocks_27_transition_single_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "27").transition_single.linear_no_bias_ab.weight
    pairformer_stack_blocks_27_transition_single_linear_out_weight = getattr(self.pairformer_stack.blocks, "27").transition_single.linear_out.weight
    pairformer_stack_blocks_27_attention_pair_bias_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "27").attention_pair_bias.single_layer_norm.weight
    pairformer_stack_blocks_27_attention_pair_bias_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "27").attention_pair_bias.single_layer_norm.bias
    pairformer_stack_blocks_27_attention_pair_bias_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "27").attention_pair_bias.pair_layer_norm.weight
    pairformer_stack_blocks_27_attention_pair_bias_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "27").attention_pair_bias.pair_layer_norm.bias
    pairformer_stack_blocks_27_attention_pair_bias_pair_linear_weight = getattr(self.pairformer_stack.blocks, "27").attention_pair_bias.pair_linear.weight
    pairformer_stack_blocks_27_attention_pair_bias_attention_query_bias = getattr(self.pairformer_stack.blocks, "27").attention_pair_bias.attention.query_bias
    pairformer_stack_blocks_27_attention_pair_bias_attention_input2qkvg_weight = getattr(self.pairformer_stack.blocks, "27").attention_pair_bias.attention.input2qkvg.weight
    pairformer_stack_blocks_27_attention_pair_bias_attention_output_proj_weight = getattr(self.pairformer_stack.blocks, "27").attention_pair_bias.attention.output_proj.weight
    pairformer_stack_blocks_28_transition_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "28").transition_pair.layer_norm.weight
    pairformer_stack_blocks_28_transition_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "28").transition_pair.layer_norm.bias
    pairformer_stack_blocks_28_transition_pair_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "28").transition_pair.linear_no_bias_ab.weight
    pairformer_stack_blocks_28_transition_pair_linear_out_weight = getattr(self.pairformer_stack.blocks, "28").transition_pair.linear_out.weight
    pairformer_stack_blocks_28_triangle_multiplication_layernorm_z_in_weight = getattr(self.pairformer_stack.blocks, "28").triangle_multiplication.layernorm_z_in.weight
    pairformer_stack_blocks_28_triangle_multiplication_layernorm_z_in_bias = getattr(self.pairformer_stack.blocks, "28").triangle_multiplication.layernorm_z_in.bias
    pairformer_stack_blocks_28_triangle_multiplication_linear_z_out_weight = getattr(self.pairformer_stack.blocks, "28").triangle_multiplication.linear_z_out.weight
    pairformer_stack_blocks_28_triangle_multiplication_merged_linear_p_weight = getattr(self.pairformer_stack.blocks, "28").triangle_multiplication.merged_linear_p.weight
    pairformer_stack_blocks_28_triangle_multiplication_merged_linear_g_weight = getattr(self.pairformer_stack.blocks, "28").triangle_multiplication.merged_linear_g.weight
    pairformer_stack_blocks_28_triangle_attention_out_scalers = getattr(self.pairformer_stack.blocks, "28").triangle_attention.out_scalers
    pairformer_stack_blocks_28_triangle_attention_pair2b_weight = getattr(self.pairformer_stack.blocks, "28").triangle_attention.pair2b.weight
    pairformer_stack_blocks_28_triangle_attention_pair2qkvg1_weight = getattr(self.pairformer_stack.blocks, "28").triangle_attention.pair2qkvg1.weight
    pairformer_stack_blocks_28_triangle_attention_pair2qkvg2_weight = getattr(self.pairformer_stack.blocks, "28").triangle_attention.pair2qkvg2.weight
    pairformer_stack_blocks_28_triangle_attention_linear_out_weight = getattr(self.pairformer_stack.blocks, "28").triangle_attention.linear_out.weight
    pairformer_stack_blocks_28_transition_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "28").transition_single.layer_norm.weight
    pairformer_stack_blocks_28_transition_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "28").transition_single.layer_norm.bias
    pairformer_stack_blocks_28_transition_single_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "28").transition_single.linear_no_bias_ab.weight
    pairformer_stack_blocks_28_transition_single_linear_out_weight = getattr(self.pairformer_stack.blocks, "28").transition_single.linear_out.weight
    pairformer_stack_blocks_28_attention_pair_bias_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "28").attention_pair_bias.single_layer_norm.weight
    pairformer_stack_blocks_28_attention_pair_bias_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "28").attention_pair_bias.single_layer_norm.bias
    pairformer_stack_blocks_28_attention_pair_bias_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "28").attention_pair_bias.pair_layer_norm.weight
    pairformer_stack_blocks_28_attention_pair_bias_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "28").attention_pair_bias.pair_layer_norm.bias
    pairformer_stack_blocks_28_attention_pair_bias_pair_linear_weight = getattr(self.pairformer_stack.blocks, "28").attention_pair_bias.pair_linear.weight
    pairformer_stack_blocks_28_attention_pair_bias_attention_query_bias = getattr(self.pairformer_stack.blocks, "28").attention_pair_bias.attention.query_bias
    pairformer_stack_blocks_28_attention_pair_bias_attention_input2qkvg_weight = getattr(self.pairformer_stack.blocks, "28").attention_pair_bias.attention.input2qkvg.weight
    pairformer_stack_blocks_28_attention_pair_bias_attention_output_proj_weight = getattr(self.pairformer_stack.blocks, "28").attention_pair_bias.attention.output_proj.weight
    pairformer_stack_blocks_29_transition_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "29").transition_pair.layer_norm.weight
    pairformer_stack_blocks_29_transition_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "29").transition_pair.layer_norm.bias
    pairformer_stack_blocks_29_transition_pair_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "29").transition_pair.linear_no_bias_ab.weight
    pairformer_stack_blocks_29_transition_pair_linear_out_weight = getattr(self.pairformer_stack.blocks, "29").transition_pair.linear_out.weight
    pairformer_stack_blocks_29_triangle_multiplication_layernorm_z_in_weight = getattr(self.pairformer_stack.blocks, "29").triangle_multiplication.layernorm_z_in.weight
    pairformer_stack_blocks_29_triangle_multiplication_layernorm_z_in_bias = getattr(self.pairformer_stack.blocks, "29").triangle_multiplication.layernorm_z_in.bias
    pairformer_stack_blocks_29_triangle_multiplication_linear_z_out_weight = getattr(self.pairformer_stack.blocks, "29").triangle_multiplication.linear_z_out.weight
    pairformer_stack_blocks_29_triangle_multiplication_merged_linear_p_weight = getattr(self.pairformer_stack.blocks, "29").triangle_multiplication.merged_linear_p.weight
    pairformer_stack_blocks_29_triangle_multiplication_merged_linear_g_weight = getattr(self.pairformer_stack.blocks, "29").triangle_multiplication.merged_linear_g.weight
    pairformer_stack_blocks_29_triangle_attention_out_scalers = getattr(self.pairformer_stack.blocks, "29").triangle_attention.out_scalers
    pairformer_stack_blocks_29_triangle_attention_pair2b_weight = getattr(self.pairformer_stack.blocks, "29").triangle_attention.pair2b.weight
    pairformer_stack_blocks_29_triangle_attention_pair2qkvg1_weight = getattr(self.pairformer_stack.blocks, "29").triangle_attention.pair2qkvg1.weight
    pairformer_stack_blocks_29_triangle_attention_pair2qkvg2_weight = getattr(self.pairformer_stack.blocks, "29").triangle_attention.pair2qkvg2.weight
    pairformer_stack_blocks_29_triangle_attention_linear_out_weight = getattr(self.pairformer_stack.blocks, "29").triangle_attention.linear_out.weight
    pairformer_stack_blocks_29_transition_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "29").transition_single.layer_norm.weight
    pairformer_stack_blocks_29_transition_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "29").transition_single.layer_norm.bias
    pairformer_stack_blocks_29_transition_single_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "29").transition_single.linear_no_bias_ab.weight
    pairformer_stack_blocks_29_transition_single_linear_out_weight = getattr(self.pairformer_stack.blocks, "29").transition_single.linear_out.weight
    pairformer_stack_blocks_29_attention_pair_bias_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "29").attention_pair_bias.single_layer_norm.weight
    pairformer_stack_blocks_29_attention_pair_bias_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "29").attention_pair_bias.single_layer_norm.bias
    pairformer_stack_blocks_29_attention_pair_bias_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "29").attention_pair_bias.pair_layer_norm.weight
    pairformer_stack_blocks_29_attention_pair_bias_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "29").attention_pair_bias.pair_layer_norm.bias
    pairformer_stack_blocks_29_attention_pair_bias_pair_linear_weight = getattr(self.pairformer_stack.blocks, "29").attention_pair_bias.pair_linear.weight
    pairformer_stack_blocks_29_attention_pair_bias_attention_query_bias = getattr(self.pairformer_stack.blocks, "29").attention_pair_bias.attention.query_bias
    pairformer_stack_blocks_29_attention_pair_bias_attention_input2qkvg_weight = getattr(self.pairformer_stack.blocks, "29").attention_pair_bias.attention.input2qkvg.weight
    pairformer_stack_blocks_29_attention_pair_bias_attention_output_proj_weight = getattr(self.pairformer_stack.blocks, "29").attention_pair_bias.attention.output_proj.weight
    pairformer_stack_blocks_30_transition_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "30").transition_pair.layer_norm.weight
    pairformer_stack_blocks_30_transition_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "30").transition_pair.layer_norm.bias
    pairformer_stack_blocks_30_transition_pair_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "30").transition_pair.linear_no_bias_ab.weight
    pairformer_stack_blocks_30_transition_pair_linear_out_weight = getattr(self.pairformer_stack.blocks, "30").transition_pair.linear_out.weight
    pairformer_stack_blocks_30_triangle_multiplication_layernorm_z_in_weight = getattr(self.pairformer_stack.blocks, "30").triangle_multiplication.layernorm_z_in.weight
    pairformer_stack_blocks_30_triangle_multiplication_layernorm_z_in_bias = getattr(self.pairformer_stack.blocks, "30").triangle_multiplication.layernorm_z_in.bias
    pairformer_stack_blocks_30_triangle_multiplication_linear_z_out_weight = getattr(self.pairformer_stack.blocks, "30").triangle_multiplication.linear_z_out.weight
    pairformer_stack_blocks_30_triangle_multiplication_merged_linear_p_weight = getattr(self.pairformer_stack.blocks, "30").triangle_multiplication.merged_linear_p.weight
    pairformer_stack_blocks_30_triangle_multiplication_merged_linear_g_weight = getattr(self.pairformer_stack.blocks, "30").triangle_multiplication.merged_linear_g.weight
    pairformer_stack_blocks_30_triangle_attention_out_scalers = getattr(self.pairformer_stack.blocks, "30").triangle_attention.out_scalers
    pairformer_stack_blocks_30_triangle_attention_pair2b_weight = getattr(self.pairformer_stack.blocks, "30").triangle_attention.pair2b.weight
    pairformer_stack_blocks_30_triangle_attention_pair2qkvg1_weight = getattr(self.pairformer_stack.blocks, "30").triangle_attention.pair2qkvg1.weight
    pairformer_stack_blocks_30_triangle_attention_pair2qkvg2_weight = getattr(self.pairformer_stack.blocks, "30").triangle_attention.pair2qkvg2.weight
    pairformer_stack_blocks_30_triangle_attention_linear_out_weight = getattr(self.pairformer_stack.blocks, "30").triangle_attention.linear_out.weight
    pairformer_stack_blocks_30_transition_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "30").transition_single.layer_norm.weight
    pairformer_stack_blocks_30_transition_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "30").transition_single.layer_norm.bias
    pairformer_stack_blocks_30_transition_single_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "30").transition_single.linear_no_bias_ab.weight
    pairformer_stack_blocks_30_transition_single_linear_out_weight = getattr(self.pairformer_stack.blocks, "30").transition_single.linear_out.weight
    pairformer_stack_blocks_30_attention_pair_bias_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "30").attention_pair_bias.single_layer_norm.weight
    pairformer_stack_blocks_30_attention_pair_bias_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "30").attention_pair_bias.single_layer_norm.bias
    pairformer_stack_blocks_30_attention_pair_bias_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "30").attention_pair_bias.pair_layer_norm.weight
    pairformer_stack_blocks_30_attention_pair_bias_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "30").attention_pair_bias.pair_layer_norm.bias
    pairformer_stack_blocks_30_attention_pair_bias_pair_linear_weight = getattr(self.pairformer_stack.blocks, "30").attention_pair_bias.pair_linear.weight
    pairformer_stack_blocks_30_attention_pair_bias_attention_query_bias = getattr(self.pairformer_stack.blocks, "30").attention_pair_bias.attention.query_bias
    pairformer_stack_blocks_30_attention_pair_bias_attention_input2qkvg_weight = getattr(self.pairformer_stack.blocks, "30").attention_pair_bias.attention.input2qkvg.weight
    pairformer_stack_blocks_30_attention_pair_bias_attention_output_proj_weight = getattr(self.pairformer_stack.blocks, "30").attention_pair_bias.attention.output_proj.weight
    pairformer_stack_blocks_31_transition_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "31").transition_pair.layer_norm.weight
    pairformer_stack_blocks_31_transition_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "31").transition_pair.layer_norm.bias
    pairformer_stack_blocks_31_transition_pair_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "31").transition_pair.linear_no_bias_ab.weight
    pairformer_stack_blocks_31_transition_pair_linear_out_weight = getattr(self.pairformer_stack.blocks, "31").transition_pair.linear_out.weight
    pairformer_stack_blocks_31_triangle_multiplication_layernorm_z_in_weight = getattr(self.pairformer_stack.blocks, "31").triangle_multiplication.layernorm_z_in.weight
    pairformer_stack_blocks_31_triangle_multiplication_layernorm_z_in_bias = getattr(self.pairformer_stack.blocks, "31").triangle_multiplication.layernorm_z_in.bias
    pairformer_stack_blocks_31_triangle_multiplication_linear_z_out_weight = getattr(self.pairformer_stack.blocks, "31").triangle_multiplication.linear_z_out.weight
    pairformer_stack_blocks_31_triangle_multiplication_merged_linear_p_weight = getattr(self.pairformer_stack.blocks, "31").triangle_multiplication.merged_linear_p.weight
    pairformer_stack_blocks_31_triangle_multiplication_merged_linear_g_weight = getattr(self.pairformer_stack.blocks, "31").triangle_multiplication.merged_linear_g.weight
    pairformer_stack_blocks_31_triangle_attention_out_scalers = getattr(self.pairformer_stack.blocks, "31").triangle_attention.out_scalers
    pairformer_stack_blocks_31_triangle_attention_pair2b_weight = getattr(self.pairformer_stack.blocks, "31").triangle_attention.pair2b.weight
    pairformer_stack_blocks_31_triangle_attention_pair2qkvg1_weight = getattr(self.pairformer_stack.blocks, "31").triangle_attention.pair2qkvg1.weight
    pairformer_stack_blocks_31_triangle_attention_pair2qkvg2_weight = getattr(self.pairformer_stack.blocks, "31").triangle_attention.pair2qkvg2.weight
    pairformer_stack_blocks_31_triangle_attention_linear_out_weight = getattr(self.pairformer_stack.blocks, "31").triangle_attention.linear_out.weight
    pairformer_stack_blocks_31_transition_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "31").transition_single.layer_norm.weight
    pairformer_stack_blocks_31_transition_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "31").transition_single.layer_norm.bias
    pairformer_stack_blocks_31_transition_single_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "31").transition_single.linear_no_bias_ab.weight
    pairformer_stack_blocks_31_transition_single_linear_out_weight = getattr(self.pairformer_stack.blocks, "31").transition_single.linear_out.weight
    pairformer_stack_blocks_31_attention_pair_bias_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "31").attention_pair_bias.single_layer_norm.weight
    pairformer_stack_blocks_31_attention_pair_bias_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "31").attention_pair_bias.single_layer_norm.bias
    pairformer_stack_blocks_31_attention_pair_bias_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "31").attention_pair_bias.pair_layer_norm.weight
    pairformer_stack_blocks_31_attention_pair_bias_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "31").attention_pair_bias.pair_layer_norm.bias
    pairformer_stack_blocks_31_attention_pair_bias_pair_linear_weight = getattr(self.pairformer_stack.blocks, "31").attention_pair_bias.pair_linear.weight
    pairformer_stack_blocks_31_attention_pair_bias_attention_query_bias = getattr(self.pairformer_stack.blocks, "31").attention_pair_bias.attention.query_bias
    pairformer_stack_blocks_31_attention_pair_bias_attention_input2qkvg_weight = getattr(self.pairformer_stack.blocks, "31").attention_pair_bias.attention.input2qkvg.weight
    pairformer_stack_blocks_31_attention_pair_bias_attention_output_proj_weight = getattr(self.pairformer_stack.blocks, "31").attention_pair_bias.attention.output_proj.weight
    pairformer_stack_blocks_32_transition_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "32").transition_pair.layer_norm.weight
    pairformer_stack_blocks_32_transition_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "32").transition_pair.layer_norm.bias
    pairformer_stack_blocks_32_transition_pair_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "32").transition_pair.linear_no_bias_ab.weight
    pairformer_stack_blocks_32_transition_pair_linear_out_weight = getattr(self.pairformer_stack.blocks, "32").transition_pair.linear_out.weight
    pairformer_stack_blocks_32_triangle_multiplication_layernorm_z_in_weight = getattr(self.pairformer_stack.blocks, "32").triangle_multiplication.layernorm_z_in.weight
    pairformer_stack_blocks_32_triangle_multiplication_layernorm_z_in_bias = getattr(self.pairformer_stack.blocks, "32").triangle_multiplication.layernorm_z_in.bias
    pairformer_stack_blocks_32_triangle_multiplication_linear_z_out_weight = getattr(self.pairformer_stack.blocks, "32").triangle_multiplication.linear_z_out.weight
    pairformer_stack_blocks_32_triangle_multiplication_merged_linear_p_weight = getattr(self.pairformer_stack.blocks, "32").triangle_multiplication.merged_linear_p.weight
    pairformer_stack_blocks_32_triangle_multiplication_merged_linear_g_weight = getattr(self.pairformer_stack.blocks, "32").triangle_multiplication.merged_linear_g.weight
    pairformer_stack_blocks_32_triangle_attention_out_scalers = getattr(self.pairformer_stack.blocks, "32").triangle_attention.out_scalers
    pairformer_stack_blocks_32_triangle_attention_pair2b_weight = getattr(self.pairformer_stack.blocks, "32").triangle_attention.pair2b.weight
    pairformer_stack_blocks_32_triangle_attention_pair2qkvg1_weight = getattr(self.pairformer_stack.blocks, "32").triangle_attention.pair2qkvg1.weight
    pairformer_stack_blocks_32_triangle_attention_pair2qkvg2_weight = getattr(self.pairformer_stack.blocks, "32").triangle_attention.pair2qkvg2.weight
    pairformer_stack_blocks_32_triangle_attention_linear_out_weight = getattr(self.pairformer_stack.blocks, "32").triangle_attention.linear_out.weight
    pairformer_stack_blocks_32_transition_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "32").transition_single.layer_norm.weight
    pairformer_stack_blocks_32_transition_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "32").transition_single.layer_norm.bias
    pairformer_stack_blocks_32_transition_single_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "32").transition_single.linear_no_bias_ab.weight
    pairformer_stack_blocks_32_transition_single_linear_out_weight = getattr(self.pairformer_stack.blocks, "32").transition_single.linear_out.weight
    pairformer_stack_blocks_32_attention_pair_bias_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "32").attention_pair_bias.single_layer_norm.weight
    pairformer_stack_blocks_32_attention_pair_bias_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "32").attention_pair_bias.single_layer_norm.bias
    pairformer_stack_blocks_32_attention_pair_bias_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "32").attention_pair_bias.pair_layer_norm.weight
    pairformer_stack_blocks_32_attention_pair_bias_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "32").attention_pair_bias.pair_layer_norm.bias
    pairformer_stack_blocks_32_attention_pair_bias_pair_linear_weight = getattr(self.pairformer_stack.blocks, "32").attention_pair_bias.pair_linear.weight
    pairformer_stack_blocks_32_attention_pair_bias_attention_query_bias = getattr(self.pairformer_stack.blocks, "32").attention_pair_bias.attention.query_bias
    pairformer_stack_blocks_32_attention_pair_bias_attention_input2qkvg_weight = getattr(self.pairformer_stack.blocks, "32").attention_pair_bias.attention.input2qkvg.weight
    pairformer_stack_blocks_32_attention_pair_bias_attention_output_proj_weight = getattr(self.pairformer_stack.blocks, "32").attention_pair_bias.attention.output_proj.weight
    pairformer_stack_blocks_33_transition_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "33").transition_pair.layer_norm.weight
    pairformer_stack_blocks_33_transition_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "33").transition_pair.layer_norm.bias
    pairformer_stack_blocks_33_transition_pair_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "33").transition_pair.linear_no_bias_ab.weight
    pairformer_stack_blocks_33_transition_pair_linear_out_weight = getattr(self.pairformer_stack.blocks, "33").transition_pair.linear_out.weight
    pairformer_stack_blocks_33_triangle_multiplication_layernorm_z_in_weight = getattr(self.pairformer_stack.blocks, "33").triangle_multiplication.layernorm_z_in.weight
    pairformer_stack_blocks_33_triangle_multiplication_layernorm_z_in_bias = getattr(self.pairformer_stack.blocks, "33").triangle_multiplication.layernorm_z_in.bias
    pairformer_stack_blocks_33_triangle_multiplication_linear_z_out_weight = getattr(self.pairformer_stack.blocks, "33").triangle_multiplication.linear_z_out.weight
    pairformer_stack_blocks_33_triangle_multiplication_merged_linear_p_weight = getattr(self.pairformer_stack.blocks, "33").triangle_multiplication.merged_linear_p.weight
    pairformer_stack_blocks_33_triangle_multiplication_merged_linear_g_weight = getattr(self.pairformer_stack.blocks, "33").triangle_multiplication.merged_linear_g.weight
    pairformer_stack_blocks_33_triangle_attention_out_scalers = getattr(self.pairformer_stack.blocks, "33").triangle_attention.out_scalers
    pairformer_stack_blocks_33_triangle_attention_pair2b_weight = getattr(self.pairformer_stack.blocks, "33").triangle_attention.pair2b.weight
    pairformer_stack_blocks_33_triangle_attention_pair2qkvg1_weight = getattr(self.pairformer_stack.blocks, "33").triangle_attention.pair2qkvg1.weight
    pairformer_stack_blocks_33_triangle_attention_pair2qkvg2_weight = getattr(self.pairformer_stack.blocks, "33").triangle_attention.pair2qkvg2.weight
    pairformer_stack_blocks_33_triangle_attention_linear_out_weight = getattr(self.pairformer_stack.blocks, "33").triangle_attention.linear_out.weight
    pairformer_stack_blocks_33_transition_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "33").transition_single.layer_norm.weight
    pairformer_stack_blocks_33_transition_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "33").transition_single.layer_norm.bias
    pairformer_stack_blocks_33_transition_single_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "33").transition_single.linear_no_bias_ab.weight
    pairformer_stack_blocks_33_transition_single_linear_out_weight = getattr(self.pairformer_stack.blocks, "33").transition_single.linear_out.weight
    pairformer_stack_blocks_33_attention_pair_bias_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "33").attention_pair_bias.single_layer_norm.weight
    pairformer_stack_blocks_33_attention_pair_bias_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "33").attention_pair_bias.single_layer_norm.bias
    pairformer_stack_blocks_33_attention_pair_bias_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "33").attention_pair_bias.pair_layer_norm.weight
    pairformer_stack_blocks_33_attention_pair_bias_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "33").attention_pair_bias.pair_layer_norm.bias
    pairformer_stack_blocks_33_attention_pair_bias_pair_linear_weight = getattr(self.pairformer_stack.blocks, "33").attention_pair_bias.pair_linear.weight
    pairformer_stack_blocks_33_attention_pair_bias_attention_query_bias = getattr(self.pairformer_stack.blocks, "33").attention_pair_bias.attention.query_bias
    pairformer_stack_blocks_33_attention_pair_bias_attention_input2qkvg_weight = getattr(self.pairformer_stack.blocks, "33").attention_pair_bias.attention.input2qkvg.weight
    pairformer_stack_blocks_33_attention_pair_bias_attention_output_proj_weight = getattr(self.pairformer_stack.blocks, "33").attention_pair_bias.attention.output_proj.weight
    pairformer_stack_blocks_34_transition_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "34").transition_pair.layer_norm.weight
    pairformer_stack_blocks_34_transition_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "34").transition_pair.layer_norm.bias
    pairformer_stack_blocks_34_transition_pair_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "34").transition_pair.linear_no_bias_ab.weight
    pairformer_stack_blocks_34_transition_pair_linear_out_weight = getattr(self.pairformer_stack.blocks, "34").transition_pair.linear_out.weight
    pairformer_stack_blocks_34_triangle_multiplication_layernorm_z_in_weight = getattr(self.pairformer_stack.blocks, "34").triangle_multiplication.layernorm_z_in.weight
    pairformer_stack_blocks_34_triangle_multiplication_layernorm_z_in_bias = getattr(self.pairformer_stack.blocks, "34").triangle_multiplication.layernorm_z_in.bias
    pairformer_stack_blocks_34_triangle_multiplication_linear_z_out_weight = getattr(self.pairformer_stack.blocks, "34").triangle_multiplication.linear_z_out.weight
    pairformer_stack_blocks_34_triangle_multiplication_merged_linear_p_weight = getattr(self.pairformer_stack.blocks, "34").triangle_multiplication.merged_linear_p.weight
    pairformer_stack_blocks_34_triangle_multiplication_merged_linear_g_weight = getattr(self.pairformer_stack.blocks, "34").triangle_multiplication.merged_linear_g.weight
    pairformer_stack_blocks_34_triangle_attention_out_scalers = getattr(self.pairformer_stack.blocks, "34").triangle_attention.out_scalers
    pairformer_stack_blocks_34_triangle_attention_pair2b_weight = getattr(self.pairformer_stack.blocks, "34").triangle_attention.pair2b.weight
    pairformer_stack_blocks_34_triangle_attention_pair2qkvg1_weight = getattr(self.pairformer_stack.blocks, "34").triangle_attention.pair2qkvg1.weight
    pairformer_stack_blocks_34_triangle_attention_pair2qkvg2_weight = getattr(self.pairformer_stack.blocks, "34").triangle_attention.pair2qkvg2.weight
    pairformer_stack_blocks_34_triangle_attention_linear_out_weight = getattr(self.pairformer_stack.blocks, "34").triangle_attention.linear_out.weight
    pairformer_stack_blocks_34_transition_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "34").transition_single.layer_norm.weight
    pairformer_stack_blocks_34_transition_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "34").transition_single.layer_norm.bias
    pairformer_stack_blocks_34_transition_single_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "34").transition_single.linear_no_bias_ab.weight
    pairformer_stack_blocks_34_transition_single_linear_out_weight = getattr(self.pairformer_stack.blocks, "34").transition_single.linear_out.weight
    pairformer_stack_blocks_34_attention_pair_bias_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "34").attention_pair_bias.single_layer_norm.weight
    pairformer_stack_blocks_34_attention_pair_bias_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "34").attention_pair_bias.single_layer_norm.bias
    pairformer_stack_blocks_34_attention_pair_bias_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "34").attention_pair_bias.pair_layer_norm.weight
    pairformer_stack_blocks_34_attention_pair_bias_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "34").attention_pair_bias.pair_layer_norm.bias
    pairformer_stack_blocks_34_attention_pair_bias_pair_linear_weight = getattr(self.pairformer_stack.blocks, "34").attention_pair_bias.pair_linear.weight
    pairformer_stack_blocks_34_attention_pair_bias_attention_query_bias = getattr(self.pairformer_stack.blocks, "34").attention_pair_bias.attention.query_bias
    pairformer_stack_blocks_34_attention_pair_bias_attention_input2qkvg_weight = getattr(self.pairformer_stack.blocks, "34").attention_pair_bias.attention.input2qkvg.weight
    pairformer_stack_blocks_34_attention_pair_bias_attention_output_proj_weight = getattr(self.pairformer_stack.blocks, "34").attention_pair_bias.attention.output_proj.weight
    pairformer_stack_blocks_35_transition_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "35").transition_pair.layer_norm.weight
    pairformer_stack_blocks_35_transition_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "35").transition_pair.layer_norm.bias
    pairformer_stack_blocks_35_transition_pair_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "35").transition_pair.linear_no_bias_ab.weight
    pairformer_stack_blocks_35_transition_pair_linear_out_weight = getattr(self.pairformer_stack.blocks, "35").transition_pair.linear_out.weight
    pairformer_stack_blocks_35_triangle_multiplication_layernorm_z_in_weight = getattr(self.pairformer_stack.blocks, "35").triangle_multiplication.layernorm_z_in.weight
    pairformer_stack_blocks_35_triangle_multiplication_layernorm_z_in_bias = getattr(self.pairformer_stack.blocks, "35").triangle_multiplication.layernorm_z_in.bias
    pairformer_stack_blocks_35_triangle_multiplication_linear_z_out_weight = getattr(self.pairformer_stack.blocks, "35").triangle_multiplication.linear_z_out.weight
    pairformer_stack_blocks_35_triangle_multiplication_merged_linear_p_weight = getattr(self.pairformer_stack.blocks, "35").triangle_multiplication.merged_linear_p.weight
    pairformer_stack_blocks_35_triangle_multiplication_merged_linear_g_weight = getattr(self.pairformer_stack.blocks, "35").triangle_multiplication.merged_linear_g.weight
    pairformer_stack_blocks_35_triangle_attention_out_scalers = getattr(self.pairformer_stack.blocks, "35").triangle_attention.out_scalers
    pairformer_stack_blocks_35_triangle_attention_pair2b_weight = getattr(self.pairformer_stack.blocks, "35").triangle_attention.pair2b.weight
    pairformer_stack_blocks_35_triangle_attention_pair2qkvg1_weight = getattr(self.pairformer_stack.blocks, "35").triangle_attention.pair2qkvg1.weight
    pairformer_stack_blocks_35_triangle_attention_pair2qkvg2_weight = getattr(self.pairformer_stack.blocks, "35").triangle_attention.pair2qkvg2.weight
    pairformer_stack_blocks_35_triangle_attention_linear_out_weight = getattr(self.pairformer_stack.blocks, "35").triangle_attention.linear_out.weight
    pairformer_stack_blocks_35_transition_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "35").transition_single.layer_norm.weight
    pairformer_stack_blocks_35_transition_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "35").transition_single.layer_norm.bias
    pairformer_stack_blocks_35_transition_single_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "35").transition_single.linear_no_bias_ab.weight
    pairformer_stack_blocks_35_transition_single_linear_out_weight = getattr(self.pairformer_stack.blocks, "35").transition_single.linear_out.weight
    pairformer_stack_blocks_35_attention_pair_bias_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "35").attention_pair_bias.single_layer_norm.weight
    pairformer_stack_blocks_35_attention_pair_bias_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "35").attention_pair_bias.single_layer_norm.bias
    pairformer_stack_blocks_35_attention_pair_bias_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "35").attention_pair_bias.pair_layer_norm.weight
    pairformer_stack_blocks_35_attention_pair_bias_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "35").attention_pair_bias.pair_layer_norm.bias
    pairformer_stack_blocks_35_attention_pair_bias_pair_linear_weight = getattr(self.pairformer_stack.blocks, "35").attention_pair_bias.pair_linear.weight
    pairformer_stack_blocks_35_attention_pair_bias_attention_query_bias = getattr(self.pairformer_stack.blocks, "35").attention_pair_bias.attention.query_bias
    pairformer_stack_blocks_35_attention_pair_bias_attention_input2qkvg_weight = getattr(self.pairformer_stack.blocks, "35").attention_pair_bias.attention.input2qkvg.weight
    pairformer_stack_blocks_35_attention_pair_bias_attention_output_proj_weight = getattr(self.pairformer_stack.blocks, "35").attention_pair_bias.attention.output_proj.weight
    pairformer_stack_blocks_36_transition_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "36").transition_pair.layer_norm.weight
    pairformer_stack_blocks_36_transition_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "36").transition_pair.layer_norm.bias
    pairformer_stack_blocks_36_transition_pair_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "36").transition_pair.linear_no_bias_ab.weight
    pairformer_stack_blocks_36_transition_pair_linear_out_weight = getattr(self.pairformer_stack.blocks, "36").transition_pair.linear_out.weight
    pairformer_stack_blocks_36_triangle_multiplication_layernorm_z_in_weight = getattr(self.pairformer_stack.blocks, "36").triangle_multiplication.layernorm_z_in.weight
    pairformer_stack_blocks_36_triangle_multiplication_layernorm_z_in_bias = getattr(self.pairformer_stack.blocks, "36").triangle_multiplication.layernorm_z_in.bias
    pairformer_stack_blocks_36_triangle_multiplication_linear_z_out_weight = getattr(self.pairformer_stack.blocks, "36").triangle_multiplication.linear_z_out.weight
    pairformer_stack_blocks_36_triangle_multiplication_merged_linear_p_weight = getattr(self.pairformer_stack.blocks, "36").triangle_multiplication.merged_linear_p.weight
    pairformer_stack_blocks_36_triangle_multiplication_merged_linear_g_weight = getattr(self.pairformer_stack.blocks, "36").triangle_multiplication.merged_linear_g.weight
    pairformer_stack_blocks_36_triangle_attention_out_scalers = getattr(self.pairformer_stack.blocks, "36").triangle_attention.out_scalers
    pairformer_stack_blocks_36_triangle_attention_pair2b_weight = getattr(self.pairformer_stack.blocks, "36").triangle_attention.pair2b.weight
    pairformer_stack_blocks_36_triangle_attention_pair2qkvg1_weight = getattr(self.pairformer_stack.blocks, "36").triangle_attention.pair2qkvg1.weight
    pairformer_stack_blocks_36_triangle_attention_pair2qkvg2_weight = getattr(self.pairformer_stack.blocks, "36").triangle_attention.pair2qkvg2.weight
    pairformer_stack_blocks_36_triangle_attention_linear_out_weight = getattr(self.pairformer_stack.blocks, "36").triangle_attention.linear_out.weight
    pairformer_stack_blocks_36_transition_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "36").transition_single.layer_norm.weight
    pairformer_stack_blocks_36_transition_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "36").transition_single.layer_norm.bias
    pairformer_stack_blocks_36_transition_single_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "36").transition_single.linear_no_bias_ab.weight
    pairformer_stack_blocks_36_transition_single_linear_out_weight = getattr(self.pairformer_stack.blocks, "36").transition_single.linear_out.weight
    pairformer_stack_blocks_36_attention_pair_bias_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "36").attention_pair_bias.single_layer_norm.weight
    pairformer_stack_blocks_36_attention_pair_bias_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "36").attention_pair_bias.single_layer_norm.bias
    pairformer_stack_blocks_36_attention_pair_bias_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "36").attention_pair_bias.pair_layer_norm.weight
    pairformer_stack_blocks_36_attention_pair_bias_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "36").attention_pair_bias.pair_layer_norm.bias
    pairformer_stack_blocks_36_attention_pair_bias_pair_linear_weight = getattr(self.pairformer_stack.blocks, "36").attention_pair_bias.pair_linear.weight
    pairformer_stack_blocks_36_attention_pair_bias_attention_query_bias = getattr(self.pairformer_stack.blocks, "36").attention_pair_bias.attention.query_bias
    pairformer_stack_blocks_36_attention_pair_bias_attention_input2qkvg_weight = getattr(self.pairformer_stack.blocks, "36").attention_pair_bias.attention.input2qkvg.weight
    pairformer_stack_blocks_36_attention_pair_bias_attention_output_proj_weight = getattr(self.pairformer_stack.blocks, "36").attention_pair_bias.attention.output_proj.weight
    pairformer_stack_blocks_37_transition_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "37").transition_pair.layer_norm.weight
    pairformer_stack_blocks_37_transition_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "37").transition_pair.layer_norm.bias
    pairformer_stack_blocks_37_transition_pair_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "37").transition_pair.linear_no_bias_ab.weight
    pairformer_stack_blocks_37_transition_pair_linear_out_weight = getattr(self.pairformer_stack.blocks, "37").transition_pair.linear_out.weight
    pairformer_stack_blocks_37_triangle_multiplication_layernorm_z_in_weight = getattr(self.pairformer_stack.blocks, "37").triangle_multiplication.layernorm_z_in.weight
    pairformer_stack_blocks_37_triangle_multiplication_layernorm_z_in_bias = getattr(self.pairformer_stack.blocks, "37").triangle_multiplication.layernorm_z_in.bias
    pairformer_stack_blocks_37_triangle_multiplication_linear_z_out_weight = getattr(self.pairformer_stack.blocks, "37").triangle_multiplication.linear_z_out.weight
    pairformer_stack_blocks_37_triangle_multiplication_merged_linear_p_weight = getattr(self.pairformer_stack.blocks, "37").triangle_multiplication.merged_linear_p.weight
    pairformer_stack_blocks_37_triangle_multiplication_merged_linear_g_weight = getattr(self.pairformer_stack.blocks, "37").triangle_multiplication.merged_linear_g.weight
    pairformer_stack_blocks_37_triangle_attention_out_scalers = getattr(self.pairformer_stack.blocks, "37").triangle_attention.out_scalers
    pairformer_stack_blocks_37_triangle_attention_pair2b_weight = getattr(self.pairformer_stack.blocks, "37").triangle_attention.pair2b.weight
    pairformer_stack_blocks_37_triangle_attention_pair2qkvg1_weight = getattr(self.pairformer_stack.blocks, "37").triangle_attention.pair2qkvg1.weight
    pairformer_stack_blocks_37_triangle_attention_pair2qkvg2_weight = getattr(self.pairformer_stack.blocks, "37").triangle_attention.pair2qkvg2.weight
    pairformer_stack_blocks_37_triangle_attention_linear_out_weight = getattr(self.pairformer_stack.blocks, "37").triangle_attention.linear_out.weight
    pairformer_stack_blocks_37_transition_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "37").transition_single.layer_norm.weight
    pairformer_stack_blocks_37_transition_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "37").transition_single.layer_norm.bias
    pairformer_stack_blocks_37_transition_single_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "37").transition_single.linear_no_bias_ab.weight
    pairformer_stack_blocks_37_transition_single_linear_out_weight = getattr(self.pairformer_stack.blocks, "37").transition_single.linear_out.weight
    pairformer_stack_blocks_37_attention_pair_bias_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "37").attention_pair_bias.single_layer_norm.weight
    pairformer_stack_blocks_37_attention_pair_bias_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "37").attention_pair_bias.single_layer_norm.bias
    pairformer_stack_blocks_37_attention_pair_bias_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "37").attention_pair_bias.pair_layer_norm.weight
    pairformer_stack_blocks_37_attention_pair_bias_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "37").attention_pair_bias.pair_layer_norm.bias
    pairformer_stack_blocks_37_attention_pair_bias_pair_linear_weight = getattr(self.pairformer_stack.blocks, "37").attention_pair_bias.pair_linear.weight
    pairformer_stack_blocks_37_attention_pair_bias_attention_query_bias = getattr(self.pairformer_stack.blocks, "37").attention_pair_bias.attention.query_bias
    pairformer_stack_blocks_37_attention_pair_bias_attention_input2qkvg_weight = getattr(self.pairformer_stack.blocks, "37").attention_pair_bias.attention.input2qkvg.weight
    pairformer_stack_blocks_37_attention_pair_bias_attention_output_proj_weight = getattr(self.pairformer_stack.blocks, "37").attention_pair_bias.attention.output_proj.weight
    pairformer_stack_blocks_38_transition_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "38").transition_pair.layer_norm.weight
    pairformer_stack_blocks_38_transition_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "38").transition_pair.layer_norm.bias
    pairformer_stack_blocks_38_transition_pair_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "38").transition_pair.linear_no_bias_ab.weight
    pairformer_stack_blocks_38_transition_pair_linear_out_weight = getattr(self.pairformer_stack.blocks, "38").transition_pair.linear_out.weight
    pairformer_stack_blocks_38_triangle_multiplication_layernorm_z_in_weight = getattr(self.pairformer_stack.blocks, "38").triangle_multiplication.layernorm_z_in.weight
    pairformer_stack_blocks_38_triangle_multiplication_layernorm_z_in_bias = getattr(self.pairformer_stack.blocks, "38").triangle_multiplication.layernorm_z_in.bias
    pairformer_stack_blocks_38_triangle_multiplication_linear_z_out_weight = getattr(self.pairformer_stack.blocks, "38").triangle_multiplication.linear_z_out.weight
    pairformer_stack_blocks_38_triangle_multiplication_merged_linear_p_weight = getattr(self.pairformer_stack.blocks, "38").triangle_multiplication.merged_linear_p.weight
    pairformer_stack_blocks_38_triangle_multiplication_merged_linear_g_weight = getattr(self.pairformer_stack.blocks, "38").triangle_multiplication.merged_linear_g.weight
    pairformer_stack_blocks_38_triangle_attention_out_scalers = getattr(self.pairformer_stack.blocks, "38").triangle_attention.out_scalers
    pairformer_stack_blocks_38_triangle_attention_pair2b_weight = getattr(self.pairformer_stack.blocks, "38").triangle_attention.pair2b.weight
    pairformer_stack_blocks_38_triangle_attention_pair2qkvg1_weight = getattr(self.pairformer_stack.blocks, "38").triangle_attention.pair2qkvg1.weight
    pairformer_stack_blocks_38_triangle_attention_pair2qkvg2_weight = getattr(self.pairformer_stack.blocks, "38").triangle_attention.pair2qkvg2.weight
    pairformer_stack_blocks_38_triangle_attention_linear_out_weight = getattr(self.pairformer_stack.blocks, "38").triangle_attention.linear_out.weight
    pairformer_stack_blocks_38_transition_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "38").transition_single.layer_norm.weight
    pairformer_stack_blocks_38_transition_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "38").transition_single.layer_norm.bias
    pairformer_stack_blocks_38_transition_single_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "38").transition_single.linear_no_bias_ab.weight
    pairformer_stack_blocks_38_transition_single_linear_out_weight = getattr(self.pairformer_stack.blocks, "38").transition_single.linear_out.weight
    pairformer_stack_blocks_38_attention_pair_bias_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "38").attention_pair_bias.single_layer_norm.weight
    pairformer_stack_blocks_38_attention_pair_bias_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "38").attention_pair_bias.single_layer_norm.bias
    pairformer_stack_blocks_38_attention_pair_bias_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "38").attention_pair_bias.pair_layer_norm.weight
    pairformer_stack_blocks_38_attention_pair_bias_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "38").attention_pair_bias.pair_layer_norm.bias
    pairformer_stack_blocks_38_attention_pair_bias_pair_linear_weight = getattr(self.pairformer_stack.blocks, "38").attention_pair_bias.pair_linear.weight
    pairformer_stack_blocks_38_attention_pair_bias_attention_query_bias = getattr(self.pairformer_stack.blocks, "38").attention_pair_bias.attention.query_bias
    pairformer_stack_blocks_38_attention_pair_bias_attention_input2qkvg_weight = getattr(self.pairformer_stack.blocks, "38").attention_pair_bias.attention.input2qkvg.weight
    pairformer_stack_blocks_38_attention_pair_bias_attention_output_proj_weight = getattr(self.pairformer_stack.blocks, "38").attention_pair_bias.attention.output_proj.weight
    pairformer_stack_blocks_39_transition_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "39").transition_pair.layer_norm.weight
    pairformer_stack_blocks_39_transition_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "39").transition_pair.layer_norm.bias
    pairformer_stack_blocks_39_transition_pair_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "39").transition_pair.linear_no_bias_ab.weight
    pairformer_stack_blocks_39_transition_pair_linear_out_weight = getattr(self.pairformer_stack.blocks, "39").transition_pair.linear_out.weight
    pairformer_stack_blocks_39_triangle_multiplication_layernorm_z_in_weight = getattr(self.pairformer_stack.blocks, "39").triangle_multiplication.layernorm_z_in.weight
    pairformer_stack_blocks_39_triangle_multiplication_layernorm_z_in_bias = getattr(self.pairformer_stack.blocks, "39").triangle_multiplication.layernorm_z_in.bias
    pairformer_stack_blocks_39_triangle_multiplication_linear_z_out_weight = getattr(self.pairformer_stack.blocks, "39").triangle_multiplication.linear_z_out.weight
    pairformer_stack_blocks_39_triangle_multiplication_merged_linear_p_weight = getattr(self.pairformer_stack.blocks, "39").triangle_multiplication.merged_linear_p.weight
    pairformer_stack_blocks_39_triangle_multiplication_merged_linear_g_weight = getattr(self.pairformer_stack.blocks, "39").triangle_multiplication.merged_linear_g.weight
    pairformer_stack_blocks_39_triangle_attention_out_scalers = getattr(self.pairformer_stack.blocks, "39").triangle_attention.out_scalers
    pairformer_stack_blocks_39_triangle_attention_pair2b_weight = getattr(self.pairformer_stack.blocks, "39").triangle_attention.pair2b.weight
    pairformer_stack_blocks_39_triangle_attention_pair2qkvg1_weight = getattr(self.pairformer_stack.blocks, "39").triangle_attention.pair2qkvg1.weight
    pairformer_stack_blocks_39_triangle_attention_pair2qkvg2_weight = getattr(self.pairformer_stack.blocks, "39").triangle_attention.pair2qkvg2.weight
    pairformer_stack_blocks_39_triangle_attention_linear_out_weight = getattr(self.pairformer_stack.blocks, "39").triangle_attention.linear_out.weight
    pairformer_stack_blocks_39_transition_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "39").transition_single.layer_norm.weight
    pairformer_stack_blocks_39_transition_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "39").transition_single.layer_norm.bias
    pairformer_stack_blocks_39_transition_single_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "39").transition_single.linear_no_bias_ab.weight
    pairformer_stack_blocks_39_transition_single_linear_out_weight = getattr(self.pairformer_stack.blocks, "39").transition_single.linear_out.weight
    pairformer_stack_blocks_39_attention_pair_bias_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "39").attention_pair_bias.single_layer_norm.weight
    pairformer_stack_blocks_39_attention_pair_bias_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "39").attention_pair_bias.single_layer_norm.bias
    pairformer_stack_blocks_39_attention_pair_bias_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "39").attention_pair_bias.pair_layer_norm.weight
    pairformer_stack_blocks_39_attention_pair_bias_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "39").attention_pair_bias.pair_layer_norm.bias
    pairformer_stack_blocks_39_attention_pair_bias_pair_linear_weight = getattr(self.pairformer_stack.blocks, "39").attention_pair_bias.pair_linear.weight
    pairformer_stack_blocks_39_attention_pair_bias_attention_query_bias = getattr(self.pairformer_stack.blocks, "39").attention_pair_bias.attention.query_bias
    pairformer_stack_blocks_39_attention_pair_bias_attention_input2qkvg_weight = getattr(self.pairformer_stack.blocks, "39").attention_pair_bias.attention.input2qkvg.weight
    pairformer_stack_blocks_39_attention_pair_bias_attention_output_proj_weight = getattr(self.pairformer_stack.blocks, "39").attention_pair_bias.attention.output_proj.weight
    pairformer_stack_blocks_40_transition_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "40").transition_pair.layer_norm.weight
    pairformer_stack_blocks_40_transition_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "40").transition_pair.layer_norm.bias
    pairformer_stack_blocks_40_transition_pair_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "40").transition_pair.linear_no_bias_ab.weight
    pairformer_stack_blocks_40_transition_pair_linear_out_weight = getattr(self.pairformer_stack.blocks, "40").transition_pair.linear_out.weight
    pairformer_stack_blocks_40_triangle_multiplication_layernorm_z_in_weight = getattr(self.pairformer_stack.blocks, "40").triangle_multiplication.layernorm_z_in.weight
    pairformer_stack_blocks_40_triangle_multiplication_layernorm_z_in_bias = getattr(self.pairformer_stack.blocks, "40").triangle_multiplication.layernorm_z_in.bias
    pairformer_stack_blocks_40_triangle_multiplication_linear_z_out_weight = getattr(self.pairformer_stack.blocks, "40").triangle_multiplication.linear_z_out.weight
    pairformer_stack_blocks_40_triangle_multiplication_merged_linear_p_weight = getattr(self.pairformer_stack.blocks, "40").triangle_multiplication.merged_linear_p.weight
    pairformer_stack_blocks_40_triangle_multiplication_merged_linear_g_weight = getattr(self.pairformer_stack.blocks, "40").triangle_multiplication.merged_linear_g.weight
    pairformer_stack_blocks_40_triangle_attention_out_scalers = getattr(self.pairformer_stack.blocks, "40").triangle_attention.out_scalers
    pairformer_stack_blocks_40_triangle_attention_pair2b_weight = getattr(self.pairformer_stack.blocks, "40").triangle_attention.pair2b.weight
    pairformer_stack_blocks_40_triangle_attention_pair2qkvg1_weight = getattr(self.pairformer_stack.blocks, "40").triangle_attention.pair2qkvg1.weight
    pairformer_stack_blocks_40_triangle_attention_pair2qkvg2_weight = getattr(self.pairformer_stack.blocks, "40").triangle_attention.pair2qkvg2.weight
    pairformer_stack_blocks_40_triangle_attention_linear_out_weight = getattr(self.pairformer_stack.blocks, "40").triangle_attention.linear_out.weight
    pairformer_stack_blocks_40_transition_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "40").transition_single.layer_norm.weight
    pairformer_stack_blocks_40_transition_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "40").transition_single.layer_norm.bias
    pairformer_stack_blocks_40_transition_single_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "40").transition_single.linear_no_bias_ab.weight
    pairformer_stack_blocks_40_transition_single_linear_out_weight = getattr(self.pairformer_stack.blocks, "40").transition_single.linear_out.weight
    pairformer_stack_blocks_40_attention_pair_bias_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "40").attention_pair_bias.single_layer_norm.weight
    pairformer_stack_blocks_40_attention_pair_bias_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "40").attention_pair_bias.single_layer_norm.bias
    pairformer_stack_blocks_40_attention_pair_bias_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "40").attention_pair_bias.pair_layer_norm.weight
    pairformer_stack_blocks_40_attention_pair_bias_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "40").attention_pair_bias.pair_layer_norm.bias
    pairformer_stack_blocks_40_attention_pair_bias_pair_linear_weight = getattr(self.pairformer_stack.blocks, "40").attention_pair_bias.pair_linear.weight
    pairformer_stack_blocks_40_attention_pair_bias_attention_query_bias = getattr(self.pairformer_stack.blocks, "40").attention_pair_bias.attention.query_bias
    pairformer_stack_blocks_40_attention_pair_bias_attention_input2qkvg_weight = getattr(self.pairformer_stack.blocks, "40").attention_pair_bias.attention.input2qkvg.weight
    pairformer_stack_blocks_40_attention_pair_bias_attention_output_proj_weight = getattr(self.pairformer_stack.blocks, "40").attention_pair_bias.attention.output_proj.weight
    pairformer_stack_blocks_41_transition_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "41").transition_pair.layer_norm.weight
    pairformer_stack_blocks_41_transition_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "41").transition_pair.layer_norm.bias
    pairformer_stack_blocks_41_transition_pair_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "41").transition_pair.linear_no_bias_ab.weight
    pairformer_stack_blocks_41_transition_pair_linear_out_weight = getattr(self.pairformer_stack.blocks, "41").transition_pair.linear_out.weight
    pairformer_stack_blocks_41_triangle_multiplication_layernorm_z_in_weight = getattr(self.pairformer_stack.blocks, "41").triangle_multiplication.layernorm_z_in.weight
    pairformer_stack_blocks_41_triangle_multiplication_layernorm_z_in_bias = getattr(self.pairformer_stack.blocks, "41").triangle_multiplication.layernorm_z_in.bias
    pairformer_stack_blocks_41_triangle_multiplication_linear_z_out_weight = getattr(self.pairformer_stack.blocks, "41").triangle_multiplication.linear_z_out.weight
    pairformer_stack_blocks_41_triangle_multiplication_merged_linear_p_weight = getattr(self.pairformer_stack.blocks, "41").triangle_multiplication.merged_linear_p.weight
    pairformer_stack_blocks_41_triangle_multiplication_merged_linear_g_weight = getattr(self.pairformer_stack.blocks, "41").triangle_multiplication.merged_linear_g.weight
    pairformer_stack_blocks_41_triangle_attention_out_scalers = getattr(self.pairformer_stack.blocks, "41").triangle_attention.out_scalers
    pairformer_stack_blocks_41_triangle_attention_pair2b_weight = getattr(self.pairformer_stack.blocks, "41").triangle_attention.pair2b.weight
    pairformer_stack_blocks_41_triangle_attention_pair2qkvg1_weight = getattr(self.pairformer_stack.blocks, "41").triangle_attention.pair2qkvg1.weight
    pairformer_stack_blocks_41_triangle_attention_pair2qkvg2_weight = getattr(self.pairformer_stack.blocks, "41").triangle_attention.pair2qkvg2.weight
    pairformer_stack_blocks_41_triangle_attention_linear_out_weight = getattr(self.pairformer_stack.blocks, "41").triangle_attention.linear_out.weight
    pairformer_stack_blocks_41_transition_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "41").transition_single.layer_norm.weight
    pairformer_stack_blocks_41_transition_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "41").transition_single.layer_norm.bias
    pairformer_stack_blocks_41_transition_single_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "41").transition_single.linear_no_bias_ab.weight
    pairformer_stack_blocks_41_transition_single_linear_out_weight = getattr(self.pairformer_stack.blocks, "41").transition_single.linear_out.weight
    pairformer_stack_blocks_41_attention_pair_bias_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "41").attention_pair_bias.single_layer_norm.weight
    pairformer_stack_blocks_41_attention_pair_bias_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "41").attention_pair_bias.single_layer_norm.bias
    pairformer_stack_blocks_41_attention_pair_bias_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "41").attention_pair_bias.pair_layer_norm.weight
    pairformer_stack_blocks_41_attention_pair_bias_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "41").attention_pair_bias.pair_layer_norm.bias
    pairformer_stack_blocks_41_attention_pair_bias_pair_linear_weight = getattr(self.pairformer_stack.blocks, "41").attention_pair_bias.pair_linear.weight
    pairformer_stack_blocks_41_attention_pair_bias_attention_query_bias = getattr(self.pairformer_stack.blocks, "41").attention_pair_bias.attention.query_bias
    pairformer_stack_blocks_41_attention_pair_bias_attention_input2qkvg_weight = getattr(self.pairformer_stack.blocks, "41").attention_pair_bias.attention.input2qkvg.weight
    pairformer_stack_blocks_41_attention_pair_bias_attention_output_proj_weight = getattr(self.pairformer_stack.blocks, "41").attention_pair_bias.attention.output_proj.weight
    pairformer_stack_blocks_42_transition_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "42").transition_pair.layer_norm.weight
    pairformer_stack_blocks_42_transition_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "42").transition_pair.layer_norm.bias
    pairformer_stack_blocks_42_transition_pair_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "42").transition_pair.linear_no_bias_ab.weight
    pairformer_stack_blocks_42_transition_pair_linear_out_weight = getattr(self.pairformer_stack.blocks, "42").transition_pair.linear_out.weight
    pairformer_stack_blocks_42_triangle_multiplication_layernorm_z_in_weight = getattr(self.pairformer_stack.blocks, "42").triangle_multiplication.layernorm_z_in.weight
    pairformer_stack_blocks_42_triangle_multiplication_layernorm_z_in_bias = getattr(self.pairformer_stack.blocks, "42").triangle_multiplication.layernorm_z_in.bias
    pairformer_stack_blocks_42_triangle_multiplication_linear_z_out_weight = getattr(self.pairformer_stack.blocks, "42").triangle_multiplication.linear_z_out.weight
    pairformer_stack_blocks_42_triangle_multiplication_merged_linear_p_weight = getattr(self.pairformer_stack.blocks, "42").triangle_multiplication.merged_linear_p.weight
    pairformer_stack_blocks_42_triangle_multiplication_merged_linear_g_weight = getattr(self.pairformer_stack.blocks, "42").triangle_multiplication.merged_linear_g.weight
    pairformer_stack_blocks_42_triangle_attention_out_scalers = getattr(self.pairformer_stack.blocks, "42").triangle_attention.out_scalers
    pairformer_stack_blocks_42_triangle_attention_pair2b_weight = getattr(self.pairformer_stack.blocks, "42").triangle_attention.pair2b.weight
    pairformer_stack_blocks_42_triangle_attention_pair2qkvg1_weight = getattr(self.pairformer_stack.blocks, "42").triangle_attention.pair2qkvg1.weight
    pairformer_stack_blocks_42_triangle_attention_pair2qkvg2_weight = getattr(self.pairformer_stack.blocks, "42").triangle_attention.pair2qkvg2.weight
    pairformer_stack_blocks_42_triangle_attention_linear_out_weight = getattr(self.pairformer_stack.blocks, "42").triangle_attention.linear_out.weight
    pairformer_stack_blocks_42_transition_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "42").transition_single.layer_norm.weight
    pairformer_stack_blocks_42_transition_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "42").transition_single.layer_norm.bias
    pairformer_stack_blocks_42_transition_single_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "42").transition_single.linear_no_bias_ab.weight
    pairformer_stack_blocks_42_transition_single_linear_out_weight = getattr(self.pairformer_stack.blocks, "42").transition_single.linear_out.weight
    pairformer_stack_blocks_42_attention_pair_bias_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "42").attention_pair_bias.single_layer_norm.weight
    pairformer_stack_blocks_42_attention_pair_bias_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "42").attention_pair_bias.single_layer_norm.bias
    pairformer_stack_blocks_42_attention_pair_bias_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "42").attention_pair_bias.pair_layer_norm.weight
    pairformer_stack_blocks_42_attention_pair_bias_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "42").attention_pair_bias.pair_layer_norm.bias
    pairformer_stack_blocks_42_attention_pair_bias_pair_linear_weight = getattr(self.pairformer_stack.blocks, "42").attention_pair_bias.pair_linear.weight
    pairformer_stack_blocks_42_attention_pair_bias_attention_query_bias = getattr(self.pairformer_stack.blocks, "42").attention_pair_bias.attention.query_bias
    pairformer_stack_blocks_42_attention_pair_bias_attention_input2qkvg_weight = getattr(self.pairformer_stack.blocks, "42").attention_pair_bias.attention.input2qkvg.weight
    pairformer_stack_blocks_42_attention_pair_bias_attention_output_proj_weight = getattr(self.pairformer_stack.blocks, "42").attention_pair_bias.attention.output_proj.weight
    pairformer_stack_blocks_43_transition_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "43").transition_pair.layer_norm.weight
    pairformer_stack_blocks_43_transition_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "43").transition_pair.layer_norm.bias
    pairformer_stack_blocks_43_transition_pair_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "43").transition_pair.linear_no_bias_ab.weight
    pairformer_stack_blocks_43_transition_pair_linear_out_weight = getattr(self.pairformer_stack.blocks, "43").transition_pair.linear_out.weight
    pairformer_stack_blocks_43_triangle_multiplication_layernorm_z_in_weight = getattr(self.pairformer_stack.blocks, "43").triangle_multiplication.layernorm_z_in.weight
    pairformer_stack_blocks_43_triangle_multiplication_layernorm_z_in_bias = getattr(self.pairformer_stack.blocks, "43").triangle_multiplication.layernorm_z_in.bias
    pairformer_stack_blocks_43_triangle_multiplication_linear_z_out_weight = getattr(self.pairformer_stack.blocks, "43").triangle_multiplication.linear_z_out.weight
    pairformer_stack_blocks_43_triangle_multiplication_merged_linear_p_weight = getattr(self.pairformer_stack.blocks, "43").triangle_multiplication.merged_linear_p.weight
    pairformer_stack_blocks_43_triangle_multiplication_merged_linear_g_weight = getattr(self.pairformer_stack.blocks, "43").triangle_multiplication.merged_linear_g.weight
    pairformer_stack_blocks_43_triangle_attention_out_scalers = getattr(self.pairformer_stack.blocks, "43").triangle_attention.out_scalers
    pairformer_stack_blocks_43_triangle_attention_pair2b_weight = getattr(self.pairformer_stack.blocks, "43").triangle_attention.pair2b.weight
    pairformer_stack_blocks_43_triangle_attention_pair2qkvg1_weight = getattr(self.pairformer_stack.blocks, "43").triangle_attention.pair2qkvg1.weight
    pairformer_stack_blocks_43_triangle_attention_pair2qkvg2_weight = getattr(self.pairformer_stack.blocks, "43").triangle_attention.pair2qkvg2.weight
    pairformer_stack_blocks_43_triangle_attention_linear_out_weight = getattr(self.pairformer_stack.blocks, "43").triangle_attention.linear_out.weight
    pairformer_stack_blocks_43_transition_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "43").transition_single.layer_norm.weight
    pairformer_stack_blocks_43_transition_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "43").transition_single.layer_norm.bias
    pairformer_stack_blocks_43_transition_single_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "43").transition_single.linear_no_bias_ab.weight
    pairformer_stack_blocks_43_transition_single_linear_out_weight = getattr(self.pairformer_stack.blocks, "43").transition_single.linear_out.weight
    pairformer_stack_blocks_43_attention_pair_bias_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "43").attention_pair_bias.single_layer_norm.weight
    pairformer_stack_blocks_43_attention_pair_bias_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "43").attention_pair_bias.single_layer_norm.bias
    pairformer_stack_blocks_43_attention_pair_bias_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "43").attention_pair_bias.pair_layer_norm.weight
    pairformer_stack_blocks_43_attention_pair_bias_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "43").attention_pair_bias.pair_layer_norm.bias
    pairformer_stack_blocks_43_attention_pair_bias_pair_linear_weight = getattr(self.pairformer_stack.blocks, "43").attention_pair_bias.pair_linear.weight
    pairformer_stack_blocks_43_attention_pair_bias_attention_query_bias = getattr(self.pairformer_stack.blocks, "43").attention_pair_bias.attention.query_bias
    pairformer_stack_blocks_43_attention_pair_bias_attention_input2qkvg_weight = getattr(self.pairformer_stack.blocks, "43").attention_pair_bias.attention.input2qkvg.weight
    pairformer_stack_blocks_43_attention_pair_bias_attention_output_proj_weight = getattr(self.pairformer_stack.blocks, "43").attention_pair_bias.attention.output_proj.weight
    pairformer_stack_blocks_44_transition_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "44").transition_pair.layer_norm.weight
    pairformer_stack_blocks_44_transition_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "44").transition_pair.layer_norm.bias
    pairformer_stack_blocks_44_transition_pair_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "44").transition_pair.linear_no_bias_ab.weight
    pairformer_stack_blocks_44_transition_pair_linear_out_weight = getattr(self.pairformer_stack.blocks, "44").transition_pair.linear_out.weight
    pairformer_stack_blocks_44_triangle_multiplication_layernorm_z_in_weight = getattr(self.pairformer_stack.blocks, "44").triangle_multiplication.layernorm_z_in.weight
    pairformer_stack_blocks_44_triangle_multiplication_layernorm_z_in_bias = getattr(self.pairformer_stack.blocks, "44").triangle_multiplication.layernorm_z_in.bias
    pairformer_stack_blocks_44_triangle_multiplication_linear_z_out_weight = getattr(self.pairformer_stack.blocks, "44").triangle_multiplication.linear_z_out.weight
    pairformer_stack_blocks_44_triangle_multiplication_merged_linear_p_weight = getattr(self.pairformer_stack.blocks, "44").triangle_multiplication.merged_linear_p.weight
    pairformer_stack_blocks_44_triangle_multiplication_merged_linear_g_weight = getattr(self.pairformer_stack.blocks, "44").triangle_multiplication.merged_linear_g.weight
    pairformer_stack_blocks_44_triangle_attention_out_scalers = getattr(self.pairformer_stack.blocks, "44").triangle_attention.out_scalers
    pairformer_stack_blocks_44_triangle_attention_pair2b_weight = getattr(self.pairformer_stack.blocks, "44").triangle_attention.pair2b.weight
    pairformer_stack_blocks_44_triangle_attention_pair2qkvg1_weight = getattr(self.pairformer_stack.blocks, "44").triangle_attention.pair2qkvg1.weight
    pairformer_stack_blocks_44_triangle_attention_pair2qkvg2_weight = getattr(self.pairformer_stack.blocks, "44").triangle_attention.pair2qkvg2.weight
    pairformer_stack_blocks_44_triangle_attention_linear_out_weight = getattr(self.pairformer_stack.blocks, "44").triangle_attention.linear_out.weight
    pairformer_stack_blocks_44_transition_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "44").transition_single.layer_norm.weight
    pairformer_stack_blocks_44_transition_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "44").transition_single.layer_norm.bias
    pairformer_stack_blocks_44_transition_single_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "44").transition_single.linear_no_bias_ab.weight
    pairformer_stack_blocks_44_transition_single_linear_out_weight = getattr(self.pairformer_stack.blocks, "44").transition_single.linear_out.weight
    pairformer_stack_blocks_44_attention_pair_bias_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "44").attention_pair_bias.single_layer_norm.weight
    pairformer_stack_blocks_44_attention_pair_bias_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "44").attention_pair_bias.single_layer_norm.bias
    pairformer_stack_blocks_44_attention_pair_bias_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "44").attention_pair_bias.pair_layer_norm.weight
    pairformer_stack_blocks_44_attention_pair_bias_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "44").attention_pair_bias.pair_layer_norm.bias
    pairformer_stack_blocks_44_attention_pair_bias_pair_linear_weight = getattr(self.pairformer_stack.blocks, "44").attention_pair_bias.pair_linear.weight
    pairformer_stack_blocks_44_attention_pair_bias_attention_query_bias = getattr(self.pairformer_stack.blocks, "44").attention_pair_bias.attention.query_bias
    pairformer_stack_blocks_44_attention_pair_bias_attention_input2qkvg_weight = getattr(self.pairformer_stack.blocks, "44").attention_pair_bias.attention.input2qkvg.weight
    pairformer_stack_blocks_44_attention_pair_bias_attention_output_proj_weight = getattr(self.pairformer_stack.blocks, "44").attention_pair_bias.attention.output_proj.weight
    pairformer_stack_blocks_45_transition_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "45").transition_pair.layer_norm.weight
    pairformer_stack_blocks_45_transition_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "45").transition_pair.layer_norm.bias
    pairformer_stack_blocks_45_transition_pair_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "45").transition_pair.linear_no_bias_ab.weight
    pairformer_stack_blocks_45_transition_pair_linear_out_weight = getattr(self.pairformer_stack.blocks, "45").transition_pair.linear_out.weight
    pairformer_stack_blocks_45_triangle_multiplication_layernorm_z_in_weight = getattr(self.pairformer_stack.blocks, "45").triangle_multiplication.layernorm_z_in.weight
    pairformer_stack_blocks_45_triangle_multiplication_layernorm_z_in_bias = getattr(self.pairformer_stack.blocks, "45").triangle_multiplication.layernorm_z_in.bias
    pairformer_stack_blocks_45_triangle_multiplication_linear_z_out_weight = getattr(self.pairformer_stack.blocks, "45").triangle_multiplication.linear_z_out.weight
    pairformer_stack_blocks_45_triangle_multiplication_merged_linear_p_weight = getattr(self.pairformer_stack.blocks, "45").triangle_multiplication.merged_linear_p.weight
    pairformer_stack_blocks_45_triangle_multiplication_merged_linear_g_weight = getattr(self.pairformer_stack.blocks, "45").triangle_multiplication.merged_linear_g.weight
    pairformer_stack_blocks_45_triangle_attention_out_scalers = getattr(self.pairformer_stack.blocks, "45").triangle_attention.out_scalers
    pairformer_stack_blocks_45_triangle_attention_pair2b_weight = getattr(self.pairformer_stack.blocks, "45").triangle_attention.pair2b.weight
    pairformer_stack_blocks_45_triangle_attention_pair2qkvg1_weight = getattr(self.pairformer_stack.blocks, "45").triangle_attention.pair2qkvg1.weight
    pairformer_stack_blocks_45_triangle_attention_pair2qkvg2_weight = getattr(self.pairformer_stack.blocks, "45").triangle_attention.pair2qkvg2.weight
    pairformer_stack_blocks_45_triangle_attention_linear_out_weight = getattr(self.pairformer_stack.blocks, "45").triangle_attention.linear_out.weight
    pairformer_stack_blocks_45_transition_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "45").transition_single.layer_norm.weight
    pairformer_stack_blocks_45_transition_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "45").transition_single.layer_norm.bias
    pairformer_stack_blocks_45_transition_single_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "45").transition_single.linear_no_bias_ab.weight
    pairformer_stack_blocks_45_transition_single_linear_out_weight = getattr(self.pairformer_stack.blocks, "45").transition_single.linear_out.weight
    pairformer_stack_blocks_45_attention_pair_bias_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "45").attention_pair_bias.single_layer_norm.weight
    pairformer_stack_blocks_45_attention_pair_bias_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "45").attention_pair_bias.single_layer_norm.bias
    pairformer_stack_blocks_45_attention_pair_bias_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "45").attention_pair_bias.pair_layer_norm.weight
    pairformer_stack_blocks_45_attention_pair_bias_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "45").attention_pair_bias.pair_layer_norm.bias
    pairformer_stack_blocks_45_attention_pair_bias_pair_linear_weight = getattr(self.pairformer_stack.blocks, "45").attention_pair_bias.pair_linear.weight
    pairformer_stack_blocks_45_attention_pair_bias_attention_query_bias = getattr(self.pairformer_stack.blocks, "45").attention_pair_bias.attention.query_bias
    pairformer_stack_blocks_45_attention_pair_bias_attention_input2qkvg_weight = getattr(self.pairformer_stack.blocks, "45").attention_pair_bias.attention.input2qkvg.weight
    pairformer_stack_blocks_45_attention_pair_bias_attention_output_proj_weight = getattr(self.pairformer_stack.blocks, "45").attention_pair_bias.attention.output_proj.weight
    pairformer_stack_blocks_46_transition_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "46").transition_pair.layer_norm.weight
    pairformer_stack_blocks_46_transition_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "46").transition_pair.layer_norm.bias
    pairformer_stack_blocks_46_transition_pair_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "46").transition_pair.linear_no_bias_ab.weight
    pairformer_stack_blocks_46_transition_pair_linear_out_weight = getattr(self.pairformer_stack.blocks, "46").transition_pair.linear_out.weight
    pairformer_stack_blocks_46_triangle_multiplication_layernorm_z_in_weight = getattr(self.pairformer_stack.blocks, "46").triangle_multiplication.layernorm_z_in.weight
    pairformer_stack_blocks_46_triangle_multiplication_layernorm_z_in_bias = getattr(self.pairformer_stack.blocks, "46").triangle_multiplication.layernorm_z_in.bias
    pairformer_stack_blocks_46_triangle_multiplication_linear_z_out_weight = getattr(self.pairformer_stack.blocks, "46").triangle_multiplication.linear_z_out.weight
    pairformer_stack_blocks_46_triangle_multiplication_merged_linear_p_weight = getattr(self.pairformer_stack.blocks, "46").triangle_multiplication.merged_linear_p.weight
    pairformer_stack_blocks_46_triangle_multiplication_merged_linear_g_weight = getattr(self.pairformer_stack.blocks, "46").triangle_multiplication.merged_linear_g.weight
    pairformer_stack_blocks_46_triangle_attention_out_scalers = getattr(self.pairformer_stack.blocks, "46").triangle_attention.out_scalers
    pairformer_stack_blocks_46_triangle_attention_pair2b_weight = getattr(self.pairformer_stack.blocks, "46").triangle_attention.pair2b.weight
    pairformer_stack_blocks_46_triangle_attention_pair2qkvg1_weight = getattr(self.pairformer_stack.blocks, "46").triangle_attention.pair2qkvg1.weight
    pairformer_stack_blocks_46_triangle_attention_pair2qkvg2_weight = getattr(self.pairformer_stack.blocks, "46").triangle_attention.pair2qkvg2.weight
    pairformer_stack_blocks_46_triangle_attention_linear_out_weight = getattr(self.pairformer_stack.blocks, "46").triangle_attention.linear_out.weight
    pairformer_stack_blocks_46_transition_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "46").transition_single.layer_norm.weight
    pairformer_stack_blocks_46_transition_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "46").transition_single.layer_norm.bias
    pairformer_stack_blocks_46_transition_single_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "46").transition_single.linear_no_bias_ab.weight
    pairformer_stack_blocks_46_transition_single_linear_out_weight = getattr(self.pairformer_stack.blocks, "46").transition_single.linear_out.weight
    pairformer_stack_blocks_46_attention_pair_bias_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "46").attention_pair_bias.single_layer_norm.weight
    pairformer_stack_blocks_46_attention_pair_bias_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "46").attention_pair_bias.single_layer_norm.bias
    pairformer_stack_blocks_46_attention_pair_bias_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "46").attention_pair_bias.pair_layer_norm.weight
    pairformer_stack_blocks_46_attention_pair_bias_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "46").attention_pair_bias.pair_layer_norm.bias
    pairformer_stack_blocks_46_attention_pair_bias_pair_linear_weight = getattr(self.pairformer_stack.blocks, "46").attention_pair_bias.pair_linear.weight
    pairformer_stack_blocks_46_attention_pair_bias_attention_query_bias = getattr(self.pairformer_stack.blocks, "46").attention_pair_bias.attention.query_bias
    pairformer_stack_blocks_46_attention_pair_bias_attention_input2qkvg_weight = getattr(self.pairformer_stack.blocks, "46").attention_pair_bias.attention.input2qkvg.weight
    pairformer_stack_blocks_46_attention_pair_bias_attention_output_proj_weight = getattr(self.pairformer_stack.blocks, "46").attention_pair_bias.attention.output_proj.weight
    pairformer_stack_blocks_47_transition_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "47").transition_pair.layer_norm.weight
    pairformer_stack_blocks_47_transition_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "47").transition_pair.layer_norm.bias
    pairformer_stack_blocks_47_transition_pair_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "47").transition_pair.linear_no_bias_ab.weight
    pairformer_stack_blocks_47_transition_pair_linear_out_weight = getattr(self.pairformer_stack.blocks, "47").transition_pair.linear_out.weight
    pairformer_stack_blocks_47_triangle_multiplication_layernorm_z_in_weight = getattr(self.pairformer_stack.blocks, "47").triangle_multiplication.layernorm_z_in.weight
    pairformer_stack_blocks_47_triangle_multiplication_layernorm_z_in_bias = getattr(self.pairformer_stack.blocks, "47").triangle_multiplication.layernorm_z_in.bias
    pairformer_stack_blocks_47_triangle_multiplication_linear_z_out_weight = getattr(self.pairformer_stack.blocks, "47").triangle_multiplication.linear_z_out.weight
    pairformer_stack_blocks_47_triangle_multiplication_merged_linear_p_weight = getattr(self.pairformer_stack.blocks, "47").triangle_multiplication.merged_linear_p.weight
    pairformer_stack_blocks_47_triangle_multiplication_merged_linear_g_weight = getattr(self.pairformer_stack.blocks, "47").triangle_multiplication.merged_linear_g.weight
    pairformer_stack_blocks_47_triangle_attention_out_scalers = getattr(self.pairformer_stack.blocks, "47").triangle_attention.out_scalers
    pairformer_stack_blocks_47_triangle_attention_pair2b_weight = getattr(self.pairformer_stack.blocks, "47").triangle_attention.pair2b.weight
    pairformer_stack_blocks_47_triangle_attention_pair2qkvg1_weight = getattr(self.pairformer_stack.blocks, "47").triangle_attention.pair2qkvg1.weight
    pairformer_stack_blocks_47_triangle_attention_pair2qkvg2_weight = getattr(self.pairformer_stack.blocks, "47").triangle_attention.pair2qkvg2.weight
    pairformer_stack_blocks_47_triangle_attention_linear_out_weight = getattr(self.pairformer_stack.blocks, "47").triangle_attention.linear_out.weight
    pairformer_stack_blocks_47_transition_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "47").transition_single.layer_norm.weight
    pairformer_stack_blocks_47_transition_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "47").transition_single.layer_norm.bias
    pairformer_stack_blocks_47_transition_single_linear_no_bias_ab_weight = getattr(self.pairformer_stack.blocks, "47").transition_single.linear_no_bias_ab.weight
    pairformer_stack_blocks_47_transition_single_linear_out_weight = getattr(self.pairformer_stack.blocks, "47").transition_single.linear_out.weight
    pairformer_stack_blocks_47_attention_pair_bias_single_layer_norm_weight = getattr(self.pairformer_stack.blocks, "47").attention_pair_bias.single_layer_norm.weight
    pairformer_stack_blocks_47_attention_pair_bias_single_layer_norm_bias = getattr(self.pairformer_stack.blocks, "47").attention_pair_bias.single_layer_norm.bias
    pairformer_stack_blocks_47_attention_pair_bias_pair_layer_norm_weight = getattr(self.pairformer_stack.blocks, "47").attention_pair_bias.pair_layer_norm.weight
    pairformer_stack_blocks_47_attention_pair_bias_pair_layer_norm_bias = getattr(self.pairformer_stack.blocks, "47").attention_pair_bias.pair_layer_norm.bias
    pairformer_stack_blocks_47_attention_pair_bias_pair_linear_weight = getattr(self.pairformer_stack.blocks, "47").attention_pair_bias.pair_linear.weight
    pairformer_stack_blocks_47_attention_pair_bias_attention_query_bias = getattr(self.pairformer_stack.blocks, "47").attention_pair_bias.attention.query_bias
    pairformer_stack_blocks_47_attention_pair_bias_attention_input2qkvg_weight = getattr(self.pairformer_stack.blocks, "47").attention_pair_bias.attention.input2qkvg.weight
    pairformer_stack_blocks_47_attention_pair_bias_attention_output_proj_weight = getattr(self.pairformer_stack.blocks, "47").attention_pair_bias.attention.output_proj.weight
    token_single_recycle_proj_0_weight = getattr(self.token_single_recycle_proj, "0").weight
    token_single_recycle_proj_0_bias = getattr(self.token_single_recycle_proj, "0").bias
    token_single_recycle_proj_1_weight = getattr(self.token_single_recycle_proj, "1").weight
    token_pair_recycle_proj_0_weight = getattr(self.token_pair_recycle_proj, "0").weight
    token_pair_recycle_proj_0_bias = getattr(self.token_pair_recycle_proj, "0").bias
    token_pair_recycle_proj_1_weight = getattr(self.token_pair_recycle_proj, "1").weight
    _to_copy = torch.ops.aten._to_copy.default(arg1401_1, dtype = torch.float32);  arg1401_1 = None
    native_layer_norm_default = torch.ops.aten.native_layer_norm.default(_to_copy, [256], token_pair_recycle_proj_0_weight, token_pair_recycle_proj_0_bias, 1e-05);  _to_copy = token_pair_recycle_proj_0_weight = token_pair_recycle_proj_0_bias = None
    getitem = native_layer_norm_default[0];  native_layer_norm_default = None
    _to_copy_1 = torch.ops.aten._to_copy.default(token_pair_recycle_proj_1_weight, dtype = torch.bfloat16);  token_pair_recycle_proj_1_weight = None
    _to_copy_2 = torch.ops.aten._to_copy.default(getitem, dtype = torch.bfloat16);  getitem = None
    t = torch.ops.aten.t.default(_to_copy_1);  _to_copy_1 = None
    view = torch.ops.aten.view.default(_to_copy_2, [147456, 256]);  _to_copy_2 = None
    mm = torch.ops.aten.mm.default(view, t);  view = t = None
    view_1 = torch.ops.aten.view.default(mm, [1, 384, 384, 256]);  mm = None
    add = torch.ops.aten.add.Tensor(arg1399_1, view_1);  arg1399_1 = view_1 = None
    _to_copy_3 = torch.ops.aten._to_copy.default(arg1400_1, dtype = torch.float32);  arg1400_1 = None
    native_layer_norm_default_1 = torch.ops.aten.native_layer_norm.default(_to_copy_3, [384], token_single_recycle_proj_0_weight, token_single_recycle_proj_0_bias, 1e-05);  _to_copy_3 = token_single_recycle_proj_0_weight = token_single_recycle_proj_0_bias = None
    getitem_3 = native_layer_norm_default_1[0];  native_layer_norm_default_1 = None
    _to_copy_4 = torch.ops.aten._to_copy.default(token_single_recycle_proj_1_weight, dtype = torch.bfloat16);  token_single_recycle_proj_1_weight = None
    _to_copy_5 = torch.ops.aten._to_copy.default(getitem_3, dtype = torch.bfloat16);  getitem_3 = None
    t_1 = torch.ops.aten.t.default(_to_copy_4);  _to_copy_4 = None
    view_2 = torch.ops.aten.view.default(_to_copy_5, [384, 384]);  _to_copy_5 = None
    mm_1 = torch.ops.aten.mm.default(view_2, t_1);  view_2 = t_1 = None
    view_3 = torch.ops.aten.view.default(mm_1, [1, 384, 384]);  mm_1 = None
    add_1 = torch.ops.aten.add.Tensor(arg1398_1, view_3);  arg1398_1 = view_3 = None
    any_1 = torch.ops.aten.any.dims(arg1405_1, dim = [-2, -1])
    sum_1 = torch.ops.aten.sum.dim_IntList(any_1, [1]);  any_1 = None
    _to_copy_6 = torch.ops.aten._to_copy.default(add, dtype = torch.float32)
    native_layer_norm_default_2 = torch.ops.aten.native_layer_norm.default(_to_copy_6, [256], template_embedder_proj_in_0_weight, template_embedder_proj_in_0_bias, 1e-05);  _to_copy_6 = template_embedder_proj_in_0_weight = template_embedder_proj_in_0_bias = None
    getitem_6 = native_layer_norm_default_2[0];  native_layer_norm_default_2 = None
    _to_copy_7 = torch.ops.aten._to_copy.default(template_embedder_proj_in_1_weight, dtype = torch.bfloat16);  template_embedder_proj_in_1_weight = None
    _to_copy_8 = torch.ops.aten._to_copy.default(getitem_6, dtype = torch.bfloat16);  getitem_6 = None
    t_2 = torch.ops.aten.t.default(_to_copy_7);  _to_copy_7 = None
    view_4 = torch.ops.aten.view.default(_to_copy_8, [147456, 256]);  _to_copy_8 = None
    mm_2 = torch.ops.aten.mm.default(view_4, t_2);  view_4 = t_2 = None
    view_5 = torch.ops.aten.view.default(mm_2, [1, 384, 384, 64]);  mm_2 = None
    unsqueeze = torch.ops.aten.unsqueeze.default(view_5, 1);  view_5 = None
    expand = torch.ops.aten.expand.default(unsqueeze, [-1, 4, -1, -1, -1]);  unsqueeze = None
    view_6 = torch.ops.aten.view.default(expand, [4, 384, 384, 64]);  expand = None
    unsqueeze_2 = torch.ops.aten.unsqueeze.default(arg1407_1, 1)
    expand_2 = torch.ops.aten.expand.default(unsqueeze_2, [-1, 4, -1, -1]);  unsqueeze_2 = None
    view_8 = torch.ops.aten.view.default(expand_2, [4, 384, 384]);  expand_2 = None
    view_9 = torch.ops.aten.view.default(arg1404_1, [4, 384, 384, 64]);  arg1404_1 = None
    view_10 = torch.ops.aten.view.default(arg1405_1, [4, 384, 384]);  arg1405_1 = None
    bitwise_and = torch.ops.aten.bitwise_and.Tensor(view_10, view_8);  view_10 = view_8 = None
    clone = torch.ops.aten.clone.default(view_6);  view_6 = None
    add_2 = torch.ops.aten.add.Tensor(clone, view_9);  clone = view_9 = None
    _to_copy_9 = torch.ops.aten._to_copy.default(add_2, dtype = torch.float32)
    native_layer_norm_default_3 = torch.ops.aten.native_layer_norm.default(_to_copy_9, [64], template_embedder_pairformer_blocks_0_triangle_multiplication_layernorm_z_in_weight, template_embedder_pairformer_blocks_0_triangle_multiplication_layernorm_z_in_bias, 1e-05);  _to_copy_9 = template_embedder_pairformer_blocks_0_triangle_multiplication_layernorm_z_in_weight = template_embedder_pairformer_blocks_0_triangle_multiplication_layernorm_z_in_bias = None
    getitem_9 = native_layer_norm_default_3[0];  native_layer_norm_default_3 = None
    split_with_sizes_default = torch.ops.aten.split_with_sizes.default(template_embedder_pairformer_blocks_0_triangle_multiplication_merged_linear_p_weight, [128, 128]);  template_embedder_pairformer_blocks_0_triangle_multiplication_merged_linear_p_weight = None
    getitem_12 = split_with_sizes_default[0]
    getitem_13 = split_with_sizes_default[1];  split_with_sizes_default = None
    split_with_sizes_default_1 = torch.ops.aten.split_with_sizes.default(template_embedder_pairformer_blocks_0_triangle_multiplication_merged_linear_g_weight, [128, 128, 64]);  template_embedder_pairformer_blocks_0_triangle_multiplication_merged_linear_g_weight = None
    getitem_14 = split_with_sizes_default_1[0]
    getitem_15 = split_with_sizes_default_1[1]
    getitem_16 = split_with_sizes_default_1[2];  split_with_sizes_default_1 = None
    _to_copy_10 = torch.ops.aten._to_copy.default(getitem_12, dtype = torch.bfloat16);  getitem_12 = None
    _to_copy_11 = torch.ops.aten._to_copy.default(getitem_9, dtype = torch.bfloat16)
    t_3 = torch.ops.aten.t.default(_to_copy_10);  _to_copy_10 = None
    view_11 = torch.ops.aten.view.default(_to_copy_11, [589824, 64]);  _to_copy_11 = None
    mm_3 = torch.ops.aten.mm.default(view_11, t_3);  view_11 = t_3 = None
    view_12 = torch.ops.aten.view.default(mm_3, [4, 384, 384, 128]);  mm_3 = None
    _to_copy_12 = torch.ops.aten._to_copy.default(getitem_14, dtype = torch.bfloat16);  getitem_14 = None
    _to_copy_13 = torch.ops.aten._to_copy.default(getitem_9, dtype = torch.bfloat16)
    t_4 = torch.ops.aten.t.default(_to_copy_12);  _to_copy_12 = None
    view_13 = torch.ops.aten.view.default(_to_copy_13, [589824, 64]);  _to_copy_13 = None
    mm_4 = torch.ops.aten.mm.default(view_13, t_4);  view_13 = t_4 = None
    view_14 = torch.ops.aten.view.default(mm_4, [4, 384, 384, 128]);  mm_4 = None
    sigmoid = torch.ops.aten.sigmoid.default(view_14);  view_14 = None
    mul = torch.ops.aten.mul.Tensor(view_12, sigmoid);  view_12 = sigmoid = None
    unsqueeze_3 = torch.ops.aten.unsqueeze.default(bitwise_and, 3)
    bitwise_not = torch.ops.aten.bitwise_not.default(unsqueeze_3);  unsqueeze_3 = None
    masked_fill = torch.ops.aten.masked_fill.Scalar(mul, bitwise_not, 0);  mul = bitwise_not = None
    split_tensor = torch.ops.aten.split.Tensor(masked_fill, 64, dim = -1)
    getitem_19 = split_tensor[0];  split_tensor = None
    unsqueeze_6 = torch.ops.aten.unsqueeze.default(getitem_19, 4);  getitem_19 = None
    permute_3 = torch.ops.aten.permute.default(unsqueeze_6, [0, 1, 4, 3, 2]);  unsqueeze_6 = None
    permute_4 = torch.ops.aten.permute.default(permute_3, [0, 3, 1, 4, 2]);  permute_3 = None
    clone_1 = torch.ops.aten.clone.default(permute_4, memory_format = torch.contiguous_format);  permute_4 = None
    _unsafe_view = torch.ops.aten._unsafe_view.default(clone_1, [256, 384, 384]);  clone_1 = None
    split_tensor_1 = torch.ops.aten.split.Tensor(masked_fill, 64, dim = -1);  masked_fill = None
    getitem_22 = split_tensor_1[1];  split_tensor_1 = None
    unsqueeze_7 = torch.ops.aten.unsqueeze.default(getitem_22, 4);  getitem_22 = None
    permute_6 = torch.ops.aten.permute.default(unsqueeze_7, [0, 4, 1, 3, 2]);  unsqueeze_7 = None
    permute_7 = torch.ops.aten.permute.default(permute_6, [0, 3, 4, 2, 1]);  permute_6 = None
    clone_2 = torch.ops.aten.clone.default(permute_7, memory_format = torch.contiguous_format);  permute_7 = None
    _unsafe_view_1 = torch.ops.aten._unsafe_view.default(clone_2, [256, 384, 384]);  clone_2 = None
    bmm = torch.ops.aten.bmm.default(_unsafe_view, _unsafe_view_1);  _unsafe_view = _unsafe_view_1 = None
    view_15 = torch.ops.aten.view.default(bmm, [4, 64, 384, 1, 384]);  bmm = None
    permute_8 = torch.ops.aten.permute.default(view_15, [0, 2, 4, 1, 3]);  view_15 = None
    view_16 = torch.ops.aten.view.default(permute_8, [4, 384, 384, 64]);  permute_8 = None
    _to_copy_14 = torch.ops.aten._to_copy.default(getitem_13, dtype = torch.bfloat16);  getitem_13 = None
    _to_copy_15 = torch.ops.aten._to_copy.default(getitem_9, dtype = torch.bfloat16)
    t_5 = torch.ops.aten.t.default(_to_copy_14);  _to_copy_14 = None
    view_17 = torch.ops.aten.view.default(_to_copy_15, [589824, 64]);  _to_copy_15 = None
    mm_5 = torch.ops.aten.mm.default(view_17, t_5);  view_17 = t_5 = None
    view_18 = torch.ops.aten.view.default(mm_5, [4, 384, 384, 128]);  mm_5 = None
    _to_copy_16 = torch.ops.aten._to_copy.default(getitem_15, dtype = torch.bfloat16);  getitem_15 = None
    _to_copy_17 = torch.ops.aten._to_copy.default(getitem_9, dtype = torch.bfloat16)
    t_6 = torch.ops.aten.t.default(_to_copy_16);  _to_copy_16 = None
    view_19 = torch.ops.aten.view.default(_to_copy_17, [589824, 64]);  _to_copy_17 = None
    mm_6 = torch.ops.aten.mm.default(view_19, t_6);  view_19 = t_6 = None
    view_20 = torch.ops.aten.view.default(mm_6, [4, 384, 384, 128]);  mm_6 = None
    sigmoid_1 = torch.ops.aten.sigmoid.default(view_20);  view_20 = None
    mul_1 = torch.ops.aten.mul.Tensor(view_18, sigmoid_1);  view_18 = sigmoid_1 = None
    view_21 = torch.ops.aten.view.default(mul_1, [589824, 128]);  mul_1 = None
    view_22 = torch.ops.aten.view.default(view_21, [4, 384, 384, 128]);  view_21 = None
    transpose = torch.ops.aten.transpose.int(bitwise_and, 1, 2)
    unsqueeze_8 = torch.ops.aten.unsqueeze.default(transpose, 3);  transpose = None
    clone_3 = torch.ops.aten.clone.default(unsqueeze_8, memory_format = torch.contiguous_format);  unsqueeze_8 = None
    bitwise_not_1 = torch.ops.aten.bitwise_not.default(clone_3);  clone_3 = None
    masked_fill_1 = torch.ops.aten.masked_fill.Scalar(view_22, bitwise_not_1, 0);  view_22 = bitwise_not_1 = None
    view_23 = torch.ops.aten.view.default(masked_fill_1, [589824, 128]);  masked_fill_1 = None
    view_25 = torch.ops.aten.view.default(view_23, [4, 384, 384, 128])
    split_tensor_2 = torch.ops.aten.split.Tensor(view_25, 64, dim = -1);  view_25 = None
    getitem_25 = split_tensor_2[0];  split_tensor_2 = None
    unsqueeze_11 = torch.ops.aten.unsqueeze.default(getitem_25, 4);  getitem_25 = None
    permute_12 = torch.ops.aten.permute.default(unsqueeze_11, [0, 2, 4, 3, 1]);  unsqueeze_11 = None
    permute_13 = torch.ops.aten.permute.default(permute_12, [0, 3, 1, 4, 2]);  permute_12 = None
    clone_4 = torch.ops.aten.clone.default(permute_13, memory_format = torch.contiguous_format);  permute_13 = None
    _unsafe_view_2 = torch.ops.aten._unsafe_view.default(clone_4, [256, 384, 384]);  clone_4 = None
    view_26 = torch.ops.aten.view.default(view_23, [4, 384, 384, 128]);  view_23 = None
    split_tensor_3 = torch.ops.aten.split.Tensor(view_26, 64, dim = -1);  view_26 = None
    getitem_28 = split_tensor_3[1];  split_tensor_3 = None
    unsqueeze_12 = torch.ops.aten.unsqueeze.default(getitem_28, 4);  getitem_28 = None
    permute_15 = torch.ops.aten.permute.default(unsqueeze_12, [0, 4, 2, 3, 1]);  unsqueeze_12 = None
    permute_16 = torch.ops.aten.permute.default(permute_15, [0, 3, 4, 2, 1]);  permute_15 = None
    clone_5 = torch.ops.aten.clone.default(permute_16, memory_format = torch.contiguous_format);  permute_16 = None
    _unsafe_view_3 = torch.ops.aten._unsafe_view.default(clone_5, [256, 384, 384]);  clone_5 = None
    bmm_1 = torch.ops.aten.bmm.default(_unsafe_view_2, _unsafe_view_3);  _unsafe_view_2 = _unsafe_view_3 = None
    view_27 = torch.ops.aten.view.default(bmm_1, [4, 64, 384, 1, 384]);  bmm_1 = None
    permute_17 = torch.ops.aten.permute.default(view_27, [0, 2, 4, 1, 3]);  view_27 = None
    view_28 = torch.ops.aten.view.default(permute_17, [4, 384, 384, 64]);  permute_17 = None
    _to_copy_18 = torch.ops.aten._to_copy.default(view_16, dtype = torch.float32);  view_16 = None
    native_layer_norm_default_4 = torch.ops.aten.native_layer_norm.default(_to_copy_18, [64], None, None, 1e-05);  _to_copy_18 = None
    getitem_29 = native_layer_norm_default_4[0];  native_layer_norm_default_4 = None
    _to_copy_19 = torch.ops.aten._to_copy.default(view_28, dtype = torch.float32);  view_28 = None
    native_layer_norm_default_5 = torch.ops.aten.native_layer_norm.default(_to_copy_19, [64], None, None, 1e-05);  _to_copy_19 = None
    getitem_32 = native_layer_norm_default_5[0];  native_layer_norm_default_5 = None
    add_3 = torch.ops.aten.add.Tensor(getitem_29, getitem_32);  getitem_29 = getitem_32 = None
    _to_copy_20 = torch.ops.aten._to_copy.default(template_embedder_pairformer_blocks_0_triangle_multiplication_linear_z_out_weight, dtype = torch.bfloat16);  template_embedder_pairformer_blocks_0_triangle_multiplication_linear_z_out_weight = None
    _to_copy_21 = torch.ops.aten._to_copy.default(add_3, dtype = torch.bfloat16);  add_3 = None
    t_7 = torch.ops.aten.t.default(_to_copy_20);  _to_copy_20 = None
    view_29 = torch.ops.aten.view.default(_to_copy_21, [589824, 64]);  _to_copy_21 = None
    mm_7 = torch.ops.aten.mm.default(view_29, t_7);  view_29 = t_7 = None
    view_30 = torch.ops.aten.view.default(mm_7, [4, 384, 384, 64]);  mm_7 = None
    _to_copy_22 = torch.ops.aten._to_copy.default(getitem_16, dtype = torch.bfloat16);  getitem_16 = None
    _to_copy_23 = torch.ops.aten._to_copy.default(getitem_9, dtype = torch.bfloat16);  getitem_9 = None
    t_8 = torch.ops.aten.t.default(_to_copy_22);  _to_copy_22 = None
    view_31 = torch.ops.aten.view.default(_to_copy_23, [589824, 64]);  _to_copy_23 = None
    mm_8 = torch.ops.aten.mm.default(view_31, t_8);  view_31 = t_8 = None
    view_32 = torch.ops.aten.view.default(mm_8, [4, 384, 384, 64]);  mm_8 = None
    sigmoid_2 = torch.ops.aten.sigmoid.default(view_32);  view_32 = None
    mul_2 = torch.ops.aten.mul.Tensor(view_30, sigmoid_2);  view_30 = sigmoid_2 = None
    add_4 = torch.ops.aten.add.Tensor(add_2, mul_2);  mul_2 = None
    _to_copy_24 = torch.ops.aten._to_copy.default(add_2, dtype = torch.float32)
    native_layer_norm_default_6 = torch.ops.aten.native_layer_norm.default(_to_copy_24, [64], None, None, 1e-05);  _to_copy_24 = None
    getitem_35 = native_layer_norm_default_6[0];  native_layer_norm_default_6 = None
    _to_copy_25 = torch.ops.aten._to_copy.default(template_embedder_pairformer_blocks_0_triangle_attention_pair2b_weight, dtype = torch.bfloat16);  template_embedder_pairformer_blocks_0_triangle_attention_pair2b_weight = None
    _to_copy_26 = torch.ops.aten._to_copy.default(getitem_35, dtype = torch.bfloat16)
    t_9 = torch.ops.aten.t.default(_to_copy_25);  _to_copy_25 = None
    view_33 = torch.ops.aten.view.default(_to_copy_26, [589824, 64]);  _to_copy_26 = None
    mm_9 = torch.ops.aten.mm.default(view_33, t_9);  view_33 = t_9 = None
    view_34 = torch.ops.aten.view.default(mm_9, [4, 384, 384, 8]);  mm_9 = None
    view_35 = torch.ops.aten.view.default(view_34, [4, 384, 384, 2, 4]);  view_34 = None
    permute_18 = torch.ops.aten.permute.default(view_35, [0, 3, 4, 1, 2]);  view_35 = None
    view_36 = torch.ops.aten.view.default(permute_18, [4, 2, 4, 1, 384, 384]);  permute_18 = None
    view_37 = torch.ops.aten.view.default(bitwise_and, [4, 1, 1, 1, 384, 384])
    bitwise_not_2 = torch.ops.aten.bitwise_not.default(view_37);  view_37 = None
    masked_fill_2 = torch.ops.aten.masked_fill.Scalar(view_36, bitwise_not_2, -10000);  view_36 = bitwise_not_2 = None
    view_38 = torch.ops.aten.view.default(masked_fill_2, [4, 2, 4, 384, 384]);  masked_fill_2 = None
    permute_19 = torch.ops.aten.permute.default(view_38, [1, 0, 2, 3, 4]);  view_38 = None
    clone_6 = torch.ops.aten.clone.default(permute_19, memory_format = torch.contiguous_format);  permute_19 = None
    _unsafe_view_4 = torch.ops.aten._unsafe_view.default(clone_6, [2, 16, 1, 384, 384]);  clone_6 = None
    _to_copy_27 = torch.ops.aten._to_copy.default(template_embedder_pairformer_blocks_0_triangle_attention_pair2qkvg1_weight, dtype = torch.bfloat16);  template_embedder_pairformer_blocks_0_triangle_attention_pair2qkvg1_weight = None
    _to_copy_28 = torch.ops.aten._to_copy.default(getitem_35, dtype = torch.bfloat16)
    t_10 = torch.ops.aten.t.default(_to_copy_27);  _to_copy_27 = None
    view_39 = torch.ops.aten.view.default(_to_copy_28, [589824, 64]);  _to_copy_28 = None
    mm_10 = torch.ops.aten.mm.default(view_39, t_10);  view_39 = t_10 = None
    view_40 = torch.ops.aten.view.default(mm_10, [4, 384, 384, 512]);  mm_10 = None
    select_1 = torch.ops.aten.select.int(_unsafe_view_4, 0, 0)
    view_41 = torch.ops.aten.view.default(view_40, [4, 384, 384, 4, 4, 32]);  view_40 = None
    permute_20 = torch.ops.aten.permute.default(view_41, [4, 0, 3, 1, 2, 5]);  view_41 = None
    clone_7 = torch.ops.aten.clone.default(permute_20, memory_format = torch.contiguous_format);  permute_20 = None
    _unsafe_view_5 = torch.ops.aten._unsafe_view.default(clone_7, [4, 16, 384, 384, 32]);  clone_7 = None
    unbind_int = torch.ops.aten.unbind.int(_unsafe_view_5);  _unsafe_view_5 = None
    getitem_38 = unbind_int[0]
    getitem_39 = unbind_int[1]
    getitem_40 = unbind_int[2]
    getitem_41 = unbind_int[3];  unbind_int = None
    expand_3 = torch.ops.aten.expand.default(select_1, [16, 384, 384, 384]);  select_1 = None
    _scaled_dot_product_efficient_attention_default = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_38, getitem_39, getitem_40, expand_3, False);  getitem_38 = getitem_39 = getitem_40 = expand_3 = None
    getitem_42 = _scaled_dot_product_efficient_attention_default[0];  _scaled_dot_product_efficient_attention_default = None
    sigmoid_3 = torch.ops.aten.sigmoid.default(getitem_41);  getitem_41 = None
    mul_3 = torch.ops.aten.mul.Tensor(getitem_42, sigmoid_3);  getitem_42 = sigmoid_3 = None
    view_42 = torch.ops.aten.view.default(mul_3, [4, 4, 384, 384, 32]);  mul_3 = None
    permute_21 = torch.ops.aten.permute.default(view_42, [0, 2, 3, 1, 4]);  view_42 = None
    clone_8 = torch.ops.aten.clone.default(permute_21, memory_format = torch.contiguous_format);  permute_21 = None
    _unsafe_view_6 = torch.ops.aten._unsafe_view.default(clone_8, [4, 384, 384, 128]);  clone_8 = None
    transpose_1 = torch.ops.aten.transpose.int(getitem_35, 1, 2);  getitem_35 = None
    _to_copy_29 = torch.ops.aten._to_copy.default(template_embedder_pairformer_blocks_0_triangle_attention_pair2qkvg2_weight, dtype = torch.bfloat16);  template_embedder_pairformer_blocks_0_triangle_attention_pair2qkvg2_weight = None
    _to_copy_30 = torch.ops.aten._to_copy.default(transpose_1, dtype = torch.bfloat16);  transpose_1 = None
    t_11 = torch.ops.aten.t.default(_to_copy_29);  _to_copy_29 = None
    expand_4 = torch.ops.aten.expand.default(_to_copy_30, [4, 384, 384, 64]);  _to_copy_30 = None
    clone_9 = torch.ops.aten.clone.default(expand_4, memory_format = torch.contiguous_format);  expand_4 = None
    _unsafe_view_7 = torch.ops.aten._unsafe_view.default(clone_9, [1536, 384, 64]);  clone_9 = None
    expand_5 = torch.ops.aten.expand.default(t_11, [4, 384, 64, 512]);  t_11 = None
    view_43 = torch.ops.aten.view.default(expand_5, [1536, 64, 512]);  expand_5 = None
    bmm_2 = torch.ops.aten.bmm.default(_unsafe_view_7, view_43);  _unsafe_view_7 = view_43 = None
    view_44 = torch.ops.aten.view.default(bmm_2, [4, 384, 384, 512]);  bmm_2 = None
    select_2 = torch.ops.aten.select.int(_unsafe_view_4, 0, 1);  _unsafe_view_4 = None
    view_45 = torch.ops.aten.view.default(view_44, [4, 384, 384, 4, 4, 32]);  view_44 = None
    permute_22 = torch.ops.aten.permute.default(view_45, [4, 0, 3, 1, 2, 5]);  view_45 = None
    clone_10 = torch.ops.aten.clone.default(permute_22, memory_format = torch.contiguous_format);  permute_22 = None
    _unsafe_view_8 = torch.ops.aten._unsafe_view.default(clone_10, [4, 16, 384, 384, 32]);  clone_10 = None
    unbind_int_1 = torch.ops.aten.unbind.int(_unsafe_view_8);  _unsafe_view_8 = None
    getitem_46 = unbind_int_1[0]
    getitem_47 = unbind_int_1[1]
    getitem_48 = unbind_int_1[2]
    getitem_49 = unbind_int_1[3];  unbind_int_1 = None
    expand_6 = torch.ops.aten.expand.default(select_2, [16, 384, 384, 384]);  select_2 = None
    _scaled_dot_product_efficient_attention_default_1 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_46, getitem_47, getitem_48, expand_6, False);  getitem_46 = getitem_47 = getitem_48 = expand_6 = None
    getitem_50 = _scaled_dot_product_efficient_attention_default_1[0];  _scaled_dot_product_efficient_attention_default_1 = None
    sigmoid_4 = torch.ops.aten.sigmoid.default(getitem_49);  getitem_49 = None
    mul_4 = torch.ops.aten.mul.Tensor(getitem_50, sigmoid_4);  getitem_50 = sigmoid_4 = None
    view_46 = torch.ops.aten.view.default(mul_4, [4, 4, 384, 384, 32]);  mul_4 = None
    permute_23 = torch.ops.aten.permute.default(view_46, [0, 2, 3, 1, 4]);  view_46 = None
    clone_11 = torch.ops.aten.clone.default(permute_23, memory_format = torch.contiguous_format);  permute_23 = None
    _unsafe_view_9 = torch.ops.aten._unsafe_view.default(clone_11, [4, 384, 384, 128]);  clone_11 = None
    cat = torch.ops.aten.cat.default([_unsafe_view_6, _unsafe_view_9], dim = -1);  _unsafe_view_6 = _unsafe_view_9 = None
    slice_3 = torch.ops.aten.slice.Tensor(template_embedder_pairformer_blocks_0_triangle_attention_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  template_embedder_pairformer_blocks_0_triangle_attention_out_scalers = None
    unsqueeze_13 = torch.ops.aten.unsqueeze.default(slice_3, 1);  slice_3 = None
    mul_5 = torch.ops.aten.mul.Tensor(template_embedder_pairformer_blocks_0_triangle_attention_linear_out_weight, unsqueeze_13);  template_embedder_pairformer_blocks_0_triangle_attention_linear_out_weight = unsqueeze_13 = None
    _to_copy_31 = torch.ops.aten._to_copy.default(mul_5, dtype = torch.bfloat16);  mul_5 = None
    t_12 = torch.ops.aten.t.default(_to_copy_31);  _to_copy_31 = None
    view_47 = torch.ops.aten.view.default(cat, [589824, 256]);  cat = None
    mm_11 = torch.ops.aten.mm.default(view_47, t_12);  view_47 = t_12 = None
    view_48 = torch.ops.aten.view.default(mm_11, [4, 384, 384, 64]);  mm_11 = None
    add_5 = torch.ops.aten.add.Tensor(add_4, view_48);  add_4 = view_48 = None
    split_tensor_4 = torch.ops.aten.split.Tensor(add_2, 384, dim = -2);  add_2 = None
    getitem_54 = split_tensor_4[0];  split_tensor_4 = None
    _to_copy_32 = torch.ops.aten._to_copy.default(getitem_54, dtype = torch.float32);  getitem_54 = None
    native_layer_norm_default_7 = torch.ops.aten.native_layer_norm.default(_to_copy_32, [64], template_embedder_pairformer_blocks_0_transition_pair_layer_norm_weight, template_embedder_pairformer_blocks_0_transition_pair_layer_norm_bias, 1e-05);  _to_copy_32 = template_embedder_pairformer_blocks_0_transition_pair_layer_norm_weight = template_embedder_pairformer_blocks_0_transition_pair_layer_norm_bias = None
    getitem_55 = native_layer_norm_default_7[0];  native_layer_norm_default_7 = None
    _to_copy_33 = torch.ops.aten._to_copy.default(template_embedder_pairformer_blocks_0_transition_pair_linear_no_bias_ab_weight, dtype = torch.bfloat16);  template_embedder_pairformer_blocks_0_transition_pair_linear_no_bias_ab_weight = None
    _to_copy_34 = torch.ops.aten._to_copy.default(getitem_55, dtype = torch.bfloat16);  getitem_55 = None
    t_13 = torch.ops.aten.t.default(_to_copy_33);  _to_copy_33 = None
    view_49 = torch.ops.aten.view.default(_to_copy_34, [589824, 64]);  _to_copy_34 = None
    mm_12 = torch.ops.aten.mm.default(view_49, t_13);  view_49 = t_13 = None
    view_50 = torch.ops.aten.view.default(mm_12, [4, 384, 384, 256]);  mm_12 = None
    split_tensor_5 = torch.ops.aten.split.Tensor(view_50, 128, dim = -1);  view_50 = None
    getitem_58 = split_tensor_5[0]
    getitem_59 = split_tensor_5[1];  split_tensor_5 = None
    silu = torch.ops.aten.silu.default(getitem_58);  getitem_58 = None
    mul_6 = torch.ops.aten.mul.Tensor(silu, getitem_59);  silu = getitem_59 = None
    _to_copy_35 = torch.ops.aten._to_copy.default(template_embedder_pairformer_blocks_0_transition_pair_linear_out_weight, dtype = torch.bfloat16);  template_embedder_pairformer_blocks_0_transition_pair_linear_out_weight = None
    t_14 = torch.ops.aten.t.default(_to_copy_35);  _to_copy_35 = None
    view_52 = torch.ops.aten.view.default(mul_6, [589824, 128]);  mul_6 = None
    mm_13 = torch.ops.aten.mm.default(view_52, t_14);  view_52 = t_14 = None
    view_53 = torch.ops.aten.view.default(mm_13, [4, 384, 384, 64]);  mm_13 = None
    add_6 = torch.ops.aten.add.Tensor(add_5, view_53);  add_5 = view_53 = None
    _to_copy_36 = torch.ops.aten._to_copy.default(add_6, dtype = torch.float32)
    native_layer_norm_default_8 = torch.ops.aten.native_layer_norm.default(_to_copy_36, [64], template_embedder_pairformer_blocks_1_triangle_multiplication_layernorm_z_in_weight, template_embedder_pairformer_blocks_1_triangle_multiplication_layernorm_z_in_bias, 1e-05);  _to_copy_36 = template_embedder_pairformer_blocks_1_triangle_multiplication_layernorm_z_in_weight = template_embedder_pairformer_blocks_1_triangle_multiplication_layernorm_z_in_bias = None
    getitem_60 = native_layer_norm_default_8[0];  native_layer_norm_default_8 = None
    split_with_sizes_default_2 = torch.ops.aten.split_with_sizes.default(template_embedder_pairformer_blocks_1_triangle_multiplication_merged_linear_p_weight, [128, 128]);  template_embedder_pairformer_blocks_1_triangle_multiplication_merged_linear_p_weight = None
    getitem_63 = split_with_sizes_default_2[0]
    getitem_64 = split_with_sizes_default_2[1];  split_with_sizes_default_2 = None
    split_with_sizes_default_3 = torch.ops.aten.split_with_sizes.default(template_embedder_pairformer_blocks_1_triangle_multiplication_merged_linear_g_weight, [128, 128, 64]);  template_embedder_pairformer_blocks_1_triangle_multiplication_merged_linear_g_weight = None
    getitem_65 = split_with_sizes_default_3[0]
    getitem_66 = split_with_sizes_default_3[1]
    getitem_67 = split_with_sizes_default_3[2];  split_with_sizes_default_3 = None
    _to_copy_37 = torch.ops.aten._to_copy.default(getitem_63, dtype = torch.bfloat16);  getitem_63 = None
    _to_copy_38 = torch.ops.aten._to_copy.default(getitem_60, dtype = torch.bfloat16)
    t_15 = torch.ops.aten.t.default(_to_copy_37);  _to_copy_37 = None
    view_54 = torch.ops.aten.view.default(_to_copy_38, [589824, 64]);  _to_copy_38 = None
    mm_14 = torch.ops.aten.mm.default(view_54, t_15);  view_54 = t_15 = None
    view_55 = torch.ops.aten.view.default(mm_14, [4, 384, 384, 128]);  mm_14 = None
    _to_copy_39 = torch.ops.aten._to_copy.default(getitem_65, dtype = torch.bfloat16);  getitem_65 = None
    _to_copy_40 = torch.ops.aten._to_copy.default(getitem_60, dtype = torch.bfloat16)
    t_16 = torch.ops.aten.t.default(_to_copy_39);  _to_copy_39 = None
    view_56 = torch.ops.aten.view.default(_to_copy_40, [589824, 64]);  _to_copy_40 = None
    mm_15 = torch.ops.aten.mm.default(view_56, t_16);  view_56 = t_16 = None
    view_57 = torch.ops.aten.view.default(mm_15, [4, 384, 384, 128]);  mm_15 = None
    sigmoid_5 = torch.ops.aten.sigmoid.default(view_57);  view_57 = None
    mul_7 = torch.ops.aten.mul.Tensor(view_55, sigmoid_5);  view_55 = sigmoid_5 = None
    unsqueeze_14 = torch.ops.aten.unsqueeze.default(bitwise_and, 3)
    bitwise_not_3 = torch.ops.aten.bitwise_not.default(unsqueeze_14);  unsqueeze_14 = None
    masked_fill_3 = torch.ops.aten.masked_fill.Scalar(mul_7, bitwise_not_3, 0);  mul_7 = bitwise_not_3 = None
    split_tensor_6 = torch.ops.aten.split.Tensor(masked_fill_3, 64, dim = -1)
    getitem_70 = split_tensor_6[0];  split_tensor_6 = None
    unsqueeze_17 = torch.ops.aten.unsqueeze.default(getitem_70, 4);  getitem_70 = None
    permute_27 = torch.ops.aten.permute.default(unsqueeze_17, [0, 1, 4, 3, 2]);  unsqueeze_17 = None
    permute_28 = torch.ops.aten.permute.default(permute_27, [0, 3, 1, 4, 2]);  permute_27 = None
    clone_12 = torch.ops.aten.clone.default(permute_28, memory_format = torch.contiguous_format);  permute_28 = None
    _unsafe_view_10 = torch.ops.aten._unsafe_view.default(clone_12, [256, 384, 384]);  clone_12 = None
    split_tensor_7 = torch.ops.aten.split.Tensor(masked_fill_3, 64, dim = -1);  masked_fill_3 = None
    getitem_73 = split_tensor_7[1];  split_tensor_7 = None
    unsqueeze_18 = torch.ops.aten.unsqueeze.default(getitem_73, 4);  getitem_73 = None
    permute_30 = torch.ops.aten.permute.default(unsqueeze_18, [0, 4, 1, 3, 2]);  unsqueeze_18 = None
    permute_31 = torch.ops.aten.permute.default(permute_30, [0, 3, 4, 2, 1]);  permute_30 = None
    clone_13 = torch.ops.aten.clone.default(permute_31, memory_format = torch.contiguous_format);  permute_31 = None
    _unsafe_view_11 = torch.ops.aten._unsafe_view.default(clone_13, [256, 384, 384]);  clone_13 = None
    bmm_3 = torch.ops.aten.bmm.default(_unsafe_view_10, _unsafe_view_11);  _unsafe_view_10 = _unsafe_view_11 = None
    view_58 = torch.ops.aten.view.default(bmm_3, [4, 64, 384, 1, 384]);  bmm_3 = None
    permute_32 = torch.ops.aten.permute.default(view_58, [0, 2, 4, 1, 3]);  view_58 = None
    view_59 = torch.ops.aten.view.default(permute_32, [4, 384, 384, 64]);  permute_32 = None
    _to_copy_41 = torch.ops.aten._to_copy.default(getitem_64, dtype = torch.bfloat16);  getitem_64 = None
    _to_copy_42 = torch.ops.aten._to_copy.default(getitem_60, dtype = torch.bfloat16)
    t_17 = torch.ops.aten.t.default(_to_copy_41);  _to_copy_41 = None
    view_60 = torch.ops.aten.view.default(_to_copy_42, [589824, 64]);  _to_copy_42 = None
    mm_16 = torch.ops.aten.mm.default(view_60, t_17);  view_60 = t_17 = None
    view_61 = torch.ops.aten.view.default(mm_16, [4, 384, 384, 128]);  mm_16 = None
    _to_copy_43 = torch.ops.aten._to_copy.default(getitem_66, dtype = torch.bfloat16);  getitem_66 = None
    _to_copy_44 = torch.ops.aten._to_copy.default(getitem_60, dtype = torch.bfloat16)
    t_18 = torch.ops.aten.t.default(_to_copy_43);  _to_copy_43 = None
    view_62 = torch.ops.aten.view.default(_to_copy_44, [589824, 64]);  _to_copy_44 = None
    mm_17 = torch.ops.aten.mm.default(view_62, t_18);  view_62 = t_18 = None
    view_63 = torch.ops.aten.view.default(mm_17, [4, 384, 384, 128]);  mm_17 = None
    sigmoid_6 = torch.ops.aten.sigmoid.default(view_63);  view_63 = None
    mul_8 = torch.ops.aten.mul.Tensor(view_61, sigmoid_6);  view_61 = sigmoid_6 = None
    view_64 = torch.ops.aten.view.default(mul_8, [589824, 128]);  mul_8 = None
    view_65 = torch.ops.aten.view.default(view_64, [4, 384, 384, 128]);  view_64 = None
    transpose_2 = torch.ops.aten.transpose.int(bitwise_and, 1, 2)
    unsqueeze_19 = torch.ops.aten.unsqueeze.default(transpose_2, 3);  transpose_2 = None
    clone_14 = torch.ops.aten.clone.default(unsqueeze_19, memory_format = torch.contiguous_format);  unsqueeze_19 = None
    bitwise_not_4 = torch.ops.aten.bitwise_not.default(clone_14);  clone_14 = None
    masked_fill_4 = torch.ops.aten.masked_fill.Scalar(view_65, bitwise_not_4, 0);  view_65 = bitwise_not_4 = None
    view_66 = torch.ops.aten.view.default(masked_fill_4, [589824, 128]);  masked_fill_4 = None
    view_68 = torch.ops.aten.view.default(view_66, [4, 384, 384, 128])
    split_tensor_8 = torch.ops.aten.split.Tensor(view_68, 64, dim = -1);  view_68 = None
    getitem_76 = split_tensor_8[0];  split_tensor_8 = None
    unsqueeze_22 = torch.ops.aten.unsqueeze.default(getitem_76, 4);  getitem_76 = None
    permute_36 = torch.ops.aten.permute.default(unsqueeze_22, [0, 2, 4, 3, 1]);  unsqueeze_22 = None
    permute_37 = torch.ops.aten.permute.default(permute_36, [0, 3, 1, 4, 2]);  permute_36 = None
    clone_15 = torch.ops.aten.clone.default(permute_37, memory_format = torch.contiguous_format);  permute_37 = None
    _unsafe_view_12 = torch.ops.aten._unsafe_view.default(clone_15, [256, 384, 384]);  clone_15 = None
    view_69 = torch.ops.aten.view.default(view_66, [4, 384, 384, 128]);  view_66 = None
    split_tensor_9 = torch.ops.aten.split.Tensor(view_69, 64, dim = -1);  view_69 = None
    getitem_79 = split_tensor_9[1];  split_tensor_9 = None
    unsqueeze_23 = torch.ops.aten.unsqueeze.default(getitem_79, 4);  getitem_79 = None
    permute_39 = torch.ops.aten.permute.default(unsqueeze_23, [0, 4, 2, 3, 1]);  unsqueeze_23 = None
    permute_40 = torch.ops.aten.permute.default(permute_39, [0, 3, 4, 2, 1]);  permute_39 = None
    clone_16 = torch.ops.aten.clone.default(permute_40, memory_format = torch.contiguous_format);  permute_40 = None
    _unsafe_view_13 = torch.ops.aten._unsafe_view.default(clone_16, [256, 384, 384]);  clone_16 = None
    bmm_4 = torch.ops.aten.bmm.default(_unsafe_view_12, _unsafe_view_13);  _unsafe_view_12 = _unsafe_view_13 = None
    view_70 = torch.ops.aten.view.default(bmm_4, [4, 64, 384, 1, 384]);  bmm_4 = None
    permute_41 = torch.ops.aten.permute.default(view_70, [0, 2, 4, 1, 3]);  view_70 = None
    view_71 = torch.ops.aten.view.default(permute_41, [4, 384, 384, 64]);  permute_41 = None
    _to_copy_45 = torch.ops.aten._to_copy.default(view_59, dtype = torch.float32);  view_59 = None
    native_layer_norm_default_9 = torch.ops.aten.native_layer_norm.default(_to_copy_45, [64], None, None, 1e-05);  _to_copy_45 = None
    getitem_80 = native_layer_norm_default_9[0];  native_layer_norm_default_9 = None
    _to_copy_46 = torch.ops.aten._to_copy.default(view_71, dtype = torch.float32);  view_71 = None
    native_layer_norm_default_10 = torch.ops.aten.native_layer_norm.default(_to_copy_46, [64], None, None, 1e-05);  _to_copy_46 = None
    getitem_83 = native_layer_norm_default_10[0];  native_layer_norm_default_10 = None
    add_7 = torch.ops.aten.add.Tensor(getitem_80, getitem_83);  getitem_80 = getitem_83 = None
    _to_copy_47 = torch.ops.aten._to_copy.default(template_embedder_pairformer_blocks_1_triangle_multiplication_linear_z_out_weight, dtype = torch.bfloat16);  template_embedder_pairformer_blocks_1_triangle_multiplication_linear_z_out_weight = None
    _to_copy_48 = torch.ops.aten._to_copy.default(add_7, dtype = torch.bfloat16);  add_7 = None
    t_19 = torch.ops.aten.t.default(_to_copy_47);  _to_copy_47 = None
    view_72 = torch.ops.aten.view.default(_to_copy_48, [589824, 64]);  _to_copy_48 = None
    mm_18 = torch.ops.aten.mm.default(view_72, t_19);  view_72 = t_19 = None
    view_73 = torch.ops.aten.view.default(mm_18, [4, 384, 384, 64]);  mm_18 = None
    _to_copy_49 = torch.ops.aten._to_copy.default(getitem_67, dtype = torch.bfloat16);  getitem_67 = None
    _to_copy_50 = torch.ops.aten._to_copy.default(getitem_60, dtype = torch.bfloat16);  getitem_60 = None
    t_20 = torch.ops.aten.t.default(_to_copy_49);  _to_copy_49 = None
    view_74 = torch.ops.aten.view.default(_to_copy_50, [589824, 64]);  _to_copy_50 = None
    mm_19 = torch.ops.aten.mm.default(view_74, t_20);  view_74 = t_20 = None
    view_75 = torch.ops.aten.view.default(mm_19, [4, 384, 384, 64]);  mm_19 = None
    sigmoid_7 = torch.ops.aten.sigmoid.default(view_75);  view_75 = None
    mul_9 = torch.ops.aten.mul.Tensor(view_73, sigmoid_7);  view_73 = sigmoid_7 = None
    add_8 = torch.ops.aten.add.Tensor(add_6, mul_9);  mul_9 = None
    _to_copy_51 = torch.ops.aten._to_copy.default(add_6, dtype = torch.float32)
    native_layer_norm_default_11 = torch.ops.aten.native_layer_norm.default(_to_copy_51, [64], None, None, 1e-05);  _to_copy_51 = None
    getitem_86 = native_layer_norm_default_11[0];  native_layer_norm_default_11 = None
    _to_copy_52 = torch.ops.aten._to_copy.default(template_embedder_pairformer_blocks_1_triangle_attention_pair2b_weight, dtype = torch.bfloat16);  template_embedder_pairformer_blocks_1_triangle_attention_pair2b_weight = None
    _to_copy_53 = torch.ops.aten._to_copy.default(getitem_86, dtype = torch.bfloat16)
    t_21 = torch.ops.aten.t.default(_to_copy_52);  _to_copy_52 = None
    view_76 = torch.ops.aten.view.default(_to_copy_53, [589824, 64]);  _to_copy_53 = None
    mm_20 = torch.ops.aten.mm.default(view_76, t_21);  view_76 = t_21 = None
    view_77 = torch.ops.aten.view.default(mm_20, [4, 384, 384, 8]);  mm_20 = None
    view_78 = torch.ops.aten.view.default(view_77, [4, 384, 384, 2, 4]);  view_77 = None
    permute_42 = torch.ops.aten.permute.default(view_78, [0, 3, 4, 1, 2]);  view_78 = None
    view_79 = torch.ops.aten.view.default(permute_42, [4, 2, 4, 1, 384, 384]);  permute_42 = None
    view_80 = torch.ops.aten.view.default(bitwise_and, [4, 1, 1, 1, 384, 384])
    bitwise_not_5 = torch.ops.aten.bitwise_not.default(view_80);  view_80 = None
    masked_fill_5 = torch.ops.aten.masked_fill.Scalar(view_79, bitwise_not_5, -10000);  view_79 = bitwise_not_5 = None
    view_81 = torch.ops.aten.view.default(masked_fill_5, [4, 2, 4, 384, 384]);  masked_fill_5 = None
    permute_43 = torch.ops.aten.permute.default(view_81, [1, 0, 2, 3, 4]);  view_81 = None
    clone_17 = torch.ops.aten.clone.default(permute_43, memory_format = torch.contiguous_format);  permute_43 = None
    _unsafe_view_14 = torch.ops.aten._unsafe_view.default(clone_17, [2, 16, 1, 384, 384]);  clone_17 = None
    _to_copy_54 = torch.ops.aten._to_copy.default(template_embedder_pairformer_blocks_1_triangle_attention_pair2qkvg1_weight, dtype = torch.bfloat16);  template_embedder_pairformer_blocks_1_triangle_attention_pair2qkvg1_weight = None
    _to_copy_55 = torch.ops.aten._to_copy.default(getitem_86, dtype = torch.bfloat16)
    t_22 = torch.ops.aten.t.default(_to_copy_54);  _to_copy_54 = None
    view_82 = torch.ops.aten.view.default(_to_copy_55, [589824, 64]);  _to_copy_55 = None
    mm_21 = torch.ops.aten.mm.default(view_82, t_22);  view_82 = t_22 = None
    view_83 = torch.ops.aten.view.default(mm_21, [4, 384, 384, 512]);  mm_21 = None
    select_3 = torch.ops.aten.select.int(_unsafe_view_14, 0, 0)
    view_84 = torch.ops.aten.view.default(view_83, [4, 384, 384, 4, 4, 32]);  view_83 = None
    permute_44 = torch.ops.aten.permute.default(view_84, [4, 0, 3, 1, 2, 5]);  view_84 = None
    clone_18 = torch.ops.aten.clone.default(permute_44, memory_format = torch.contiguous_format);  permute_44 = None
    _unsafe_view_15 = torch.ops.aten._unsafe_view.default(clone_18, [4, 16, 384, 384, 32]);  clone_18 = None
    unbind_int_2 = torch.ops.aten.unbind.int(_unsafe_view_15);  _unsafe_view_15 = None
    getitem_89 = unbind_int_2[0]
    getitem_90 = unbind_int_2[1]
    getitem_91 = unbind_int_2[2]
    getitem_92 = unbind_int_2[3];  unbind_int_2 = None
    expand_7 = torch.ops.aten.expand.default(select_3, [16, 384, 384, 384]);  select_3 = None
    _scaled_dot_product_efficient_attention_default_2 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_89, getitem_90, getitem_91, expand_7, False);  getitem_89 = getitem_90 = getitem_91 = expand_7 = None
    getitem_93 = _scaled_dot_product_efficient_attention_default_2[0];  _scaled_dot_product_efficient_attention_default_2 = None
    sigmoid_8 = torch.ops.aten.sigmoid.default(getitem_92);  getitem_92 = None
    mul_10 = torch.ops.aten.mul.Tensor(getitem_93, sigmoid_8);  getitem_93 = sigmoid_8 = None
    view_85 = torch.ops.aten.view.default(mul_10, [4, 4, 384, 384, 32]);  mul_10 = None
    permute_45 = torch.ops.aten.permute.default(view_85, [0, 2, 3, 1, 4]);  view_85 = None
    clone_19 = torch.ops.aten.clone.default(permute_45, memory_format = torch.contiguous_format);  permute_45 = None
    _unsafe_view_16 = torch.ops.aten._unsafe_view.default(clone_19, [4, 384, 384, 128]);  clone_19 = None
    transpose_3 = torch.ops.aten.transpose.int(getitem_86, 1, 2);  getitem_86 = None
    _to_copy_56 = torch.ops.aten._to_copy.default(template_embedder_pairformer_blocks_1_triangle_attention_pair2qkvg2_weight, dtype = torch.bfloat16);  template_embedder_pairformer_blocks_1_triangle_attention_pair2qkvg2_weight = None
    _to_copy_57 = torch.ops.aten._to_copy.default(transpose_3, dtype = torch.bfloat16);  transpose_3 = None
    t_23 = torch.ops.aten.t.default(_to_copy_56);  _to_copy_56 = None
    expand_8 = torch.ops.aten.expand.default(_to_copy_57, [4, 384, 384, 64]);  _to_copy_57 = None
    clone_20 = torch.ops.aten.clone.default(expand_8, memory_format = torch.contiguous_format);  expand_8 = None
    _unsafe_view_17 = torch.ops.aten._unsafe_view.default(clone_20, [1536, 384, 64]);  clone_20 = None
    expand_9 = torch.ops.aten.expand.default(t_23, [4, 384, 64, 512]);  t_23 = None
    view_86 = torch.ops.aten.view.default(expand_9, [1536, 64, 512]);  expand_9 = None
    bmm_5 = torch.ops.aten.bmm.default(_unsafe_view_17, view_86);  _unsafe_view_17 = view_86 = None
    view_87 = torch.ops.aten.view.default(bmm_5, [4, 384, 384, 512]);  bmm_5 = None
    select_4 = torch.ops.aten.select.int(_unsafe_view_14, 0, 1);  _unsafe_view_14 = None
    view_88 = torch.ops.aten.view.default(view_87, [4, 384, 384, 4, 4, 32]);  view_87 = None
    permute_46 = torch.ops.aten.permute.default(view_88, [4, 0, 3, 1, 2, 5]);  view_88 = None
    clone_21 = torch.ops.aten.clone.default(permute_46, memory_format = torch.contiguous_format);  permute_46 = None
    _unsafe_view_18 = torch.ops.aten._unsafe_view.default(clone_21, [4, 16, 384, 384, 32]);  clone_21 = None
    unbind_int_3 = torch.ops.aten.unbind.int(_unsafe_view_18);  _unsafe_view_18 = None
    getitem_97 = unbind_int_3[0]
    getitem_98 = unbind_int_3[1]
    getitem_99 = unbind_int_3[2]
    getitem_100 = unbind_int_3[3];  unbind_int_3 = None
    expand_10 = torch.ops.aten.expand.default(select_4, [16, 384, 384, 384]);  select_4 = None
    _scaled_dot_product_efficient_attention_default_3 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_97, getitem_98, getitem_99, expand_10, False);  getitem_97 = getitem_98 = getitem_99 = expand_10 = None
    getitem_101 = _scaled_dot_product_efficient_attention_default_3[0];  _scaled_dot_product_efficient_attention_default_3 = None
    sigmoid_9 = torch.ops.aten.sigmoid.default(getitem_100);  getitem_100 = None
    mul_11 = torch.ops.aten.mul.Tensor(getitem_101, sigmoid_9);  getitem_101 = sigmoid_9 = None
    view_89 = torch.ops.aten.view.default(mul_11, [4, 4, 384, 384, 32]);  mul_11 = None
    permute_47 = torch.ops.aten.permute.default(view_89, [0, 2, 3, 1, 4]);  view_89 = None
    clone_22 = torch.ops.aten.clone.default(permute_47, memory_format = torch.contiguous_format);  permute_47 = None
    _unsafe_view_19 = torch.ops.aten._unsafe_view.default(clone_22, [4, 384, 384, 128]);  clone_22 = None
    cat_1 = torch.ops.aten.cat.default([_unsafe_view_16, _unsafe_view_19], dim = -1);  _unsafe_view_16 = _unsafe_view_19 = None
    slice_4 = torch.ops.aten.slice.Tensor(template_embedder_pairformer_blocks_1_triangle_attention_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  template_embedder_pairformer_blocks_1_triangle_attention_out_scalers = None
    unsqueeze_24 = torch.ops.aten.unsqueeze.default(slice_4, 1);  slice_4 = None
    mul_12 = torch.ops.aten.mul.Tensor(template_embedder_pairformer_blocks_1_triangle_attention_linear_out_weight, unsqueeze_24);  template_embedder_pairformer_blocks_1_triangle_attention_linear_out_weight = unsqueeze_24 = None
    _to_copy_58 = torch.ops.aten._to_copy.default(mul_12, dtype = torch.bfloat16);  mul_12 = None
    t_24 = torch.ops.aten.t.default(_to_copy_58);  _to_copy_58 = None
    view_90 = torch.ops.aten.view.default(cat_1, [589824, 256]);  cat_1 = None
    mm_22 = torch.ops.aten.mm.default(view_90, t_24);  view_90 = t_24 = None
    view_91 = torch.ops.aten.view.default(mm_22, [4, 384, 384, 64]);  mm_22 = None
    add_9 = torch.ops.aten.add.Tensor(add_8, view_91);  add_8 = view_91 = None
    split_tensor_10 = torch.ops.aten.split.Tensor(add_6, 384, dim = -2);  add_6 = None
    getitem_105 = split_tensor_10[0];  split_tensor_10 = None
    _to_copy_59 = torch.ops.aten._to_copy.default(getitem_105, dtype = torch.float32);  getitem_105 = None
    native_layer_norm_default_12 = torch.ops.aten.native_layer_norm.default(_to_copy_59, [64], template_embedder_pairformer_blocks_1_transition_pair_layer_norm_weight, template_embedder_pairformer_blocks_1_transition_pair_layer_norm_bias, 1e-05);  _to_copy_59 = template_embedder_pairformer_blocks_1_transition_pair_layer_norm_weight = template_embedder_pairformer_blocks_1_transition_pair_layer_norm_bias = None
    getitem_106 = native_layer_norm_default_12[0];  native_layer_norm_default_12 = None
    _to_copy_60 = torch.ops.aten._to_copy.default(template_embedder_pairformer_blocks_1_transition_pair_linear_no_bias_ab_weight, dtype = torch.bfloat16);  template_embedder_pairformer_blocks_1_transition_pair_linear_no_bias_ab_weight = None
    _to_copy_61 = torch.ops.aten._to_copy.default(getitem_106, dtype = torch.bfloat16);  getitem_106 = None
    t_25 = torch.ops.aten.t.default(_to_copy_60);  _to_copy_60 = None
    view_92 = torch.ops.aten.view.default(_to_copy_61, [589824, 64]);  _to_copy_61 = None
    mm_23 = torch.ops.aten.mm.default(view_92, t_25);  view_92 = t_25 = None
    view_93 = torch.ops.aten.view.default(mm_23, [4, 384, 384, 256]);  mm_23 = None
    split_tensor_11 = torch.ops.aten.split.Tensor(view_93, 128, dim = -1);  view_93 = None
    getitem_109 = split_tensor_11[0]
    getitem_110 = split_tensor_11[1];  split_tensor_11 = None
    silu_1 = torch.ops.aten.silu.default(getitem_109);  getitem_109 = None
    mul_13 = torch.ops.aten.mul.Tensor(silu_1, getitem_110);  silu_1 = getitem_110 = None
    _to_copy_62 = torch.ops.aten._to_copy.default(template_embedder_pairformer_blocks_1_transition_pair_linear_out_weight, dtype = torch.bfloat16);  template_embedder_pairformer_blocks_1_transition_pair_linear_out_weight = None
    t_26 = torch.ops.aten.t.default(_to_copy_62);  _to_copy_62 = None
    view_95 = torch.ops.aten.view.default(mul_13, [589824, 128]);  mul_13 = None
    mm_24 = torch.ops.aten.mm.default(view_95, t_26);  view_95 = t_26 = None
    view_96 = torch.ops.aten.view.default(mm_24, [4, 384, 384, 64]);  mm_24 = None
    add_10 = torch.ops.aten.add.Tensor(add_9, view_96);  add_9 = view_96 = None
    view_97 = torch.ops.aten.view.default(add_10, [1, 4, 384, 384, 64]);  add_10 = None
    view_98 = torch.ops.aten.view.default(bitwise_and, [1, 4, 384, 384]);  bitwise_and = None
    _to_copy_63 = torch.ops.aten._to_copy.default(view_97, dtype = torch.float32);  view_97 = None
    native_layer_norm_default_13 = torch.ops.aten.native_layer_norm.default(_to_copy_63, [64], template_embedder_template_layernorm_weight, template_embedder_template_layernorm_bias, 1e-05);  _to_copy_63 = template_embedder_template_layernorm_weight = template_embedder_template_layernorm_bias = None
    getitem_111 = native_layer_norm_default_13[0];  native_layer_norm_default_13 = None
    unsqueeze_25 = torch.ops.aten.unsqueeze.default(view_98, -1);  view_98 = None
    mul_14 = torch.ops.aten.mul.Tensor(getitem_111, unsqueeze_25);  getitem_111 = unsqueeze_25 = None
    sum_2 = torch.ops.aten.sum.dim_IntList(mul_14, [1], dtype = torch.float32);  mul_14 = None
    clamp_min = torch.ops.aten.clamp_min.default(sum_1, 1);  sum_1 = None
    view_99 = torch.ops.aten.view.default(clamp_min, [1, 1, 1, 1]);  clamp_min = None
    div = torch.ops.aten.div.Tensor(sum_2, view_99);  sum_2 = view_99 = None
    relu = torch.ops.aten.relu.default(div);  div = None
    _to_copy_64 = torch.ops.aten._to_copy.default(template_embedder_proj_out_1_weight, dtype = torch.bfloat16);  template_embedder_proj_out_1_weight = None
    _to_copy_65 = torch.ops.aten._to_copy.default(relu, dtype = torch.bfloat16);  relu = None
    t_27 = torch.ops.aten.t.default(_to_copy_64);  _to_copy_64 = None
    view_100 = torch.ops.aten.view.default(_to_copy_65, [147456, 64]);  _to_copy_65 = None
    mm_25 = torch.ops.aten.mm.default(view_100, t_27);  view_100 = t_27 = None
    view_101 = torch.ops.aten.view.default(mm_25, [1, 384, 384, 256]);  mm_25 = None
    add_11 = torch.ops.aten.add.Tensor(add, view_101);  add = view_101 = None
    _to_copy_66 = torch.ops.aten._to_copy.default(msa_module_linear_s2m_weight, dtype = torch.bfloat16);  msa_module_linear_s2m_weight = None
    t_28 = torch.ops.aten.t.default(_to_copy_66);  _to_copy_66 = None
    view_102 = torch.ops.aten.view.default(add_1, [384, 384])
    mm_26 = torch.ops.aten.mm.default(view_102, t_28);  view_102 = t_28 = None
    view_103 = torch.ops.aten.view.default(mm_26, [1, 384, 64]);  mm_26 = None
    view_104 = torch.ops.aten.view.default(view_103, [1, 1, 384, 64]);  view_103 = None
    add_12 = torch.ops.aten.add.Tensor(arg1402_1, view_104);  arg1402_1 = view_104 = None
    slice_5 = torch.ops.aten.slice.Tensor(add_12, dim = 0, start = 0, end = 9223372036854775807)
    slice_6 = torch.ops.aten.slice.Tensor(slice_5, dim = 1, start = 0, end = 4096);  slice_5 = None
    slice_7 = torch.ops.aten.slice.Tensor(slice_6, dim = 2, start = 0, end = 9223372036854775807);  slice_6 = None
    slice_8 = torch.ops.aten.slice.Tensor(slice_7, dim = 3, start = 0, end = 9223372036854775807);  slice_7 = None
    slice_9 = torch.ops.aten.slice.Tensor(arg1403_1, dim = 0, start = 0, end = 9223372036854775807)
    slice_10 = torch.ops.aten.slice.Tensor(slice_9, dim = 1, start = 0, end = 4096);  slice_9 = None
    slice_11 = torch.ops.aten.slice.Tensor(slice_10, dim = 2, start = 0, end = 9223372036854775807);  slice_10 = None
    _to_copy_67 = torch.ops.aten._to_copy.default(slice_8, dtype = torch.float32);  slice_8 = None
    native_layer_norm_default_14 = torch.ops.aten.native_layer_norm.default(_to_copy_67, [64], None, None, 1e-05);  _to_copy_67 = None
    getitem_114 = native_layer_norm_default_14[0];  native_layer_norm_default_14 = None
    view_105 = torch.ops.aten.view.default(slice_11, [1, 4096, 384, 1]);  slice_11 = None
    bitwise_not_6 = torch.ops.aten.bitwise_not.default(view_105);  view_105 = None
    masked_fill_6 = torch.ops.aten.masked_fill.Scalar(getitem_114, bitwise_not_6, 0);  getitem_114 = bitwise_not_6 = None
    unbind_int_4 = torch.ops.aten.unbind.int(msa_module_outer_product_mean_0_weight_ab)
    getitem_117 = unbind_int_4[0]
    getitem_118 = unbind_int_4[1];  unbind_int_4 = None
    _to_copy_68 = torch.ops.aten._to_copy.default(getitem_117, dtype = torch.bfloat16);  getitem_117 = None
    _to_copy_69 = torch.ops.aten._to_copy.default(masked_fill_6, dtype = torch.bfloat16)
    unsqueeze_26 = torch.ops.aten.unsqueeze.default(_to_copy_68, 3);  _to_copy_68 = None
    unsqueeze_27 = torch.ops.aten.unsqueeze.default(unsqueeze_26, 4);  unsqueeze_26 = None
    unsqueeze_28 = torch.ops.aten.unsqueeze.default(unsqueeze_27, 5);  unsqueeze_27 = None
    permute_48 = torch.ops.aten.permute.default(unsqueeze_28, [0, 1, 3, 4, 5, 2]);  unsqueeze_28 = None
    unsqueeze_29 = torch.ops.aten.unsqueeze.default(_to_copy_69, 4);  _to_copy_69 = None
    unsqueeze_30 = torch.ops.aten.unsqueeze.default(unsqueeze_29, 5);  unsqueeze_29 = None
    permute_49 = torch.ops.aten.permute.default(unsqueeze_30, [4, 5, 0, 1, 2, 3]);  unsqueeze_30 = None
    permute_50 = torch.ops.aten.permute.default(permute_48, [0, 1, 5, 2, 3, 4]);  permute_48 = None
    view_106 = torch.ops.aten.view.default(permute_50, [1, 64, 64]);  permute_50 = None
    permute_51 = torch.ops.aten.permute.default(permute_49, [5, 2, 3, 4, 0, 1]);  permute_49 = None
    view_107 = torch.ops.aten.view.default(permute_51, [1, 64, 1572864]);  permute_51 = None
    bmm_6 = torch.ops.aten.bmm.default(view_106, view_107);  view_106 = view_107 = None
    view_108 = torch.ops.aten.view.default(bmm_6, [8, 8, 1, 1, 4096, 384]);  bmm_6 = None
    permute_52 = torch.ops.aten.permute.default(view_108, [0, 1, 3, 4, 5, 2]);  view_108 = None
    view_109 = torch.ops.aten.view.default(permute_52, [8, 8, 1, 4096, 384]);  permute_52 = None
    _to_copy_70 = torch.ops.aten._to_copy.default(getitem_118, dtype = torch.bfloat16);  getitem_118 = None
    _to_copy_71 = torch.ops.aten._to_copy.default(masked_fill_6, dtype = torch.bfloat16);  masked_fill_6 = None
    unsqueeze_31 = torch.ops.aten.unsqueeze.default(_to_copy_70, 3);  _to_copy_70 = None
    unsqueeze_32 = torch.ops.aten.unsqueeze.default(unsqueeze_31, 4);  unsqueeze_31 = None
    unsqueeze_33 = torch.ops.aten.unsqueeze.default(unsqueeze_32, 5);  unsqueeze_32 = None
    permute_53 = torch.ops.aten.permute.default(unsqueeze_33, [0, 1, 3, 4, 5, 2]);  unsqueeze_33 = None
    unsqueeze_34 = torch.ops.aten.unsqueeze.default(_to_copy_71, 4);  _to_copy_71 = None
    unsqueeze_35 = torch.ops.aten.unsqueeze.default(unsqueeze_34, 5);  unsqueeze_34 = None
    permute_54 = torch.ops.aten.permute.default(unsqueeze_35, [4, 5, 0, 1, 2, 3]);  unsqueeze_35 = None
    permute_55 = torch.ops.aten.permute.default(permute_53, [0, 1, 5, 2, 3, 4]);  permute_53 = None
    view_110 = torch.ops.aten.view.default(permute_55, [1, 64, 64]);  permute_55 = None
    permute_56 = torch.ops.aten.permute.default(permute_54, [5, 2, 3, 4, 0, 1]);  permute_54 = None
    view_111 = torch.ops.aten.view.default(permute_56, [1, 64, 1572864]);  permute_56 = None
    bmm_7 = torch.ops.aten.bmm.default(view_110, view_111);  view_110 = view_111 = None
    view_112 = torch.ops.aten.view.default(bmm_7, [8, 8, 1, 1, 4096, 384]);  bmm_7 = None
    permute_57 = torch.ops.aten.permute.default(view_112, [0, 1, 3, 4, 5, 2]);  view_112 = None
    view_113 = torch.ops.aten.view.default(permute_57, [8, 8, 1, 4096, 384]);  permute_57 = None
    unsqueeze_36 = torch.ops.aten.unsqueeze.default(view_109, 5);  view_109 = None
    unsqueeze_37 = torch.ops.aten.unsqueeze.default(unsqueeze_36, 6);  unsqueeze_36 = None
    permute_58 = torch.ops.aten.permute.default(unsqueeze_37, [2, 4, 5, 0, 1, 6, 3]);  unsqueeze_37 = None
    unsqueeze_38 = torch.ops.aten.unsqueeze.default(view_113, 5);  view_113 = None
    unsqueeze_39 = torch.ops.aten.unsqueeze.default(unsqueeze_38, 6);  unsqueeze_38 = None
    permute_59 = torch.ops.aten.permute.default(unsqueeze_39, [2, 5, 4, 0, 6, 1, 3]);  unsqueeze_39 = None
    permute_60 = torch.ops.aten.permute.default(permute_58, [3, 1, 4, 6, 0, 2, 5]);  permute_58 = None
    clone_23 = torch.ops.aten.clone.default(permute_60, memory_format = torch.contiguous_format);  permute_60 = None
    _unsafe_view_20 = torch.ops.aten._unsafe_view.default(clone_23, [8, 3072, 4096]);  clone_23 = None
    permute_61 = torch.ops.aten.permute.default(permute_59, [3, 6, 0, 2, 5, 1, 4]);  permute_59 = None
    clone_24 = torch.ops.aten.clone.default(permute_61, memory_format = torch.contiguous_format);  permute_61 = None
    _unsafe_view_21 = torch.ops.aten._unsafe_view.default(clone_24, [8, 4096, 3072]);  clone_24 = None
    bmm_8 = torch.ops.aten.bmm.default(_unsafe_view_20, _unsafe_view_21);  _unsafe_view_20 = _unsafe_view_21 = None
    view_114 = torch.ops.aten.view.default(bmm_8, [8, 384, 8, 1, 1, 384, 8]);  bmm_8 = None
    permute_62 = torch.ops.aten.permute.default(view_114, [4, 1, 5, 0, 2, 6, 3]);  view_114 = None
    view_115 = torch.ops.aten.view.default(permute_62, [1, 384, 384, 8, 8, 8]);  permute_62 = None
    clone_25 = torch.ops.aten.clone.default(view_115, memory_format = torch.contiguous_format);  view_115 = None
    _unsafe_view_22 = torch.ops.aten._unsafe_view.default(clone_25, [1, 384, 384, 512]);  clone_25 = None
    add_13 = torch.ops.aten.add.Tensor(_unsafe_view_22, 0);  _unsafe_view_22 = None
    slice_12 = torch.ops.aten.slice.Tensor(add_12, dim = 0, start = 0, end = 9223372036854775807)
    slice_13 = torch.ops.aten.slice.Tensor(slice_12, dim = 1, start = 4096, end = 8192);  slice_12 = None
    slice_14 = torch.ops.aten.slice.Tensor(slice_13, dim = 2, start = 0, end = 9223372036854775807);  slice_13 = None
    slice_15 = torch.ops.aten.slice.Tensor(slice_14, dim = 3, start = 0, end = 9223372036854775807);  slice_14 = None
    slice_16 = torch.ops.aten.slice.Tensor(arg1403_1, dim = 0, start = 0, end = 9223372036854775807)
    slice_17 = torch.ops.aten.slice.Tensor(slice_16, dim = 1, start = 4096, end = 8192);  slice_16 = None
    slice_18 = torch.ops.aten.slice.Tensor(slice_17, dim = 2, start = 0, end = 9223372036854775807);  slice_17 = None
    _to_copy_72 = torch.ops.aten._to_copy.default(slice_15, dtype = torch.float32);  slice_15 = None
    native_layer_norm_default_15 = torch.ops.aten.native_layer_norm.default(_to_copy_72, [64], None, None, 1e-05);  _to_copy_72 = None
    getitem_119 = native_layer_norm_default_15[0];  native_layer_norm_default_15 = None
    view_116 = torch.ops.aten.view.default(slice_18, [1, 4096, 384, 1]);  slice_18 = None
    bitwise_not_7 = torch.ops.aten.bitwise_not.default(view_116);  view_116 = None
    masked_fill_7 = torch.ops.aten.masked_fill.Scalar(getitem_119, bitwise_not_7, 0);  getitem_119 = bitwise_not_7 = None
    unbind_int_5 = torch.ops.aten.unbind.int(msa_module_outer_product_mean_0_weight_ab)
    getitem_122 = unbind_int_5[0]
    getitem_123 = unbind_int_5[1];  unbind_int_5 = None
    _to_copy_73 = torch.ops.aten._to_copy.default(getitem_122, dtype = torch.bfloat16);  getitem_122 = None
    _to_copy_74 = torch.ops.aten._to_copy.default(masked_fill_7, dtype = torch.bfloat16)
    unsqueeze_40 = torch.ops.aten.unsqueeze.default(_to_copy_73, 3);  _to_copy_73 = None
    unsqueeze_41 = torch.ops.aten.unsqueeze.default(unsqueeze_40, 4);  unsqueeze_40 = None
    unsqueeze_42 = torch.ops.aten.unsqueeze.default(unsqueeze_41, 5);  unsqueeze_41 = None
    permute_63 = torch.ops.aten.permute.default(unsqueeze_42, [0, 1, 3, 4, 5, 2]);  unsqueeze_42 = None
    unsqueeze_43 = torch.ops.aten.unsqueeze.default(_to_copy_74, 4);  _to_copy_74 = None
    unsqueeze_44 = torch.ops.aten.unsqueeze.default(unsqueeze_43, 5);  unsqueeze_43 = None
    permute_64 = torch.ops.aten.permute.default(unsqueeze_44, [4, 5, 0, 1, 2, 3]);  unsqueeze_44 = None
    permute_65 = torch.ops.aten.permute.default(permute_63, [0, 1, 5, 2, 3, 4]);  permute_63 = None
    view_117 = torch.ops.aten.view.default(permute_65, [1, 64, 64]);  permute_65 = None
    permute_66 = torch.ops.aten.permute.default(permute_64, [5, 2, 3, 4, 0, 1]);  permute_64 = None
    view_118 = torch.ops.aten.view.default(permute_66, [1, 64, 1572864]);  permute_66 = None
    bmm_9 = torch.ops.aten.bmm.default(view_117, view_118);  view_117 = view_118 = None
    view_119 = torch.ops.aten.view.default(bmm_9, [8, 8, 1, 1, 4096, 384]);  bmm_9 = None
    permute_67 = torch.ops.aten.permute.default(view_119, [0, 1, 3, 4, 5, 2]);  view_119 = None
    view_120 = torch.ops.aten.view.default(permute_67, [8, 8, 1, 4096, 384]);  permute_67 = None
    _to_copy_75 = torch.ops.aten._to_copy.default(getitem_123, dtype = torch.bfloat16);  getitem_123 = None
    _to_copy_76 = torch.ops.aten._to_copy.default(masked_fill_7, dtype = torch.bfloat16);  masked_fill_7 = None
    unsqueeze_45 = torch.ops.aten.unsqueeze.default(_to_copy_75, 3);  _to_copy_75 = None
    unsqueeze_46 = torch.ops.aten.unsqueeze.default(unsqueeze_45, 4);  unsqueeze_45 = None
    unsqueeze_47 = torch.ops.aten.unsqueeze.default(unsqueeze_46, 5);  unsqueeze_46 = None
    permute_68 = torch.ops.aten.permute.default(unsqueeze_47, [0, 1, 3, 4, 5, 2]);  unsqueeze_47 = None
    unsqueeze_48 = torch.ops.aten.unsqueeze.default(_to_copy_76, 4);  _to_copy_76 = None
    unsqueeze_49 = torch.ops.aten.unsqueeze.default(unsqueeze_48, 5);  unsqueeze_48 = None
    permute_69 = torch.ops.aten.permute.default(unsqueeze_49, [4, 5, 0, 1, 2, 3]);  unsqueeze_49 = None
    permute_70 = torch.ops.aten.permute.default(permute_68, [0, 1, 5, 2, 3, 4]);  permute_68 = None
    view_121 = torch.ops.aten.view.default(permute_70, [1, 64, 64]);  permute_70 = None
    permute_71 = torch.ops.aten.permute.default(permute_69, [5, 2, 3, 4, 0, 1]);  permute_69 = None
    view_122 = torch.ops.aten.view.default(permute_71, [1, 64, 1572864]);  permute_71 = None
    bmm_10 = torch.ops.aten.bmm.default(view_121, view_122);  view_121 = view_122 = None
    view_123 = torch.ops.aten.view.default(bmm_10, [8, 8, 1, 1, 4096, 384]);  bmm_10 = None
    permute_72 = torch.ops.aten.permute.default(view_123, [0, 1, 3, 4, 5, 2]);  view_123 = None
    view_124 = torch.ops.aten.view.default(permute_72, [8, 8, 1, 4096, 384]);  permute_72 = None
    unsqueeze_50 = torch.ops.aten.unsqueeze.default(view_120, 5);  view_120 = None
    unsqueeze_51 = torch.ops.aten.unsqueeze.default(unsqueeze_50, 6);  unsqueeze_50 = None
    permute_73 = torch.ops.aten.permute.default(unsqueeze_51, [2, 4, 5, 0, 1, 6, 3]);  unsqueeze_51 = None
    unsqueeze_52 = torch.ops.aten.unsqueeze.default(view_124, 5);  view_124 = None
    unsqueeze_53 = torch.ops.aten.unsqueeze.default(unsqueeze_52, 6);  unsqueeze_52 = None
    permute_74 = torch.ops.aten.permute.default(unsqueeze_53, [2, 5, 4, 0, 6, 1, 3]);  unsqueeze_53 = None
    permute_75 = torch.ops.aten.permute.default(permute_73, [3, 1, 4, 6, 0, 2, 5]);  permute_73 = None
    clone_26 = torch.ops.aten.clone.default(permute_75, memory_format = torch.contiguous_format);  permute_75 = None
    _unsafe_view_23 = torch.ops.aten._unsafe_view.default(clone_26, [8, 3072, 4096]);  clone_26 = None
    permute_76 = torch.ops.aten.permute.default(permute_74, [3, 6, 0, 2, 5, 1, 4]);  permute_74 = None
    clone_27 = torch.ops.aten.clone.default(permute_76, memory_format = torch.contiguous_format);  permute_76 = None
    _unsafe_view_24 = torch.ops.aten._unsafe_view.default(clone_27, [8, 4096, 3072]);  clone_27 = None
    bmm_11 = torch.ops.aten.bmm.default(_unsafe_view_23, _unsafe_view_24);  _unsafe_view_23 = _unsafe_view_24 = None
    view_125 = torch.ops.aten.view.default(bmm_11, [8, 384, 8, 1, 1, 384, 8]);  bmm_11 = None
    permute_77 = torch.ops.aten.permute.default(view_125, [4, 1, 5, 0, 2, 6, 3]);  view_125 = None
    view_126 = torch.ops.aten.view.default(permute_77, [1, 384, 384, 8, 8, 8]);  permute_77 = None
    clone_28 = torch.ops.aten.clone.default(view_126, memory_format = torch.contiguous_format);  view_126 = None
    _unsafe_view_25 = torch.ops.aten._unsafe_view.default(clone_28, [1, 384, 384, 512]);  clone_28 = None
    add_14 = torch.ops.aten.add.Tensor(add_13, _unsafe_view_25);  add_13 = _unsafe_view_25 = None
    slice_19 = torch.ops.aten.slice.Tensor(add_12, dim = 0, start = 0, end = 9223372036854775807)
    slice_20 = torch.ops.aten.slice.Tensor(slice_19, dim = 1, start = 8192, end = 12288);  slice_19 = None
    slice_21 = torch.ops.aten.slice.Tensor(slice_20, dim = 2, start = 0, end = 9223372036854775807);  slice_20 = None
    slice_22 = torch.ops.aten.slice.Tensor(slice_21, dim = 3, start = 0, end = 9223372036854775807);  slice_21 = None
    slice_23 = torch.ops.aten.slice.Tensor(arg1403_1, dim = 0, start = 0, end = 9223372036854775807)
    slice_24 = torch.ops.aten.slice.Tensor(slice_23, dim = 1, start = 8192, end = 12288);  slice_23 = None
    slice_25 = torch.ops.aten.slice.Tensor(slice_24, dim = 2, start = 0, end = 9223372036854775807);  slice_24 = None
    _to_copy_77 = torch.ops.aten._to_copy.default(slice_22, dtype = torch.float32);  slice_22 = None
    native_layer_norm_default_16 = torch.ops.aten.native_layer_norm.default(_to_copy_77, [64], None, None, 1e-05);  _to_copy_77 = None
    getitem_124 = native_layer_norm_default_16[0];  native_layer_norm_default_16 = None
    view_127 = torch.ops.aten.view.default(slice_25, [1, 4096, 384, 1]);  slice_25 = None
    bitwise_not_8 = torch.ops.aten.bitwise_not.default(view_127);  view_127 = None
    masked_fill_8 = torch.ops.aten.masked_fill.Scalar(getitem_124, bitwise_not_8, 0);  getitem_124 = bitwise_not_8 = None
    unbind_int_6 = torch.ops.aten.unbind.int(msa_module_outer_product_mean_0_weight_ab)
    getitem_127 = unbind_int_6[0]
    getitem_128 = unbind_int_6[1];  unbind_int_6 = None
    _to_copy_78 = torch.ops.aten._to_copy.default(getitem_127, dtype = torch.bfloat16);  getitem_127 = None
    _to_copy_79 = torch.ops.aten._to_copy.default(masked_fill_8, dtype = torch.bfloat16)
    unsqueeze_54 = torch.ops.aten.unsqueeze.default(_to_copy_78, 3);  _to_copy_78 = None
    unsqueeze_55 = torch.ops.aten.unsqueeze.default(unsqueeze_54, 4);  unsqueeze_54 = None
    unsqueeze_56 = torch.ops.aten.unsqueeze.default(unsqueeze_55, 5);  unsqueeze_55 = None
    permute_78 = torch.ops.aten.permute.default(unsqueeze_56, [0, 1, 3, 4, 5, 2]);  unsqueeze_56 = None
    unsqueeze_57 = torch.ops.aten.unsqueeze.default(_to_copy_79, 4);  _to_copy_79 = None
    unsqueeze_58 = torch.ops.aten.unsqueeze.default(unsqueeze_57, 5);  unsqueeze_57 = None
    permute_79 = torch.ops.aten.permute.default(unsqueeze_58, [4, 5, 0, 1, 2, 3]);  unsqueeze_58 = None
    permute_80 = torch.ops.aten.permute.default(permute_78, [0, 1, 5, 2, 3, 4]);  permute_78 = None
    view_128 = torch.ops.aten.view.default(permute_80, [1, 64, 64]);  permute_80 = None
    permute_81 = torch.ops.aten.permute.default(permute_79, [5, 2, 3, 4, 0, 1]);  permute_79 = None
    view_129 = torch.ops.aten.view.default(permute_81, [1, 64, 1572864]);  permute_81 = None
    bmm_12 = torch.ops.aten.bmm.default(view_128, view_129);  view_128 = view_129 = None
    view_130 = torch.ops.aten.view.default(bmm_12, [8, 8, 1, 1, 4096, 384]);  bmm_12 = None
    permute_82 = torch.ops.aten.permute.default(view_130, [0, 1, 3, 4, 5, 2]);  view_130 = None
    view_131 = torch.ops.aten.view.default(permute_82, [8, 8, 1, 4096, 384]);  permute_82 = None
    _to_copy_80 = torch.ops.aten._to_copy.default(getitem_128, dtype = torch.bfloat16);  getitem_128 = None
    _to_copy_81 = torch.ops.aten._to_copy.default(masked_fill_8, dtype = torch.bfloat16);  masked_fill_8 = None
    unsqueeze_59 = torch.ops.aten.unsqueeze.default(_to_copy_80, 3);  _to_copy_80 = None
    unsqueeze_60 = torch.ops.aten.unsqueeze.default(unsqueeze_59, 4);  unsqueeze_59 = None
    unsqueeze_61 = torch.ops.aten.unsqueeze.default(unsqueeze_60, 5);  unsqueeze_60 = None
    permute_83 = torch.ops.aten.permute.default(unsqueeze_61, [0, 1, 3, 4, 5, 2]);  unsqueeze_61 = None
    unsqueeze_62 = torch.ops.aten.unsqueeze.default(_to_copy_81, 4);  _to_copy_81 = None
    unsqueeze_63 = torch.ops.aten.unsqueeze.default(unsqueeze_62, 5);  unsqueeze_62 = None
    permute_84 = torch.ops.aten.permute.default(unsqueeze_63, [4, 5, 0, 1, 2, 3]);  unsqueeze_63 = None
    permute_85 = torch.ops.aten.permute.default(permute_83, [0, 1, 5, 2, 3, 4]);  permute_83 = None
    view_132 = torch.ops.aten.view.default(permute_85, [1, 64, 64]);  permute_85 = None
    permute_86 = torch.ops.aten.permute.default(permute_84, [5, 2, 3, 4, 0, 1]);  permute_84 = None
    view_133 = torch.ops.aten.view.default(permute_86, [1, 64, 1572864]);  permute_86 = None
    bmm_13 = torch.ops.aten.bmm.default(view_132, view_133);  view_132 = view_133 = None
    view_134 = torch.ops.aten.view.default(bmm_13, [8, 8, 1, 1, 4096, 384]);  bmm_13 = None
    permute_87 = torch.ops.aten.permute.default(view_134, [0, 1, 3, 4, 5, 2]);  view_134 = None
    view_135 = torch.ops.aten.view.default(permute_87, [8, 8, 1, 4096, 384]);  permute_87 = None
    unsqueeze_64 = torch.ops.aten.unsqueeze.default(view_131, 5);  view_131 = None
    unsqueeze_65 = torch.ops.aten.unsqueeze.default(unsqueeze_64, 6);  unsqueeze_64 = None
    permute_88 = torch.ops.aten.permute.default(unsqueeze_65, [2, 4, 5, 0, 1, 6, 3]);  unsqueeze_65 = None
    unsqueeze_66 = torch.ops.aten.unsqueeze.default(view_135, 5);  view_135 = None
    unsqueeze_67 = torch.ops.aten.unsqueeze.default(unsqueeze_66, 6);  unsqueeze_66 = None
    permute_89 = torch.ops.aten.permute.default(unsqueeze_67, [2, 5, 4, 0, 6, 1, 3]);  unsqueeze_67 = None
    permute_90 = torch.ops.aten.permute.default(permute_88, [3, 1, 4, 6, 0, 2, 5]);  permute_88 = None
    clone_29 = torch.ops.aten.clone.default(permute_90, memory_format = torch.contiguous_format);  permute_90 = None
    _unsafe_view_26 = torch.ops.aten._unsafe_view.default(clone_29, [8, 3072, 4096]);  clone_29 = None
    permute_91 = torch.ops.aten.permute.default(permute_89, [3, 6, 0, 2, 5, 1, 4]);  permute_89 = None
    clone_30 = torch.ops.aten.clone.default(permute_91, memory_format = torch.contiguous_format);  permute_91 = None
    _unsafe_view_27 = torch.ops.aten._unsafe_view.default(clone_30, [8, 4096, 3072]);  clone_30 = None
    bmm_14 = torch.ops.aten.bmm.default(_unsafe_view_26, _unsafe_view_27);  _unsafe_view_26 = _unsafe_view_27 = None
    view_136 = torch.ops.aten.view.default(bmm_14, [8, 384, 8, 1, 1, 384, 8]);  bmm_14 = None
    permute_92 = torch.ops.aten.permute.default(view_136, [4, 1, 5, 0, 2, 6, 3]);  view_136 = None
    view_137 = torch.ops.aten.view.default(permute_92, [1, 384, 384, 8, 8, 8]);  permute_92 = None
    clone_31 = torch.ops.aten.clone.default(view_137, memory_format = torch.contiguous_format);  view_137 = None
    _unsafe_view_28 = torch.ops.aten._unsafe_view.default(clone_31, [1, 384, 384, 512]);  clone_31 = None
    add_15 = torch.ops.aten.add.Tensor(add_14, _unsafe_view_28);  add_14 = _unsafe_view_28 = None
    slice_26 = torch.ops.aten.slice.Tensor(add_12, dim = 0, start = 0, end = 9223372036854775807)
    slice_27 = torch.ops.aten.slice.Tensor(slice_26, dim = 1, start = 12288, end = 16384);  slice_26 = None
    slice_28 = torch.ops.aten.slice.Tensor(slice_27, dim = 2, start = 0, end = 9223372036854775807);  slice_27 = None
    slice_29 = torch.ops.aten.slice.Tensor(slice_28, dim = 3, start = 0, end = 9223372036854775807);  slice_28 = None
    slice_30 = torch.ops.aten.slice.Tensor(arg1403_1, dim = 0, start = 0, end = 9223372036854775807)
    slice_31 = torch.ops.aten.slice.Tensor(slice_30, dim = 1, start = 12288, end = 16384);  slice_30 = None
    slice_32 = torch.ops.aten.slice.Tensor(slice_31, dim = 2, start = 0, end = 9223372036854775807);  slice_31 = None
    _to_copy_82 = torch.ops.aten._to_copy.default(slice_29, dtype = torch.float32);  slice_29 = None
    native_layer_norm_default_17 = torch.ops.aten.native_layer_norm.default(_to_copy_82, [64], None, None, 1e-05);  _to_copy_82 = None
    getitem_129 = native_layer_norm_default_17[0];  native_layer_norm_default_17 = None
    view_138 = torch.ops.aten.view.default(slice_32, [1, 4096, 384, 1]);  slice_32 = None
    bitwise_not_9 = torch.ops.aten.bitwise_not.default(view_138);  view_138 = None
    masked_fill_9 = torch.ops.aten.masked_fill.Scalar(getitem_129, bitwise_not_9, 0);  getitem_129 = bitwise_not_9 = None
    unbind_int_7 = torch.ops.aten.unbind.int(msa_module_outer_product_mean_0_weight_ab);  msa_module_outer_product_mean_0_weight_ab = None
    getitem_132 = unbind_int_7[0]
    getitem_133 = unbind_int_7[1];  unbind_int_7 = None
    _to_copy_83 = torch.ops.aten._to_copy.default(getitem_132, dtype = torch.bfloat16);  getitem_132 = None
    _to_copy_84 = torch.ops.aten._to_copy.default(masked_fill_9, dtype = torch.bfloat16)
    unsqueeze_68 = torch.ops.aten.unsqueeze.default(_to_copy_83, 3);  _to_copy_83 = None
    unsqueeze_69 = torch.ops.aten.unsqueeze.default(unsqueeze_68, 4);  unsqueeze_68 = None
    unsqueeze_70 = torch.ops.aten.unsqueeze.default(unsqueeze_69, 5);  unsqueeze_69 = None
    permute_93 = torch.ops.aten.permute.default(unsqueeze_70, [0, 1, 3, 4, 5, 2]);  unsqueeze_70 = None
    unsqueeze_71 = torch.ops.aten.unsqueeze.default(_to_copy_84, 4);  _to_copy_84 = None
    unsqueeze_72 = torch.ops.aten.unsqueeze.default(unsqueeze_71, 5);  unsqueeze_71 = None
    permute_94 = torch.ops.aten.permute.default(unsqueeze_72, [4, 5, 0, 1, 2, 3]);  unsqueeze_72 = None
    permute_95 = torch.ops.aten.permute.default(permute_93, [0, 1, 5, 2, 3, 4]);  permute_93 = None
    view_139 = torch.ops.aten.view.default(permute_95, [1, 64, 64]);  permute_95 = None
    permute_96 = torch.ops.aten.permute.default(permute_94, [5, 2, 3, 4, 0, 1]);  permute_94 = None
    view_140 = torch.ops.aten.view.default(permute_96, [1, 64, 1572864]);  permute_96 = None
    bmm_15 = torch.ops.aten.bmm.default(view_139, view_140);  view_139 = view_140 = None
    view_141 = torch.ops.aten.view.default(bmm_15, [8, 8, 1, 1, 4096, 384]);  bmm_15 = None
    permute_97 = torch.ops.aten.permute.default(view_141, [0, 1, 3, 4, 5, 2]);  view_141 = None
    view_142 = torch.ops.aten.view.default(permute_97, [8, 8, 1, 4096, 384]);  permute_97 = None
    _to_copy_85 = torch.ops.aten._to_copy.default(getitem_133, dtype = torch.bfloat16);  getitem_133 = None
    _to_copy_86 = torch.ops.aten._to_copy.default(masked_fill_9, dtype = torch.bfloat16);  masked_fill_9 = None
    unsqueeze_73 = torch.ops.aten.unsqueeze.default(_to_copy_85, 3);  _to_copy_85 = None
    unsqueeze_74 = torch.ops.aten.unsqueeze.default(unsqueeze_73, 4);  unsqueeze_73 = None
    unsqueeze_75 = torch.ops.aten.unsqueeze.default(unsqueeze_74, 5);  unsqueeze_74 = None
    permute_98 = torch.ops.aten.permute.default(unsqueeze_75, [0, 1, 3, 4, 5, 2]);  unsqueeze_75 = None
    unsqueeze_76 = torch.ops.aten.unsqueeze.default(_to_copy_86, 4);  _to_copy_86 = None
    unsqueeze_77 = torch.ops.aten.unsqueeze.default(unsqueeze_76, 5);  unsqueeze_76 = None
    permute_99 = torch.ops.aten.permute.default(unsqueeze_77, [4, 5, 0, 1, 2, 3]);  unsqueeze_77 = None
    permute_100 = torch.ops.aten.permute.default(permute_98, [0, 1, 5, 2, 3, 4]);  permute_98 = None
    view_143 = torch.ops.aten.view.default(permute_100, [1, 64, 64]);  permute_100 = None
    permute_101 = torch.ops.aten.permute.default(permute_99, [5, 2, 3, 4, 0, 1]);  permute_99 = None
    view_144 = torch.ops.aten.view.default(permute_101, [1, 64, 1572864]);  permute_101 = None
    bmm_16 = torch.ops.aten.bmm.default(view_143, view_144);  view_143 = view_144 = None
    view_145 = torch.ops.aten.view.default(bmm_16, [8, 8, 1, 1, 4096, 384]);  bmm_16 = None
    permute_102 = torch.ops.aten.permute.default(view_145, [0, 1, 3, 4, 5, 2]);  view_145 = None
    view_146 = torch.ops.aten.view.default(permute_102, [8, 8, 1, 4096, 384]);  permute_102 = None
    unsqueeze_78 = torch.ops.aten.unsqueeze.default(view_142, 5);  view_142 = None
    unsqueeze_79 = torch.ops.aten.unsqueeze.default(unsqueeze_78, 6);  unsqueeze_78 = None
    permute_103 = torch.ops.aten.permute.default(unsqueeze_79, [2, 4, 5, 0, 1, 6, 3]);  unsqueeze_79 = None
    unsqueeze_80 = torch.ops.aten.unsqueeze.default(view_146, 5);  view_146 = None
    unsqueeze_81 = torch.ops.aten.unsqueeze.default(unsqueeze_80, 6);  unsqueeze_80 = None
    permute_104 = torch.ops.aten.permute.default(unsqueeze_81, [2, 5, 4, 0, 6, 1, 3]);  unsqueeze_81 = None
    permute_105 = torch.ops.aten.permute.default(permute_103, [3, 1, 4, 6, 0, 2, 5]);  permute_103 = None
    clone_32 = torch.ops.aten.clone.default(permute_105, memory_format = torch.contiguous_format);  permute_105 = None
    _unsafe_view_29 = torch.ops.aten._unsafe_view.default(clone_32, [8, 3072, 4096]);  clone_32 = None
    permute_106 = torch.ops.aten.permute.default(permute_104, [3, 6, 0, 2, 5, 1, 4]);  permute_104 = None
    clone_33 = torch.ops.aten.clone.default(permute_106, memory_format = torch.contiguous_format);  permute_106 = None
    _unsafe_view_30 = torch.ops.aten._unsafe_view.default(clone_33, [8, 4096, 3072]);  clone_33 = None
    bmm_17 = torch.ops.aten.bmm.default(_unsafe_view_29, _unsafe_view_30);  _unsafe_view_29 = _unsafe_view_30 = None
    view_147 = torch.ops.aten.view.default(bmm_17, [8, 384, 8, 1, 1, 384, 8]);  bmm_17 = None
    permute_107 = torch.ops.aten.permute.default(view_147, [4, 1, 5, 0, 2, 6, 3]);  view_147 = None
    view_148 = torch.ops.aten.view.default(permute_107, [1, 384, 384, 8, 8, 8]);  permute_107 = None
    clone_34 = torch.ops.aten.clone.default(view_148, memory_format = torch.contiguous_format);  view_148 = None
    _unsafe_view_31 = torch.ops.aten._unsafe_view.default(clone_34, [1, 384, 384, 512]);  clone_34 = None
    add_16 = torch.ops.aten.add.Tensor(add_15, _unsafe_view_31);  add_15 = _unsafe_view_31 = None
    _to_copy_87 = torch.ops.aten._to_copy.default(add_16, dtype = torch.float32);  add_16 = None
    native_layer_norm_default_18 = torch.ops.aten.native_layer_norm.default(_to_copy_87, [512], msa_module_outer_product_mean_0_ln_out_weight, msa_module_outer_product_mean_0_ln_out_bias, 0.1);  _to_copy_87 = msa_module_outer_product_mean_0_ln_out_weight = msa_module_outer_product_mean_0_ln_out_bias = None
    getitem_134 = native_layer_norm_default_18[0];  native_layer_norm_default_18 = None
    _to_copy_88 = torch.ops.aten._to_copy.default(msa_module_outer_product_mean_0_linear_out_bias, dtype = torch.bfloat16);  msa_module_outer_product_mean_0_linear_out_bias = None
    _to_copy_89 = torch.ops.aten._to_copy.default(msa_module_outer_product_mean_0_linear_out_weight, dtype = torch.bfloat16);  msa_module_outer_product_mean_0_linear_out_weight = None
    _to_copy_90 = torch.ops.aten._to_copy.default(getitem_134, dtype = torch.bfloat16);  getitem_134 = None
    view_149 = torch.ops.aten.view.default(_to_copy_90, [147456, 512]);  _to_copy_90 = None
    t_29 = torch.ops.aten.t.default(_to_copy_89);  _to_copy_89 = None
    addmm = torch.ops.aten.addmm.default(_to_copy_88, view_149, t_29);  _to_copy_88 = view_149 = t_29 = None
    view_150 = torch.ops.aten.view.default(addmm, [1, 384, 384, 256]);  addmm = None
    add_17 = torch.ops.aten.add.Tensor(add_11, view_150);  view_150 = None
    split_tensor_12 = torch.ops.aten.split.Tensor(add_12, 128, dim = -2)
    getitem_137 = split_tensor_12[0]
    getitem_138 = split_tensor_12[1]
    getitem_139 = split_tensor_12[2];  split_tensor_12 = None
    _to_copy_91 = torch.ops.aten._to_copy.default(getitem_137, dtype = torch.float32);  getitem_137 = None
    native_layer_norm_default_19 = torch.ops.aten.native_layer_norm.default(_to_copy_91, [64], msa_module_msa_transition_0_layer_norm_weight, msa_module_msa_transition_0_layer_norm_bias, 1e-05);  _to_copy_91 = None
    getitem_140 = native_layer_norm_default_19[0];  native_layer_norm_default_19 = None
    _to_copy_92 = torch.ops.aten._to_copy.default(msa_module_msa_transition_0_linear_no_bias_ab_weight, dtype = torch.bfloat16)
    _to_copy_93 = torch.ops.aten._to_copy.default(getitem_140, dtype = torch.bfloat16);  getitem_140 = None
    t_30 = torch.ops.aten.t.default(_to_copy_92);  _to_copy_92 = None
    view_151 = torch.ops.aten.view.default(_to_copy_93, [2097152, 64]);  _to_copy_93 = None
    mm_27 = torch.ops.aten.mm.default(view_151, t_30);  view_151 = t_30 = None
    view_152 = torch.ops.aten.view.default(mm_27, [1, 16384, 128, 512]);  mm_27 = None
    split_tensor_13 = torch.ops.aten.split.Tensor(view_152, 256, dim = -1);  view_152 = None
    getitem_143 = split_tensor_13[0]
    getitem_144 = split_tensor_13[1];  split_tensor_13 = None
    silu_2 = torch.ops.aten.silu.default(getitem_143);  getitem_143 = None
    mul_15 = torch.ops.aten.mul.Tensor(silu_2, getitem_144);  silu_2 = getitem_144 = None
    _to_copy_94 = torch.ops.aten._to_copy.default(msa_module_msa_transition_0_linear_out_weight, dtype = torch.bfloat16)
    t_31 = torch.ops.aten.t.default(_to_copy_94);  _to_copy_94 = None
    view_154 = torch.ops.aten.view.default(mul_15, [2097152, 256]);  mul_15 = None
    mm_28 = torch.ops.aten.mm.default(view_154, t_31);  view_154 = t_31 = None
    view_155 = torch.ops.aten.view.default(mm_28, [1, 16384, 128, 64]);  mm_28 = None
    _to_copy_95 = torch.ops.aten._to_copy.default(getitem_138, dtype = torch.float32);  getitem_138 = None
    native_layer_norm_default_20 = torch.ops.aten.native_layer_norm.default(_to_copy_95, [64], msa_module_msa_transition_0_layer_norm_weight, msa_module_msa_transition_0_layer_norm_bias, 1e-05);  _to_copy_95 = None
    getitem_145 = native_layer_norm_default_20[0];  native_layer_norm_default_20 = None
    _to_copy_96 = torch.ops.aten._to_copy.default(msa_module_msa_transition_0_linear_no_bias_ab_weight, dtype = torch.bfloat16)
    _to_copy_97 = torch.ops.aten._to_copy.default(getitem_145, dtype = torch.bfloat16);  getitem_145 = None
    t_32 = torch.ops.aten.t.default(_to_copy_96);  _to_copy_96 = None
    view_156 = torch.ops.aten.view.default(_to_copy_97, [2097152, 64]);  _to_copy_97 = None
    mm_29 = torch.ops.aten.mm.default(view_156, t_32);  view_156 = t_32 = None
    view_157 = torch.ops.aten.view.default(mm_29, [1, 16384, 128, 512]);  mm_29 = None
    split_tensor_14 = torch.ops.aten.split.Tensor(view_157, 256, dim = -1);  view_157 = None
    getitem_148 = split_tensor_14[0]
    getitem_149 = split_tensor_14[1];  split_tensor_14 = None
    silu_3 = torch.ops.aten.silu.default(getitem_148);  getitem_148 = None
    mul_16 = torch.ops.aten.mul.Tensor(silu_3, getitem_149);  silu_3 = getitem_149 = None
    _to_copy_98 = torch.ops.aten._to_copy.default(msa_module_msa_transition_0_linear_out_weight, dtype = torch.bfloat16)
    t_33 = torch.ops.aten.t.default(_to_copy_98);  _to_copy_98 = None
    view_159 = torch.ops.aten.view.default(mul_16, [2097152, 256]);  mul_16 = None
    mm_30 = torch.ops.aten.mm.default(view_159, t_33);  view_159 = t_33 = None
    view_160 = torch.ops.aten.view.default(mm_30, [1, 16384, 128, 64]);  mm_30 = None
    _to_copy_99 = torch.ops.aten._to_copy.default(getitem_139, dtype = torch.float32);  getitem_139 = None
    native_layer_norm_default_21 = torch.ops.aten.native_layer_norm.default(_to_copy_99, [64], msa_module_msa_transition_0_layer_norm_weight, msa_module_msa_transition_0_layer_norm_bias, 1e-05);  _to_copy_99 = msa_module_msa_transition_0_layer_norm_weight = msa_module_msa_transition_0_layer_norm_bias = None
    getitem_150 = native_layer_norm_default_21[0];  native_layer_norm_default_21 = None
    _to_copy_100 = torch.ops.aten._to_copy.default(msa_module_msa_transition_0_linear_no_bias_ab_weight, dtype = torch.bfloat16);  msa_module_msa_transition_0_linear_no_bias_ab_weight = None
    _to_copy_101 = torch.ops.aten._to_copy.default(getitem_150, dtype = torch.bfloat16);  getitem_150 = None
    t_34 = torch.ops.aten.t.default(_to_copy_100);  _to_copy_100 = None
    view_161 = torch.ops.aten.view.default(_to_copy_101, [2097152, 64]);  _to_copy_101 = None
    mm_31 = torch.ops.aten.mm.default(view_161, t_34);  view_161 = t_34 = None
    view_162 = torch.ops.aten.view.default(mm_31, [1, 16384, 128, 512]);  mm_31 = None
    split_tensor_15 = torch.ops.aten.split.Tensor(view_162, 256, dim = -1);  view_162 = None
    getitem_153 = split_tensor_15[0]
    getitem_154 = split_tensor_15[1];  split_tensor_15 = None
    silu_4 = torch.ops.aten.silu.default(getitem_153);  getitem_153 = None
    mul_17 = torch.ops.aten.mul.Tensor(silu_4, getitem_154);  silu_4 = getitem_154 = None
    _to_copy_102 = torch.ops.aten._to_copy.default(msa_module_msa_transition_0_linear_out_weight, dtype = torch.bfloat16);  msa_module_msa_transition_0_linear_out_weight = None
    t_35 = torch.ops.aten.t.default(_to_copy_102);  _to_copy_102 = None
    view_164 = torch.ops.aten.view.default(mul_17, [2097152, 256]);  mul_17 = None
    mm_32 = torch.ops.aten.mm.default(view_164, t_35);  view_164 = t_35 = None
    view_165 = torch.ops.aten.view.default(mm_32, [1, 16384, 128, 64]);  mm_32 = None
    cat_2 = torch.ops.aten.cat.default([view_155, view_160, view_165], dim = -2);  view_155 = view_160 = view_165 = None
    add_18 = torch.ops.aten.add.Tensor(add_12, cat_2);  cat_2 = None
    slice_33 = torch.ops.aten.slice.Tensor(add_12, dim = 0, start = 0, end = 9223372036854775807)
    slice_34 = torch.ops.aten.slice.Tensor(slice_33, dim = 1, start = 0, end = 8192);  slice_33 = None
    slice_35 = torch.ops.aten.slice.Tensor(slice_34, dim = 2, start = 0, end = 9223372036854775807);  slice_34 = None
    slice_36 = torch.ops.aten.slice.Tensor(slice_35, dim = 3, start = 0, end = 9223372036854775807);  slice_35 = None
    slice_37 = torch.ops.aten.slice.Tensor(arg1403_1, dim = 0, start = 0, end = 9223372036854775807)
    slice_38 = torch.ops.aten.slice.Tensor(slice_37, dim = 1, start = 0, end = 8192);  slice_37 = None
    slice_39 = torch.ops.aten.slice.Tensor(slice_38, dim = 2, start = 0, end = 9223372036854775807);  slice_38 = None
    _to_copy_103 = torch.ops.aten._to_copy.default(add_17, dtype = torch.float32)
    native_layer_norm_default_22 = torch.ops.aten.native_layer_norm.default(_to_copy_103, [256], msa_module_msa_pair_weighted_averaging_0_layernorm_pair_weight, msa_module_msa_pair_weighted_averaging_0_layernorm_pair_bias, 1e-05);  _to_copy_103 = None
    getitem_155 = native_layer_norm_default_22[0];  native_layer_norm_default_22 = None
    _to_copy_104 = torch.ops.aten._to_copy.default(msa_module_msa_pair_weighted_averaging_0_linear_pair_weight, dtype = torch.bfloat16)
    _to_copy_105 = torch.ops.aten._to_copy.default(getitem_155, dtype = torch.bfloat16);  getitem_155 = None
    t_36 = torch.ops.aten.t.default(_to_copy_104);  _to_copy_104 = None
    view_166 = torch.ops.aten.view.default(_to_copy_105, [147456, 256]);  _to_copy_105 = None
    mm_33 = torch.ops.aten.mm.default(view_166, t_36);  view_166 = t_36 = None
    view_167 = torch.ops.aten.view.default(mm_33, [1, 384, 384, 8]);  mm_33 = None
    permute_108 = torch.ops.aten.permute.default(view_167, [0, 3, 1, 2]);  view_167 = None
    view_168 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384])
    bitwise_not_10 = torch.ops.aten.bitwise_not.default(view_168);  view_168 = None
    masked_fill_10 = torch.ops.aten.masked_fill.Scalar(permute_108, bitwise_not_10, -10000);  permute_108 = bitwise_not_10 = None
    _to_copy_106 = torch.ops.aten._to_copy.default(masked_fill_10, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0));  masked_fill_10 = None
    _softmax = torch.ops.aten._softmax.default(_to_copy_106, -1, False);  _to_copy_106 = None
    _to_copy_107 = torch.ops.aten._to_copy.default(slice_36, dtype = torch.float32);  slice_36 = None
    native_layer_norm_default_23 = torch.ops.aten.native_layer_norm.default(_to_copy_107, [64], msa_module_msa_pair_weighted_averaging_0_layernorm_msa_weight, msa_module_msa_pair_weighted_averaging_0_layernorm_msa_bias, 1e-05);  _to_copy_107 = None
    getitem_158 = native_layer_norm_default_23[0];  native_layer_norm_default_23 = None
    _to_copy_108 = torch.ops.aten._to_copy.default(msa_module_msa_pair_weighted_averaging_0_linear_msa2vg_weight, dtype = torch.bfloat16)
    _to_copy_109 = torch.ops.aten._to_copy.default(getitem_158, dtype = torch.bfloat16);  getitem_158 = None
    t_37 = torch.ops.aten.t.default(_to_copy_108);  _to_copy_108 = None
    view_169 = torch.ops.aten.view.default(_to_copy_109, [3145728, 64]);  _to_copy_109 = None
    mm_34 = torch.ops.aten.mm.default(view_169, t_37);  view_169 = t_37 = None
    view_170 = torch.ops.aten.view.default(mm_34, [1, 8192, 384, 512]);  mm_34 = None
    view_171 = torch.ops.aten.view.default(view_170, [1, 8192, 384, 2, 8, 32]);  view_170 = None
    permute_109 = torch.ops.aten.permute.default(view_171, [3, 0, 1, 2, 4, 5]);  view_171 = None
    unbind_int_8 = torch.ops.aten.unbind.int(permute_109);  permute_109 = None
    getitem_161 = unbind_int_8[0]
    getitem_162 = unbind_int_8[1];  unbind_int_8 = None
    sigmoid_10 = torch.ops.aten.sigmoid.default(getitem_162);  getitem_162 = None
    bitwise_not_11 = torch.ops.aten.bitwise_not.default(slice_39);  slice_39 = None
    view_172 = torch.ops.aten.view.default(bitwise_not_11, [1, 8192, 384, 1, 1]);  bitwise_not_11 = None
    masked_fill_11 = torch.ops.aten.masked_fill.Scalar(getitem_161, view_172, 0);  getitem_161 = view_172 = None
    _to_copy_110 = torch.ops.aten._to_copy.default(_softmax, dtype = torch.bfloat16);  _softmax = None
    unsqueeze_82 = torch.ops.aten.unsqueeze.default(_to_copy_110, 4);  _to_copy_110 = None
    unsqueeze_83 = torch.ops.aten.unsqueeze.default(unsqueeze_82, 5);  unsqueeze_82 = None
    permute_110 = torch.ops.aten.permute.default(unsqueeze_83, [0, 4, 2, 1, 5, 3]);  unsqueeze_83 = None
    unsqueeze_84 = torch.ops.aten.unsqueeze.default(masked_fill_11, 5);  masked_fill_11 = None
    permute_111 = torch.ops.aten.permute.default(unsqueeze_84, [0, 1, 5, 3, 4, 2]);  unsqueeze_84 = None
    permute_112 = torch.ops.aten.permute.default(permute_110, [3, 2, 5, 0, 1, 4]);  permute_110 = None
    view_173 = torch.ops.aten.view.default(permute_112, [8, 384, 384]);  permute_112 = None
    permute_113 = torch.ops.aten.permute.default(permute_111, [3, 5, 0, 1, 4, 2]);  permute_111 = None
    clone_35 = torch.ops.aten.clone.default(permute_113, memory_format = torch.contiguous_format);  permute_113 = None
    _unsafe_view_32 = torch.ops.aten._unsafe_view.default(clone_35, [8, 384, 262144]);  clone_35 = None
    bmm_18 = torch.ops.aten.bmm.default(view_173, _unsafe_view_32);  view_173 = _unsafe_view_32 = None
    view_174 = torch.ops.aten.view.default(bmm_18, [8, 384, 1, 1, 8192, 32]);  bmm_18 = None
    permute_114 = torch.ops.aten.permute.default(view_174, [3, 4, 1, 0, 5, 2]);  view_174 = None
    view_175 = torch.ops.aten.view.default(permute_114, [1, 8192, 384, 8, 32]);  permute_114 = None
    mul_18 = torch.ops.aten.mul.Tensor(sigmoid_10, view_175);  sigmoid_10 = view_175 = None
    view_176 = torch.ops.aten.view.default(mul_18, [1, 8192, 384, 256]);  mul_18 = None
    _to_copy_111 = torch.ops.aten._to_copy.default(msa_module_msa_pair_weighted_averaging_0_linear_out_no_bias_weight, dtype = torch.bfloat16)
    t_38 = torch.ops.aten.t.default(_to_copy_111);  _to_copy_111 = None
    view_177 = torch.ops.aten.view.default(view_176, [3145728, 256]);  view_176 = None
    mm_35 = torch.ops.aten.mm.default(view_177, t_38);  view_177 = t_38 = None
    view_178 = torch.ops.aten.view.default(mm_35, [1, 8192, 384, 64]);  mm_35 = None
    slice_40 = torch.ops.aten.slice.Tensor(add_12, dim = 0, start = 0, end = 9223372036854775807);  add_12 = None
    slice_41 = torch.ops.aten.slice.Tensor(slice_40, dim = 1, start = 8192, end = 16384);  slice_40 = None
    slice_42 = torch.ops.aten.slice.Tensor(slice_41, dim = 2, start = 0, end = 9223372036854775807);  slice_41 = None
    slice_43 = torch.ops.aten.slice.Tensor(slice_42, dim = 3, start = 0, end = 9223372036854775807);  slice_42 = None
    slice_44 = torch.ops.aten.slice.Tensor(arg1403_1, dim = 0, start = 0, end = 9223372036854775807)
    slice_45 = torch.ops.aten.slice.Tensor(slice_44, dim = 1, start = 8192, end = 16384);  slice_44 = None
    slice_46 = torch.ops.aten.slice.Tensor(slice_45, dim = 2, start = 0, end = 9223372036854775807);  slice_45 = None
    _to_copy_112 = torch.ops.aten._to_copy.default(add_17, dtype = torch.float32)
    native_layer_norm_default_24 = torch.ops.aten.native_layer_norm.default(_to_copy_112, [256], msa_module_msa_pair_weighted_averaging_0_layernorm_pair_weight, msa_module_msa_pair_weighted_averaging_0_layernorm_pair_bias, 1e-05);  _to_copy_112 = msa_module_msa_pair_weighted_averaging_0_layernorm_pair_weight = msa_module_msa_pair_weighted_averaging_0_layernorm_pair_bias = None
    getitem_163 = native_layer_norm_default_24[0];  native_layer_norm_default_24 = None
    _to_copy_113 = torch.ops.aten._to_copy.default(msa_module_msa_pair_weighted_averaging_0_linear_pair_weight, dtype = torch.bfloat16);  msa_module_msa_pair_weighted_averaging_0_linear_pair_weight = None
    _to_copy_114 = torch.ops.aten._to_copy.default(getitem_163, dtype = torch.bfloat16);  getitem_163 = None
    t_39 = torch.ops.aten.t.default(_to_copy_113);  _to_copy_113 = None
    view_179 = torch.ops.aten.view.default(_to_copy_114, [147456, 256]);  _to_copy_114 = None
    mm_36 = torch.ops.aten.mm.default(view_179, t_39);  view_179 = t_39 = None
    view_180 = torch.ops.aten.view.default(mm_36, [1, 384, 384, 8]);  mm_36 = None
    permute_115 = torch.ops.aten.permute.default(view_180, [0, 3, 1, 2]);  view_180 = None
    view_181 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384])
    bitwise_not_12 = torch.ops.aten.bitwise_not.default(view_181);  view_181 = None
    masked_fill_12 = torch.ops.aten.masked_fill.Scalar(permute_115, bitwise_not_12, -10000);  permute_115 = bitwise_not_12 = None
    _to_copy_115 = torch.ops.aten._to_copy.default(masked_fill_12, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0));  masked_fill_12 = None
    _softmax_1 = torch.ops.aten._softmax.default(_to_copy_115, -1, False);  _to_copy_115 = None
    _to_copy_116 = torch.ops.aten._to_copy.default(slice_43, dtype = torch.float32);  slice_43 = None
    native_layer_norm_default_25 = torch.ops.aten.native_layer_norm.default(_to_copy_116, [64], msa_module_msa_pair_weighted_averaging_0_layernorm_msa_weight, msa_module_msa_pair_weighted_averaging_0_layernorm_msa_bias, 1e-05);  _to_copy_116 = msa_module_msa_pair_weighted_averaging_0_layernorm_msa_weight = msa_module_msa_pair_weighted_averaging_0_layernorm_msa_bias = None
    getitem_166 = native_layer_norm_default_25[0];  native_layer_norm_default_25 = None
    _to_copy_117 = torch.ops.aten._to_copy.default(msa_module_msa_pair_weighted_averaging_0_linear_msa2vg_weight, dtype = torch.bfloat16);  msa_module_msa_pair_weighted_averaging_0_linear_msa2vg_weight = None
    _to_copy_118 = torch.ops.aten._to_copy.default(getitem_166, dtype = torch.bfloat16);  getitem_166 = None
    t_40 = torch.ops.aten.t.default(_to_copy_117);  _to_copy_117 = None
    view_182 = torch.ops.aten.view.default(_to_copy_118, [3145728, 64]);  _to_copy_118 = None
    mm_37 = torch.ops.aten.mm.default(view_182, t_40);  view_182 = t_40 = None
    view_183 = torch.ops.aten.view.default(mm_37, [1, 8192, 384, 512]);  mm_37 = None
    view_184 = torch.ops.aten.view.default(view_183, [1, 8192, 384, 2, 8, 32]);  view_183 = None
    permute_116 = torch.ops.aten.permute.default(view_184, [3, 0, 1, 2, 4, 5]);  view_184 = None
    unbind_int_9 = torch.ops.aten.unbind.int(permute_116);  permute_116 = None
    getitem_169 = unbind_int_9[0]
    getitem_170 = unbind_int_9[1];  unbind_int_9 = None
    sigmoid_11 = torch.ops.aten.sigmoid.default(getitem_170);  getitem_170 = None
    bitwise_not_13 = torch.ops.aten.bitwise_not.default(slice_46);  slice_46 = None
    view_185 = torch.ops.aten.view.default(bitwise_not_13, [1, 8192, 384, 1, 1]);  bitwise_not_13 = None
    masked_fill_13 = torch.ops.aten.masked_fill.Scalar(getitem_169, view_185, 0);  getitem_169 = view_185 = None
    _to_copy_119 = torch.ops.aten._to_copy.default(_softmax_1, dtype = torch.bfloat16);  _softmax_1 = None
    unsqueeze_85 = torch.ops.aten.unsqueeze.default(_to_copy_119, 4);  _to_copy_119 = None
    unsqueeze_86 = torch.ops.aten.unsqueeze.default(unsqueeze_85, 5);  unsqueeze_85 = None
    permute_117 = torch.ops.aten.permute.default(unsqueeze_86, [0, 4, 2, 1, 5, 3]);  unsqueeze_86 = None
    unsqueeze_87 = torch.ops.aten.unsqueeze.default(masked_fill_13, 5);  masked_fill_13 = None
    permute_118 = torch.ops.aten.permute.default(unsqueeze_87, [0, 1, 5, 3, 4, 2]);  unsqueeze_87 = None
    permute_119 = torch.ops.aten.permute.default(permute_117, [3, 2, 5, 0, 1, 4]);  permute_117 = None
    view_186 = torch.ops.aten.view.default(permute_119, [8, 384, 384]);  permute_119 = None
    permute_120 = torch.ops.aten.permute.default(permute_118, [3, 5, 0, 1, 4, 2]);  permute_118 = None
    clone_36 = torch.ops.aten.clone.default(permute_120, memory_format = torch.contiguous_format);  permute_120 = None
    _unsafe_view_33 = torch.ops.aten._unsafe_view.default(clone_36, [8, 384, 262144]);  clone_36 = None
    bmm_19 = torch.ops.aten.bmm.default(view_186, _unsafe_view_33);  view_186 = _unsafe_view_33 = None
    view_187 = torch.ops.aten.view.default(bmm_19, [8, 384, 1, 1, 8192, 32]);  bmm_19 = None
    permute_121 = torch.ops.aten.permute.default(view_187, [3, 4, 1, 0, 5, 2]);  view_187 = None
    view_188 = torch.ops.aten.view.default(permute_121, [1, 8192, 384, 8, 32]);  permute_121 = None
    mul_19 = torch.ops.aten.mul.Tensor(sigmoid_11, view_188);  sigmoid_11 = view_188 = None
    view_189 = torch.ops.aten.view.default(mul_19, [1, 8192, 384, 256]);  mul_19 = None
    _to_copy_120 = torch.ops.aten._to_copy.default(msa_module_msa_pair_weighted_averaging_0_linear_out_no_bias_weight, dtype = torch.bfloat16);  msa_module_msa_pair_weighted_averaging_0_linear_out_no_bias_weight = None
    t_41 = torch.ops.aten.t.default(_to_copy_120);  _to_copy_120 = None
    view_190 = torch.ops.aten.view.default(view_189, [3145728, 256]);  view_189 = None
    mm_38 = torch.ops.aten.mm.default(view_190, t_41);  view_190 = t_41 = None
    view_191 = torch.ops.aten.view.default(mm_38, [1, 8192, 384, 64]);  mm_38 = None
    cat_3 = torch.ops.aten.cat.default([view_178, view_191], dim = 1);  view_178 = view_191 = None
    add_19 = torch.ops.aten.add.Tensor(add_18, cat_3);  add_18 = cat_3 = None
    _to_copy_121 = torch.ops.aten._to_copy.default(add_17, dtype = torch.float32)
    native_layer_norm_default_26 = torch.ops.aten.native_layer_norm.default(_to_copy_121, [256], msa_module_triangular_multiplication_0_layernorm_z_in_weight, msa_module_triangular_multiplication_0_layernorm_z_in_bias, 1e-05);  _to_copy_121 = msa_module_triangular_multiplication_0_layernorm_z_in_weight = msa_module_triangular_multiplication_0_layernorm_z_in_bias = None
    getitem_171 = native_layer_norm_default_26[0];  native_layer_norm_default_26 = None
    split_with_sizes_default_4 = torch.ops.aten.split_with_sizes.default(msa_module_triangular_multiplication_0_merged_linear_p_weight, [512, 512]);  msa_module_triangular_multiplication_0_merged_linear_p_weight = None
    getitem_174 = split_with_sizes_default_4[0]
    getitem_175 = split_with_sizes_default_4[1];  split_with_sizes_default_4 = None
    split_with_sizes_default_5 = torch.ops.aten.split_with_sizes.default(msa_module_triangular_multiplication_0_merged_linear_g_weight, [512, 512, 256]);  msa_module_triangular_multiplication_0_merged_linear_g_weight = None
    getitem_176 = split_with_sizes_default_5[0]
    getitem_177 = split_with_sizes_default_5[1]
    getitem_178 = split_with_sizes_default_5[2];  split_with_sizes_default_5 = None
    _to_copy_122 = torch.ops.aten._to_copy.default(getitem_174, dtype = torch.bfloat16);  getitem_174 = None
    _to_copy_123 = torch.ops.aten._to_copy.default(getitem_171, dtype = torch.bfloat16)
    t_42 = torch.ops.aten.t.default(_to_copy_122);  _to_copy_122 = None
    view_192 = torch.ops.aten.view.default(_to_copy_123, [147456, 256]);  _to_copy_123 = None
    mm_39 = torch.ops.aten.mm.default(view_192, t_42);  view_192 = t_42 = None
    view_193 = torch.ops.aten.view.default(mm_39, [1, 384, 384, 512]);  mm_39 = None
    _to_copy_124 = torch.ops.aten._to_copy.default(getitem_176, dtype = torch.bfloat16);  getitem_176 = None
    _to_copy_125 = torch.ops.aten._to_copy.default(getitem_171, dtype = torch.bfloat16)
    t_43 = torch.ops.aten.t.default(_to_copy_124);  _to_copy_124 = None
    view_194 = torch.ops.aten.view.default(_to_copy_125, [147456, 256]);  _to_copy_125 = None
    mm_40 = torch.ops.aten.mm.default(view_194, t_43);  view_194 = t_43 = None
    view_195 = torch.ops.aten.view.default(mm_40, [1, 384, 384, 512]);  mm_40 = None
    sigmoid_12 = torch.ops.aten.sigmoid.default(view_195);  view_195 = None
    mul_20 = torch.ops.aten.mul.Tensor(view_193, sigmoid_12);  view_193 = sigmoid_12 = None
    unsqueeze_88 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
    bitwise_not_14 = torch.ops.aten.bitwise_not.default(unsqueeze_88);  unsqueeze_88 = None
    masked_fill_14 = torch.ops.aten.masked_fill.Scalar(mul_20, bitwise_not_14, 0);  mul_20 = bitwise_not_14 = None
    split_tensor_16 = torch.ops.aten.split.Tensor(masked_fill_14, 256, dim = -1)
    getitem_181 = split_tensor_16[0];  split_tensor_16 = None
    unsqueeze_91 = torch.ops.aten.unsqueeze.default(getitem_181, 4);  getitem_181 = None
    permute_126 = torch.ops.aten.permute.default(unsqueeze_91, [0, 1, 4, 3, 2]);  unsqueeze_91 = None
    permute_127 = torch.ops.aten.permute.default(permute_126, [3, 1, 4, 0, 2]);  permute_126 = None
    view_198 = torch.ops.aten.view.default(permute_127, [256, 384, 384]);  permute_127 = None
    split_tensor_17 = torch.ops.aten.split.Tensor(masked_fill_14, 256, dim = -1);  masked_fill_14 = None
    getitem_184 = split_tensor_17[1];  split_tensor_17 = None
    unsqueeze_92 = torch.ops.aten.unsqueeze.default(getitem_184, 4);  getitem_184 = None
    permute_128 = torch.ops.aten.permute.default(unsqueeze_92, [0, 4, 1, 3, 2]);  unsqueeze_92 = None
    permute_129 = torch.ops.aten.permute.default(permute_128, [3, 4, 0, 2, 1]);  permute_128 = None
    view_199 = torch.ops.aten.view.default(permute_129, [256, 384, 384]);  permute_129 = None
    bmm_20 = torch.ops.aten.bmm.default(view_198, view_199);  view_198 = view_199 = None
    view_200 = torch.ops.aten.view.default(bmm_20, [256, 384, 1, 1, 384]);  bmm_20 = None
    permute_130 = torch.ops.aten.permute.default(view_200, [3, 1, 4, 0, 2]);  view_200 = None
    view_201 = torch.ops.aten.view.default(permute_130, [1, 384, 384, 256]);  permute_130 = None
    _to_copy_126 = torch.ops.aten._to_copy.default(getitem_175, dtype = torch.bfloat16);  getitem_175 = None
    _to_copy_127 = torch.ops.aten._to_copy.default(getitem_171, dtype = torch.bfloat16)
    t_44 = torch.ops.aten.t.default(_to_copy_126);  _to_copy_126 = None
    view_202 = torch.ops.aten.view.default(_to_copy_127, [147456, 256]);  _to_copy_127 = None
    mm_41 = torch.ops.aten.mm.default(view_202, t_44);  view_202 = t_44 = None
    view_203 = torch.ops.aten.view.default(mm_41, [1, 384, 384, 512]);  mm_41 = None
    _to_copy_128 = torch.ops.aten._to_copy.default(getitem_177, dtype = torch.bfloat16);  getitem_177 = None
    _to_copy_129 = torch.ops.aten._to_copy.default(getitem_171, dtype = torch.bfloat16)
    t_45 = torch.ops.aten.t.default(_to_copy_128);  _to_copy_128 = None
    view_204 = torch.ops.aten.view.default(_to_copy_129, [147456, 256]);  _to_copy_129 = None
    mm_42 = torch.ops.aten.mm.default(view_204, t_45);  view_204 = t_45 = None
    view_205 = torch.ops.aten.view.default(mm_42, [1, 384, 384, 512]);  mm_42 = None
    sigmoid_13 = torch.ops.aten.sigmoid.default(view_205);  view_205 = None
    mul_21 = torch.ops.aten.mul.Tensor(view_203, sigmoid_13);  view_203 = sigmoid_13 = None
    view_206 = torch.ops.aten.view.default(mul_21, [147456, 512]);  mul_21 = None
    view_207 = torch.ops.aten.view.default(view_206, [1, 384, 384, 512]);  view_206 = None
    transpose_4 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
    unsqueeze_93 = torch.ops.aten.unsqueeze.default(transpose_4, 3);  transpose_4 = None
    clone_37 = torch.ops.aten.clone.default(unsqueeze_93, memory_format = torch.contiguous_format);  unsqueeze_93 = None
    bitwise_not_15 = torch.ops.aten.bitwise_not.default(clone_37);  clone_37 = None
    masked_fill_15 = torch.ops.aten.masked_fill.Scalar(view_207, bitwise_not_15, 0);  view_207 = bitwise_not_15 = None
    view_208 = torch.ops.aten.view.default(masked_fill_15, [147456, 512]);  masked_fill_15 = None
    view_212 = torch.ops.aten.view.default(view_208, [1, 384, 384, 512])
    split_tensor_18 = torch.ops.aten.split.Tensor(view_212, 256, dim = -1);  view_212 = None
    getitem_187 = split_tensor_18[0];  split_tensor_18 = None
    unsqueeze_96 = torch.ops.aten.unsqueeze.default(getitem_187, 4);  getitem_187 = None
    permute_135 = torch.ops.aten.permute.default(unsqueeze_96, [0, 2, 4, 3, 1]);  unsqueeze_96 = None
    permute_136 = torch.ops.aten.permute.default(permute_135, [3, 1, 4, 0, 2]);  permute_135 = None
    view_213 = torch.ops.aten.view.default(permute_136, [256, 384, 384]);  permute_136 = None
    view_214 = torch.ops.aten.view.default(view_208, [1, 384, 384, 512]);  view_208 = None
    split_tensor_19 = torch.ops.aten.split.Tensor(view_214, 256, dim = -1);  view_214 = None
    getitem_190 = split_tensor_19[1];  split_tensor_19 = None
    unsqueeze_97 = torch.ops.aten.unsqueeze.default(getitem_190, 4);  getitem_190 = None
    permute_137 = torch.ops.aten.permute.default(unsqueeze_97, [0, 4, 2, 3, 1]);  unsqueeze_97 = None
    permute_138 = torch.ops.aten.permute.default(permute_137, [3, 4, 0, 2, 1]);  permute_137 = None
    view_215 = torch.ops.aten.view.default(permute_138, [256, 384, 384]);  permute_138 = None
    bmm_21 = torch.ops.aten.bmm.default(view_213, view_215);  view_213 = view_215 = None
    view_216 = torch.ops.aten.view.default(bmm_21, [256, 384, 1, 1, 384]);  bmm_21 = None
    permute_139 = torch.ops.aten.permute.default(view_216, [3, 1, 4, 0, 2]);  view_216 = None
    view_217 = torch.ops.aten.view.default(permute_139, [1, 384, 384, 256]);  permute_139 = None
    _to_copy_130 = torch.ops.aten._to_copy.default(view_201, dtype = torch.float32);  view_201 = None
    native_layer_norm_default_27 = torch.ops.aten.native_layer_norm.default(_to_copy_130, [256], None, None, 1e-05);  _to_copy_130 = None
    getitem_191 = native_layer_norm_default_27[0];  native_layer_norm_default_27 = None
    _to_copy_131 = torch.ops.aten._to_copy.default(view_217, dtype = torch.float32);  view_217 = None
    native_layer_norm_default_28 = torch.ops.aten.native_layer_norm.default(_to_copy_131, [256], None, None, 1e-05);  _to_copy_131 = None
    getitem_194 = native_layer_norm_default_28[0];  native_layer_norm_default_28 = None
    add_20 = torch.ops.aten.add.Tensor(getitem_191, getitem_194);  getitem_191 = getitem_194 = None
    _to_copy_132 = torch.ops.aten._to_copy.default(msa_module_triangular_multiplication_0_linear_z_out_weight, dtype = torch.bfloat16);  msa_module_triangular_multiplication_0_linear_z_out_weight = None
    _to_copy_133 = torch.ops.aten._to_copy.default(add_20, dtype = torch.bfloat16);  add_20 = None
    t_46 = torch.ops.aten.t.default(_to_copy_132);  _to_copy_132 = None
    view_218 = torch.ops.aten.view.default(_to_copy_133, [147456, 256]);  _to_copy_133 = None
    mm_43 = torch.ops.aten.mm.default(view_218, t_46);  view_218 = t_46 = None
    view_219 = torch.ops.aten.view.default(mm_43, [1, 384, 384, 256]);  mm_43 = None
    _to_copy_134 = torch.ops.aten._to_copy.default(getitem_178, dtype = torch.bfloat16);  getitem_178 = None
    _to_copy_135 = torch.ops.aten._to_copy.default(getitem_171, dtype = torch.bfloat16);  getitem_171 = None
    t_47 = torch.ops.aten.t.default(_to_copy_134);  _to_copy_134 = None
    view_220 = torch.ops.aten.view.default(_to_copy_135, [147456, 256]);  _to_copy_135 = None
    mm_44 = torch.ops.aten.mm.default(view_220, t_47);  view_220 = t_47 = None
    view_221 = torch.ops.aten.view.default(mm_44, [1, 384, 384, 256]);  mm_44 = None
    sigmoid_14 = torch.ops.aten.sigmoid.default(view_221);  view_221 = None
    mul_22 = torch.ops.aten.mul.Tensor(view_219, sigmoid_14);  view_219 = sigmoid_14 = None
    add_21 = torch.ops.aten.add.Tensor(add_17, mul_22);  mul_22 = None
    split_tensor_20 = torch.ops.aten.split.Tensor(add_17, 384, dim = -2);  add_17 = None
    getitem_197 = split_tensor_20[0];  split_tensor_20 = None
    _to_copy_136 = torch.ops.aten._to_copy.default(getitem_197, dtype = torch.float32);  getitem_197 = None
    native_layer_norm_default_29 = torch.ops.aten.native_layer_norm.default(_to_copy_136, [256], msa_module_pair_transition_0_layer_norm_weight, msa_module_pair_transition_0_layer_norm_bias, 1e-05);  _to_copy_136 = msa_module_pair_transition_0_layer_norm_weight = msa_module_pair_transition_0_layer_norm_bias = None
    getitem_198 = native_layer_norm_default_29[0];  native_layer_norm_default_29 = None
    _to_copy_137 = torch.ops.aten._to_copy.default(msa_module_pair_transition_0_linear_no_bias_ab_weight, dtype = torch.bfloat16);  msa_module_pair_transition_0_linear_no_bias_ab_weight = None
    _to_copy_138 = torch.ops.aten._to_copy.default(getitem_198, dtype = torch.bfloat16);  getitem_198 = None
    t_48 = torch.ops.aten.t.default(_to_copy_137);  _to_copy_137 = None
    view_222 = torch.ops.aten.view.default(_to_copy_138, [147456, 256]);  _to_copy_138 = None
    mm_45 = torch.ops.aten.mm.default(view_222, t_48);  view_222 = t_48 = None
    view_223 = torch.ops.aten.view.default(mm_45, [1, 384, 384, 2048]);  mm_45 = None
    split_tensor_21 = torch.ops.aten.split.Tensor(view_223, 1024, dim = -1);  view_223 = None
    getitem_201 = split_tensor_21[0]
    getitem_202 = split_tensor_21[1];  split_tensor_21 = None
    silu_5 = torch.ops.aten.silu.default(getitem_201);  getitem_201 = None
    mul_23 = torch.ops.aten.mul.Tensor(silu_5, getitem_202);  silu_5 = getitem_202 = None
    _to_copy_139 = torch.ops.aten._to_copy.default(msa_module_pair_transition_0_linear_out_weight, dtype = torch.bfloat16);  msa_module_pair_transition_0_linear_out_weight = None
    t_49 = torch.ops.aten.t.default(_to_copy_139);  _to_copy_139 = None
    view_225 = torch.ops.aten.view.default(mul_23, [147456, 1024]);  mul_23 = None
    mm_46 = torch.ops.aten.mm.default(view_225, t_49);  view_225 = t_49 = None
    view_226 = torch.ops.aten.view.default(mm_46, [1, 384, 384, 256]);  mm_46 = None
    add_22 = torch.ops.aten.add.Tensor(add_21, view_226);  add_21 = view_226 = None
    _to_copy_140 = torch.ops.aten._to_copy.default(add_22, dtype = torch.float32)
    native_layer_norm_default_30 = torch.ops.aten.native_layer_norm.default(_to_copy_140, [256], None, None, 1e-05);  _to_copy_140 = None
    getitem_203 = native_layer_norm_default_30[0];  native_layer_norm_default_30 = None
    _to_copy_141 = torch.ops.aten._to_copy.default(msa_module_triangular_attention_0_pair2b_weight, dtype = torch.bfloat16);  msa_module_triangular_attention_0_pair2b_weight = None
    _to_copy_142 = torch.ops.aten._to_copy.default(getitem_203, dtype = torch.bfloat16)
    t_50 = torch.ops.aten.t.default(_to_copy_141);  _to_copy_141 = None
    view_227 = torch.ops.aten.view.default(_to_copy_142, [147456, 256]);  _to_copy_142 = None
    mm_47 = torch.ops.aten.mm.default(view_227, t_50);  view_227 = t_50 = None
    view_228 = torch.ops.aten.view.default(mm_47, [1, 384, 384, 8]);  mm_47 = None
    view_229 = torch.ops.aten.view.default(view_228, [1, 384, 384, 2, 4]);  view_228 = None
    permute_140 = torch.ops.aten.permute.default(view_229, [0, 3, 4, 1, 2]);  view_229 = None
    view_230 = torch.ops.aten.view.default(permute_140, [1, 2, 4, 1, 384, 384]);  permute_140 = None
    view_231 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 384, 384])
    bitwise_not_16 = torch.ops.aten.bitwise_not.default(view_231);  view_231 = None
    masked_fill_16 = torch.ops.aten.masked_fill.Scalar(view_230, bitwise_not_16, -10000);  view_230 = bitwise_not_16 = None
    view_232 = torch.ops.aten.view.default(masked_fill_16, [1, 2, 4, 384, 384]);  masked_fill_16 = None
    permute_141 = torch.ops.aten.permute.default(view_232, [1, 0, 2, 3, 4]);  view_232 = None
    view_233 = torch.ops.aten.view.default(permute_141, [2, 4, 1, 384, 384]);  permute_141 = None
    _to_copy_143 = torch.ops.aten._to_copy.default(msa_module_triangular_attention_0_pair2qkvg1_weight, dtype = torch.bfloat16);  msa_module_triangular_attention_0_pair2qkvg1_weight = None
    _to_copy_144 = torch.ops.aten._to_copy.default(getitem_203, dtype = torch.bfloat16)
    t_51 = torch.ops.aten.t.default(_to_copy_143);  _to_copy_143 = None
    view_234 = torch.ops.aten.view.default(_to_copy_144, [147456, 256]);  _to_copy_144 = None
    mm_48 = torch.ops.aten.mm.default(view_234, t_51);  view_234 = t_51 = None
    view_235 = torch.ops.aten.view.default(mm_48, [1, 384, 384, 1024]);  mm_48 = None
    select_5 = torch.ops.aten.select.int(view_233, 0, 0)
    view_236 = torch.ops.aten.view.default(view_235, [1, 384, 384, 4, 4, 64]);  view_235 = None
    permute_142 = torch.ops.aten.permute.default(view_236, [4, 0, 3, 1, 2, 5]);  view_236 = None
    view_237 = torch.ops.aten.view.default(permute_142, [4, 4, 384, 384, 64]);  permute_142 = None
    unbind_int_10 = torch.ops.aten.unbind.int(view_237);  view_237 = None
    getitem_206 = unbind_int_10[0]
    getitem_207 = unbind_int_10[1]
    getitem_208 = unbind_int_10[2]
    getitem_209 = unbind_int_10[3];  unbind_int_10 = None
    expand_11 = torch.ops.aten.expand.default(select_5, [4, 384, 384, 384]);  select_5 = None
    _scaled_dot_product_efficient_attention_default_4 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_206, getitem_207, getitem_208, expand_11, False);  getitem_206 = getitem_207 = getitem_208 = expand_11 = None
    getitem_210 = _scaled_dot_product_efficient_attention_default_4[0];  _scaled_dot_product_efficient_attention_default_4 = None
    sigmoid_15 = torch.ops.aten.sigmoid.default(getitem_209);  getitem_209 = None
    mul_24 = torch.ops.aten.mul.Tensor(getitem_210, sigmoid_15);  getitem_210 = sigmoid_15 = None
    view_238 = torch.ops.aten.view.default(mul_24, [1, 4, 384, 384, 64]);  mul_24 = None
    permute_143 = torch.ops.aten.permute.default(view_238, [0, 2, 3, 1, 4]);  view_238 = None
    clone_38 = torch.ops.aten.clone.default(permute_143, memory_format = torch.contiguous_format);  permute_143 = None
    _unsafe_view_34 = torch.ops.aten._unsafe_view.default(clone_38, [1, 384, 384, 256]);  clone_38 = None
    transpose_5 = torch.ops.aten.transpose.int(getitem_203, 1, 2);  getitem_203 = None
    _to_copy_145 = torch.ops.aten._to_copy.default(msa_module_triangular_attention_0_pair2qkvg2_weight, dtype = torch.bfloat16);  msa_module_triangular_attention_0_pair2qkvg2_weight = None
    _to_copy_146 = torch.ops.aten._to_copy.default(transpose_5, dtype = torch.bfloat16);  transpose_5 = None
    t_52 = torch.ops.aten.t.default(_to_copy_145);  _to_copy_145 = None
    expand_12 = torch.ops.aten.expand.default(_to_copy_146, [1, 384, 384, 256]);  _to_copy_146 = None
    view_239 = torch.ops.aten.view.default(expand_12, [384, 384, 256]);  expand_12 = None
    expand_13 = torch.ops.aten.expand.default(t_52, [1, 384, 256, 1024]);  t_52 = None
    view_240 = torch.ops.aten.view.default(expand_13, [384, 256, 1024]);  expand_13 = None
    bmm_22 = torch.ops.aten.bmm.default(view_239, view_240);  view_239 = view_240 = None
    view_241 = torch.ops.aten.view.default(bmm_22, [1, 384, 384, 1024]);  bmm_22 = None
    select_6 = torch.ops.aten.select.int(view_233, 0, 1);  view_233 = None
    view_242 = torch.ops.aten.view.default(view_241, [1, 384, 384, 4, 4, 64]);  view_241 = None
    permute_144 = torch.ops.aten.permute.default(view_242, [4, 0, 3, 1, 2, 5]);  view_242 = None
    view_243 = torch.ops.aten.view.default(permute_144, [4, 4, 384, 384, 64]);  permute_144 = None
    unbind_int_11 = torch.ops.aten.unbind.int(view_243);  view_243 = None
    getitem_214 = unbind_int_11[0]
    getitem_215 = unbind_int_11[1]
    getitem_216 = unbind_int_11[2]
    getitem_217 = unbind_int_11[3];  unbind_int_11 = None
    expand_14 = torch.ops.aten.expand.default(select_6, [4, 384, 384, 384]);  select_6 = None
    _scaled_dot_product_efficient_attention_default_5 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_214, getitem_215, getitem_216, expand_14, False);  getitem_214 = getitem_215 = getitem_216 = expand_14 = None
    getitem_218 = _scaled_dot_product_efficient_attention_default_5[0];  _scaled_dot_product_efficient_attention_default_5 = None
    sigmoid_16 = torch.ops.aten.sigmoid.default(getitem_217);  getitem_217 = None
    mul_25 = torch.ops.aten.mul.Tensor(getitem_218, sigmoid_16);  getitem_218 = sigmoid_16 = None
    view_244 = torch.ops.aten.view.default(mul_25, [1, 4, 384, 384, 64]);  mul_25 = None
    permute_145 = torch.ops.aten.permute.default(view_244, [0, 2, 3, 1, 4]);  view_244 = None
    clone_39 = torch.ops.aten.clone.default(permute_145, memory_format = torch.contiguous_format);  permute_145 = None
    _unsafe_view_35 = torch.ops.aten._unsafe_view.default(clone_39, [1, 384, 384, 256]);  clone_39 = None
    cat_4 = torch.ops.aten.cat.default([_unsafe_view_34, _unsafe_view_35], dim = -1);  _unsafe_view_34 = _unsafe_view_35 = None
    slice_47 = torch.ops.aten.slice.Tensor(msa_module_triangular_attention_0_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  msa_module_triangular_attention_0_out_scalers = None
    unsqueeze_98 = torch.ops.aten.unsqueeze.default(slice_47, 1);  slice_47 = None
    mul_26 = torch.ops.aten.mul.Tensor(msa_module_triangular_attention_0_linear_out_weight, unsqueeze_98);  msa_module_triangular_attention_0_linear_out_weight = unsqueeze_98 = None
    _to_copy_147 = torch.ops.aten._to_copy.default(mul_26, dtype = torch.bfloat16);  mul_26 = None
    t_53 = torch.ops.aten.t.default(_to_copy_147);  _to_copy_147 = None
    view_245 = torch.ops.aten.view.default(cat_4, [147456, 512]);  cat_4 = None
    mm_49 = torch.ops.aten.mm.default(view_245, t_53);  view_245 = t_53 = None
    view_246 = torch.ops.aten.view.default(mm_49, [1, 384, 384, 256]);  mm_49 = None
    add_23 = torch.ops.aten.add.Tensor(add_22, view_246);  add_22 = view_246 = None
    slice_48 = torch.ops.aten.slice.Tensor(add_19, dim = 0, start = 0, end = 9223372036854775807)
    slice_49 = torch.ops.aten.slice.Tensor(slice_48, dim = 1, start = 0, end = 4096);  slice_48 = None
    slice_50 = torch.ops.aten.slice.Tensor(slice_49, dim = 2, start = 0, end = 9223372036854775807);  slice_49 = None
    slice_51 = torch.ops.aten.slice.Tensor(slice_50, dim = 3, start = 0, end = 9223372036854775807);  slice_50 = None
    slice_52 = torch.ops.aten.slice.Tensor(arg1403_1, dim = 0, start = 0, end = 9223372036854775807)
    slice_53 = torch.ops.aten.slice.Tensor(slice_52, dim = 1, start = 0, end = 4096);  slice_52 = None
    slice_54 = torch.ops.aten.slice.Tensor(slice_53, dim = 2, start = 0, end = 9223372036854775807);  slice_53 = None
    _to_copy_148 = torch.ops.aten._to_copy.default(slice_51, dtype = torch.float32);  slice_51 = None
    native_layer_norm_default_31 = torch.ops.aten.native_layer_norm.default(_to_copy_148, [64], None, None, 1e-05);  _to_copy_148 = None
    getitem_222 = native_layer_norm_default_31[0];  native_layer_norm_default_31 = None
    view_247 = torch.ops.aten.view.default(slice_54, [1, 4096, 384, 1]);  slice_54 = None
    bitwise_not_17 = torch.ops.aten.bitwise_not.default(view_247);  view_247 = None
    masked_fill_17 = torch.ops.aten.masked_fill.Scalar(getitem_222, bitwise_not_17, 0);  getitem_222 = bitwise_not_17 = None
    unbind_int_12 = torch.ops.aten.unbind.int(msa_module_outer_product_mean_1_weight_ab)
    getitem_225 = unbind_int_12[0]
    getitem_226 = unbind_int_12[1];  unbind_int_12 = None
    _to_copy_149 = torch.ops.aten._to_copy.default(getitem_225, dtype = torch.bfloat16);  getitem_225 = None
    _to_copy_150 = torch.ops.aten._to_copy.default(masked_fill_17, dtype = torch.bfloat16)
    unsqueeze_99 = torch.ops.aten.unsqueeze.default(_to_copy_149, 3);  _to_copy_149 = None
    unsqueeze_100 = torch.ops.aten.unsqueeze.default(unsqueeze_99, 4);  unsqueeze_99 = None
    unsqueeze_101 = torch.ops.aten.unsqueeze.default(unsqueeze_100, 5);  unsqueeze_100 = None
    permute_146 = torch.ops.aten.permute.default(unsqueeze_101, [0, 1, 3, 4, 5, 2]);  unsqueeze_101 = None
    unsqueeze_102 = torch.ops.aten.unsqueeze.default(_to_copy_150, 4);  _to_copy_150 = None
    unsqueeze_103 = torch.ops.aten.unsqueeze.default(unsqueeze_102, 5);  unsqueeze_102 = None
    permute_147 = torch.ops.aten.permute.default(unsqueeze_103, [4, 5, 0, 1, 2, 3]);  unsqueeze_103 = None
    permute_148 = torch.ops.aten.permute.default(permute_146, [0, 1, 5, 2, 3, 4]);  permute_146 = None
    view_248 = torch.ops.aten.view.default(permute_148, [1, 64, 64]);  permute_148 = None
    permute_149 = torch.ops.aten.permute.default(permute_147, [5, 2, 3, 4, 0, 1]);  permute_147 = None
    view_249 = torch.ops.aten.view.default(permute_149, [1, 64, 1572864]);  permute_149 = None
    bmm_23 = torch.ops.aten.bmm.default(view_248, view_249);  view_248 = view_249 = None
    view_250 = torch.ops.aten.view.default(bmm_23, [8, 8, 1, 1, 4096, 384]);  bmm_23 = None
    permute_150 = torch.ops.aten.permute.default(view_250, [0, 1, 3, 4, 5, 2]);  view_250 = None
    view_251 = torch.ops.aten.view.default(permute_150, [8, 8, 1, 4096, 384]);  permute_150 = None
    _to_copy_151 = torch.ops.aten._to_copy.default(getitem_226, dtype = torch.bfloat16);  getitem_226 = None
    _to_copy_152 = torch.ops.aten._to_copy.default(masked_fill_17, dtype = torch.bfloat16);  masked_fill_17 = None
    unsqueeze_104 = torch.ops.aten.unsqueeze.default(_to_copy_151, 3);  _to_copy_151 = None
    unsqueeze_105 = torch.ops.aten.unsqueeze.default(unsqueeze_104, 4);  unsqueeze_104 = None
    unsqueeze_106 = torch.ops.aten.unsqueeze.default(unsqueeze_105, 5);  unsqueeze_105 = None
    permute_151 = torch.ops.aten.permute.default(unsqueeze_106, [0, 1, 3, 4, 5, 2]);  unsqueeze_106 = None
    unsqueeze_107 = torch.ops.aten.unsqueeze.default(_to_copy_152, 4);  _to_copy_152 = None
    unsqueeze_108 = torch.ops.aten.unsqueeze.default(unsqueeze_107, 5);  unsqueeze_107 = None
    permute_152 = torch.ops.aten.permute.default(unsqueeze_108, [4, 5, 0, 1, 2, 3]);  unsqueeze_108 = None
    permute_153 = torch.ops.aten.permute.default(permute_151, [0, 1, 5, 2, 3, 4]);  permute_151 = None
    view_252 = torch.ops.aten.view.default(permute_153, [1, 64, 64]);  permute_153 = None
    permute_154 = torch.ops.aten.permute.default(permute_152, [5, 2, 3, 4, 0, 1]);  permute_152 = None
    view_253 = torch.ops.aten.view.default(permute_154, [1, 64, 1572864]);  permute_154 = None
    bmm_24 = torch.ops.aten.bmm.default(view_252, view_253);  view_252 = view_253 = None
    view_254 = torch.ops.aten.view.default(bmm_24, [8, 8, 1, 1, 4096, 384]);  bmm_24 = None
    permute_155 = torch.ops.aten.permute.default(view_254, [0, 1, 3, 4, 5, 2]);  view_254 = None
    view_255 = torch.ops.aten.view.default(permute_155, [8, 8, 1, 4096, 384]);  permute_155 = None
    unsqueeze_109 = torch.ops.aten.unsqueeze.default(view_251, 5);  view_251 = None
    unsqueeze_110 = torch.ops.aten.unsqueeze.default(unsqueeze_109, 6);  unsqueeze_109 = None
    permute_156 = torch.ops.aten.permute.default(unsqueeze_110, [2, 4, 5, 0, 1, 6, 3]);  unsqueeze_110 = None
    unsqueeze_111 = torch.ops.aten.unsqueeze.default(view_255, 5);  view_255 = None
    unsqueeze_112 = torch.ops.aten.unsqueeze.default(unsqueeze_111, 6);  unsqueeze_111 = None
    permute_157 = torch.ops.aten.permute.default(unsqueeze_112, [2, 5, 4, 0, 6, 1, 3]);  unsqueeze_112 = None
    permute_158 = torch.ops.aten.permute.default(permute_156, [3, 1, 4, 6, 0, 2, 5]);  permute_156 = None
    clone_40 = torch.ops.aten.clone.default(permute_158, memory_format = torch.contiguous_format);  permute_158 = None
    _unsafe_view_36 = torch.ops.aten._unsafe_view.default(clone_40, [8, 3072, 4096]);  clone_40 = None
    permute_159 = torch.ops.aten.permute.default(permute_157, [3, 6, 0, 2, 5, 1, 4]);  permute_157 = None
    clone_41 = torch.ops.aten.clone.default(permute_159, memory_format = torch.contiguous_format);  permute_159 = None
    _unsafe_view_37 = torch.ops.aten._unsafe_view.default(clone_41, [8, 4096, 3072]);  clone_41 = None
    bmm_25 = torch.ops.aten.bmm.default(_unsafe_view_36, _unsafe_view_37);  _unsafe_view_36 = _unsafe_view_37 = None
    view_256 = torch.ops.aten.view.default(bmm_25, [8, 384, 8, 1, 1, 384, 8]);  bmm_25 = None
    permute_160 = torch.ops.aten.permute.default(view_256, [4, 1, 5, 0, 2, 6, 3]);  view_256 = None
    view_257 = torch.ops.aten.view.default(permute_160, [1, 384, 384, 8, 8, 8]);  permute_160 = None
    clone_42 = torch.ops.aten.clone.default(view_257, memory_format = torch.contiguous_format);  view_257 = None
    _unsafe_view_38 = torch.ops.aten._unsafe_view.default(clone_42, [1, 384, 384, 512]);  clone_42 = None
    add_24 = torch.ops.aten.add.Tensor(_unsafe_view_38, 0);  _unsafe_view_38 = None
    slice_55 = torch.ops.aten.slice.Tensor(add_19, dim = 0, start = 0, end = 9223372036854775807)
    slice_56 = torch.ops.aten.slice.Tensor(slice_55, dim = 1, start = 4096, end = 8192);  slice_55 = None
    slice_57 = torch.ops.aten.slice.Tensor(slice_56, dim = 2, start = 0, end = 9223372036854775807);  slice_56 = None
    slice_58 = torch.ops.aten.slice.Tensor(slice_57, dim = 3, start = 0, end = 9223372036854775807);  slice_57 = None
    slice_59 = torch.ops.aten.slice.Tensor(arg1403_1, dim = 0, start = 0, end = 9223372036854775807)
    slice_60 = torch.ops.aten.slice.Tensor(slice_59, dim = 1, start = 4096, end = 8192);  slice_59 = None
    slice_61 = torch.ops.aten.slice.Tensor(slice_60, dim = 2, start = 0, end = 9223372036854775807);  slice_60 = None
    _to_copy_153 = torch.ops.aten._to_copy.default(slice_58, dtype = torch.float32);  slice_58 = None
    native_layer_norm_default_32 = torch.ops.aten.native_layer_norm.default(_to_copy_153, [64], None, None, 1e-05);  _to_copy_153 = None
    getitem_227 = native_layer_norm_default_32[0];  native_layer_norm_default_32 = None
    view_258 = torch.ops.aten.view.default(slice_61, [1, 4096, 384, 1]);  slice_61 = None
    bitwise_not_18 = torch.ops.aten.bitwise_not.default(view_258);  view_258 = None
    masked_fill_18 = torch.ops.aten.masked_fill.Scalar(getitem_227, bitwise_not_18, 0);  getitem_227 = bitwise_not_18 = None
    unbind_int_13 = torch.ops.aten.unbind.int(msa_module_outer_product_mean_1_weight_ab)
    getitem_230 = unbind_int_13[0]
    getitem_231 = unbind_int_13[1];  unbind_int_13 = None
    _to_copy_154 = torch.ops.aten._to_copy.default(getitem_230, dtype = torch.bfloat16);  getitem_230 = None
    _to_copy_155 = torch.ops.aten._to_copy.default(masked_fill_18, dtype = torch.bfloat16)
    unsqueeze_113 = torch.ops.aten.unsqueeze.default(_to_copy_154, 3);  _to_copy_154 = None
    unsqueeze_114 = torch.ops.aten.unsqueeze.default(unsqueeze_113, 4);  unsqueeze_113 = None
    unsqueeze_115 = torch.ops.aten.unsqueeze.default(unsqueeze_114, 5);  unsqueeze_114 = None
    permute_161 = torch.ops.aten.permute.default(unsqueeze_115, [0, 1, 3, 4, 5, 2]);  unsqueeze_115 = None
    unsqueeze_116 = torch.ops.aten.unsqueeze.default(_to_copy_155, 4);  _to_copy_155 = None
    unsqueeze_117 = torch.ops.aten.unsqueeze.default(unsqueeze_116, 5);  unsqueeze_116 = None
    permute_162 = torch.ops.aten.permute.default(unsqueeze_117, [4, 5, 0, 1, 2, 3]);  unsqueeze_117 = None
    permute_163 = torch.ops.aten.permute.default(permute_161, [0, 1, 5, 2, 3, 4]);  permute_161 = None
    view_259 = torch.ops.aten.view.default(permute_163, [1, 64, 64]);  permute_163 = None
    permute_164 = torch.ops.aten.permute.default(permute_162, [5, 2, 3, 4, 0, 1]);  permute_162 = None
    view_260 = torch.ops.aten.view.default(permute_164, [1, 64, 1572864]);  permute_164 = None
    bmm_26 = torch.ops.aten.bmm.default(view_259, view_260);  view_259 = view_260 = None
    view_261 = torch.ops.aten.view.default(bmm_26, [8, 8, 1, 1, 4096, 384]);  bmm_26 = None
    permute_165 = torch.ops.aten.permute.default(view_261, [0, 1, 3, 4, 5, 2]);  view_261 = None
    view_262 = torch.ops.aten.view.default(permute_165, [8, 8, 1, 4096, 384]);  permute_165 = None
    _to_copy_156 = torch.ops.aten._to_copy.default(getitem_231, dtype = torch.bfloat16);  getitem_231 = None
    _to_copy_157 = torch.ops.aten._to_copy.default(masked_fill_18, dtype = torch.bfloat16);  masked_fill_18 = None
    unsqueeze_118 = torch.ops.aten.unsqueeze.default(_to_copy_156, 3);  _to_copy_156 = None
    unsqueeze_119 = torch.ops.aten.unsqueeze.default(unsqueeze_118, 4);  unsqueeze_118 = None
    unsqueeze_120 = torch.ops.aten.unsqueeze.default(unsqueeze_119, 5);  unsqueeze_119 = None
    permute_166 = torch.ops.aten.permute.default(unsqueeze_120, [0, 1, 3, 4, 5, 2]);  unsqueeze_120 = None
    unsqueeze_121 = torch.ops.aten.unsqueeze.default(_to_copy_157, 4);  _to_copy_157 = None
    unsqueeze_122 = torch.ops.aten.unsqueeze.default(unsqueeze_121, 5);  unsqueeze_121 = None
    permute_167 = torch.ops.aten.permute.default(unsqueeze_122, [4, 5, 0, 1, 2, 3]);  unsqueeze_122 = None
    permute_168 = torch.ops.aten.permute.default(permute_166, [0, 1, 5, 2, 3, 4]);  permute_166 = None
    view_263 = torch.ops.aten.view.default(permute_168, [1, 64, 64]);  permute_168 = None
    permute_169 = torch.ops.aten.permute.default(permute_167, [5, 2, 3, 4, 0, 1]);  permute_167 = None
    view_264 = torch.ops.aten.view.default(permute_169, [1, 64, 1572864]);  permute_169 = None
    bmm_27 = torch.ops.aten.bmm.default(view_263, view_264);  view_263 = view_264 = None
    view_265 = torch.ops.aten.view.default(bmm_27, [8, 8, 1, 1, 4096, 384]);  bmm_27 = None
    permute_170 = torch.ops.aten.permute.default(view_265, [0, 1, 3, 4, 5, 2]);  view_265 = None
    view_266 = torch.ops.aten.view.default(permute_170, [8, 8, 1, 4096, 384]);  permute_170 = None
    unsqueeze_123 = torch.ops.aten.unsqueeze.default(view_262, 5);  view_262 = None
    unsqueeze_124 = torch.ops.aten.unsqueeze.default(unsqueeze_123, 6);  unsqueeze_123 = None
    permute_171 = torch.ops.aten.permute.default(unsqueeze_124, [2, 4, 5, 0, 1, 6, 3]);  unsqueeze_124 = None
    unsqueeze_125 = torch.ops.aten.unsqueeze.default(view_266, 5);  view_266 = None
    unsqueeze_126 = torch.ops.aten.unsqueeze.default(unsqueeze_125, 6);  unsqueeze_125 = None
    permute_172 = torch.ops.aten.permute.default(unsqueeze_126, [2, 5, 4, 0, 6, 1, 3]);  unsqueeze_126 = None
    permute_173 = torch.ops.aten.permute.default(permute_171, [3, 1, 4, 6, 0, 2, 5]);  permute_171 = None
    clone_43 = torch.ops.aten.clone.default(permute_173, memory_format = torch.contiguous_format);  permute_173 = None
    _unsafe_view_39 = torch.ops.aten._unsafe_view.default(clone_43, [8, 3072, 4096]);  clone_43 = None
    permute_174 = torch.ops.aten.permute.default(permute_172, [3, 6, 0, 2, 5, 1, 4]);  permute_172 = None
    clone_44 = torch.ops.aten.clone.default(permute_174, memory_format = torch.contiguous_format);  permute_174 = None
    _unsafe_view_40 = torch.ops.aten._unsafe_view.default(clone_44, [8, 4096, 3072]);  clone_44 = None
    bmm_28 = torch.ops.aten.bmm.default(_unsafe_view_39, _unsafe_view_40);  _unsafe_view_39 = _unsafe_view_40 = None
    view_267 = torch.ops.aten.view.default(bmm_28, [8, 384, 8, 1, 1, 384, 8]);  bmm_28 = None
    permute_175 = torch.ops.aten.permute.default(view_267, [4, 1, 5, 0, 2, 6, 3]);  view_267 = None
    view_268 = torch.ops.aten.view.default(permute_175, [1, 384, 384, 8, 8, 8]);  permute_175 = None
    clone_45 = torch.ops.aten.clone.default(view_268, memory_format = torch.contiguous_format);  view_268 = None
    _unsafe_view_41 = torch.ops.aten._unsafe_view.default(clone_45, [1, 384, 384, 512]);  clone_45 = None
    add_25 = torch.ops.aten.add.Tensor(add_24, _unsafe_view_41);  add_24 = _unsafe_view_41 = None
    slice_62 = torch.ops.aten.slice.Tensor(add_19, dim = 0, start = 0, end = 9223372036854775807)
    slice_63 = torch.ops.aten.slice.Tensor(slice_62, dim = 1, start = 8192, end = 12288);  slice_62 = None
    slice_64 = torch.ops.aten.slice.Tensor(slice_63, dim = 2, start = 0, end = 9223372036854775807);  slice_63 = None
    slice_65 = torch.ops.aten.slice.Tensor(slice_64, dim = 3, start = 0, end = 9223372036854775807);  slice_64 = None
    slice_66 = torch.ops.aten.slice.Tensor(arg1403_1, dim = 0, start = 0, end = 9223372036854775807)
    slice_67 = torch.ops.aten.slice.Tensor(slice_66, dim = 1, start = 8192, end = 12288);  slice_66 = None
    slice_68 = torch.ops.aten.slice.Tensor(slice_67, dim = 2, start = 0, end = 9223372036854775807);  slice_67 = None
    _to_copy_158 = torch.ops.aten._to_copy.default(slice_65, dtype = torch.float32);  slice_65 = None
    native_layer_norm_default_33 = torch.ops.aten.native_layer_norm.default(_to_copy_158, [64], None, None, 1e-05);  _to_copy_158 = None
    getitem_232 = native_layer_norm_default_33[0];  native_layer_norm_default_33 = None
    view_269 = torch.ops.aten.view.default(slice_68, [1, 4096, 384, 1]);  slice_68 = None
    bitwise_not_19 = torch.ops.aten.bitwise_not.default(view_269);  view_269 = None
    masked_fill_19 = torch.ops.aten.masked_fill.Scalar(getitem_232, bitwise_not_19, 0);  getitem_232 = bitwise_not_19 = None
    unbind_int_14 = torch.ops.aten.unbind.int(msa_module_outer_product_mean_1_weight_ab)
    getitem_235 = unbind_int_14[0]
    getitem_236 = unbind_int_14[1];  unbind_int_14 = None
    _to_copy_159 = torch.ops.aten._to_copy.default(getitem_235, dtype = torch.bfloat16);  getitem_235 = None
    _to_copy_160 = torch.ops.aten._to_copy.default(masked_fill_19, dtype = torch.bfloat16)
    unsqueeze_127 = torch.ops.aten.unsqueeze.default(_to_copy_159, 3);  _to_copy_159 = None
    unsqueeze_128 = torch.ops.aten.unsqueeze.default(unsqueeze_127, 4);  unsqueeze_127 = None
    unsqueeze_129 = torch.ops.aten.unsqueeze.default(unsqueeze_128, 5);  unsqueeze_128 = None
    permute_176 = torch.ops.aten.permute.default(unsqueeze_129, [0, 1, 3, 4, 5, 2]);  unsqueeze_129 = None
    unsqueeze_130 = torch.ops.aten.unsqueeze.default(_to_copy_160, 4);  _to_copy_160 = None
    unsqueeze_131 = torch.ops.aten.unsqueeze.default(unsqueeze_130, 5);  unsqueeze_130 = None
    permute_177 = torch.ops.aten.permute.default(unsqueeze_131, [4, 5, 0, 1, 2, 3]);  unsqueeze_131 = None
    permute_178 = torch.ops.aten.permute.default(permute_176, [0, 1, 5, 2, 3, 4]);  permute_176 = None
    view_270 = torch.ops.aten.view.default(permute_178, [1, 64, 64]);  permute_178 = None
    permute_179 = torch.ops.aten.permute.default(permute_177, [5, 2, 3, 4, 0, 1]);  permute_177 = None
    view_271 = torch.ops.aten.view.default(permute_179, [1, 64, 1572864]);  permute_179 = None
    bmm_29 = torch.ops.aten.bmm.default(view_270, view_271);  view_270 = view_271 = None
    view_272 = torch.ops.aten.view.default(bmm_29, [8, 8, 1, 1, 4096, 384]);  bmm_29 = None
    permute_180 = torch.ops.aten.permute.default(view_272, [0, 1, 3, 4, 5, 2]);  view_272 = None
    view_273 = torch.ops.aten.view.default(permute_180, [8, 8, 1, 4096, 384]);  permute_180 = None
    _to_copy_161 = torch.ops.aten._to_copy.default(getitem_236, dtype = torch.bfloat16);  getitem_236 = None
    _to_copy_162 = torch.ops.aten._to_copy.default(masked_fill_19, dtype = torch.bfloat16);  masked_fill_19 = None
    unsqueeze_132 = torch.ops.aten.unsqueeze.default(_to_copy_161, 3);  _to_copy_161 = None
    unsqueeze_133 = torch.ops.aten.unsqueeze.default(unsqueeze_132, 4);  unsqueeze_132 = None
    unsqueeze_134 = torch.ops.aten.unsqueeze.default(unsqueeze_133, 5);  unsqueeze_133 = None
    permute_181 = torch.ops.aten.permute.default(unsqueeze_134, [0, 1, 3, 4, 5, 2]);  unsqueeze_134 = None
    unsqueeze_135 = torch.ops.aten.unsqueeze.default(_to_copy_162, 4);  _to_copy_162 = None
    unsqueeze_136 = torch.ops.aten.unsqueeze.default(unsqueeze_135, 5);  unsqueeze_135 = None
    permute_182 = torch.ops.aten.permute.default(unsqueeze_136, [4, 5, 0, 1, 2, 3]);  unsqueeze_136 = None
    permute_183 = torch.ops.aten.permute.default(permute_181, [0, 1, 5, 2, 3, 4]);  permute_181 = None
    view_274 = torch.ops.aten.view.default(permute_183, [1, 64, 64]);  permute_183 = None
    permute_184 = torch.ops.aten.permute.default(permute_182, [5, 2, 3, 4, 0, 1]);  permute_182 = None
    view_275 = torch.ops.aten.view.default(permute_184, [1, 64, 1572864]);  permute_184 = None
    bmm_30 = torch.ops.aten.bmm.default(view_274, view_275);  view_274 = view_275 = None
    view_276 = torch.ops.aten.view.default(bmm_30, [8, 8, 1, 1, 4096, 384]);  bmm_30 = None
    permute_185 = torch.ops.aten.permute.default(view_276, [0, 1, 3, 4, 5, 2]);  view_276 = None
    view_277 = torch.ops.aten.view.default(permute_185, [8, 8, 1, 4096, 384]);  permute_185 = None
    unsqueeze_137 = torch.ops.aten.unsqueeze.default(view_273, 5);  view_273 = None
    unsqueeze_138 = torch.ops.aten.unsqueeze.default(unsqueeze_137, 6);  unsqueeze_137 = None
    permute_186 = torch.ops.aten.permute.default(unsqueeze_138, [2, 4, 5, 0, 1, 6, 3]);  unsqueeze_138 = None
    unsqueeze_139 = torch.ops.aten.unsqueeze.default(view_277, 5);  view_277 = None
    unsqueeze_140 = torch.ops.aten.unsqueeze.default(unsqueeze_139, 6);  unsqueeze_139 = None
    permute_187 = torch.ops.aten.permute.default(unsqueeze_140, [2, 5, 4, 0, 6, 1, 3]);  unsqueeze_140 = None
    permute_188 = torch.ops.aten.permute.default(permute_186, [3, 1, 4, 6, 0, 2, 5]);  permute_186 = None
    clone_46 = torch.ops.aten.clone.default(permute_188, memory_format = torch.contiguous_format);  permute_188 = None
    _unsafe_view_42 = torch.ops.aten._unsafe_view.default(clone_46, [8, 3072, 4096]);  clone_46 = None
    permute_189 = torch.ops.aten.permute.default(permute_187, [3, 6, 0, 2, 5, 1, 4]);  permute_187 = None
    clone_47 = torch.ops.aten.clone.default(permute_189, memory_format = torch.contiguous_format);  permute_189 = None
    _unsafe_view_43 = torch.ops.aten._unsafe_view.default(clone_47, [8, 4096, 3072]);  clone_47 = None
    bmm_31 = torch.ops.aten.bmm.default(_unsafe_view_42, _unsafe_view_43);  _unsafe_view_42 = _unsafe_view_43 = None
    view_278 = torch.ops.aten.view.default(bmm_31, [8, 384, 8, 1, 1, 384, 8]);  bmm_31 = None
    permute_190 = torch.ops.aten.permute.default(view_278, [4, 1, 5, 0, 2, 6, 3]);  view_278 = None
    view_279 = torch.ops.aten.view.default(permute_190, [1, 384, 384, 8, 8, 8]);  permute_190 = None
    clone_48 = torch.ops.aten.clone.default(view_279, memory_format = torch.contiguous_format);  view_279 = None
    _unsafe_view_44 = torch.ops.aten._unsafe_view.default(clone_48, [1, 384, 384, 512]);  clone_48 = None
    add_26 = torch.ops.aten.add.Tensor(add_25, _unsafe_view_44);  add_25 = _unsafe_view_44 = None
    slice_69 = torch.ops.aten.slice.Tensor(add_19, dim = 0, start = 0, end = 9223372036854775807)
    slice_70 = torch.ops.aten.slice.Tensor(slice_69, dim = 1, start = 12288, end = 16384);  slice_69 = None
    slice_71 = torch.ops.aten.slice.Tensor(slice_70, dim = 2, start = 0, end = 9223372036854775807);  slice_70 = None
    slice_72 = torch.ops.aten.slice.Tensor(slice_71, dim = 3, start = 0, end = 9223372036854775807);  slice_71 = None
    slice_73 = torch.ops.aten.slice.Tensor(arg1403_1, dim = 0, start = 0, end = 9223372036854775807)
    slice_74 = torch.ops.aten.slice.Tensor(slice_73, dim = 1, start = 12288, end = 16384);  slice_73 = None
    slice_75 = torch.ops.aten.slice.Tensor(slice_74, dim = 2, start = 0, end = 9223372036854775807);  slice_74 = None
    _to_copy_163 = torch.ops.aten._to_copy.default(slice_72, dtype = torch.float32);  slice_72 = None
    native_layer_norm_default_34 = torch.ops.aten.native_layer_norm.default(_to_copy_163, [64], None, None, 1e-05);  _to_copy_163 = None
    getitem_237 = native_layer_norm_default_34[0];  native_layer_norm_default_34 = None
    view_280 = torch.ops.aten.view.default(slice_75, [1, 4096, 384, 1]);  slice_75 = None
    bitwise_not_20 = torch.ops.aten.bitwise_not.default(view_280);  view_280 = None
    masked_fill_20 = torch.ops.aten.masked_fill.Scalar(getitem_237, bitwise_not_20, 0);  getitem_237 = bitwise_not_20 = None
    unbind_int_15 = torch.ops.aten.unbind.int(msa_module_outer_product_mean_1_weight_ab);  msa_module_outer_product_mean_1_weight_ab = None
    getitem_240 = unbind_int_15[0]
    getitem_241 = unbind_int_15[1];  unbind_int_15 = None
    _to_copy_164 = torch.ops.aten._to_copy.default(getitem_240, dtype = torch.bfloat16);  getitem_240 = None
    _to_copy_165 = torch.ops.aten._to_copy.default(masked_fill_20, dtype = torch.bfloat16)
    unsqueeze_141 = torch.ops.aten.unsqueeze.default(_to_copy_164, 3);  _to_copy_164 = None
    unsqueeze_142 = torch.ops.aten.unsqueeze.default(unsqueeze_141, 4);  unsqueeze_141 = None
    unsqueeze_143 = torch.ops.aten.unsqueeze.default(unsqueeze_142, 5);  unsqueeze_142 = None
    permute_191 = torch.ops.aten.permute.default(unsqueeze_143, [0, 1, 3, 4, 5, 2]);  unsqueeze_143 = None
    unsqueeze_144 = torch.ops.aten.unsqueeze.default(_to_copy_165, 4);  _to_copy_165 = None
    unsqueeze_145 = torch.ops.aten.unsqueeze.default(unsqueeze_144, 5);  unsqueeze_144 = None
    permute_192 = torch.ops.aten.permute.default(unsqueeze_145, [4, 5, 0, 1, 2, 3]);  unsqueeze_145 = None
    permute_193 = torch.ops.aten.permute.default(permute_191, [0, 1, 5, 2, 3, 4]);  permute_191 = None
    view_281 = torch.ops.aten.view.default(permute_193, [1, 64, 64]);  permute_193 = None
    permute_194 = torch.ops.aten.permute.default(permute_192, [5, 2, 3, 4, 0, 1]);  permute_192 = None
    view_282 = torch.ops.aten.view.default(permute_194, [1, 64, 1572864]);  permute_194 = None
    bmm_32 = torch.ops.aten.bmm.default(view_281, view_282);  view_281 = view_282 = None
    view_283 = torch.ops.aten.view.default(bmm_32, [8, 8, 1, 1, 4096, 384]);  bmm_32 = None
    permute_195 = torch.ops.aten.permute.default(view_283, [0, 1, 3, 4, 5, 2]);  view_283 = None
    view_284 = torch.ops.aten.view.default(permute_195, [8, 8, 1, 4096, 384]);  permute_195 = None
    _to_copy_166 = torch.ops.aten._to_copy.default(getitem_241, dtype = torch.bfloat16);  getitem_241 = None
    _to_copy_167 = torch.ops.aten._to_copy.default(masked_fill_20, dtype = torch.bfloat16);  masked_fill_20 = None
    unsqueeze_146 = torch.ops.aten.unsqueeze.default(_to_copy_166, 3);  _to_copy_166 = None
    unsqueeze_147 = torch.ops.aten.unsqueeze.default(unsqueeze_146, 4);  unsqueeze_146 = None
    unsqueeze_148 = torch.ops.aten.unsqueeze.default(unsqueeze_147, 5);  unsqueeze_147 = None
    permute_196 = torch.ops.aten.permute.default(unsqueeze_148, [0, 1, 3, 4, 5, 2]);  unsqueeze_148 = None
    unsqueeze_149 = torch.ops.aten.unsqueeze.default(_to_copy_167, 4);  _to_copy_167 = None
    unsqueeze_150 = torch.ops.aten.unsqueeze.default(unsqueeze_149, 5);  unsqueeze_149 = None
    permute_197 = torch.ops.aten.permute.default(unsqueeze_150, [4, 5, 0, 1, 2, 3]);  unsqueeze_150 = None
    permute_198 = torch.ops.aten.permute.default(permute_196, [0, 1, 5, 2, 3, 4]);  permute_196 = None
    view_285 = torch.ops.aten.view.default(permute_198, [1, 64, 64]);  permute_198 = None
    permute_199 = torch.ops.aten.permute.default(permute_197, [5, 2, 3, 4, 0, 1]);  permute_197 = None
    view_286 = torch.ops.aten.view.default(permute_199, [1, 64, 1572864]);  permute_199 = None
    bmm_33 = torch.ops.aten.bmm.default(view_285, view_286);  view_285 = view_286 = None
    view_287 = torch.ops.aten.view.default(bmm_33, [8, 8, 1, 1, 4096, 384]);  bmm_33 = None
    permute_200 = torch.ops.aten.permute.default(view_287, [0, 1, 3, 4, 5, 2]);  view_287 = None
    view_288 = torch.ops.aten.view.default(permute_200, [8, 8, 1, 4096, 384]);  permute_200 = None
    unsqueeze_151 = torch.ops.aten.unsqueeze.default(view_284, 5);  view_284 = None
    unsqueeze_152 = torch.ops.aten.unsqueeze.default(unsqueeze_151, 6);  unsqueeze_151 = None
    permute_201 = torch.ops.aten.permute.default(unsqueeze_152, [2, 4, 5, 0, 1, 6, 3]);  unsqueeze_152 = None
    unsqueeze_153 = torch.ops.aten.unsqueeze.default(view_288, 5);  view_288 = None
    unsqueeze_154 = torch.ops.aten.unsqueeze.default(unsqueeze_153, 6);  unsqueeze_153 = None
    permute_202 = torch.ops.aten.permute.default(unsqueeze_154, [2, 5, 4, 0, 6, 1, 3]);  unsqueeze_154 = None
    permute_203 = torch.ops.aten.permute.default(permute_201, [3, 1, 4, 6, 0, 2, 5]);  permute_201 = None
    clone_49 = torch.ops.aten.clone.default(permute_203, memory_format = torch.contiguous_format);  permute_203 = None
    _unsafe_view_45 = torch.ops.aten._unsafe_view.default(clone_49, [8, 3072, 4096]);  clone_49 = None
    permute_204 = torch.ops.aten.permute.default(permute_202, [3, 6, 0, 2, 5, 1, 4]);  permute_202 = None
    clone_50 = torch.ops.aten.clone.default(permute_204, memory_format = torch.contiguous_format);  permute_204 = None
    _unsafe_view_46 = torch.ops.aten._unsafe_view.default(clone_50, [8, 4096, 3072]);  clone_50 = None
    bmm_34 = torch.ops.aten.bmm.default(_unsafe_view_45, _unsafe_view_46);  _unsafe_view_45 = _unsafe_view_46 = None
    view_289 = torch.ops.aten.view.default(bmm_34, [8, 384, 8, 1, 1, 384, 8]);  bmm_34 = None
    permute_205 = torch.ops.aten.permute.default(view_289, [4, 1, 5, 0, 2, 6, 3]);  view_289 = None
    view_290 = torch.ops.aten.view.default(permute_205, [1, 384, 384, 8, 8, 8]);  permute_205 = None
    clone_51 = torch.ops.aten.clone.default(view_290, memory_format = torch.contiguous_format);  view_290 = None
    _unsafe_view_47 = torch.ops.aten._unsafe_view.default(clone_51, [1, 384, 384, 512]);  clone_51 = None
    add_27 = torch.ops.aten.add.Tensor(add_26, _unsafe_view_47);  add_26 = _unsafe_view_47 = None
    _to_copy_168 = torch.ops.aten._to_copy.default(add_27, dtype = torch.float32);  add_27 = None
    native_layer_norm_default_35 = torch.ops.aten.native_layer_norm.default(_to_copy_168, [512], msa_module_outer_product_mean_1_ln_out_weight, msa_module_outer_product_mean_1_ln_out_bias, 0.1);  _to_copy_168 = msa_module_outer_product_mean_1_ln_out_weight = msa_module_outer_product_mean_1_ln_out_bias = None
    getitem_242 = native_layer_norm_default_35[0];  native_layer_norm_default_35 = None
    _to_copy_169 = torch.ops.aten._to_copy.default(msa_module_outer_product_mean_1_linear_out_bias, dtype = torch.bfloat16);  msa_module_outer_product_mean_1_linear_out_bias = None
    _to_copy_170 = torch.ops.aten._to_copy.default(msa_module_outer_product_mean_1_linear_out_weight, dtype = torch.bfloat16);  msa_module_outer_product_mean_1_linear_out_weight = None
    _to_copy_171 = torch.ops.aten._to_copy.default(getitem_242, dtype = torch.bfloat16);  getitem_242 = None
    view_291 = torch.ops.aten.view.default(_to_copy_171, [147456, 512]);  _to_copy_171 = None
    t_54 = torch.ops.aten.t.default(_to_copy_170);  _to_copy_170 = None
    addmm_1 = torch.ops.aten.addmm.default(_to_copy_169, view_291, t_54);  _to_copy_169 = view_291 = t_54 = None
    view_292 = torch.ops.aten.view.default(addmm_1, [1, 384, 384, 256]);  addmm_1 = None
    add_28 = torch.ops.aten.add.Tensor(add_23, view_292);  add_23 = view_292 = None
    split_tensor_22 = torch.ops.aten.split.Tensor(add_19, 128, dim = -2)
    getitem_245 = split_tensor_22[0]
    getitem_246 = split_tensor_22[1]
    getitem_247 = split_tensor_22[2];  split_tensor_22 = None
    _to_copy_172 = torch.ops.aten._to_copy.default(getitem_245, dtype = torch.float32);  getitem_245 = None
    native_layer_norm_default_36 = torch.ops.aten.native_layer_norm.default(_to_copy_172, [64], msa_module_msa_transition_1_layer_norm_weight, msa_module_msa_transition_1_layer_norm_bias, 1e-05);  _to_copy_172 = None
    getitem_248 = native_layer_norm_default_36[0];  native_layer_norm_default_36 = None
    _to_copy_173 = torch.ops.aten._to_copy.default(msa_module_msa_transition_1_linear_no_bias_ab_weight, dtype = torch.bfloat16)
    _to_copy_174 = torch.ops.aten._to_copy.default(getitem_248, dtype = torch.bfloat16);  getitem_248 = None
    t_55 = torch.ops.aten.t.default(_to_copy_173);  _to_copy_173 = None
    view_293 = torch.ops.aten.view.default(_to_copy_174, [2097152, 64]);  _to_copy_174 = None
    mm_50 = torch.ops.aten.mm.default(view_293, t_55);  view_293 = t_55 = None
    view_294 = torch.ops.aten.view.default(mm_50, [1, 16384, 128, 512]);  mm_50 = None
    split_tensor_23 = torch.ops.aten.split.Tensor(view_294, 256, dim = -1);  view_294 = None
    getitem_251 = split_tensor_23[0]
    getitem_252 = split_tensor_23[1];  split_tensor_23 = None
    silu_6 = torch.ops.aten.silu.default(getitem_251);  getitem_251 = None
    mul_27 = torch.ops.aten.mul.Tensor(silu_6, getitem_252);  silu_6 = getitem_252 = None
    _to_copy_175 = torch.ops.aten._to_copy.default(msa_module_msa_transition_1_linear_out_weight, dtype = torch.bfloat16)
    t_56 = torch.ops.aten.t.default(_to_copy_175);  _to_copy_175 = None
    view_296 = torch.ops.aten.view.default(mul_27, [2097152, 256]);  mul_27 = None
    mm_51 = torch.ops.aten.mm.default(view_296, t_56);  view_296 = t_56 = None
    view_297 = torch.ops.aten.view.default(mm_51, [1, 16384, 128, 64]);  mm_51 = None
    _to_copy_176 = torch.ops.aten._to_copy.default(getitem_246, dtype = torch.float32);  getitem_246 = None
    native_layer_norm_default_37 = torch.ops.aten.native_layer_norm.default(_to_copy_176, [64], msa_module_msa_transition_1_layer_norm_weight, msa_module_msa_transition_1_layer_norm_bias, 1e-05);  _to_copy_176 = None
    getitem_253 = native_layer_norm_default_37[0];  native_layer_norm_default_37 = None
    _to_copy_177 = torch.ops.aten._to_copy.default(msa_module_msa_transition_1_linear_no_bias_ab_weight, dtype = torch.bfloat16)
    _to_copy_178 = torch.ops.aten._to_copy.default(getitem_253, dtype = torch.bfloat16);  getitem_253 = None
    t_57 = torch.ops.aten.t.default(_to_copy_177);  _to_copy_177 = None
    view_298 = torch.ops.aten.view.default(_to_copy_178, [2097152, 64]);  _to_copy_178 = None
    mm_52 = torch.ops.aten.mm.default(view_298, t_57);  view_298 = t_57 = None
    view_299 = torch.ops.aten.view.default(mm_52, [1, 16384, 128, 512]);  mm_52 = None
    split_tensor_24 = torch.ops.aten.split.Tensor(view_299, 256, dim = -1);  view_299 = None
    getitem_256 = split_tensor_24[0]
    getitem_257 = split_tensor_24[1];  split_tensor_24 = None
    silu_7 = torch.ops.aten.silu.default(getitem_256);  getitem_256 = None
    mul_28 = torch.ops.aten.mul.Tensor(silu_7, getitem_257);  silu_7 = getitem_257 = None
    _to_copy_179 = torch.ops.aten._to_copy.default(msa_module_msa_transition_1_linear_out_weight, dtype = torch.bfloat16)
    t_58 = torch.ops.aten.t.default(_to_copy_179);  _to_copy_179 = None
    view_301 = torch.ops.aten.view.default(mul_28, [2097152, 256]);  mul_28 = None
    mm_53 = torch.ops.aten.mm.default(view_301, t_58);  view_301 = t_58 = None
    view_302 = torch.ops.aten.view.default(mm_53, [1, 16384, 128, 64]);  mm_53 = None
    _to_copy_180 = torch.ops.aten._to_copy.default(getitem_247, dtype = torch.float32);  getitem_247 = None
    native_layer_norm_default_38 = torch.ops.aten.native_layer_norm.default(_to_copy_180, [64], msa_module_msa_transition_1_layer_norm_weight, msa_module_msa_transition_1_layer_norm_bias, 1e-05);  _to_copy_180 = msa_module_msa_transition_1_layer_norm_weight = msa_module_msa_transition_1_layer_norm_bias = None
    getitem_258 = native_layer_norm_default_38[0];  native_layer_norm_default_38 = None
    _to_copy_181 = torch.ops.aten._to_copy.default(msa_module_msa_transition_1_linear_no_bias_ab_weight, dtype = torch.bfloat16);  msa_module_msa_transition_1_linear_no_bias_ab_weight = None
    _to_copy_182 = torch.ops.aten._to_copy.default(getitem_258, dtype = torch.bfloat16);  getitem_258 = None
    t_59 = torch.ops.aten.t.default(_to_copy_181);  _to_copy_181 = None
    view_303 = torch.ops.aten.view.default(_to_copy_182, [2097152, 64]);  _to_copy_182 = None
    mm_54 = torch.ops.aten.mm.default(view_303, t_59);  view_303 = t_59 = None
    view_304 = torch.ops.aten.view.default(mm_54, [1, 16384, 128, 512]);  mm_54 = None
    split_tensor_25 = torch.ops.aten.split.Tensor(view_304, 256, dim = -1);  view_304 = None
    getitem_261 = split_tensor_25[0]
    getitem_262 = split_tensor_25[1];  split_tensor_25 = None
    silu_8 = torch.ops.aten.silu.default(getitem_261);  getitem_261 = None
    mul_29 = torch.ops.aten.mul.Tensor(silu_8, getitem_262);  silu_8 = getitem_262 = None
    _to_copy_183 = torch.ops.aten._to_copy.default(msa_module_msa_transition_1_linear_out_weight, dtype = torch.bfloat16);  msa_module_msa_transition_1_linear_out_weight = None
    t_60 = torch.ops.aten.t.default(_to_copy_183);  _to_copy_183 = None
    view_306 = torch.ops.aten.view.default(mul_29, [2097152, 256]);  mul_29 = None
    mm_55 = torch.ops.aten.mm.default(view_306, t_60);  view_306 = t_60 = None
    view_307 = torch.ops.aten.view.default(mm_55, [1, 16384, 128, 64]);  mm_55 = None
    cat_5 = torch.ops.aten.cat.default([view_297, view_302, view_307], dim = -2);  view_297 = view_302 = view_307 = None
    add_29 = torch.ops.aten.add.Tensor(add_19, cat_5);  cat_5 = None
    slice_76 = torch.ops.aten.slice.Tensor(add_19, dim = 0, start = 0, end = 9223372036854775807)
    slice_77 = torch.ops.aten.slice.Tensor(slice_76, dim = 1, start = 0, end = 8192);  slice_76 = None
    slice_78 = torch.ops.aten.slice.Tensor(slice_77, dim = 2, start = 0, end = 9223372036854775807);  slice_77 = None
    slice_79 = torch.ops.aten.slice.Tensor(slice_78, dim = 3, start = 0, end = 9223372036854775807);  slice_78 = None
    slice_80 = torch.ops.aten.slice.Tensor(arg1403_1, dim = 0, start = 0, end = 9223372036854775807)
    slice_81 = torch.ops.aten.slice.Tensor(slice_80, dim = 1, start = 0, end = 8192);  slice_80 = None
    slice_82 = torch.ops.aten.slice.Tensor(slice_81, dim = 2, start = 0, end = 9223372036854775807);  slice_81 = None
    _to_copy_184 = torch.ops.aten._to_copy.default(add_28, dtype = torch.float32)
    native_layer_norm_default_39 = torch.ops.aten.native_layer_norm.default(_to_copy_184, [256], msa_module_msa_pair_weighted_averaging_1_layernorm_pair_weight, msa_module_msa_pair_weighted_averaging_1_layernorm_pair_bias, 1e-05);  _to_copy_184 = None
    getitem_263 = native_layer_norm_default_39[0];  native_layer_norm_default_39 = None
    _to_copy_185 = torch.ops.aten._to_copy.default(msa_module_msa_pair_weighted_averaging_1_linear_pair_weight, dtype = torch.bfloat16)
    _to_copy_186 = torch.ops.aten._to_copy.default(getitem_263, dtype = torch.bfloat16);  getitem_263 = None
    t_61 = torch.ops.aten.t.default(_to_copy_185);  _to_copy_185 = None
    view_308 = torch.ops.aten.view.default(_to_copy_186, [147456, 256]);  _to_copy_186 = None
    mm_56 = torch.ops.aten.mm.default(view_308, t_61);  view_308 = t_61 = None
    view_309 = torch.ops.aten.view.default(mm_56, [1, 384, 384, 8]);  mm_56 = None
    permute_206 = torch.ops.aten.permute.default(view_309, [0, 3, 1, 2]);  view_309 = None
    view_310 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384])
    bitwise_not_21 = torch.ops.aten.bitwise_not.default(view_310);  view_310 = None
    masked_fill_21 = torch.ops.aten.masked_fill.Scalar(permute_206, bitwise_not_21, -10000);  permute_206 = bitwise_not_21 = None
    _to_copy_187 = torch.ops.aten._to_copy.default(masked_fill_21, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0));  masked_fill_21 = None
    _softmax_2 = torch.ops.aten._softmax.default(_to_copy_187, -1, False);  _to_copy_187 = None
    _to_copy_188 = torch.ops.aten._to_copy.default(slice_79, dtype = torch.float32);  slice_79 = None
    native_layer_norm_default_40 = torch.ops.aten.native_layer_norm.default(_to_copy_188, [64], msa_module_msa_pair_weighted_averaging_1_layernorm_msa_weight, msa_module_msa_pair_weighted_averaging_1_layernorm_msa_bias, 1e-05);  _to_copy_188 = None
    getitem_266 = native_layer_norm_default_40[0];  native_layer_norm_default_40 = None
    _to_copy_189 = torch.ops.aten._to_copy.default(msa_module_msa_pair_weighted_averaging_1_linear_msa2vg_weight, dtype = torch.bfloat16)
    _to_copy_190 = torch.ops.aten._to_copy.default(getitem_266, dtype = torch.bfloat16);  getitem_266 = None
    t_62 = torch.ops.aten.t.default(_to_copy_189);  _to_copy_189 = None
    view_311 = torch.ops.aten.view.default(_to_copy_190, [3145728, 64]);  _to_copy_190 = None
    mm_57 = torch.ops.aten.mm.default(view_311, t_62);  view_311 = t_62 = None
    view_312 = torch.ops.aten.view.default(mm_57, [1, 8192, 384, 512]);  mm_57 = None
    view_313 = torch.ops.aten.view.default(view_312, [1, 8192, 384, 2, 8, 32]);  view_312 = None
    permute_207 = torch.ops.aten.permute.default(view_313, [3, 0, 1, 2, 4, 5]);  view_313 = None
    unbind_int_16 = torch.ops.aten.unbind.int(permute_207);  permute_207 = None
    getitem_269 = unbind_int_16[0]
    getitem_270 = unbind_int_16[1];  unbind_int_16 = None
    sigmoid_17 = torch.ops.aten.sigmoid.default(getitem_270);  getitem_270 = None
    bitwise_not_22 = torch.ops.aten.bitwise_not.default(slice_82);  slice_82 = None
    view_314 = torch.ops.aten.view.default(bitwise_not_22, [1, 8192, 384, 1, 1]);  bitwise_not_22 = None
    masked_fill_22 = torch.ops.aten.masked_fill.Scalar(getitem_269, view_314, 0);  getitem_269 = view_314 = None
    _to_copy_191 = torch.ops.aten._to_copy.default(_softmax_2, dtype = torch.bfloat16);  _softmax_2 = None
    unsqueeze_155 = torch.ops.aten.unsqueeze.default(_to_copy_191, 4);  _to_copy_191 = None
    unsqueeze_156 = torch.ops.aten.unsqueeze.default(unsqueeze_155, 5);  unsqueeze_155 = None
    permute_208 = torch.ops.aten.permute.default(unsqueeze_156, [0, 4, 2, 1, 5, 3]);  unsqueeze_156 = None
    unsqueeze_157 = torch.ops.aten.unsqueeze.default(masked_fill_22, 5);  masked_fill_22 = None
    permute_209 = torch.ops.aten.permute.default(unsqueeze_157, [0, 1, 5, 3, 4, 2]);  unsqueeze_157 = None
    permute_210 = torch.ops.aten.permute.default(permute_208, [3, 2, 5, 0, 1, 4]);  permute_208 = None
    view_315 = torch.ops.aten.view.default(permute_210, [8, 384, 384]);  permute_210 = None
    permute_211 = torch.ops.aten.permute.default(permute_209, [3, 5, 0, 1, 4, 2]);  permute_209 = None
    clone_52 = torch.ops.aten.clone.default(permute_211, memory_format = torch.contiguous_format);  permute_211 = None
    _unsafe_view_48 = torch.ops.aten._unsafe_view.default(clone_52, [8, 384, 262144]);  clone_52 = None
    bmm_35 = torch.ops.aten.bmm.default(view_315, _unsafe_view_48);  view_315 = _unsafe_view_48 = None
    view_316 = torch.ops.aten.view.default(bmm_35, [8, 384, 1, 1, 8192, 32]);  bmm_35 = None
    permute_212 = torch.ops.aten.permute.default(view_316, [3, 4, 1, 0, 5, 2]);  view_316 = None
    view_317 = torch.ops.aten.view.default(permute_212, [1, 8192, 384, 8, 32]);  permute_212 = None
    mul_30 = torch.ops.aten.mul.Tensor(sigmoid_17, view_317);  sigmoid_17 = view_317 = None
    view_318 = torch.ops.aten.view.default(mul_30, [1, 8192, 384, 256]);  mul_30 = None
    _to_copy_192 = torch.ops.aten._to_copy.default(msa_module_msa_pair_weighted_averaging_1_linear_out_no_bias_weight, dtype = torch.bfloat16)
    t_63 = torch.ops.aten.t.default(_to_copy_192);  _to_copy_192 = None
    view_319 = torch.ops.aten.view.default(view_318, [3145728, 256]);  view_318 = None
    mm_58 = torch.ops.aten.mm.default(view_319, t_63);  view_319 = t_63 = None
    view_320 = torch.ops.aten.view.default(mm_58, [1, 8192, 384, 64]);  mm_58 = None
    slice_83 = torch.ops.aten.slice.Tensor(add_19, dim = 0, start = 0, end = 9223372036854775807);  add_19 = None
    slice_84 = torch.ops.aten.slice.Tensor(slice_83, dim = 1, start = 8192, end = 16384);  slice_83 = None
    slice_85 = torch.ops.aten.slice.Tensor(slice_84, dim = 2, start = 0, end = 9223372036854775807);  slice_84 = None
    slice_86 = torch.ops.aten.slice.Tensor(slice_85, dim = 3, start = 0, end = 9223372036854775807);  slice_85 = None
    slice_87 = torch.ops.aten.slice.Tensor(arg1403_1, dim = 0, start = 0, end = 9223372036854775807)
    slice_88 = torch.ops.aten.slice.Tensor(slice_87, dim = 1, start = 8192, end = 16384);  slice_87 = None
    slice_89 = torch.ops.aten.slice.Tensor(slice_88, dim = 2, start = 0, end = 9223372036854775807);  slice_88 = None
    _to_copy_193 = torch.ops.aten._to_copy.default(add_28, dtype = torch.float32)
    native_layer_norm_default_41 = torch.ops.aten.native_layer_norm.default(_to_copy_193, [256], msa_module_msa_pair_weighted_averaging_1_layernorm_pair_weight, msa_module_msa_pair_weighted_averaging_1_layernorm_pair_bias, 1e-05);  _to_copy_193 = msa_module_msa_pair_weighted_averaging_1_layernorm_pair_weight = msa_module_msa_pair_weighted_averaging_1_layernorm_pair_bias = None
    getitem_271 = native_layer_norm_default_41[0];  native_layer_norm_default_41 = None
    _to_copy_194 = torch.ops.aten._to_copy.default(msa_module_msa_pair_weighted_averaging_1_linear_pair_weight, dtype = torch.bfloat16);  msa_module_msa_pair_weighted_averaging_1_linear_pair_weight = None
    _to_copy_195 = torch.ops.aten._to_copy.default(getitem_271, dtype = torch.bfloat16);  getitem_271 = None
    t_64 = torch.ops.aten.t.default(_to_copy_194);  _to_copy_194 = None
    view_321 = torch.ops.aten.view.default(_to_copy_195, [147456, 256]);  _to_copy_195 = None
    mm_59 = torch.ops.aten.mm.default(view_321, t_64);  view_321 = t_64 = None
    view_322 = torch.ops.aten.view.default(mm_59, [1, 384, 384, 8]);  mm_59 = None
    permute_213 = torch.ops.aten.permute.default(view_322, [0, 3, 1, 2]);  view_322 = None
    view_323 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384])
    bitwise_not_23 = torch.ops.aten.bitwise_not.default(view_323);  view_323 = None
    masked_fill_23 = torch.ops.aten.masked_fill.Scalar(permute_213, bitwise_not_23, -10000);  permute_213 = bitwise_not_23 = None
    _to_copy_196 = torch.ops.aten._to_copy.default(masked_fill_23, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0));  masked_fill_23 = None
    _softmax_3 = torch.ops.aten._softmax.default(_to_copy_196, -1, False);  _to_copy_196 = None
    _to_copy_197 = torch.ops.aten._to_copy.default(slice_86, dtype = torch.float32);  slice_86 = None
    native_layer_norm_default_42 = torch.ops.aten.native_layer_norm.default(_to_copy_197, [64], msa_module_msa_pair_weighted_averaging_1_layernorm_msa_weight, msa_module_msa_pair_weighted_averaging_1_layernorm_msa_bias, 1e-05);  _to_copy_197 = msa_module_msa_pair_weighted_averaging_1_layernorm_msa_weight = msa_module_msa_pair_weighted_averaging_1_layernorm_msa_bias = None
    getitem_274 = native_layer_norm_default_42[0];  native_layer_norm_default_42 = None
    _to_copy_198 = torch.ops.aten._to_copy.default(msa_module_msa_pair_weighted_averaging_1_linear_msa2vg_weight, dtype = torch.bfloat16);  msa_module_msa_pair_weighted_averaging_1_linear_msa2vg_weight = None
    _to_copy_199 = torch.ops.aten._to_copy.default(getitem_274, dtype = torch.bfloat16);  getitem_274 = None
    t_65 = torch.ops.aten.t.default(_to_copy_198);  _to_copy_198 = None
    view_324 = torch.ops.aten.view.default(_to_copy_199, [3145728, 64]);  _to_copy_199 = None
    mm_60 = torch.ops.aten.mm.default(view_324, t_65);  view_324 = t_65 = None
    view_325 = torch.ops.aten.view.default(mm_60, [1, 8192, 384, 512]);  mm_60 = None
    view_326 = torch.ops.aten.view.default(view_325, [1, 8192, 384, 2, 8, 32]);  view_325 = None
    permute_214 = torch.ops.aten.permute.default(view_326, [3, 0, 1, 2, 4, 5]);  view_326 = None
    unbind_int_17 = torch.ops.aten.unbind.int(permute_214);  permute_214 = None
    getitem_277 = unbind_int_17[0]
    getitem_278 = unbind_int_17[1];  unbind_int_17 = None
    sigmoid_18 = torch.ops.aten.sigmoid.default(getitem_278);  getitem_278 = None
    bitwise_not_24 = torch.ops.aten.bitwise_not.default(slice_89);  slice_89 = None
    view_327 = torch.ops.aten.view.default(bitwise_not_24, [1, 8192, 384, 1, 1]);  bitwise_not_24 = None
    masked_fill_24 = torch.ops.aten.masked_fill.Scalar(getitem_277, view_327, 0);  getitem_277 = view_327 = None
    _to_copy_200 = torch.ops.aten._to_copy.default(_softmax_3, dtype = torch.bfloat16);  _softmax_3 = None
    unsqueeze_158 = torch.ops.aten.unsqueeze.default(_to_copy_200, 4);  _to_copy_200 = None
    unsqueeze_159 = torch.ops.aten.unsqueeze.default(unsqueeze_158, 5);  unsqueeze_158 = None
    permute_215 = torch.ops.aten.permute.default(unsqueeze_159, [0, 4, 2, 1, 5, 3]);  unsqueeze_159 = None
    unsqueeze_160 = torch.ops.aten.unsqueeze.default(masked_fill_24, 5);  masked_fill_24 = None
    permute_216 = torch.ops.aten.permute.default(unsqueeze_160, [0, 1, 5, 3, 4, 2]);  unsqueeze_160 = None
    permute_217 = torch.ops.aten.permute.default(permute_215, [3, 2, 5, 0, 1, 4]);  permute_215 = None
    view_328 = torch.ops.aten.view.default(permute_217, [8, 384, 384]);  permute_217 = None
    permute_218 = torch.ops.aten.permute.default(permute_216, [3, 5, 0, 1, 4, 2]);  permute_216 = None
    clone_53 = torch.ops.aten.clone.default(permute_218, memory_format = torch.contiguous_format);  permute_218 = None
    _unsafe_view_49 = torch.ops.aten._unsafe_view.default(clone_53, [8, 384, 262144]);  clone_53 = None
    bmm_36 = torch.ops.aten.bmm.default(view_328, _unsafe_view_49);  view_328 = _unsafe_view_49 = None
    view_329 = torch.ops.aten.view.default(bmm_36, [8, 384, 1, 1, 8192, 32]);  bmm_36 = None
    permute_219 = torch.ops.aten.permute.default(view_329, [3, 4, 1, 0, 5, 2]);  view_329 = None
    view_330 = torch.ops.aten.view.default(permute_219, [1, 8192, 384, 8, 32]);  permute_219 = None
    mul_31 = torch.ops.aten.mul.Tensor(sigmoid_18, view_330);  sigmoid_18 = view_330 = None
    view_331 = torch.ops.aten.view.default(mul_31, [1, 8192, 384, 256]);  mul_31 = None
    _to_copy_201 = torch.ops.aten._to_copy.default(msa_module_msa_pair_weighted_averaging_1_linear_out_no_bias_weight, dtype = torch.bfloat16);  msa_module_msa_pair_weighted_averaging_1_linear_out_no_bias_weight = None
    t_66 = torch.ops.aten.t.default(_to_copy_201);  _to_copy_201 = None
    view_332 = torch.ops.aten.view.default(view_331, [3145728, 256]);  view_331 = None
    mm_61 = torch.ops.aten.mm.default(view_332, t_66);  view_332 = t_66 = None
    view_333 = torch.ops.aten.view.default(mm_61, [1, 8192, 384, 64]);  mm_61 = None
    cat_6 = torch.ops.aten.cat.default([view_320, view_333], dim = 1);  view_320 = view_333 = None
    add_30 = torch.ops.aten.add.Tensor(add_29, cat_6);  add_29 = cat_6 = None
    _to_copy_202 = torch.ops.aten._to_copy.default(add_28, dtype = torch.float32)
    native_layer_norm_default_43 = torch.ops.aten.native_layer_norm.default(_to_copy_202, [256], msa_module_triangular_multiplication_1_layernorm_z_in_weight, msa_module_triangular_multiplication_1_layernorm_z_in_bias, 1e-05);  _to_copy_202 = msa_module_triangular_multiplication_1_layernorm_z_in_weight = msa_module_triangular_multiplication_1_layernorm_z_in_bias = None
    getitem_279 = native_layer_norm_default_43[0];  native_layer_norm_default_43 = None
    split_with_sizes_default_6 = torch.ops.aten.split_with_sizes.default(msa_module_triangular_multiplication_1_merged_linear_p_weight, [512, 512]);  msa_module_triangular_multiplication_1_merged_linear_p_weight = None
    getitem_282 = split_with_sizes_default_6[0]
    getitem_283 = split_with_sizes_default_6[1];  split_with_sizes_default_6 = None
    split_with_sizes_default_7 = torch.ops.aten.split_with_sizes.default(msa_module_triangular_multiplication_1_merged_linear_g_weight, [512, 512, 256]);  msa_module_triangular_multiplication_1_merged_linear_g_weight = None
    getitem_284 = split_with_sizes_default_7[0]
    getitem_285 = split_with_sizes_default_7[1]
    getitem_286 = split_with_sizes_default_7[2];  split_with_sizes_default_7 = None
    _to_copy_203 = torch.ops.aten._to_copy.default(getitem_282, dtype = torch.bfloat16);  getitem_282 = None
    _to_copy_204 = torch.ops.aten._to_copy.default(getitem_279, dtype = torch.bfloat16)
    t_67 = torch.ops.aten.t.default(_to_copy_203);  _to_copy_203 = None
    view_334 = torch.ops.aten.view.default(_to_copy_204, [147456, 256]);  _to_copy_204 = None
    mm_62 = torch.ops.aten.mm.default(view_334, t_67);  view_334 = t_67 = None
    view_335 = torch.ops.aten.view.default(mm_62, [1, 384, 384, 512]);  mm_62 = None
    _to_copy_205 = torch.ops.aten._to_copy.default(getitem_284, dtype = torch.bfloat16);  getitem_284 = None
    _to_copy_206 = torch.ops.aten._to_copy.default(getitem_279, dtype = torch.bfloat16)
    t_68 = torch.ops.aten.t.default(_to_copy_205);  _to_copy_205 = None
    view_336 = torch.ops.aten.view.default(_to_copy_206, [147456, 256]);  _to_copy_206 = None
    mm_63 = torch.ops.aten.mm.default(view_336, t_68);  view_336 = t_68 = None
    view_337 = torch.ops.aten.view.default(mm_63, [1, 384, 384, 512]);  mm_63 = None
    sigmoid_19 = torch.ops.aten.sigmoid.default(view_337);  view_337 = None
    mul_32 = torch.ops.aten.mul.Tensor(view_335, sigmoid_19);  view_335 = sigmoid_19 = None
    unsqueeze_161 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
    bitwise_not_25 = torch.ops.aten.bitwise_not.default(unsqueeze_161);  unsqueeze_161 = None
    masked_fill_25 = torch.ops.aten.masked_fill.Scalar(mul_32, bitwise_not_25, 0);  mul_32 = bitwise_not_25 = None
    split_tensor_26 = torch.ops.aten.split.Tensor(masked_fill_25, 256, dim = -1)
    getitem_289 = split_tensor_26[0];  split_tensor_26 = None
    unsqueeze_164 = torch.ops.aten.unsqueeze.default(getitem_289, 4);  getitem_289 = None
    permute_224 = torch.ops.aten.permute.default(unsqueeze_164, [0, 1, 4, 3, 2]);  unsqueeze_164 = None
    permute_225 = torch.ops.aten.permute.default(permute_224, [3, 1, 4, 0, 2]);  permute_224 = None
    view_340 = torch.ops.aten.view.default(permute_225, [256, 384, 384]);  permute_225 = None
    split_tensor_27 = torch.ops.aten.split.Tensor(masked_fill_25, 256, dim = -1);  masked_fill_25 = None
    getitem_292 = split_tensor_27[1];  split_tensor_27 = None
    unsqueeze_165 = torch.ops.aten.unsqueeze.default(getitem_292, 4);  getitem_292 = None
    permute_226 = torch.ops.aten.permute.default(unsqueeze_165, [0, 4, 1, 3, 2]);  unsqueeze_165 = None
    permute_227 = torch.ops.aten.permute.default(permute_226, [3, 4, 0, 2, 1]);  permute_226 = None
    view_341 = torch.ops.aten.view.default(permute_227, [256, 384, 384]);  permute_227 = None
    bmm_37 = torch.ops.aten.bmm.default(view_340, view_341);  view_340 = view_341 = None
    view_342 = torch.ops.aten.view.default(bmm_37, [256, 384, 1, 1, 384]);  bmm_37 = None
    permute_228 = torch.ops.aten.permute.default(view_342, [3, 1, 4, 0, 2]);  view_342 = None
    view_343 = torch.ops.aten.view.default(permute_228, [1, 384, 384, 256]);  permute_228 = None
    _to_copy_207 = torch.ops.aten._to_copy.default(getitem_283, dtype = torch.bfloat16);  getitem_283 = None
    _to_copy_208 = torch.ops.aten._to_copy.default(getitem_279, dtype = torch.bfloat16)
    t_69 = torch.ops.aten.t.default(_to_copy_207);  _to_copy_207 = None
    view_344 = torch.ops.aten.view.default(_to_copy_208, [147456, 256]);  _to_copy_208 = None
    mm_64 = torch.ops.aten.mm.default(view_344, t_69);  view_344 = t_69 = None
    view_345 = torch.ops.aten.view.default(mm_64, [1, 384, 384, 512]);  mm_64 = None
    _to_copy_209 = torch.ops.aten._to_copy.default(getitem_285, dtype = torch.bfloat16);  getitem_285 = None
    _to_copy_210 = torch.ops.aten._to_copy.default(getitem_279, dtype = torch.bfloat16)
    t_70 = torch.ops.aten.t.default(_to_copy_209);  _to_copy_209 = None
    view_346 = torch.ops.aten.view.default(_to_copy_210, [147456, 256]);  _to_copy_210 = None
    mm_65 = torch.ops.aten.mm.default(view_346, t_70);  view_346 = t_70 = None
    view_347 = torch.ops.aten.view.default(mm_65, [1, 384, 384, 512]);  mm_65 = None
    sigmoid_20 = torch.ops.aten.sigmoid.default(view_347);  view_347 = None
    mul_33 = torch.ops.aten.mul.Tensor(view_345, sigmoid_20);  view_345 = sigmoid_20 = None
    view_348 = torch.ops.aten.view.default(mul_33, [147456, 512]);  mul_33 = None
    view_349 = torch.ops.aten.view.default(view_348, [1, 384, 384, 512]);  view_348 = None
    transpose_6 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
    unsqueeze_166 = torch.ops.aten.unsqueeze.default(transpose_6, 3);  transpose_6 = None
    clone_54 = torch.ops.aten.clone.default(unsqueeze_166, memory_format = torch.contiguous_format);  unsqueeze_166 = None
    bitwise_not_26 = torch.ops.aten.bitwise_not.default(clone_54);  clone_54 = None
    masked_fill_26 = torch.ops.aten.masked_fill.Scalar(view_349, bitwise_not_26, 0);  view_349 = bitwise_not_26 = None
    view_350 = torch.ops.aten.view.default(masked_fill_26, [147456, 512]);  masked_fill_26 = None
    view_354 = torch.ops.aten.view.default(view_350, [1, 384, 384, 512])
    split_tensor_28 = torch.ops.aten.split.Tensor(view_354, 256, dim = -1);  view_354 = None
    getitem_295 = split_tensor_28[0];  split_tensor_28 = None
    unsqueeze_169 = torch.ops.aten.unsqueeze.default(getitem_295, 4);  getitem_295 = None
    permute_233 = torch.ops.aten.permute.default(unsqueeze_169, [0, 2, 4, 3, 1]);  unsqueeze_169 = None
    permute_234 = torch.ops.aten.permute.default(permute_233, [3, 1, 4, 0, 2]);  permute_233 = None
    view_355 = torch.ops.aten.view.default(permute_234, [256, 384, 384]);  permute_234 = None
    view_356 = torch.ops.aten.view.default(view_350, [1, 384, 384, 512]);  view_350 = None
    split_tensor_29 = torch.ops.aten.split.Tensor(view_356, 256, dim = -1);  view_356 = None
    getitem_298 = split_tensor_29[1];  split_tensor_29 = None
    unsqueeze_170 = torch.ops.aten.unsqueeze.default(getitem_298, 4);  getitem_298 = None
    permute_235 = torch.ops.aten.permute.default(unsqueeze_170, [0, 4, 2, 3, 1]);  unsqueeze_170 = None
    permute_236 = torch.ops.aten.permute.default(permute_235, [3, 4, 0, 2, 1]);  permute_235 = None
    view_357 = torch.ops.aten.view.default(permute_236, [256, 384, 384]);  permute_236 = None
    bmm_38 = torch.ops.aten.bmm.default(view_355, view_357);  view_355 = view_357 = None
    view_358 = torch.ops.aten.view.default(bmm_38, [256, 384, 1, 1, 384]);  bmm_38 = None
    permute_237 = torch.ops.aten.permute.default(view_358, [3, 1, 4, 0, 2]);  view_358 = None
    view_359 = torch.ops.aten.view.default(permute_237, [1, 384, 384, 256]);  permute_237 = None
    _to_copy_211 = torch.ops.aten._to_copy.default(view_343, dtype = torch.float32);  view_343 = None
    native_layer_norm_default_44 = torch.ops.aten.native_layer_norm.default(_to_copy_211, [256], None, None, 1e-05);  _to_copy_211 = None
    getitem_299 = native_layer_norm_default_44[0];  native_layer_norm_default_44 = None
    _to_copy_212 = torch.ops.aten._to_copy.default(view_359, dtype = torch.float32);  view_359 = None
    native_layer_norm_default_45 = torch.ops.aten.native_layer_norm.default(_to_copy_212, [256], None, None, 1e-05);  _to_copy_212 = None
    getitem_302 = native_layer_norm_default_45[0];  native_layer_norm_default_45 = None
    add_31 = torch.ops.aten.add.Tensor(getitem_299, getitem_302);  getitem_299 = getitem_302 = None
    _to_copy_213 = torch.ops.aten._to_copy.default(msa_module_triangular_multiplication_1_linear_z_out_weight, dtype = torch.bfloat16);  msa_module_triangular_multiplication_1_linear_z_out_weight = None
    _to_copy_214 = torch.ops.aten._to_copy.default(add_31, dtype = torch.bfloat16);  add_31 = None
    t_71 = torch.ops.aten.t.default(_to_copy_213);  _to_copy_213 = None
    view_360 = torch.ops.aten.view.default(_to_copy_214, [147456, 256]);  _to_copy_214 = None
    mm_66 = torch.ops.aten.mm.default(view_360, t_71);  view_360 = t_71 = None
    view_361 = torch.ops.aten.view.default(mm_66, [1, 384, 384, 256]);  mm_66 = None
    _to_copy_215 = torch.ops.aten._to_copy.default(getitem_286, dtype = torch.bfloat16);  getitem_286 = None
    _to_copy_216 = torch.ops.aten._to_copy.default(getitem_279, dtype = torch.bfloat16);  getitem_279 = None
    t_72 = torch.ops.aten.t.default(_to_copy_215);  _to_copy_215 = None
    view_362 = torch.ops.aten.view.default(_to_copy_216, [147456, 256]);  _to_copy_216 = None
    mm_67 = torch.ops.aten.mm.default(view_362, t_72);  view_362 = t_72 = None
    view_363 = torch.ops.aten.view.default(mm_67, [1, 384, 384, 256]);  mm_67 = None
    sigmoid_21 = torch.ops.aten.sigmoid.default(view_363);  view_363 = None
    mul_34 = torch.ops.aten.mul.Tensor(view_361, sigmoid_21);  view_361 = sigmoid_21 = None
    add_32 = torch.ops.aten.add.Tensor(add_28, mul_34);  mul_34 = None
    split_tensor_30 = torch.ops.aten.split.Tensor(add_28, 384, dim = -2);  add_28 = None
    getitem_305 = split_tensor_30[0];  split_tensor_30 = None
    _to_copy_217 = torch.ops.aten._to_copy.default(getitem_305, dtype = torch.float32);  getitem_305 = None
    native_layer_norm_default_46 = torch.ops.aten.native_layer_norm.default(_to_copy_217, [256], msa_module_pair_transition_1_layer_norm_weight, msa_module_pair_transition_1_layer_norm_bias, 1e-05);  _to_copy_217 = msa_module_pair_transition_1_layer_norm_weight = msa_module_pair_transition_1_layer_norm_bias = None
    getitem_306 = native_layer_norm_default_46[0];  native_layer_norm_default_46 = None
    _to_copy_218 = torch.ops.aten._to_copy.default(msa_module_pair_transition_1_linear_no_bias_ab_weight, dtype = torch.bfloat16);  msa_module_pair_transition_1_linear_no_bias_ab_weight = None
    _to_copy_219 = torch.ops.aten._to_copy.default(getitem_306, dtype = torch.bfloat16);  getitem_306 = None
    t_73 = torch.ops.aten.t.default(_to_copy_218);  _to_copy_218 = None
    view_364 = torch.ops.aten.view.default(_to_copy_219, [147456, 256]);  _to_copy_219 = None
    mm_68 = torch.ops.aten.mm.default(view_364, t_73);  view_364 = t_73 = None
    view_365 = torch.ops.aten.view.default(mm_68, [1, 384, 384, 2048]);  mm_68 = None
    split_tensor_31 = torch.ops.aten.split.Tensor(view_365, 1024, dim = -1);  view_365 = None
    getitem_309 = split_tensor_31[0]
    getitem_310 = split_tensor_31[1];  split_tensor_31 = None
    silu_9 = torch.ops.aten.silu.default(getitem_309);  getitem_309 = None
    mul_35 = torch.ops.aten.mul.Tensor(silu_9, getitem_310);  silu_9 = getitem_310 = None
    _to_copy_220 = torch.ops.aten._to_copy.default(msa_module_pair_transition_1_linear_out_weight, dtype = torch.bfloat16);  msa_module_pair_transition_1_linear_out_weight = None
    t_74 = torch.ops.aten.t.default(_to_copy_220);  _to_copy_220 = None
    view_367 = torch.ops.aten.view.default(mul_35, [147456, 1024]);  mul_35 = None
    mm_69 = torch.ops.aten.mm.default(view_367, t_74);  view_367 = t_74 = None
    view_368 = torch.ops.aten.view.default(mm_69, [1, 384, 384, 256]);  mm_69 = None
    add_33 = torch.ops.aten.add.Tensor(add_32, view_368);  add_32 = view_368 = None
    _to_copy_221 = torch.ops.aten._to_copy.default(add_33, dtype = torch.float32)
    native_layer_norm_default_47 = torch.ops.aten.native_layer_norm.default(_to_copy_221, [256], None, None, 1e-05);  _to_copy_221 = None
    getitem_311 = native_layer_norm_default_47[0];  native_layer_norm_default_47 = None
    _to_copy_222 = torch.ops.aten._to_copy.default(msa_module_triangular_attention_1_pair2b_weight, dtype = torch.bfloat16);  msa_module_triangular_attention_1_pair2b_weight = None
    _to_copy_223 = torch.ops.aten._to_copy.default(getitem_311, dtype = torch.bfloat16)
    t_75 = torch.ops.aten.t.default(_to_copy_222);  _to_copy_222 = None
    view_369 = torch.ops.aten.view.default(_to_copy_223, [147456, 256]);  _to_copy_223 = None
    mm_70 = torch.ops.aten.mm.default(view_369, t_75);  view_369 = t_75 = None
    view_370 = torch.ops.aten.view.default(mm_70, [1, 384, 384, 8]);  mm_70 = None
    view_371 = torch.ops.aten.view.default(view_370, [1, 384, 384, 2, 4]);  view_370 = None
    permute_238 = torch.ops.aten.permute.default(view_371, [0, 3, 4, 1, 2]);  view_371 = None
    view_372 = torch.ops.aten.view.default(permute_238, [1, 2, 4, 1, 384, 384]);  permute_238 = None
    view_373 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 384, 384])
    bitwise_not_27 = torch.ops.aten.bitwise_not.default(view_373);  view_373 = None
    masked_fill_27 = torch.ops.aten.masked_fill.Scalar(view_372, bitwise_not_27, -10000);  view_372 = bitwise_not_27 = None
    view_374 = torch.ops.aten.view.default(masked_fill_27, [1, 2, 4, 384, 384]);  masked_fill_27 = None
    permute_239 = torch.ops.aten.permute.default(view_374, [1, 0, 2, 3, 4]);  view_374 = None
    view_375 = torch.ops.aten.view.default(permute_239, [2, 4, 1, 384, 384]);  permute_239 = None
    _to_copy_224 = torch.ops.aten._to_copy.default(msa_module_triangular_attention_1_pair2qkvg1_weight, dtype = torch.bfloat16);  msa_module_triangular_attention_1_pair2qkvg1_weight = None
    _to_copy_225 = torch.ops.aten._to_copy.default(getitem_311, dtype = torch.bfloat16)
    t_76 = torch.ops.aten.t.default(_to_copy_224);  _to_copy_224 = None
    view_376 = torch.ops.aten.view.default(_to_copy_225, [147456, 256]);  _to_copy_225 = None
    mm_71 = torch.ops.aten.mm.default(view_376, t_76);  view_376 = t_76 = None
    view_377 = torch.ops.aten.view.default(mm_71, [1, 384, 384, 1024]);  mm_71 = None
    select_7 = torch.ops.aten.select.int(view_375, 0, 0)
    view_378 = torch.ops.aten.view.default(view_377, [1, 384, 384, 4, 4, 64]);  view_377 = None
    permute_240 = torch.ops.aten.permute.default(view_378, [4, 0, 3, 1, 2, 5]);  view_378 = None
    view_379 = torch.ops.aten.view.default(permute_240, [4, 4, 384, 384, 64]);  permute_240 = None
    unbind_int_18 = torch.ops.aten.unbind.int(view_379);  view_379 = None
    getitem_314 = unbind_int_18[0]
    getitem_315 = unbind_int_18[1]
    getitem_316 = unbind_int_18[2]
    getitem_317 = unbind_int_18[3];  unbind_int_18 = None
    expand_15 = torch.ops.aten.expand.default(select_7, [4, 384, 384, 384]);  select_7 = None
    _scaled_dot_product_efficient_attention_default_6 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_314, getitem_315, getitem_316, expand_15, False);  getitem_314 = getitem_315 = getitem_316 = expand_15 = None
    getitem_318 = _scaled_dot_product_efficient_attention_default_6[0];  _scaled_dot_product_efficient_attention_default_6 = None
    sigmoid_22 = torch.ops.aten.sigmoid.default(getitem_317);  getitem_317 = None
    mul_36 = torch.ops.aten.mul.Tensor(getitem_318, sigmoid_22);  getitem_318 = sigmoid_22 = None
    view_380 = torch.ops.aten.view.default(mul_36, [1, 4, 384, 384, 64]);  mul_36 = None
    permute_241 = torch.ops.aten.permute.default(view_380, [0, 2, 3, 1, 4]);  view_380 = None
    clone_55 = torch.ops.aten.clone.default(permute_241, memory_format = torch.contiguous_format);  permute_241 = None
    _unsafe_view_50 = torch.ops.aten._unsafe_view.default(clone_55, [1, 384, 384, 256]);  clone_55 = None
    transpose_7 = torch.ops.aten.transpose.int(getitem_311, 1, 2);  getitem_311 = None
    _to_copy_226 = torch.ops.aten._to_copy.default(msa_module_triangular_attention_1_pair2qkvg2_weight, dtype = torch.bfloat16);  msa_module_triangular_attention_1_pair2qkvg2_weight = None
    _to_copy_227 = torch.ops.aten._to_copy.default(transpose_7, dtype = torch.bfloat16);  transpose_7 = None
    t_77 = torch.ops.aten.t.default(_to_copy_226);  _to_copy_226 = None
    expand_16 = torch.ops.aten.expand.default(_to_copy_227, [1, 384, 384, 256]);  _to_copy_227 = None
    view_381 = torch.ops.aten.view.default(expand_16, [384, 384, 256]);  expand_16 = None
    expand_17 = torch.ops.aten.expand.default(t_77, [1, 384, 256, 1024]);  t_77 = None
    view_382 = torch.ops.aten.view.default(expand_17, [384, 256, 1024]);  expand_17 = None
    bmm_39 = torch.ops.aten.bmm.default(view_381, view_382);  view_381 = view_382 = None
    view_383 = torch.ops.aten.view.default(bmm_39, [1, 384, 384, 1024]);  bmm_39 = None
    select_8 = torch.ops.aten.select.int(view_375, 0, 1);  view_375 = None
    view_384 = torch.ops.aten.view.default(view_383, [1, 384, 384, 4, 4, 64]);  view_383 = None
    permute_242 = torch.ops.aten.permute.default(view_384, [4, 0, 3, 1, 2, 5]);  view_384 = None
    view_385 = torch.ops.aten.view.default(permute_242, [4, 4, 384, 384, 64]);  permute_242 = None
    unbind_int_19 = torch.ops.aten.unbind.int(view_385);  view_385 = None
    getitem_322 = unbind_int_19[0]
    getitem_323 = unbind_int_19[1]
    getitem_324 = unbind_int_19[2]
    getitem_325 = unbind_int_19[3];  unbind_int_19 = None
    expand_18 = torch.ops.aten.expand.default(select_8, [4, 384, 384, 384]);  select_8 = None
    _scaled_dot_product_efficient_attention_default_7 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_322, getitem_323, getitem_324, expand_18, False);  getitem_322 = getitem_323 = getitem_324 = expand_18 = None
    getitem_326 = _scaled_dot_product_efficient_attention_default_7[0];  _scaled_dot_product_efficient_attention_default_7 = None
    sigmoid_23 = torch.ops.aten.sigmoid.default(getitem_325);  getitem_325 = None
    mul_37 = torch.ops.aten.mul.Tensor(getitem_326, sigmoid_23);  getitem_326 = sigmoid_23 = None
    view_386 = torch.ops.aten.view.default(mul_37, [1, 4, 384, 384, 64]);  mul_37 = None
    permute_243 = torch.ops.aten.permute.default(view_386, [0, 2, 3, 1, 4]);  view_386 = None
    clone_56 = torch.ops.aten.clone.default(permute_243, memory_format = torch.contiguous_format);  permute_243 = None
    _unsafe_view_51 = torch.ops.aten._unsafe_view.default(clone_56, [1, 384, 384, 256]);  clone_56 = None
    cat_7 = torch.ops.aten.cat.default([_unsafe_view_50, _unsafe_view_51], dim = -1);  _unsafe_view_50 = _unsafe_view_51 = None
    slice_90 = torch.ops.aten.slice.Tensor(msa_module_triangular_attention_1_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  msa_module_triangular_attention_1_out_scalers = None
    unsqueeze_171 = torch.ops.aten.unsqueeze.default(slice_90, 1);  slice_90 = None
    mul_38 = torch.ops.aten.mul.Tensor(msa_module_triangular_attention_1_linear_out_weight, unsqueeze_171);  msa_module_triangular_attention_1_linear_out_weight = unsqueeze_171 = None
    _to_copy_228 = torch.ops.aten._to_copy.default(mul_38, dtype = torch.bfloat16);  mul_38 = None
    t_78 = torch.ops.aten.t.default(_to_copy_228);  _to_copy_228 = None
    view_387 = torch.ops.aten.view.default(cat_7, [147456, 512]);  cat_7 = None
    mm_72 = torch.ops.aten.mm.default(view_387, t_78);  view_387 = t_78 = None
    view_388 = torch.ops.aten.view.default(mm_72, [1, 384, 384, 256]);  mm_72 = None
    add_34 = torch.ops.aten.add.Tensor(add_33, view_388);  add_33 = view_388 = None
    slice_91 = torch.ops.aten.slice.Tensor(add_30, dim = 0, start = 0, end = 9223372036854775807)
    slice_92 = torch.ops.aten.slice.Tensor(slice_91, dim = 1, start = 0, end = 4096);  slice_91 = None
    slice_93 = torch.ops.aten.slice.Tensor(slice_92, dim = 2, start = 0, end = 9223372036854775807);  slice_92 = None
    slice_94 = torch.ops.aten.slice.Tensor(slice_93, dim = 3, start = 0, end = 9223372036854775807);  slice_93 = None
    slice_95 = torch.ops.aten.slice.Tensor(arg1403_1, dim = 0, start = 0, end = 9223372036854775807)
    slice_96 = torch.ops.aten.slice.Tensor(slice_95, dim = 1, start = 0, end = 4096);  slice_95 = None
    slice_97 = torch.ops.aten.slice.Tensor(slice_96, dim = 2, start = 0, end = 9223372036854775807);  slice_96 = None
    _to_copy_229 = torch.ops.aten._to_copy.default(slice_94, dtype = torch.float32);  slice_94 = None
    native_layer_norm_default_48 = torch.ops.aten.native_layer_norm.default(_to_copy_229, [64], None, None, 1e-05);  _to_copy_229 = None
    getitem_330 = native_layer_norm_default_48[0];  native_layer_norm_default_48 = None
    view_389 = torch.ops.aten.view.default(slice_97, [1, 4096, 384, 1]);  slice_97 = None
    bitwise_not_28 = torch.ops.aten.bitwise_not.default(view_389);  view_389 = None
    masked_fill_28 = torch.ops.aten.masked_fill.Scalar(getitem_330, bitwise_not_28, 0);  getitem_330 = bitwise_not_28 = None
    unbind_int_20 = torch.ops.aten.unbind.int(msa_module_outer_product_mean_2_weight_ab)
    getitem_333 = unbind_int_20[0]
    getitem_334 = unbind_int_20[1];  unbind_int_20 = None
    _to_copy_230 = torch.ops.aten._to_copy.default(getitem_333, dtype = torch.bfloat16);  getitem_333 = None
    _to_copy_231 = torch.ops.aten._to_copy.default(masked_fill_28, dtype = torch.bfloat16)
    unsqueeze_172 = torch.ops.aten.unsqueeze.default(_to_copy_230, 3);  _to_copy_230 = None
    unsqueeze_173 = torch.ops.aten.unsqueeze.default(unsqueeze_172, 4);  unsqueeze_172 = None
    unsqueeze_174 = torch.ops.aten.unsqueeze.default(unsqueeze_173, 5);  unsqueeze_173 = None
    permute_244 = torch.ops.aten.permute.default(unsqueeze_174, [0, 1, 3, 4, 5, 2]);  unsqueeze_174 = None
    unsqueeze_175 = torch.ops.aten.unsqueeze.default(_to_copy_231, 4);  _to_copy_231 = None
    unsqueeze_176 = torch.ops.aten.unsqueeze.default(unsqueeze_175, 5);  unsqueeze_175 = None
    permute_245 = torch.ops.aten.permute.default(unsqueeze_176, [4, 5, 0, 1, 2, 3]);  unsqueeze_176 = None
    permute_246 = torch.ops.aten.permute.default(permute_244, [0, 1, 5, 2, 3, 4]);  permute_244 = None
    view_390 = torch.ops.aten.view.default(permute_246, [1, 64, 64]);  permute_246 = None
    permute_247 = torch.ops.aten.permute.default(permute_245, [5, 2, 3, 4, 0, 1]);  permute_245 = None
    view_391 = torch.ops.aten.view.default(permute_247, [1, 64, 1572864]);  permute_247 = None
    bmm_40 = torch.ops.aten.bmm.default(view_390, view_391);  view_390 = view_391 = None
    view_392 = torch.ops.aten.view.default(bmm_40, [8, 8, 1, 1, 4096, 384]);  bmm_40 = None
    permute_248 = torch.ops.aten.permute.default(view_392, [0, 1, 3, 4, 5, 2]);  view_392 = None
    view_393 = torch.ops.aten.view.default(permute_248, [8, 8, 1, 4096, 384]);  permute_248 = None
    _to_copy_232 = torch.ops.aten._to_copy.default(getitem_334, dtype = torch.bfloat16);  getitem_334 = None
    _to_copy_233 = torch.ops.aten._to_copy.default(masked_fill_28, dtype = torch.bfloat16);  masked_fill_28 = None
    unsqueeze_177 = torch.ops.aten.unsqueeze.default(_to_copy_232, 3);  _to_copy_232 = None
    unsqueeze_178 = torch.ops.aten.unsqueeze.default(unsqueeze_177, 4);  unsqueeze_177 = None
    unsqueeze_179 = torch.ops.aten.unsqueeze.default(unsqueeze_178, 5);  unsqueeze_178 = None
    permute_249 = torch.ops.aten.permute.default(unsqueeze_179, [0, 1, 3, 4, 5, 2]);  unsqueeze_179 = None
    unsqueeze_180 = torch.ops.aten.unsqueeze.default(_to_copy_233, 4);  _to_copy_233 = None
    unsqueeze_181 = torch.ops.aten.unsqueeze.default(unsqueeze_180, 5);  unsqueeze_180 = None
    permute_250 = torch.ops.aten.permute.default(unsqueeze_181, [4, 5, 0, 1, 2, 3]);  unsqueeze_181 = None
    permute_251 = torch.ops.aten.permute.default(permute_249, [0, 1, 5, 2, 3, 4]);  permute_249 = None
    view_394 = torch.ops.aten.view.default(permute_251, [1, 64, 64]);  permute_251 = None
    permute_252 = torch.ops.aten.permute.default(permute_250, [5, 2, 3, 4, 0, 1]);  permute_250 = None
    view_395 = torch.ops.aten.view.default(permute_252, [1, 64, 1572864]);  permute_252 = None
    bmm_41 = torch.ops.aten.bmm.default(view_394, view_395);  view_394 = view_395 = None
    view_396 = torch.ops.aten.view.default(bmm_41, [8, 8, 1, 1, 4096, 384]);  bmm_41 = None
    permute_253 = torch.ops.aten.permute.default(view_396, [0, 1, 3, 4, 5, 2]);  view_396 = None
    view_397 = torch.ops.aten.view.default(permute_253, [8, 8, 1, 4096, 384]);  permute_253 = None
    unsqueeze_182 = torch.ops.aten.unsqueeze.default(view_393, 5);  view_393 = None
    unsqueeze_183 = torch.ops.aten.unsqueeze.default(unsqueeze_182, 6);  unsqueeze_182 = None
    permute_254 = torch.ops.aten.permute.default(unsqueeze_183, [2, 4, 5, 0, 1, 6, 3]);  unsqueeze_183 = None
    unsqueeze_184 = torch.ops.aten.unsqueeze.default(view_397, 5);  view_397 = None
    unsqueeze_185 = torch.ops.aten.unsqueeze.default(unsqueeze_184, 6);  unsqueeze_184 = None
    permute_255 = torch.ops.aten.permute.default(unsqueeze_185, [2, 5, 4, 0, 6, 1, 3]);  unsqueeze_185 = None
    permute_256 = torch.ops.aten.permute.default(permute_254, [3, 1, 4, 6, 0, 2, 5]);  permute_254 = None
    clone_57 = torch.ops.aten.clone.default(permute_256, memory_format = torch.contiguous_format);  permute_256 = None
    _unsafe_view_52 = torch.ops.aten._unsafe_view.default(clone_57, [8, 3072, 4096]);  clone_57 = None
    permute_257 = torch.ops.aten.permute.default(permute_255, [3, 6, 0, 2, 5, 1, 4]);  permute_255 = None
    clone_58 = torch.ops.aten.clone.default(permute_257, memory_format = torch.contiguous_format);  permute_257 = None
    _unsafe_view_53 = torch.ops.aten._unsafe_view.default(clone_58, [8, 4096, 3072]);  clone_58 = None
    bmm_42 = torch.ops.aten.bmm.default(_unsafe_view_52, _unsafe_view_53);  _unsafe_view_52 = _unsafe_view_53 = None
    view_398 = torch.ops.aten.view.default(bmm_42, [8, 384, 8, 1, 1, 384, 8]);  bmm_42 = None
    permute_258 = torch.ops.aten.permute.default(view_398, [4, 1, 5, 0, 2, 6, 3]);  view_398 = None
    view_399 = torch.ops.aten.view.default(permute_258, [1, 384, 384, 8, 8, 8]);  permute_258 = None
    clone_59 = torch.ops.aten.clone.default(view_399, memory_format = torch.contiguous_format);  view_399 = None
    _unsafe_view_54 = torch.ops.aten._unsafe_view.default(clone_59, [1, 384, 384, 512]);  clone_59 = None
    add_35 = torch.ops.aten.add.Tensor(_unsafe_view_54, 0);  _unsafe_view_54 = None
    slice_98 = torch.ops.aten.slice.Tensor(add_30, dim = 0, start = 0, end = 9223372036854775807)
    slice_99 = torch.ops.aten.slice.Tensor(slice_98, dim = 1, start = 4096, end = 8192);  slice_98 = None
    slice_100 = torch.ops.aten.slice.Tensor(slice_99, dim = 2, start = 0, end = 9223372036854775807);  slice_99 = None
    slice_101 = torch.ops.aten.slice.Tensor(slice_100, dim = 3, start = 0, end = 9223372036854775807);  slice_100 = None
    slice_102 = torch.ops.aten.slice.Tensor(arg1403_1, dim = 0, start = 0, end = 9223372036854775807)
    slice_103 = torch.ops.aten.slice.Tensor(slice_102, dim = 1, start = 4096, end = 8192);  slice_102 = None
    slice_104 = torch.ops.aten.slice.Tensor(slice_103, dim = 2, start = 0, end = 9223372036854775807);  slice_103 = None
    _to_copy_234 = torch.ops.aten._to_copy.default(slice_101, dtype = torch.float32);  slice_101 = None
    native_layer_norm_default_49 = torch.ops.aten.native_layer_norm.default(_to_copy_234, [64], None, None, 1e-05);  _to_copy_234 = None
    getitem_335 = native_layer_norm_default_49[0];  native_layer_norm_default_49 = None
    view_400 = torch.ops.aten.view.default(slice_104, [1, 4096, 384, 1]);  slice_104 = None
    bitwise_not_29 = torch.ops.aten.bitwise_not.default(view_400);  view_400 = None
    masked_fill_29 = torch.ops.aten.masked_fill.Scalar(getitem_335, bitwise_not_29, 0);  getitem_335 = bitwise_not_29 = None
    unbind_int_21 = torch.ops.aten.unbind.int(msa_module_outer_product_mean_2_weight_ab)
    getitem_338 = unbind_int_21[0]
    getitem_339 = unbind_int_21[1];  unbind_int_21 = None
    _to_copy_235 = torch.ops.aten._to_copy.default(getitem_338, dtype = torch.bfloat16);  getitem_338 = None
    _to_copy_236 = torch.ops.aten._to_copy.default(masked_fill_29, dtype = torch.bfloat16)
    unsqueeze_186 = torch.ops.aten.unsqueeze.default(_to_copy_235, 3);  _to_copy_235 = None
    unsqueeze_187 = torch.ops.aten.unsqueeze.default(unsqueeze_186, 4);  unsqueeze_186 = None
    unsqueeze_188 = torch.ops.aten.unsqueeze.default(unsqueeze_187, 5);  unsqueeze_187 = None
    permute_259 = torch.ops.aten.permute.default(unsqueeze_188, [0, 1, 3, 4, 5, 2]);  unsqueeze_188 = None
    unsqueeze_189 = torch.ops.aten.unsqueeze.default(_to_copy_236, 4);  _to_copy_236 = None
    unsqueeze_190 = torch.ops.aten.unsqueeze.default(unsqueeze_189, 5);  unsqueeze_189 = None
    permute_260 = torch.ops.aten.permute.default(unsqueeze_190, [4, 5, 0, 1, 2, 3]);  unsqueeze_190 = None
    permute_261 = torch.ops.aten.permute.default(permute_259, [0, 1, 5, 2, 3, 4]);  permute_259 = None
    view_401 = torch.ops.aten.view.default(permute_261, [1, 64, 64]);  permute_261 = None
    permute_262 = torch.ops.aten.permute.default(permute_260, [5, 2, 3, 4, 0, 1]);  permute_260 = None
    view_402 = torch.ops.aten.view.default(permute_262, [1, 64, 1572864]);  permute_262 = None
    bmm_43 = torch.ops.aten.bmm.default(view_401, view_402);  view_401 = view_402 = None
    view_403 = torch.ops.aten.view.default(bmm_43, [8, 8, 1, 1, 4096, 384]);  bmm_43 = None
    permute_263 = torch.ops.aten.permute.default(view_403, [0, 1, 3, 4, 5, 2]);  view_403 = None
    view_404 = torch.ops.aten.view.default(permute_263, [8, 8, 1, 4096, 384]);  permute_263 = None
    _to_copy_237 = torch.ops.aten._to_copy.default(getitem_339, dtype = torch.bfloat16);  getitem_339 = None
    _to_copy_238 = torch.ops.aten._to_copy.default(masked_fill_29, dtype = torch.bfloat16);  masked_fill_29 = None
    unsqueeze_191 = torch.ops.aten.unsqueeze.default(_to_copy_237, 3);  _to_copy_237 = None
    unsqueeze_192 = torch.ops.aten.unsqueeze.default(unsqueeze_191, 4);  unsqueeze_191 = None
    unsqueeze_193 = torch.ops.aten.unsqueeze.default(unsqueeze_192, 5);  unsqueeze_192 = None
    permute_264 = torch.ops.aten.permute.default(unsqueeze_193, [0, 1, 3, 4, 5, 2]);  unsqueeze_193 = None
    unsqueeze_194 = torch.ops.aten.unsqueeze.default(_to_copy_238, 4);  _to_copy_238 = None
    unsqueeze_195 = torch.ops.aten.unsqueeze.default(unsqueeze_194, 5);  unsqueeze_194 = None
    permute_265 = torch.ops.aten.permute.default(unsqueeze_195, [4, 5, 0, 1, 2, 3]);  unsqueeze_195 = None
    permute_266 = torch.ops.aten.permute.default(permute_264, [0, 1, 5, 2, 3, 4]);  permute_264 = None
    view_405 = torch.ops.aten.view.default(permute_266, [1, 64, 64]);  permute_266 = None
    permute_267 = torch.ops.aten.permute.default(permute_265, [5, 2, 3, 4, 0, 1]);  permute_265 = None
    view_406 = torch.ops.aten.view.default(permute_267, [1, 64, 1572864]);  permute_267 = None
    bmm_44 = torch.ops.aten.bmm.default(view_405, view_406);  view_405 = view_406 = None
    view_407 = torch.ops.aten.view.default(bmm_44, [8, 8, 1, 1, 4096, 384]);  bmm_44 = None
    permute_268 = torch.ops.aten.permute.default(view_407, [0, 1, 3, 4, 5, 2]);  view_407 = None
    view_408 = torch.ops.aten.view.default(permute_268, [8, 8, 1, 4096, 384]);  permute_268 = None
    unsqueeze_196 = torch.ops.aten.unsqueeze.default(view_404, 5);  view_404 = None
    unsqueeze_197 = torch.ops.aten.unsqueeze.default(unsqueeze_196, 6);  unsqueeze_196 = None
    permute_269 = torch.ops.aten.permute.default(unsqueeze_197, [2, 4, 5, 0, 1, 6, 3]);  unsqueeze_197 = None
    unsqueeze_198 = torch.ops.aten.unsqueeze.default(view_408, 5);  view_408 = None
    unsqueeze_199 = torch.ops.aten.unsqueeze.default(unsqueeze_198, 6);  unsqueeze_198 = None
    permute_270 = torch.ops.aten.permute.default(unsqueeze_199, [2, 5, 4, 0, 6, 1, 3]);  unsqueeze_199 = None
    permute_271 = torch.ops.aten.permute.default(permute_269, [3, 1, 4, 6, 0, 2, 5]);  permute_269 = None
    clone_60 = torch.ops.aten.clone.default(permute_271, memory_format = torch.contiguous_format);  permute_271 = None
    _unsafe_view_55 = torch.ops.aten._unsafe_view.default(clone_60, [8, 3072, 4096]);  clone_60 = None
    permute_272 = torch.ops.aten.permute.default(permute_270, [3, 6, 0, 2, 5, 1, 4]);  permute_270 = None
    clone_61 = torch.ops.aten.clone.default(permute_272, memory_format = torch.contiguous_format);  permute_272 = None
    _unsafe_view_56 = torch.ops.aten._unsafe_view.default(clone_61, [8, 4096, 3072]);  clone_61 = None
    bmm_45 = torch.ops.aten.bmm.default(_unsafe_view_55, _unsafe_view_56);  _unsafe_view_55 = _unsafe_view_56 = None
    view_409 = torch.ops.aten.view.default(bmm_45, [8, 384, 8, 1, 1, 384, 8]);  bmm_45 = None
    permute_273 = torch.ops.aten.permute.default(view_409, [4, 1, 5, 0, 2, 6, 3]);  view_409 = None
    view_410 = torch.ops.aten.view.default(permute_273, [1, 384, 384, 8, 8, 8]);  permute_273 = None
    clone_62 = torch.ops.aten.clone.default(view_410, memory_format = torch.contiguous_format);  view_410 = None
    _unsafe_view_57 = torch.ops.aten._unsafe_view.default(clone_62, [1, 384, 384, 512]);  clone_62 = None
    add_36 = torch.ops.aten.add.Tensor(add_35, _unsafe_view_57);  add_35 = _unsafe_view_57 = None
    slice_105 = torch.ops.aten.slice.Tensor(add_30, dim = 0, start = 0, end = 9223372036854775807)
    slice_106 = torch.ops.aten.slice.Tensor(slice_105, dim = 1, start = 8192, end = 12288);  slice_105 = None
    slice_107 = torch.ops.aten.slice.Tensor(slice_106, dim = 2, start = 0, end = 9223372036854775807);  slice_106 = None
    slice_108 = torch.ops.aten.slice.Tensor(slice_107, dim = 3, start = 0, end = 9223372036854775807);  slice_107 = None
    slice_109 = torch.ops.aten.slice.Tensor(arg1403_1, dim = 0, start = 0, end = 9223372036854775807)
    slice_110 = torch.ops.aten.slice.Tensor(slice_109, dim = 1, start = 8192, end = 12288);  slice_109 = None
    slice_111 = torch.ops.aten.slice.Tensor(slice_110, dim = 2, start = 0, end = 9223372036854775807);  slice_110 = None
    _to_copy_239 = torch.ops.aten._to_copy.default(slice_108, dtype = torch.float32);  slice_108 = None
    native_layer_norm_default_50 = torch.ops.aten.native_layer_norm.default(_to_copy_239, [64], None, None, 1e-05);  _to_copy_239 = None
    getitem_340 = native_layer_norm_default_50[0];  native_layer_norm_default_50 = None
    view_411 = torch.ops.aten.view.default(slice_111, [1, 4096, 384, 1]);  slice_111 = None
    bitwise_not_30 = torch.ops.aten.bitwise_not.default(view_411);  view_411 = None
    masked_fill_30 = torch.ops.aten.masked_fill.Scalar(getitem_340, bitwise_not_30, 0);  getitem_340 = bitwise_not_30 = None
    unbind_int_22 = torch.ops.aten.unbind.int(msa_module_outer_product_mean_2_weight_ab)
    getitem_343 = unbind_int_22[0]
    getitem_344 = unbind_int_22[1];  unbind_int_22 = None
    _to_copy_240 = torch.ops.aten._to_copy.default(getitem_343, dtype = torch.bfloat16);  getitem_343 = None
    _to_copy_241 = torch.ops.aten._to_copy.default(masked_fill_30, dtype = torch.bfloat16)
    unsqueeze_200 = torch.ops.aten.unsqueeze.default(_to_copy_240, 3);  _to_copy_240 = None
    unsqueeze_201 = torch.ops.aten.unsqueeze.default(unsqueeze_200, 4);  unsqueeze_200 = None
    unsqueeze_202 = torch.ops.aten.unsqueeze.default(unsqueeze_201, 5);  unsqueeze_201 = None
    permute_274 = torch.ops.aten.permute.default(unsqueeze_202, [0, 1, 3, 4, 5, 2]);  unsqueeze_202 = None
    unsqueeze_203 = torch.ops.aten.unsqueeze.default(_to_copy_241, 4);  _to_copy_241 = None
    unsqueeze_204 = torch.ops.aten.unsqueeze.default(unsqueeze_203, 5);  unsqueeze_203 = None
    permute_275 = torch.ops.aten.permute.default(unsqueeze_204, [4, 5, 0, 1, 2, 3]);  unsqueeze_204 = None
    permute_276 = torch.ops.aten.permute.default(permute_274, [0, 1, 5, 2, 3, 4]);  permute_274 = None
    view_412 = torch.ops.aten.view.default(permute_276, [1, 64, 64]);  permute_276 = None
    permute_277 = torch.ops.aten.permute.default(permute_275, [5, 2, 3, 4, 0, 1]);  permute_275 = None
    view_413 = torch.ops.aten.view.default(permute_277, [1, 64, 1572864]);  permute_277 = None
    bmm_46 = torch.ops.aten.bmm.default(view_412, view_413);  view_412 = view_413 = None
    view_414 = torch.ops.aten.view.default(bmm_46, [8, 8, 1, 1, 4096, 384]);  bmm_46 = None
    permute_278 = torch.ops.aten.permute.default(view_414, [0, 1, 3, 4, 5, 2]);  view_414 = None
    view_415 = torch.ops.aten.view.default(permute_278, [8, 8, 1, 4096, 384]);  permute_278 = None
    _to_copy_242 = torch.ops.aten._to_copy.default(getitem_344, dtype = torch.bfloat16);  getitem_344 = None
    _to_copy_243 = torch.ops.aten._to_copy.default(masked_fill_30, dtype = torch.bfloat16);  masked_fill_30 = None
    unsqueeze_205 = torch.ops.aten.unsqueeze.default(_to_copy_242, 3);  _to_copy_242 = None
    unsqueeze_206 = torch.ops.aten.unsqueeze.default(unsqueeze_205, 4);  unsqueeze_205 = None
    unsqueeze_207 = torch.ops.aten.unsqueeze.default(unsqueeze_206, 5);  unsqueeze_206 = None
    permute_279 = torch.ops.aten.permute.default(unsqueeze_207, [0, 1, 3, 4, 5, 2]);  unsqueeze_207 = None
    unsqueeze_208 = torch.ops.aten.unsqueeze.default(_to_copy_243, 4);  _to_copy_243 = None
    unsqueeze_209 = torch.ops.aten.unsqueeze.default(unsqueeze_208, 5);  unsqueeze_208 = None
    permute_280 = torch.ops.aten.permute.default(unsqueeze_209, [4, 5, 0, 1, 2, 3]);  unsqueeze_209 = None
    permute_281 = torch.ops.aten.permute.default(permute_279, [0, 1, 5, 2, 3, 4]);  permute_279 = None
    view_416 = torch.ops.aten.view.default(permute_281, [1, 64, 64]);  permute_281 = None
    permute_282 = torch.ops.aten.permute.default(permute_280, [5, 2, 3, 4, 0, 1]);  permute_280 = None
    view_417 = torch.ops.aten.view.default(permute_282, [1, 64, 1572864]);  permute_282 = None
    bmm_47 = torch.ops.aten.bmm.default(view_416, view_417);  view_416 = view_417 = None
    view_418 = torch.ops.aten.view.default(bmm_47, [8, 8, 1, 1, 4096, 384]);  bmm_47 = None
    permute_283 = torch.ops.aten.permute.default(view_418, [0, 1, 3, 4, 5, 2]);  view_418 = None
    view_419 = torch.ops.aten.view.default(permute_283, [8, 8, 1, 4096, 384]);  permute_283 = None
    unsqueeze_210 = torch.ops.aten.unsqueeze.default(view_415, 5);  view_415 = None
    unsqueeze_211 = torch.ops.aten.unsqueeze.default(unsqueeze_210, 6);  unsqueeze_210 = None
    permute_284 = torch.ops.aten.permute.default(unsqueeze_211, [2, 4, 5, 0, 1, 6, 3]);  unsqueeze_211 = None
    unsqueeze_212 = torch.ops.aten.unsqueeze.default(view_419, 5);  view_419 = None
    unsqueeze_213 = torch.ops.aten.unsqueeze.default(unsqueeze_212, 6);  unsqueeze_212 = None
    permute_285 = torch.ops.aten.permute.default(unsqueeze_213, [2, 5, 4, 0, 6, 1, 3]);  unsqueeze_213 = None
    permute_286 = torch.ops.aten.permute.default(permute_284, [3, 1, 4, 6, 0, 2, 5]);  permute_284 = None
    clone_63 = torch.ops.aten.clone.default(permute_286, memory_format = torch.contiguous_format);  permute_286 = None
    _unsafe_view_58 = torch.ops.aten._unsafe_view.default(clone_63, [8, 3072, 4096]);  clone_63 = None
    permute_287 = torch.ops.aten.permute.default(permute_285, [3, 6, 0, 2, 5, 1, 4]);  permute_285 = None
    clone_64 = torch.ops.aten.clone.default(permute_287, memory_format = torch.contiguous_format);  permute_287 = None
    _unsafe_view_59 = torch.ops.aten._unsafe_view.default(clone_64, [8, 4096, 3072]);  clone_64 = None
    bmm_48 = torch.ops.aten.bmm.default(_unsafe_view_58, _unsafe_view_59);  _unsafe_view_58 = _unsafe_view_59 = None
    view_420 = torch.ops.aten.view.default(bmm_48, [8, 384, 8, 1, 1, 384, 8]);  bmm_48 = None
    permute_288 = torch.ops.aten.permute.default(view_420, [4, 1, 5, 0, 2, 6, 3]);  view_420 = None
    view_421 = torch.ops.aten.view.default(permute_288, [1, 384, 384, 8, 8, 8]);  permute_288 = None
    clone_65 = torch.ops.aten.clone.default(view_421, memory_format = torch.contiguous_format);  view_421 = None
    _unsafe_view_60 = torch.ops.aten._unsafe_view.default(clone_65, [1, 384, 384, 512]);  clone_65 = None
    add_37 = torch.ops.aten.add.Tensor(add_36, _unsafe_view_60);  add_36 = _unsafe_view_60 = None
    slice_112 = torch.ops.aten.slice.Tensor(add_30, dim = 0, start = 0, end = 9223372036854775807)
    slice_113 = torch.ops.aten.slice.Tensor(slice_112, dim = 1, start = 12288, end = 16384);  slice_112 = None
    slice_114 = torch.ops.aten.slice.Tensor(slice_113, dim = 2, start = 0, end = 9223372036854775807);  slice_113 = None
    slice_115 = torch.ops.aten.slice.Tensor(slice_114, dim = 3, start = 0, end = 9223372036854775807);  slice_114 = None
    slice_116 = torch.ops.aten.slice.Tensor(arg1403_1, dim = 0, start = 0, end = 9223372036854775807)
    slice_117 = torch.ops.aten.slice.Tensor(slice_116, dim = 1, start = 12288, end = 16384);  slice_116 = None
    slice_118 = torch.ops.aten.slice.Tensor(slice_117, dim = 2, start = 0, end = 9223372036854775807);  slice_117 = None
    _to_copy_244 = torch.ops.aten._to_copy.default(slice_115, dtype = torch.float32);  slice_115 = None
    native_layer_norm_default_51 = torch.ops.aten.native_layer_norm.default(_to_copy_244, [64], None, None, 1e-05);  _to_copy_244 = None
    getitem_345 = native_layer_norm_default_51[0];  native_layer_norm_default_51 = None
    view_422 = torch.ops.aten.view.default(slice_118, [1, 4096, 384, 1]);  slice_118 = None
    bitwise_not_31 = torch.ops.aten.bitwise_not.default(view_422);  view_422 = None
    masked_fill_31 = torch.ops.aten.masked_fill.Scalar(getitem_345, bitwise_not_31, 0);  getitem_345 = bitwise_not_31 = None
    unbind_int_23 = torch.ops.aten.unbind.int(msa_module_outer_product_mean_2_weight_ab);  msa_module_outer_product_mean_2_weight_ab = None
    getitem_348 = unbind_int_23[0]
    getitem_349 = unbind_int_23[1];  unbind_int_23 = None
    _to_copy_245 = torch.ops.aten._to_copy.default(getitem_348, dtype = torch.bfloat16);  getitem_348 = None
    _to_copy_246 = torch.ops.aten._to_copy.default(masked_fill_31, dtype = torch.bfloat16)
    unsqueeze_214 = torch.ops.aten.unsqueeze.default(_to_copy_245, 3);  _to_copy_245 = None
    unsqueeze_215 = torch.ops.aten.unsqueeze.default(unsqueeze_214, 4);  unsqueeze_214 = None
    unsqueeze_216 = torch.ops.aten.unsqueeze.default(unsqueeze_215, 5);  unsqueeze_215 = None
    permute_289 = torch.ops.aten.permute.default(unsqueeze_216, [0, 1, 3, 4, 5, 2]);  unsqueeze_216 = None
    unsqueeze_217 = torch.ops.aten.unsqueeze.default(_to_copy_246, 4);  _to_copy_246 = None
    unsqueeze_218 = torch.ops.aten.unsqueeze.default(unsqueeze_217, 5);  unsqueeze_217 = None
    permute_290 = torch.ops.aten.permute.default(unsqueeze_218, [4, 5, 0, 1, 2, 3]);  unsqueeze_218 = None
    permute_291 = torch.ops.aten.permute.default(permute_289, [0, 1, 5, 2, 3, 4]);  permute_289 = None
    view_423 = torch.ops.aten.view.default(permute_291, [1, 64, 64]);  permute_291 = None
    permute_292 = torch.ops.aten.permute.default(permute_290, [5, 2, 3, 4, 0, 1]);  permute_290 = None
    view_424 = torch.ops.aten.view.default(permute_292, [1, 64, 1572864]);  permute_292 = None
    bmm_49 = torch.ops.aten.bmm.default(view_423, view_424);  view_423 = view_424 = None
    view_425 = torch.ops.aten.view.default(bmm_49, [8, 8, 1, 1, 4096, 384]);  bmm_49 = None
    permute_293 = torch.ops.aten.permute.default(view_425, [0, 1, 3, 4, 5, 2]);  view_425 = None
    view_426 = torch.ops.aten.view.default(permute_293, [8, 8, 1, 4096, 384]);  permute_293 = None
    _to_copy_247 = torch.ops.aten._to_copy.default(getitem_349, dtype = torch.bfloat16);  getitem_349 = None
    _to_copy_248 = torch.ops.aten._to_copy.default(masked_fill_31, dtype = torch.bfloat16);  masked_fill_31 = None
    unsqueeze_219 = torch.ops.aten.unsqueeze.default(_to_copy_247, 3);  _to_copy_247 = None
    unsqueeze_220 = torch.ops.aten.unsqueeze.default(unsqueeze_219, 4);  unsqueeze_219 = None
    unsqueeze_221 = torch.ops.aten.unsqueeze.default(unsqueeze_220, 5);  unsqueeze_220 = None
    permute_294 = torch.ops.aten.permute.default(unsqueeze_221, [0, 1, 3, 4, 5, 2]);  unsqueeze_221 = None
    unsqueeze_222 = torch.ops.aten.unsqueeze.default(_to_copy_248, 4);  _to_copy_248 = None
    unsqueeze_223 = torch.ops.aten.unsqueeze.default(unsqueeze_222, 5);  unsqueeze_222 = None
    permute_295 = torch.ops.aten.permute.default(unsqueeze_223, [4, 5, 0, 1, 2, 3]);  unsqueeze_223 = None
    permute_296 = torch.ops.aten.permute.default(permute_294, [0, 1, 5, 2, 3, 4]);  permute_294 = None
    view_427 = torch.ops.aten.view.default(permute_296, [1, 64, 64]);  permute_296 = None
    permute_297 = torch.ops.aten.permute.default(permute_295, [5, 2, 3, 4, 0, 1]);  permute_295 = None
    view_428 = torch.ops.aten.view.default(permute_297, [1, 64, 1572864]);  permute_297 = None
    bmm_50 = torch.ops.aten.bmm.default(view_427, view_428);  view_427 = view_428 = None
    view_429 = torch.ops.aten.view.default(bmm_50, [8, 8, 1, 1, 4096, 384]);  bmm_50 = None
    permute_298 = torch.ops.aten.permute.default(view_429, [0, 1, 3, 4, 5, 2]);  view_429 = None
    view_430 = torch.ops.aten.view.default(permute_298, [8, 8, 1, 4096, 384]);  permute_298 = None
    unsqueeze_224 = torch.ops.aten.unsqueeze.default(view_426, 5);  view_426 = None
    unsqueeze_225 = torch.ops.aten.unsqueeze.default(unsqueeze_224, 6);  unsqueeze_224 = None
    permute_299 = torch.ops.aten.permute.default(unsqueeze_225, [2, 4, 5, 0, 1, 6, 3]);  unsqueeze_225 = None
    unsqueeze_226 = torch.ops.aten.unsqueeze.default(view_430, 5);  view_430 = None
    unsqueeze_227 = torch.ops.aten.unsqueeze.default(unsqueeze_226, 6);  unsqueeze_226 = None
    permute_300 = torch.ops.aten.permute.default(unsqueeze_227, [2, 5, 4, 0, 6, 1, 3]);  unsqueeze_227 = None
    permute_301 = torch.ops.aten.permute.default(permute_299, [3, 1, 4, 6, 0, 2, 5]);  permute_299 = None
    clone_66 = torch.ops.aten.clone.default(permute_301, memory_format = torch.contiguous_format);  permute_301 = None
    _unsafe_view_61 = torch.ops.aten._unsafe_view.default(clone_66, [8, 3072, 4096]);  clone_66 = None
    permute_302 = torch.ops.aten.permute.default(permute_300, [3, 6, 0, 2, 5, 1, 4]);  permute_300 = None
    clone_67 = torch.ops.aten.clone.default(permute_302, memory_format = torch.contiguous_format);  permute_302 = None
    _unsafe_view_62 = torch.ops.aten._unsafe_view.default(clone_67, [8, 4096, 3072]);  clone_67 = None
    bmm_51 = torch.ops.aten.bmm.default(_unsafe_view_61, _unsafe_view_62);  _unsafe_view_61 = _unsafe_view_62 = None
    view_431 = torch.ops.aten.view.default(bmm_51, [8, 384, 8, 1, 1, 384, 8]);  bmm_51 = None
    permute_303 = torch.ops.aten.permute.default(view_431, [4, 1, 5, 0, 2, 6, 3]);  view_431 = None
    view_432 = torch.ops.aten.view.default(permute_303, [1, 384, 384, 8, 8, 8]);  permute_303 = None
    clone_68 = torch.ops.aten.clone.default(view_432, memory_format = torch.contiguous_format);  view_432 = None
    _unsafe_view_63 = torch.ops.aten._unsafe_view.default(clone_68, [1, 384, 384, 512]);  clone_68 = None
    add_38 = torch.ops.aten.add.Tensor(add_37, _unsafe_view_63);  add_37 = _unsafe_view_63 = None
    _to_copy_249 = torch.ops.aten._to_copy.default(add_38, dtype = torch.float32);  add_38 = None
    native_layer_norm_default_52 = torch.ops.aten.native_layer_norm.default(_to_copy_249, [512], msa_module_outer_product_mean_2_ln_out_weight, msa_module_outer_product_mean_2_ln_out_bias, 0.1);  _to_copy_249 = msa_module_outer_product_mean_2_ln_out_weight = msa_module_outer_product_mean_2_ln_out_bias = None
    getitem_350 = native_layer_norm_default_52[0];  native_layer_norm_default_52 = None
    _to_copy_250 = torch.ops.aten._to_copy.default(msa_module_outer_product_mean_2_linear_out_bias, dtype = torch.bfloat16);  msa_module_outer_product_mean_2_linear_out_bias = None
    _to_copy_251 = torch.ops.aten._to_copy.default(msa_module_outer_product_mean_2_linear_out_weight, dtype = torch.bfloat16);  msa_module_outer_product_mean_2_linear_out_weight = None
    _to_copy_252 = torch.ops.aten._to_copy.default(getitem_350, dtype = torch.bfloat16);  getitem_350 = None
    view_433 = torch.ops.aten.view.default(_to_copy_252, [147456, 512]);  _to_copy_252 = None
    t_79 = torch.ops.aten.t.default(_to_copy_251);  _to_copy_251 = None
    addmm_2 = torch.ops.aten.addmm.default(_to_copy_250, view_433, t_79);  _to_copy_250 = view_433 = t_79 = None
    view_434 = torch.ops.aten.view.default(addmm_2, [1, 384, 384, 256]);  addmm_2 = None
    add_39 = torch.ops.aten.add.Tensor(add_34, view_434);  add_34 = view_434 = None
    split_tensor_32 = torch.ops.aten.split.Tensor(add_30, 128, dim = -2)
    getitem_353 = split_tensor_32[0]
    getitem_354 = split_tensor_32[1]
    getitem_355 = split_tensor_32[2];  split_tensor_32 = None
    _to_copy_253 = torch.ops.aten._to_copy.default(getitem_353, dtype = torch.float32);  getitem_353 = None
    native_layer_norm_default_53 = torch.ops.aten.native_layer_norm.default(_to_copy_253, [64], msa_module_msa_transition_2_layer_norm_weight, msa_module_msa_transition_2_layer_norm_bias, 1e-05);  _to_copy_253 = None
    getitem_356 = native_layer_norm_default_53[0];  native_layer_norm_default_53 = None
    _to_copy_254 = torch.ops.aten._to_copy.default(msa_module_msa_transition_2_linear_no_bias_ab_weight, dtype = torch.bfloat16)
    _to_copy_255 = torch.ops.aten._to_copy.default(getitem_356, dtype = torch.bfloat16);  getitem_356 = None
    t_80 = torch.ops.aten.t.default(_to_copy_254);  _to_copy_254 = None
    view_435 = torch.ops.aten.view.default(_to_copy_255, [2097152, 64]);  _to_copy_255 = None
    mm_73 = torch.ops.aten.mm.default(view_435, t_80);  view_435 = t_80 = None
    view_436 = torch.ops.aten.view.default(mm_73, [1, 16384, 128, 512]);  mm_73 = None
    split_tensor_33 = torch.ops.aten.split.Tensor(view_436, 256, dim = -1);  view_436 = None
    getitem_359 = split_tensor_33[0]
    getitem_360 = split_tensor_33[1];  split_tensor_33 = None
    silu_10 = torch.ops.aten.silu.default(getitem_359);  getitem_359 = None
    mul_39 = torch.ops.aten.mul.Tensor(silu_10, getitem_360);  silu_10 = getitem_360 = None
    _to_copy_256 = torch.ops.aten._to_copy.default(msa_module_msa_transition_2_linear_out_weight, dtype = torch.bfloat16)
    t_81 = torch.ops.aten.t.default(_to_copy_256);  _to_copy_256 = None
    view_438 = torch.ops.aten.view.default(mul_39, [2097152, 256]);  mul_39 = None
    mm_74 = torch.ops.aten.mm.default(view_438, t_81);  view_438 = t_81 = None
    view_439 = torch.ops.aten.view.default(mm_74, [1, 16384, 128, 64]);  mm_74 = None
    _to_copy_257 = torch.ops.aten._to_copy.default(getitem_354, dtype = torch.float32);  getitem_354 = None
    native_layer_norm_default_54 = torch.ops.aten.native_layer_norm.default(_to_copy_257, [64], msa_module_msa_transition_2_layer_norm_weight, msa_module_msa_transition_2_layer_norm_bias, 1e-05);  _to_copy_257 = None
    getitem_361 = native_layer_norm_default_54[0];  native_layer_norm_default_54 = None
    _to_copy_258 = torch.ops.aten._to_copy.default(msa_module_msa_transition_2_linear_no_bias_ab_weight, dtype = torch.bfloat16)
    _to_copy_259 = torch.ops.aten._to_copy.default(getitem_361, dtype = torch.bfloat16);  getitem_361 = None
    t_82 = torch.ops.aten.t.default(_to_copy_258);  _to_copy_258 = None
    view_440 = torch.ops.aten.view.default(_to_copy_259, [2097152, 64]);  _to_copy_259 = None
    mm_75 = torch.ops.aten.mm.default(view_440, t_82);  view_440 = t_82 = None
    view_441 = torch.ops.aten.view.default(mm_75, [1, 16384, 128, 512]);  mm_75 = None
    split_tensor_34 = torch.ops.aten.split.Tensor(view_441, 256, dim = -1);  view_441 = None
    getitem_364 = split_tensor_34[0]
    getitem_365 = split_tensor_34[1];  split_tensor_34 = None
    silu_11 = torch.ops.aten.silu.default(getitem_364);  getitem_364 = None
    mul_40 = torch.ops.aten.mul.Tensor(silu_11, getitem_365);  silu_11 = getitem_365 = None
    _to_copy_260 = torch.ops.aten._to_copy.default(msa_module_msa_transition_2_linear_out_weight, dtype = torch.bfloat16)
    t_83 = torch.ops.aten.t.default(_to_copy_260);  _to_copy_260 = None
    view_443 = torch.ops.aten.view.default(mul_40, [2097152, 256]);  mul_40 = None
    mm_76 = torch.ops.aten.mm.default(view_443, t_83);  view_443 = t_83 = None
    view_444 = torch.ops.aten.view.default(mm_76, [1, 16384, 128, 64]);  mm_76 = None
    _to_copy_261 = torch.ops.aten._to_copy.default(getitem_355, dtype = torch.float32);  getitem_355 = None
    native_layer_norm_default_55 = torch.ops.aten.native_layer_norm.default(_to_copy_261, [64], msa_module_msa_transition_2_layer_norm_weight, msa_module_msa_transition_2_layer_norm_bias, 1e-05);  _to_copy_261 = msa_module_msa_transition_2_layer_norm_weight = msa_module_msa_transition_2_layer_norm_bias = None
    getitem_366 = native_layer_norm_default_55[0];  native_layer_norm_default_55 = None
    _to_copy_262 = torch.ops.aten._to_copy.default(msa_module_msa_transition_2_linear_no_bias_ab_weight, dtype = torch.bfloat16);  msa_module_msa_transition_2_linear_no_bias_ab_weight = None
    _to_copy_263 = torch.ops.aten._to_copy.default(getitem_366, dtype = torch.bfloat16);  getitem_366 = None
    t_84 = torch.ops.aten.t.default(_to_copy_262);  _to_copy_262 = None
    view_445 = torch.ops.aten.view.default(_to_copy_263, [2097152, 64]);  _to_copy_263 = None
    mm_77 = torch.ops.aten.mm.default(view_445, t_84);  view_445 = t_84 = None
    view_446 = torch.ops.aten.view.default(mm_77, [1, 16384, 128, 512]);  mm_77 = None
    split_tensor_35 = torch.ops.aten.split.Tensor(view_446, 256, dim = -1);  view_446 = None
    getitem_369 = split_tensor_35[0]
    getitem_370 = split_tensor_35[1];  split_tensor_35 = None
    silu_12 = torch.ops.aten.silu.default(getitem_369);  getitem_369 = None
    mul_41 = torch.ops.aten.mul.Tensor(silu_12, getitem_370);  silu_12 = getitem_370 = None
    _to_copy_264 = torch.ops.aten._to_copy.default(msa_module_msa_transition_2_linear_out_weight, dtype = torch.bfloat16);  msa_module_msa_transition_2_linear_out_weight = None
    t_85 = torch.ops.aten.t.default(_to_copy_264);  _to_copy_264 = None
    view_448 = torch.ops.aten.view.default(mul_41, [2097152, 256]);  mul_41 = None
    mm_78 = torch.ops.aten.mm.default(view_448, t_85);  view_448 = t_85 = None
    view_449 = torch.ops.aten.view.default(mm_78, [1, 16384, 128, 64]);  mm_78 = None
    cat_8 = torch.ops.aten.cat.default([view_439, view_444, view_449], dim = -2);  view_439 = view_444 = view_449 = None
    add_40 = torch.ops.aten.add.Tensor(add_30, cat_8);  cat_8 = None
    slice_119 = torch.ops.aten.slice.Tensor(add_30, dim = 0, start = 0, end = 9223372036854775807)
    slice_120 = torch.ops.aten.slice.Tensor(slice_119, dim = 1, start = 0, end = 8192);  slice_119 = None
    slice_121 = torch.ops.aten.slice.Tensor(slice_120, dim = 2, start = 0, end = 9223372036854775807);  slice_120 = None
    slice_122 = torch.ops.aten.slice.Tensor(slice_121, dim = 3, start = 0, end = 9223372036854775807);  slice_121 = None
    slice_123 = torch.ops.aten.slice.Tensor(arg1403_1, dim = 0, start = 0, end = 9223372036854775807)
    slice_124 = torch.ops.aten.slice.Tensor(slice_123, dim = 1, start = 0, end = 8192);  slice_123 = None
    slice_125 = torch.ops.aten.slice.Tensor(slice_124, dim = 2, start = 0, end = 9223372036854775807);  slice_124 = None
    _to_copy_265 = torch.ops.aten._to_copy.default(add_39, dtype = torch.float32)
    native_layer_norm_default_56 = torch.ops.aten.native_layer_norm.default(_to_copy_265, [256], msa_module_msa_pair_weighted_averaging_2_layernorm_pair_weight, msa_module_msa_pair_weighted_averaging_2_layernorm_pair_bias, 1e-05);  _to_copy_265 = None
    getitem_371 = native_layer_norm_default_56[0];  native_layer_norm_default_56 = None
    _to_copy_266 = torch.ops.aten._to_copy.default(msa_module_msa_pair_weighted_averaging_2_linear_pair_weight, dtype = torch.bfloat16)
    _to_copy_267 = torch.ops.aten._to_copy.default(getitem_371, dtype = torch.bfloat16);  getitem_371 = None
    t_86 = torch.ops.aten.t.default(_to_copy_266);  _to_copy_266 = None
    view_450 = torch.ops.aten.view.default(_to_copy_267, [147456, 256]);  _to_copy_267 = None
    mm_79 = torch.ops.aten.mm.default(view_450, t_86);  view_450 = t_86 = None
    view_451 = torch.ops.aten.view.default(mm_79, [1, 384, 384, 8]);  mm_79 = None
    permute_304 = torch.ops.aten.permute.default(view_451, [0, 3, 1, 2]);  view_451 = None
    view_452 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384])
    bitwise_not_32 = torch.ops.aten.bitwise_not.default(view_452);  view_452 = None
    masked_fill_32 = torch.ops.aten.masked_fill.Scalar(permute_304, bitwise_not_32, -10000);  permute_304 = bitwise_not_32 = None
    _to_copy_268 = torch.ops.aten._to_copy.default(masked_fill_32, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0));  masked_fill_32 = None
    _softmax_4 = torch.ops.aten._softmax.default(_to_copy_268, -1, False);  _to_copy_268 = None
    _to_copy_269 = torch.ops.aten._to_copy.default(slice_122, dtype = torch.float32);  slice_122 = None
    native_layer_norm_default_57 = torch.ops.aten.native_layer_norm.default(_to_copy_269, [64], msa_module_msa_pair_weighted_averaging_2_layernorm_msa_weight, msa_module_msa_pair_weighted_averaging_2_layernorm_msa_bias, 1e-05);  _to_copy_269 = None
    getitem_374 = native_layer_norm_default_57[0];  native_layer_norm_default_57 = None
    _to_copy_270 = torch.ops.aten._to_copy.default(msa_module_msa_pair_weighted_averaging_2_linear_msa2vg_weight, dtype = torch.bfloat16)
    _to_copy_271 = torch.ops.aten._to_copy.default(getitem_374, dtype = torch.bfloat16);  getitem_374 = None
    t_87 = torch.ops.aten.t.default(_to_copy_270);  _to_copy_270 = None
    view_453 = torch.ops.aten.view.default(_to_copy_271, [3145728, 64]);  _to_copy_271 = None
    mm_80 = torch.ops.aten.mm.default(view_453, t_87);  view_453 = t_87 = None
    view_454 = torch.ops.aten.view.default(mm_80, [1, 8192, 384, 512]);  mm_80 = None
    view_455 = torch.ops.aten.view.default(view_454, [1, 8192, 384, 2, 8, 32]);  view_454 = None
    permute_305 = torch.ops.aten.permute.default(view_455, [3, 0, 1, 2, 4, 5]);  view_455 = None
    unbind_int_24 = torch.ops.aten.unbind.int(permute_305);  permute_305 = None
    getitem_377 = unbind_int_24[0]
    getitem_378 = unbind_int_24[1];  unbind_int_24 = None
    sigmoid_24 = torch.ops.aten.sigmoid.default(getitem_378);  getitem_378 = None
    bitwise_not_33 = torch.ops.aten.bitwise_not.default(slice_125);  slice_125 = None
    view_456 = torch.ops.aten.view.default(bitwise_not_33, [1, 8192, 384, 1, 1]);  bitwise_not_33 = None
    masked_fill_33 = torch.ops.aten.masked_fill.Scalar(getitem_377, view_456, 0);  getitem_377 = view_456 = None
    _to_copy_272 = torch.ops.aten._to_copy.default(_softmax_4, dtype = torch.bfloat16);  _softmax_4 = None
    unsqueeze_228 = torch.ops.aten.unsqueeze.default(_to_copy_272, 4);  _to_copy_272 = None
    unsqueeze_229 = torch.ops.aten.unsqueeze.default(unsqueeze_228, 5);  unsqueeze_228 = None
    permute_306 = torch.ops.aten.permute.default(unsqueeze_229, [0, 4, 2, 1, 5, 3]);  unsqueeze_229 = None
    unsqueeze_230 = torch.ops.aten.unsqueeze.default(masked_fill_33, 5);  masked_fill_33 = None
    permute_307 = torch.ops.aten.permute.default(unsqueeze_230, [0, 1, 5, 3, 4, 2]);  unsqueeze_230 = None
    permute_308 = torch.ops.aten.permute.default(permute_306, [3, 2, 5, 0, 1, 4]);  permute_306 = None
    view_457 = torch.ops.aten.view.default(permute_308, [8, 384, 384]);  permute_308 = None
    permute_309 = torch.ops.aten.permute.default(permute_307, [3, 5, 0, 1, 4, 2]);  permute_307 = None
    clone_69 = torch.ops.aten.clone.default(permute_309, memory_format = torch.contiguous_format);  permute_309 = None
    _unsafe_view_64 = torch.ops.aten._unsafe_view.default(clone_69, [8, 384, 262144]);  clone_69 = None
    bmm_52 = torch.ops.aten.bmm.default(view_457, _unsafe_view_64);  view_457 = _unsafe_view_64 = None
    view_458 = torch.ops.aten.view.default(bmm_52, [8, 384, 1, 1, 8192, 32]);  bmm_52 = None
    permute_310 = torch.ops.aten.permute.default(view_458, [3, 4, 1, 0, 5, 2]);  view_458 = None
    view_459 = torch.ops.aten.view.default(permute_310, [1, 8192, 384, 8, 32]);  permute_310 = None
    mul_42 = torch.ops.aten.mul.Tensor(sigmoid_24, view_459);  sigmoid_24 = view_459 = None
    view_460 = torch.ops.aten.view.default(mul_42, [1, 8192, 384, 256]);  mul_42 = None
    _to_copy_273 = torch.ops.aten._to_copy.default(msa_module_msa_pair_weighted_averaging_2_linear_out_no_bias_weight, dtype = torch.bfloat16)
    t_88 = torch.ops.aten.t.default(_to_copy_273);  _to_copy_273 = None
    view_461 = torch.ops.aten.view.default(view_460, [3145728, 256]);  view_460 = None
    mm_81 = torch.ops.aten.mm.default(view_461, t_88);  view_461 = t_88 = None
    view_462 = torch.ops.aten.view.default(mm_81, [1, 8192, 384, 64]);  mm_81 = None
    slice_126 = torch.ops.aten.slice.Tensor(add_30, dim = 0, start = 0, end = 9223372036854775807);  add_30 = None
    slice_127 = torch.ops.aten.slice.Tensor(slice_126, dim = 1, start = 8192, end = 16384);  slice_126 = None
    slice_128 = torch.ops.aten.slice.Tensor(slice_127, dim = 2, start = 0, end = 9223372036854775807);  slice_127 = None
    slice_129 = torch.ops.aten.slice.Tensor(slice_128, dim = 3, start = 0, end = 9223372036854775807);  slice_128 = None
    slice_130 = torch.ops.aten.slice.Tensor(arg1403_1, dim = 0, start = 0, end = 9223372036854775807)
    slice_131 = torch.ops.aten.slice.Tensor(slice_130, dim = 1, start = 8192, end = 16384);  slice_130 = None
    slice_132 = torch.ops.aten.slice.Tensor(slice_131, dim = 2, start = 0, end = 9223372036854775807);  slice_131 = None
    _to_copy_274 = torch.ops.aten._to_copy.default(add_39, dtype = torch.float32)
    native_layer_norm_default_58 = torch.ops.aten.native_layer_norm.default(_to_copy_274, [256], msa_module_msa_pair_weighted_averaging_2_layernorm_pair_weight, msa_module_msa_pair_weighted_averaging_2_layernorm_pair_bias, 1e-05);  _to_copy_274 = msa_module_msa_pair_weighted_averaging_2_layernorm_pair_weight = msa_module_msa_pair_weighted_averaging_2_layernorm_pair_bias = None
    getitem_379 = native_layer_norm_default_58[0];  native_layer_norm_default_58 = None
    _to_copy_275 = torch.ops.aten._to_copy.default(msa_module_msa_pair_weighted_averaging_2_linear_pair_weight, dtype = torch.bfloat16);  msa_module_msa_pair_weighted_averaging_2_linear_pair_weight = None
    _to_copy_276 = torch.ops.aten._to_copy.default(getitem_379, dtype = torch.bfloat16);  getitem_379 = None
    t_89 = torch.ops.aten.t.default(_to_copy_275);  _to_copy_275 = None
    view_463 = torch.ops.aten.view.default(_to_copy_276, [147456, 256]);  _to_copy_276 = None
    mm_82 = torch.ops.aten.mm.default(view_463, t_89);  view_463 = t_89 = None
    view_464 = torch.ops.aten.view.default(mm_82, [1, 384, 384, 8]);  mm_82 = None
    permute_311 = torch.ops.aten.permute.default(view_464, [0, 3, 1, 2]);  view_464 = None
    view_465 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384])
    bitwise_not_34 = torch.ops.aten.bitwise_not.default(view_465);  view_465 = None
    masked_fill_34 = torch.ops.aten.masked_fill.Scalar(permute_311, bitwise_not_34, -10000);  permute_311 = bitwise_not_34 = None
    _to_copy_277 = torch.ops.aten._to_copy.default(masked_fill_34, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0));  masked_fill_34 = None
    _softmax_5 = torch.ops.aten._softmax.default(_to_copy_277, -1, False);  _to_copy_277 = None
    _to_copy_278 = torch.ops.aten._to_copy.default(slice_129, dtype = torch.float32);  slice_129 = None
    native_layer_norm_default_59 = torch.ops.aten.native_layer_norm.default(_to_copy_278, [64], msa_module_msa_pair_weighted_averaging_2_layernorm_msa_weight, msa_module_msa_pair_weighted_averaging_2_layernorm_msa_bias, 1e-05);  _to_copy_278 = msa_module_msa_pair_weighted_averaging_2_layernorm_msa_weight = msa_module_msa_pair_weighted_averaging_2_layernorm_msa_bias = None
    getitem_382 = native_layer_norm_default_59[0];  native_layer_norm_default_59 = None
    _to_copy_279 = torch.ops.aten._to_copy.default(msa_module_msa_pair_weighted_averaging_2_linear_msa2vg_weight, dtype = torch.bfloat16);  msa_module_msa_pair_weighted_averaging_2_linear_msa2vg_weight = None
    _to_copy_280 = torch.ops.aten._to_copy.default(getitem_382, dtype = torch.bfloat16);  getitem_382 = None
    t_90 = torch.ops.aten.t.default(_to_copy_279);  _to_copy_279 = None
    view_466 = torch.ops.aten.view.default(_to_copy_280, [3145728, 64]);  _to_copy_280 = None
    mm_83 = torch.ops.aten.mm.default(view_466, t_90);  view_466 = t_90 = None
    view_467 = torch.ops.aten.view.default(mm_83, [1, 8192, 384, 512]);  mm_83 = None
    view_468 = torch.ops.aten.view.default(view_467, [1, 8192, 384, 2, 8, 32]);  view_467 = None
    permute_312 = torch.ops.aten.permute.default(view_468, [3, 0, 1, 2, 4, 5]);  view_468 = None
    unbind_int_25 = torch.ops.aten.unbind.int(permute_312);  permute_312 = None
    getitem_385 = unbind_int_25[0]
    getitem_386 = unbind_int_25[1];  unbind_int_25 = None
    sigmoid_25 = torch.ops.aten.sigmoid.default(getitem_386);  getitem_386 = None
    bitwise_not_35 = torch.ops.aten.bitwise_not.default(slice_132);  slice_132 = None
    view_469 = torch.ops.aten.view.default(bitwise_not_35, [1, 8192, 384, 1, 1]);  bitwise_not_35 = None
    masked_fill_35 = torch.ops.aten.masked_fill.Scalar(getitem_385, view_469, 0);  getitem_385 = view_469 = None
    _to_copy_281 = torch.ops.aten._to_copy.default(_softmax_5, dtype = torch.bfloat16);  _softmax_5 = None
    unsqueeze_231 = torch.ops.aten.unsqueeze.default(_to_copy_281, 4);  _to_copy_281 = None
    unsqueeze_232 = torch.ops.aten.unsqueeze.default(unsqueeze_231, 5);  unsqueeze_231 = None
    permute_313 = torch.ops.aten.permute.default(unsqueeze_232, [0, 4, 2, 1, 5, 3]);  unsqueeze_232 = None
    unsqueeze_233 = torch.ops.aten.unsqueeze.default(masked_fill_35, 5);  masked_fill_35 = None
    permute_314 = torch.ops.aten.permute.default(unsqueeze_233, [0, 1, 5, 3, 4, 2]);  unsqueeze_233 = None
    permute_315 = torch.ops.aten.permute.default(permute_313, [3, 2, 5, 0, 1, 4]);  permute_313 = None
    view_470 = torch.ops.aten.view.default(permute_315, [8, 384, 384]);  permute_315 = None
    permute_316 = torch.ops.aten.permute.default(permute_314, [3, 5, 0, 1, 4, 2]);  permute_314 = None
    clone_70 = torch.ops.aten.clone.default(permute_316, memory_format = torch.contiguous_format);  permute_316 = None
    _unsafe_view_65 = torch.ops.aten._unsafe_view.default(clone_70, [8, 384, 262144]);  clone_70 = None
    bmm_53 = torch.ops.aten.bmm.default(view_470, _unsafe_view_65);  view_470 = _unsafe_view_65 = None
    view_471 = torch.ops.aten.view.default(bmm_53, [8, 384, 1, 1, 8192, 32]);  bmm_53 = None
    permute_317 = torch.ops.aten.permute.default(view_471, [3, 4, 1, 0, 5, 2]);  view_471 = None
    view_472 = torch.ops.aten.view.default(permute_317, [1, 8192, 384, 8, 32]);  permute_317 = None
    mul_43 = torch.ops.aten.mul.Tensor(sigmoid_25, view_472);  sigmoid_25 = view_472 = None
    view_473 = torch.ops.aten.view.default(mul_43, [1, 8192, 384, 256]);  mul_43 = None
    _to_copy_282 = torch.ops.aten._to_copy.default(msa_module_msa_pair_weighted_averaging_2_linear_out_no_bias_weight, dtype = torch.bfloat16);  msa_module_msa_pair_weighted_averaging_2_linear_out_no_bias_weight = None
    t_91 = torch.ops.aten.t.default(_to_copy_282);  _to_copy_282 = None
    view_474 = torch.ops.aten.view.default(view_473, [3145728, 256]);  view_473 = None
    mm_84 = torch.ops.aten.mm.default(view_474, t_91);  view_474 = t_91 = None
    view_475 = torch.ops.aten.view.default(mm_84, [1, 8192, 384, 64]);  mm_84 = None
    cat_9 = torch.ops.aten.cat.default([view_462, view_475], dim = 1);  view_462 = view_475 = None
    add_41 = torch.ops.aten.add.Tensor(add_40, cat_9);  add_40 = cat_9 = None
    _to_copy_283 = torch.ops.aten._to_copy.default(add_39, dtype = torch.float32)
    native_layer_norm_default_60 = torch.ops.aten.native_layer_norm.default(_to_copy_283, [256], msa_module_triangular_multiplication_2_layernorm_z_in_weight, msa_module_triangular_multiplication_2_layernorm_z_in_bias, 1e-05);  _to_copy_283 = msa_module_triangular_multiplication_2_layernorm_z_in_weight = msa_module_triangular_multiplication_2_layernorm_z_in_bias = None
    getitem_387 = native_layer_norm_default_60[0];  native_layer_norm_default_60 = None
    split_with_sizes_default_8 = torch.ops.aten.split_with_sizes.default(msa_module_triangular_multiplication_2_merged_linear_p_weight, [512, 512]);  msa_module_triangular_multiplication_2_merged_linear_p_weight = None
    getitem_390 = split_with_sizes_default_8[0]
    getitem_391 = split_with_sizes_default_8[1];  split_with_sizes_default_8 = None
    split_with_sizes_default_9 = torch.ops.aten.split_with_sizes.default(msa_module_triangular_multiplication_2_merged_linear_g_weight, [512, 512, 256]);  msa_module_triangular_multiplication_2_merged_linear_g_weight = None
    getitem_392 = split_with_sizes_default_9[0]
    getitem_393 = split_with_sizes_default_9[1]
    getitem_394 = split_with_sizes_default_9[2];  split_with_sizes_default_9 = None
    _to_copy_284 = torch.ops.aten._to_copy.default(getitem_390, dtype = torch.bfloat16);  getitem_390 = None
    _to_copy_285 = torch.ops.aten._to_copy.default(getitem_387, dtype = torch.bfloat16)
    t_92 = torch.ops.aten.t.default(_to_copy_284);  _to_copy_284 = None
    view_476 = torch.ops.aten.view.default(_to_copy_285, [147456, 256]);  _to_copy_285 = None
    mm_85 = torch.ops.aten.mm.default(view_476, t_92);  view_476 = t_92 = None
    view_477 = torch.ops.aten.view.default(mm_85, [1, 384, 384, 512]);  mm_85 = None
    _to_copy_286 = torch.ops.aten._to_copy.default(getitem_392, dtype = torch.bfloat16);  getitem_392 = None
    _to_copy_287 = torch.ops.aten._to_copy.default(getitem_387, dtype = torch.bfloat16)
    t_93 = torch.ops.aten.t.default(_to_copy_286);  _to_copy_286 = None
    view_478 = torch.ops.aten.view.default(_to_copy_287, [147456, 256]);  _to_copy_287 = None
    mm_86 = torch.ops.aten.mm.default(view_478, t_93);  view_478 = t_93 = None
    view_479 = torch.ops.aten.view.default(mm_86, [1, 384, 384, 512]);  mm_86 = None
    sigmoid_26 = torch.ops.aten.sigmoid.default(view_479);  view_479 = None
    mul_44 = torch.ops.aten.mul.Tensor(view_477, sigmoid_26);  view_477 = sigmoid_26 = None
    unsqueeze_234 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
    bitwise_not_36 = torch.ops.aten.bitwise_not.default(unsqueeze_234);  unsqueeze_234 = None
    masked_fill_36 = torch.ops.aten.masked_fill.Scalar(mul_44, bitwise_not_36, 0);  mul_44 = bitwise_not_36 = None
    split_tensor_36 = torch.ops.aten.split.Tensor(masked_fill_36, 256, dim = -1)
    getitem_397 = split_tensor_36[0];  split_tensor_36 = None
    unsqueeze_237 = torch.ops.aten.unsqueeze.default(getitem_397, 4);  getitem_397 = None
    permute_322 = torch.ops.aten.permute.default(unsqueeze_237, [0, 1, 4, 3, 2]);  unsqueeze_237 = None
    permute_323 = torch.ops.aten.permute.default(permute_322, [3, 1, 4, 0, 2]);  permute_322 = None
    view_482 = torch.ops.aten.view.default(permute_323, [256, 384, 384]);  permute_323 = None
    split_tensor_37 = torch.ops.aten.split.Tensor(masked_fill_36, 256, dim = -1);  masked_fill_36 = None
    getitem_400 = split_tensor_37[1];  split_tensor_37 = None
    unsqueeze_238 = torch.ops.aten.unsqueeze.default(getitem_400, 4);  getitem_400 = None
    permute_324 = torch.ops.aten.permute.default(unsqueeze_238, [0, 4, 1, 3, 2]);  unsqueeze_238 = None
    permute_325 = torch.ops.aten.permute.default(permute_324, [3, 4, 0, 2, 1]);  permute_324 = None
    view_483 = torch.ops.aten.view.default(permute_325, [256, 384, 384]);  permute_325 = None
    bmm_54 = torch.ops.aten.bmm.default(view_482, view_483);  view_482 = view_483 = None
    view_484 = torch.ops.aten.view.default(bmm_54, [256, 384, 1, 1, 384]);  bmm_54 = None
    permute_326 = torch.ops.aten.permute.default(view_484, [3, 1, 4, 0, 2]);  view_484 = None
    view_485 = torch.ops.aten.view.default(permute_326, [1, 384, 384, 256]);  permute_326 = None
    _to_copy_288 = torch.ops.aten._to_copy.default(getitem_391, dtype = torch.bfloat16);  getitem_391 = None
    _to_copy_289 = torch.ops.aten._to_copy.default(getitem_387, dtype = torch.bfloat16)
    t_94 = torch.ops.aten.t.default(_to_copy_288);  _to_copy_288 = None
    view_486 = torch.ops.aten.view.default(_to_copy_289, [147456, 256]);  _to_copy_289 = None
    mm_87 = torch.ops.aten.mm.default(view_486, t_94);  view_486 = t_94 = None
    view_487 = torch.ops.aten.view.default(mm_87, [1, 384, 384, 512]);  mm_87 = None
    _to_copy_290 = torch.ops.aten._to_copy.default(getitem_393, dtype = torch.bfloat16);  getitem_393 = None
    _to_copy_291 = torch.ops.aten._to_copy.default(getitem_387, dtype = torch.bfloat16)
    t_95 = torch.ops.aten.t.default(_to_copy_290);  _to_copy_290 = None
    view_488 = torch.ops.aten.view.default(_to_copy_291, [147456, 256]);  _to_copy_291 = None
    mm_88 = torch.ops.aten.mm.default(view_488, t_95);  view_488 = t_95 = None
    view_489 = torch.ops.aten.view.default(mm_88, [1, 384, 384, 512]);  mm_88 = None
    sigmoid_27 = torch.ops.aten.sigmoid.default(view_489);  view_489 = None
    mul_45 = torch.ops.aten.mul.Tensor(view_487, sigmoid_27);  view_487 = sigmoid_27 = None
    view_490 = torch.ops.aten.view.default(mul_45, [147456, 512]);  mul_45 = None
    view_491 = torch.ops.aten.view.default(view_490, [1, 384, 384, 512]);  view_490 = None
    transpose_8 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
    unsqueeze_239 = torch.ops.aten.unsqueeze.default(transpose_8, 3);  transpose_8 = None
    clone_71 = torch.ops.aten.clone.default(unsqueeze_239, memory_format = torch.contiguous_format);  unsqueeze_239 = None
    bitwise_not_37 = torch.ops.aten.bitwise_not.default(clone_71);  clone_71 = None
    masked_fill_37 = torch.ops.aten.masked_fill.Scalar(view_491, bitwise_not_37, 0);  view_491 = bitwise_not_37 = None
    view_492 = torch.ops.aten.view.default(masked_fill_37, [147456, 512]);  masked_fill_37 = None
    view_496 = torch.ops.aten.view.default(view_492, [1, 384, 384, 512])
    split_tensor_38 = torch.ops.aten.split.Tensor(view_496, 256, dim = -1);  view_496 = None
    getitem_403 = split_tensor_38[0];  split_tensor_38 = None
    unsqueeze_242 = torch.ops.aten.unsqueeze.default(getitem_403, 4);  getitem_403 = None
    permute_331 = torch.ops.aten.permute.default(unsqueeze_242, [0, 2, 4, 3, 1]);  unsqueeze_242 = None
    permute_332 = torch.ops.aten.permute.default(permute_331, [3, 1, 4, 0, 2]);  permute_331 = None
    view_497 = torch.ops.aten.view.default(permute_332, [256, 384, 384]);  permute_332 = None
    view_498 = torch.ops.aten.view.default(view_492, [1, 384, 384, 512]);  view_492 = None
    split_tensor_39 = torch.ops.aten.split.Tensor(view_498, 256, dim = -1);  view_498 = None
    getitem_406 = split_tensor_39[1];  split_tensor_39 = None
    unsqueeze_243 = torch.ops.aten.unsqueeze.default(getitem_406, 4);  getitem_406 = None
    permute_333 = torch.ops.aten.permute.default(unsqueeze_243, [0, 4, 2, 3, 1]);  unsqueeze_243 = None
    permute_334 = torch.ops.aten.permute.default(permute_333, [3, 4, 0, 2, 1]);  permute_333 = None
    view_499 = torch.ops.aten.view.default(permute_334, [256, 384, 384]);  permute_334 = None
    bmm_55 = torch.ops.aten.bmm.default(view_497, view_499);  view_497 = view_499 = None
    view_500 = torch.ops.aten.view.default(bmm_55, [256, 384, 1, 1, 384]);  bmm_55 = None
    permute_335 = torch.ops.aten.permute.default(view_500, [3, 1, 4, 0, 2]);  view_500 = None
    view_501 = torch.ops.aten.view.default(permute_335, [1, 384, 384, 256]);  permute_335 = None
    _to_copy_292 = torch.ops.aten._to_copy.default(view_485, dtype = torch.float32);  view_485 = None
    native_layer_norm_default_61 = torch.ops.aten.native_layer_norm.default(_to_copy_292, [256], None, None, 1e-05);  _to_copy_292 = None
    getitem_407 = native_layer_norm_default_61[0];  native_layer_norm_default_61 = None
    _to_copy_293 = torch.ops.aten._to_copy.default(view_501, dtype = torch.float32);  view_501 = None
    native_layer_norm_default_62 = torch.ops.aten.native_layer_norm.default(_to_copy_293, [256], None, None, 1e-05);  _to_copy_293 = None
    getitem_410 = native_layer_norm_default_62[0];  native_layer_norm_default_62 = None
    add_42 = torch.ops.aten.add.Tensor(getitem_407, getitem_410);  getitem_407 = getitem_410 = None
    _to_copy_294 = torch.ops.aten._to_copy.default(msa_module_triangular_multiplication_2_linear_z_out_weight, dtype = torch.bfloat16);  msa_module_triangular_multiplication_2_linear_z_out_weight = None
    _to_copy_295 = torch.ops.aten._to_copy.default(add_42, dtype = torch.bfloat16);  add_42 = None
    t_96 = torch.ops.aten.t.default(_to_copy_294);  _to_copy_294 = None
    view_502 = torch.ops.aten.view.default(_to_copy_295, [147456, 256]);  _to_copy_295 = None
    mm_89 = torch.ops.aten.mm.default(view_502, t_96);  view_502 = t_96 = None
    view_503 = torch.ops.aten.view.default(mm_89, [1, 384, 384, 256]);  mm_89 = None
    _to_copy_296 = torch.ops.aten._to_copy.default(getitem_394, dtype = torch.bfloat16);  getitem_394 = None
    _to_copy_297 = torch.ops.aten._to_copy.default(getitem_387, dtype = torch.bfloat16);  getitem_387 = None
    t_97 = torch.ops.aten.t.default(_to_copy_296);  _to_copy_296 = None
    view_504 = torch.ops.aten.view.default(_to_copy_297, [147456, 256]);  _to_copy_297 = None
    mm_90 = torch.ops.aten.mm.default(view_504, t_97);  view_504 = t_97 = None
    view_505 = torch.ops.aten.view.default(mm_90, [1, 384, 384, 256]);  mm_90 = None
    sigmoid_28 = torch.ops.aten.sigmoid.default(view_505);  view_505 = None
    mul_46 = torch.ops.aten.mul.Tensor(view_503, sigmoid_28);  view_503 = sigmoid_28 = None
    add_43 = torch.ops.aten.add.Tensor(add_39, mul_46);  mul_46 = None
    split_tensor_40 = torch.ops.aten.split.Tensor(add_39, 384, dim = -2);  add_39 = None
    getitem_413 = split_tensor_40[0];  split_tensor_40 = None
    _to_copy_298 = torch.ops.aten._to_copy.default(getitem_413, dtype = torch.float32);  getitem_413 = None
    native_layer_norm_default_63 = torch.ops.aten.native_layer_norm.default(_to_copy_298, [256], msa_module_pair_transition_2_layer_norm_weight, msa_module_pair_transition_2_layer_norm_bias, 1e-05);  _to_copy_298 = msa_module_pair_transition_2_layer_norm_weight = msa_module_pair_transition_2_layer_norm_bias = None
    getitem_414 = native_layer_norm_default_63[0];  native_layer_norm_default_63 = None
    _to_copy_299 = torch.ops.aten._to_copy.default(msa_module_pair_transition_2_linear_no_bias_ab_weight, dtype = torch.bfloat16);  msa_module_pair_transition_2_linear_no_bias_ab_weight = None
    _to_copy_300 = torch.ops.aten._to_copy.default(getitem_414, dtype = torch.bfloat16);  getitem_414 = None
    t_98 = torch.ops.aten.t.default(_to_copy_299);  _to_copy_299 = None
    view_506 = torch.ops.aten.view.default(_to_copy_300, [147456, 256]);  _to_copy_300 = None
    mm_91 = torch.ops.aten.mm.default(view_506, t_98);  view_506 = t_98 = None
    view_507 = torch.ops.aten.view.default(mm_91, [1, 384, 384, 2048]);  mm_91 = None
    split_tensor_41 = torch.ops.aten.split.Tensor(view_507, 1024, dim = -1);  view_507 = None
    getitem_417 = split_tensor_41[0]
    getitem_418 = split_tensor_41[1];  split_tensor_41 = None
    silu_13 = torch.ops.aten.silu.default(getitem_417);  getitem_417 = None
    mul_47 = torch.ops.aten.mul.Tensor(silu_13, getitem_418);  silu_13 = getitem_418 = None
    _to_copy_301 = torch.ops.aten._to_copy.default(msa_module_pair_transition_2_linear_out_weight, dtype = torch.bfloat16);  msa_module_pair_transition_2_linear_out_weight = None
    t_99 = torch.ops.aten.t.default(_to_copy_301);  _to_copy_301 = None
    view_509 = torch.ops.aten.view.default(mul_47, [147456, 1024]);  mul_47 = None
    mm_92 = torch.ops.aten.mm.default(view_509, t_99);  view_509 = t_99 = None
    view_510 = torch.ops.aten.view.default(mm_92, [1, 384, 384, 256]);  mm_92 = None
    add_44 = torch.ops.aten.add.Tensor(add_43, view_510);  add_43 = view_510 = None
    _to_copy_302 = torch.ops.aten._to_copy.default(add_44, dtype = torch.float32)
    native_layer_norm_default_64 = torch.ops.aten.native_layer_norm.default(_to_copy_302, [256], None, None, 1e-05);  _to_copy_302 = None
    getitem_419 = native_layer_norm_default_64[0];  native_layer_norm_default_64 = None
    _to_copy_303 = torch.ops.aten._to_copy.default(msa_module_triangular_attention_2_pair2b_weight, dtype = torch.bfloat16);  msa_module_triangular_attention_2_pair2b_weight = None
    _to_copy_304 = torch.ops.aten._to_copy.default(getitem_419, dtype = torch.bfloat16)
    t_100 = torch.ops.aten.t.default(_to_copy_303);  _to_copy_303 = None
    view_511 = torch.ops.aten.view.default(_to_copy_304, [147456, 256]);  _to_copy_304 = None
    mm_93 = torch.ops.aten.mm.default(view_511, t_100);  view_511 = t_100 = None
    view_512 = torch.ops.aten.view.default(mm_93, [1, 384, 384, 8]);  mm_93 = None
    view_513 = torch.ops.aten.view.default(view_512, [1, 384, 384, 2, 4]);  view_512 = None
    permute_336 = torch.ops.aten.permute.default(view_513, [0, 3, 4, 1, 2]);  view_513 = None
    view_514 = torch.ops.aten.view.default(permute_336, [1, 2, 4, 1, 384, 384]);  permute_336 = None
    view_515 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 384, 384])
    bitwise_not_38 = torch.ops.aten.bitwise_not.default(view_515);  view_515 = None
    masked_fill_38 = torch.ops.aten.masked_fill.Scalar(view_514, bitwise_not_38, -10000);  view_514 = bitwise_not_38 = None
    view_516 = torch.ops.aten.view.default(masked_fill_38, [1, 2, 4, 384, 384]);  masked_fill_38 = None
    permute_337 = torch.ops.aten.permute.default(view_516, [1, 0, 2, 3, 4]);  view_516 = None
    view_517 = torch.ops.aten.view.default(permute_337, [2, 4, 1, 384, 384]);  permute_337 = None
    _to_copy_305 = torch.ops.aten._to_copy.default(msa_module_triangular_attention_2_pair2qkvg1_weight, dtype = torch.bfloat16);  msa_module_triangular_attention_2_pair2qkvg1_weight = None
    _to_copy_306 = torch.ops.aten._to_copy.default(getitem_419, dtype = torch.bfloat16)
    t_101 = torch.ops.aten.t.default(_to_copy_305);  _to_copy_305 = None
    view_518 = torch.ops.aten.view.default(_to_copy_306, [147456, 256]);  _to_copy_306 = None
    mm_94 = torch.ops.aten.mm.default(view_518, t_101);  view_518 = t_101 = None
    view_519 = torch.ops.aten.view.default(mm_94, [1, 384, 384, 1024]);  mm_94 = None
    select_9 = torch.ops.aten.select.int(view_517, 0, 0)
    view_520 = torch.ops.aten.view.default(view_519, [1, 384, 384, 4, 4, 64]);  view_519 = None
    permute_338 = torch.ops.aten.permute.default(view_520, [4, 0, 3, 1, 2, 5]);  view_520 = None
    view_521 = torch.ops.aten.view.default(permute_338, [4, 4, 384, 384, 64]);  permute_338 = None
    unbind_int_26 = torch.ops.aten.unbind.int(view_521);  view_521 = None
    getitem_422 = unbind_int_26[0]
    getitem_423 = unbind_int_26[1]
    getitem_424 = unbind_int_26[2]
    getitem_425 = unbind_int_26[3];  unbind_int_26 = None
    expand_19 = torch.ops.aten.expand.default(select_9, [4, 384, 384, 384]);  select_9 = None
    _scaled_dot_product_efficient_attention_default_8 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_422, getitem_423, getitem_424, expand_19, False);  getitem_422 = getitem_423 = getitem_424 = expand_19 = None
    getitem_426 = _scaled_dot_product_efficient_attention_default_8[0];  _scaled_dot_product_efficient_attention_default_8 = None
    sigmoid_29 = torch.ops.aten.sigmoid.default(getitem_425);  getitem_425 = None
    mul_48 = torch.ops.aten.mul.Tensor(getitem_426, sigmoid_29);  getitem_426 = sigmoid_29 = None
    view_522 = torch.ops.aten.view.default(mul_48, [1, 4, 384, 384, 64]);  mul_48 = None
    permute_339 = torch.ops.aten.permute.default(view_522, [0, 2, 3, 1, 4]);  view_522 = None
    clone_72 = torch.ops.aten.clone.default(permute_339, memory_format = torch.contiguous_format);  permute_339 = None
    _unsafe_view_66 = torch.ops.aten._unsafe_view.default(clone_72, [1, 384, 384, 256]);  clone_72 = None
    transpose_9 = torch.ops.aten.transpose.int(getitem_419, 1, 2);  getitem_419 = None
    _to_copy_307 = torch.ops.aten._to_copy.default(msa_module_triangular_attention_2_pair2qkvg2_weight, dtype = torch.bfloat16);  msa_module_triangular_attention_2_pair2qkvg2_weight = None
    _to_copy_308 = torch.ops.aten._to_copy.default(transpose_9, dtype = torch.bfloat16);  transpose_9 = None
    t_102 = torch.ops.aten.t.default(_to_copy_307);  _to_copy_307 = None
    expand_20 = torch.ops.aten.expand.default(_to_copy_308, [1, 384, 384, 256]);  _to_copy_308 = None
    view_523 = torch.ops.aten.view.default(expand_20, [384, 384, 256]);  expand_20 = None
    expand_21 = torch.ops.aten.expand.default(t_102, [1, 384, 256, 1024]);  t_102 = None
    view_524 = torch.ops.aten.view.default(expand_21, [384, 256, 1024]);  expand_21 = None
    bmm_56 = torch.ops.aten.bmm.default(view_523, view_524);  view_523 = view_524 = None
    view_525 = torch.ops.aten.view.default(bmm_56, [1, 384, 384, 1024]);  bmm_56 = None
    select_10 = torch.ops.aten.select.int(view_517, 0, 1);  view_517 = None
    view_526 = torch.ops.aten.view.default(view_525, [1, 384, 384, 4, 4, 64]);  view_525 = None
    permute_340 = torch.ops.aten.permute.default(view_526, [4, 0, 3, 1, 2, 5]);  view_526 = None
    view_527 = torch.ops.aten.view.default(permute_340, [4, 4, 384, 384, 64]);  permute_340 = None
    unbind_int_27 = torch.ops.aten.unbind.int(view_527);  view_527 = None
    getitem_430 = unbind_int_27[0]
    getitem_431 = unbind_int_27[1]
    getitem_432 = unbind_int_27[2]
    getitem_433 = unbind_int_27[3];  unbind_int_27 = None
    expand_22 = torch.ops.aten.expand.default(select_10, [4, 384, 384, 384]);  select_10 = None
    _scaled_dot_product_efficient_attention_default_9 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_430, getitem_431, getitem_432, expand_22, False);  getitem_430 = getitem_431 = getitem_432 = expand_22 = None
    getitem_434 = _scaled_dot_product_efficient_attention_default_9[0];  _scaled_dot_product_efficient_attention_default_9 = None
    sigmoid_30 = torch.ops.aten.sigmoid.default(getitem_433);  getitem_433 = None
    mul_49 = torch.ops.aten.mul.Tensor(getitem_434, sigmoid_30);  getitem_434 = sigmoid_30 = None
    view_528 = torch.ops.aten.view.default(mul_49, [1, 4, 384, 384, 64]);  mul_49 = None
    permute_341 = torch.ops.aten.permute.default(view_528, [0, 2, 3, 1, 4]);  view_528 = None
    clone_73 = torch.ops.aten.clone.default(permute_341, memory_format = torch.contiguous_format);  permute_341 = None
    _unsafe_view_67 = torch.ops.aten._unsafe_view.default(clone_73, [1, 384, 384, 256]);  clone_73 = None
    cat_10 = torch.ops.aten.cat.default([_unsafe_view_66, _unsafe_view_67], dim = -1);  _unsafe_view_66 = _unsafe_view_67 = None
    slice_133 = torch.ops.aten.slice.Tensor(msa_module_triangular_attention_2_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  msa_module_triangular_attention_2_out_scalers = None
    unsqueeze_244 = torch.ops.aten.unsqueeze.default(slice_133, 1);  slice_133 = None
    mul_50 = torch.ops.aten.mul.Tensor(msa_module_triangular_attention_2_linear_out_weight, unsqueeze_244);  msa_module_triangular_attention_2_linear_out_weight = unsqueeze_244 = None
    _to_copy_309 = torch.ops.aten._to_copy.default(mul_50, dtype = torch.bfloat16);  mul_50 = None
    t_103 = torch.ops.aten.t.default(_to_copy_309);  _to_copy_309 = None
    view_529 = torch.ops.aten.view.default(cat_10, [147456, 512]);  cat_10 = None
    mm_95 = torch.ops.aten.mm.default(view_529, t_103);  view_529 = t_103 = None
    view_530 = torch.ops.aten.view.default(mm_95, [1, 384, 384, 256]);  mm_95 = None
    add_45 = torch.ops.aten.add.Tensor(add_44, view_530);  add_44 = view_530 = None
    slice_134 = torch.ops.aten.slice.Tensor(add_41, dim = 0, start = 0, end = 9223372036854775807)
    slice_135 = torch.ops.aten.slice.Tensor(slice_134, dim = 1, start = 0, end = 4096);  slice_134 = None
    slice_136 = torch.ops.aten.slice.Tensor(slice_135, dim = 2, start = 0, end = 9223372036854775807);  slice_135 = None
    slice_137 = torch.ops.aten.slice.Tensor(slice_136, dim = 3, start = 0, end = 9223372036854775807);  slice_136 = None
    slice_138 = torch.ops.aten.slice.Tensor(arg1403_1, dim = 0, start = 0, end = 9223372036854775807)
    slice_139 = torch.ops.aten.slice.Tensor(slice_138, dim = 1, start = 0, end = 4096);  slice_138 = None
    slice_140 = torch.ops.aten.slice.Tensor(slice_139, dim = 2, start = 0, end = 9223372036854775807);  slice_139 = None
    _to_copy_310 = torch.ops.aten._to_copy.default(slice_137, dtype = torch.float32);  slice_137 = None
    native_layer_norm_default_65 = torch.ops.aten.native_layer_norm.default(_to_copy_310, [64], None, None, 1e-05);  _to_copy_310 = None
    getitem_438 = native_layer_norm_default_65[0];  native_layer_norm_default_65 = None
    view_531 = torch.ops.aten.view.default(slice_140, [1, 4096, 384, 1]);  slice_140 = None
    bitwise_not_39 = torch.ops.aten.bitwise_not.default(view_531);  view_531 = None
    masked_fill_39 = torch.ops.aten.masked_fill.Scalar(getitem_438, bitwise_not_39, 0);  getitem_438 = bitwise_not_39 = None
    unbind_int_28 = torch.ops.aten.unbind.int(msa_module_outer_product_mean_3_weight_ab)
    getitem_441 = unbind_int_28[0]
    getitem_442 = unbind_int_28[1];  unbind_int_28 = None
    _to_copy_311 = torch.ops.aten._to_copy.default(getitem_441, dtype = torch.bfloat16);  getitem_441 = None
    _to_copy_312 = torch.ops.aten._to_copy.default(masked_fill_39, dtype = torch.bfloat16)
    unsqueeze_245 = torch.ops.aten.unsqueeze.default(_to_copy_311, 3);  _to_copy_311 = None
    unsqueeze_246 = torch.ops.aten.unsqueeze.default(unsqueeze_245, 4);  unsqueeze_245 = None
    unsqueeze_247 = torch.ops.aten.unsqueeze.default(unsqueeze_246, 5);  unsqueeze_246 = None
    permute_342 = torch.ops.aten.permute.default(unsqueeze_247, [0, 1, 3, 4, 5, 2]);  unsqueeze_247 = None
    unsqueeze_248 = torch.ops.aten.unsqueeze.default(_to_copy_312, 4);  _to_copy_312 = None
    unsqueeze_249 = torch.ops.aten.unsqueeze.default(unsqueeze_248, 5);  unsqueeze_248 = None
    permute_343 = torch.ops.aten.permute.default(unsqueeze_249, [4, 5, 0, 1, 2, 3]);  unsqueeze_249 = None
    permute_344 = torch.ops.aten.permute.default(permute_342, [0, 1, 5, 2, 3, 4]);  permute_342 = None
    view_532 = torch.ops.aten.view.default(permute_344, [1, 64, 64]);  permute_344 = None
    permute_345 = torch.ops.aten.permute.default(permute_343, [5, 2, 3, 4, 0, 1]);  permute_343 = None
    view_533 = torch.ops.aten.view.default(permute_345, [1, 64, 1572864]);  permute_345 = None
    bmm_57 = torch.ops.aten.bmm.default(view_532, view_533);  view_532 = view_533 = None
    view_534 = torch.ops.aten.view.default(bmm_57, [8, 8, 1, 1, 4096, 384]);  bmm_57 = None
    permute_346 = torch.ops.aten.permute.default(view_534, [0, 1, 3, 4, 5, 2]);  view_534 = None
    view_535 = torch.ops.aten.view.default(permute_346, [8, 8, 1, 4096, 384]);  permute_346 = None
    _to_copy_313 = torch.ops.aten._to_copy.default(getitem_442, dtype = torch.bfloat16);  getitem_442 = None
    _to_copy_314 = torch.ops.aten._to_copy.default(masked_fill_39, dtype = torch.bfloat16);  masked_fill_39 = None
    unsqueeze_250 = torch.ops.aten.unsqueeze.default(_to_copy_313, 3);  _to_copy_313 = None
    unsqueeze_251 = torch.ops.aten.unsqueeze.default(unsqueeze_250, 4);  unsqueeze_250 = None
    unsqueeze_252 = torch.ops.aten.unsqueeze.default(unsqueeze_251, 5);  unsqueeze_251 = None
    permute_347 = torch.ops.aten.permute.default(unsqueeze_252, [0, 1, 3, 4, 5, 2]);  unsqueeze_252 = None
    unsqueeze_253 = torch.ops.aten.unsqueeze.default(_to_copy_314, 4);  _to_copy_314 = None
    unsqueeze_254 = torch.ops.aten.unsqueeze.default(unsqueeze_253, 5);  unsqueeze_253 = None
    permute_348 = torch.ops.aten.permute.default(unsqueeze_254, [4, 5, 0, 1, 2, 3]);  unsqueeze_254 = None
    permute_349 = torch.ops.aten.permute.default(permute_347, [0, 1, 5, 2, 3, 4]);  permute_347 = None
    view_536 = torch.ops.aten.view.default(permute_349, [1, 64, 64]);  permute_349 = None
    permute_350 = torch.ops.aten.permute.default(permute_348, [5, 2, 3, 4, 0, 1]);  permute_348 = None
    view_537 = torch.ops.aten.view.default(permute_350, [1, 64, 1572864]);  permute_350 = None
    bmm_58 = torch.ops.aten.bmm.default(view_536, view_537);  view_536 = view_537 = None
    view_538 = torch.ops.aten.view.default(bmm_58, [8, 8, 1, 1, 4096, 384]);  bmm_58 = None
    permute_351 = torch.ops.aten.permute.default(view_538, [0, 1, 3, 4, 5, 2]);  view_538 = None
    view_539 = torch.ops.aten.view.default(permute_351, [8, 8, 1, 4096, 384]);  permute_351 = None
    unsqueeze_255 = torch.ops.aten.unsqueeze.default(view_535, 5);  view_535 = None
    unsqueeze_256 = torch.ops.aten.unsqueeze.default(unsqueeze_255, 6);  unsqueeze_255 = None
    permute_352 = torch.ops.aten.permute.default(unsqueeze_256, [2, 4, 5, 0, 1, 6, 3]);  unsqueeze_256 = None
    unsqueeze_257 = torch.ops.aten.unsqueeze.default(view_539, 5);  view_539 = None
    unsqueeze_258 = torch.ops.aten.unsqueeze.default(unsqueeze_257, 6);  unsqueeze_257 = None
    permute_353 = torch.ops.aten.permute.default(unsqueeze_258, [2, 5, 4, 0, 6, 1, 3]);  unsqueeze_258 = None
    permute_354 = torch.ops.aten.permute.default(permute_352, [3, 1, 4, 6, 0, 2, 5]);  permute_352 = None
    clone_74 = torch.ops.aten.clone.default(permute_354, memory_format = torch.contiguous_format);  permute_354 = None
    _unsafe_view_68 = torch.ops.aten._unsafe_view.default(clone_74, [8, 3072, 4096]);  clone_74 = None
    permute_355 = torch.ops.aten.permute.default(permute_353, [3, 6, 0, 2, 5, 1, 4]);  permute_353 = None
    clone_75 = torch.ops.aten.clone.default(permute_355, memory_format = torch.contiguous_format);  permute_355 = None
    _unsafe_view_69 = torch.ops.aten._unsafe_view.default(clone_75, [8, 4096, 3072]);  clone_75 = None
    bmm_59 = torch.ops.aten.bmm.default(_unsafe_view_68, _unsafe_view_69);  _unsafe_view_68 = _unsafe_view_69 = None
    view_540 = torch.ops.aten.view.default(bmm_59, [8, 384, 8, 1, 1, 384, 8]);  bmm_59 = None
    permute_356 = torch.ops.aten.permute.default(view_540, [4, 1, 5, 0, 2, 6, 3]);  view_540 = None
    view_541 = torch.ops.aten.view.default(permute_356, [1, 384, 384, 8, 8, 8]);  permute_356 = None
    clone_76 = torch.ops.aten.clone.default(view_541, memory_format = torch.contiguous_format);  view_541 = None
    _unsafe_view_70 = torch.ops.aten._unsafe_view.default(clone_76, [1, 384, 384, 512]);  clone_76 = None
    add_46 = torch.ops.aten.add.Tensor(_unsafe_view_70, 0);  _unsafe_view_70 = None
    slice_141 = torch.ops.aten.slice.Tensor(add_41, dim = 0, start = 0, end = 9223372036854775807)
    slice_142 = torch.ops.aten.slice.Tensor(slice_141, dim = 1, start = 4096, end = 8192);  slice_141 = None
    slice_143 = torch.ops.aten.slice.Tensor(slice_142, dim = 2, start = 0, end = 9223372036854775807);  slice_142 = None
    slice_144 = torch.ops.aten.slice.Tensor(slice_143, dim = 3, start = 0, end = 9223372036854775807);  slice_143 = None
    slice_145 = torch.ops.aten.slice.Tensor(arg1403_1, dim = 0, start = 0, end = 9223372036854775807)
    slice_146 = torch.ops.aten.slice.Tensor(slice_145, dim = 1, start = 4096, end = 8192);  slice_145 = None
    slice_147 = torch.ops.aten.slice.Tensor(slice_146, dim = 2, start = 0, end = 9223372036854775807);  slice_146 = None
    _to_copy_315 = torch.ops.aten._to_copy.default(slice_144, dtype = torch.float32);  slice_144 = None
    native_layer_norm_default_66 = torch.ops.aten.native_layer_norm.default(_to_copy_315, [64], None, None, 1e-05);  _to_copy_315 = None
    getitem_443 = native_layer_norm_default_66[0];  native_layer_norm_default_66 = None
    view_542 = torch.ops.aten.view.default(slice_147, [1, 4096, 384, 1]);  slice_147 = None
    bitwise_not_40 = torch.ops.aten.bitwise_not.default(view_542);  view_542 = None
    masked_fill_40 = torch.ops.aten.masked_fill.Scalar(getitem_443, bitwise_not_40, 0);  getitem_443 = bitwise_not_40 = None
    unbind_int_29 = torch.ops.aten.unbind.int(msa_module_outer_product_mean_3_weight_ab)
    getitem_446 = unbind_int_29[0]
    getitem_447 = unbind_int_29[1];  unbind_int_29 = None
    _to_copy_316 = torch.ops.aten._to_copy.default(getitem_446, dtype = torch.bfloat16);  getitem_446 = None
    _to_copy_317 = torch.ops.aten._to_copy.default(masked_fill_40, dtype = torch.bfloat16)
    unsqueeze_259 = torch.ops.aten.unsqueeze.default(_to_copy_316, 3);  _to_copy_316 = None
    unsqueeze_260 = torch.ops.aten.unsqueeze.default(unsqueeze_259, 4);  unsqueeze_259 = None
    unsqueeze_261 = torch.ops.aten.unsqueeze.default(unsqueeze_260, 5);  unsqueeze_260 = None
    permute_357 = torch.ops.aten.permute.default(unsqueeze_261, [0, 1, 3, 4, 5, 2]);  unsqueeze_261 = None
    unsqueeze_262 = torch.ops.aten.unsqueeze.default(_to_copy_317, 4);  _to_copy_317 = None
    unsqueeze_263 = torch.ops.aten.unsqueeze.default(unsqueeze_262, 5);  unsqueeze_262 = None
    permute_358 = torch.ops.aten.permute.default(unsqueeze_263, [4, 5, 0, 1, 2, 3]);  unsqueeze_263 = None
    permute_359 = torch.ops.aten.permute.default(permute_357, [0, 1, 5, 2, 3, 4]);  permute_357 = None
    view_543 = torch.ops.aten.view.default(permute_359, [1, 64, 64]);  permute_359 = None
    permute_360 = torch.ops.aten.permute.default(permute_358, [5, 2, 3, 4, 0, 1]);  permute_358 = None
    view_544 = torch.ops.aten.view.default(permute_360, [1, 64, 1572864]);  permute_360 = None
    bmm_60 = torch.ops.aten.bmm.default(view_543, view_544);  view_543 = view_544 = None
    view_545 = torch.ops.aten.view.default(bmm_60, [8, 8, 1, 1, 4096, 384]);  bmm_60 = None
    permute_361 = torch.ops.aten.permute.default(view_545, [0, 1, 3, 4, 5, 2]);  view_545 = None
    view_546 = torch.ops.aten.view.default(permute_361, [8, 8, 1, 4096, 384]);  permute_361 = None
    _to_copy_318 = torch.ops.aten._to_copy.default(getitem_447, dtype = torch.bfloat16);  getitem_447 = None
    _to_copy_319 = torch.ops.aten._to_copy.default(masked_fill_40, dtype = torch.bfloat16);  masked_fill_40 = None
    unsqueeze_264 = torch.ops.aten.unsqueeze.default(_to_copy_318, 3);  _to_copy_318 = None
    unsqueeze_265 = torch.ops.aten.unsqueeze.default(unsqueeze_264, 4);  unsqueeze_264 = None
    unsqueeze_266 = torch.ops.aten.unsqueeze.default(unsqueeze_265, 5);  unsqueeze_265 = None
    permute_362 = torch.ops.aten.permute.default(unsqueeze_266, [0, 1, 3, 4, 5, 2]);  unsqueeze_266 = None
    unsqueeze_267 = torch.ops.aten.unsqueeze.default(_to_copy_319, 4);  _to_copy_319 = None
    unsqueeze_268 = torch.ops.aten.unsqueeze.default(unsqueeze_267, 5);  unsqueeze_267 = None
    permute_363 = torch.ops.aten.permute.default(unsqueeze_268, [4, 5, 0, 1, 2, 3]);  unsqueeze_268 = None
    permute_364 = torch.ops.aten.permute.default(permute_362, [0, 1, 5, 2, 3, 4]);  permute_362 = None
    view_547 = torch.ops.aten.view.default(permute_364, [1, 64, 64]);  permute_364 = None
    permute_365 = torch.ops.aten.permute.default(permute_363, [5, 2, 3, 4, 0, 1]);  permute_363 = None
    view_548 = torch.ops.aten.view.default(permute_365, [1, 64, 1572864]);  permute_365 = None
    bmm_61 = torch.ops.aten.bmm.default(view_547, view_548);  view_547 = view_548 = None
    view_549 = torch.ops.aten.view.default(bmm_61, [8, 8, 1, 1, 4096, 384]);  bmm_61 = None
    permute_366 = torch.ops.aten.permute.default(view_549, [0, 1, 3, 4, 5, 2]);  view_549 = None
    view_550 = torch.ops.aten.view.default(permute_366, [8, 8, 1, 4096, 384]);  permute_366 = None
    unsqueeze_269 = torch.ops.aten.unsqueeze.default(view_546, 5);  view_546 = None
    unsqueeze_270 = torch.ops.aten.unsqueeze.default(unsqueeze_269, 6);  unsqueeze_269 = None
    permute_367 = torch.ops.aten.permute.default(unsqueeze_270, [2, 4, 5, 0, 1, 6, 3]);  unsqueeze_270 = None
    unsqueeze_271 = torch.ops.aten.unsqueeze.default(view_550, 5);  view_550 = None
    unsqueeze_272 = torch.ops.aten.unsqueeze.default(unsqueeze_271, 6);  unsqueeze_271 = None
    permute_368 = torch.ops.aten.permute.default(unsqueeze_272, [2, 5, 4, 0, 6, 1, 3]);  unsqueeze_272 = None
    permute_369 = torch.ops.aten.permute.default(permute_367, [3, 1, 4, 6, 0, 2, 5]);  permute_367 = None
    clone_77 = torch.ops.aten.clone.default(permute_369, memory_format = torch.contiguous_format);  permute_369 = None
    _unsafe_view_71 = torch.ops.aten._unsafe_view.default(clone_77, [8, 3072, 4096]);  clone_77 = None
    permute_370 = torch.ops.aten.permute.default(permute_368, [3, 6, 0, 2, 5, 1, 4]);  permute_368 = None
    clone_78 = torch.ops.aten.clone.default(permute_370, memory_format = torch.contiguous_format);  permute_370 = None
    _unsafe_view_72 = torch.ops.aten._unsafe_view.default(clone_78, [8, 4096, 3072]);  clone_78 = None
    bmm_62 = torch.ops.aten.bmm.default(_unsafe_view_71, _unsafe_view_72);  _unsafe_view_71 = _unsafe_view_72 = None
    view_551 = torch.ops.aten.view.default(bmm_62, [8, 384, 8, 1, 1, 384, 8]);  bmm_62 = None
    permute_371 = torch.ops.aten.permute.default(view_551, [4, 1, 5, 0, 2, 6, 3]);  view_551 = None
    view_552 = torch.ops.aten.view.default(permute_371, [1, 384, 384, 8, 8, 8]);  permute_371 = None
    clone_79 = torch.ops.aten.clone.default(view_552, memory_format = torch.contiguous_format);  view_552 = None
    _unsafe_view_73 = torch.ops.aten._unsafe_view.default(clone_79, [1, 384, 384, 512]);  clone_79 = None
    add_47 = torch.ops.aten.add.Tensor(add_46, _unsafe_view_73);  add_46 = _unsafe_view_73 = None
    slice_148 = torch.ops.aten.slice.Tensor(add_41, dim = 0, start = 0, end = 9223372036854775807)
    slice_149 = torch.ops.aten.slice.Tensor(slice_148, dim = 1, start = 8192, end = 12288);  slice_148 = None
    slice_150 = torch.ops.aten.slice.Tensor(slice_149, dim = 2, start = 0, end = 9223372036854775807);  slice_149 = None
    slice_151 = torch.ops.aten.slice.Tensor(slice_150, dim = 3, start = 0, end = 9223372036854775807);  slice_150 = None
    slice_152 = torch.ops.aten.slice.Tensor(arg1403_1, dim = 0, start = 0, end = 9223372036854775807)
    slice_153 = torch.ops.aten.slice.Tensor(slice_152, dim = 1, start = 8192, end = 12288);  slice_152 = None
    slice_154 = torch.ops.aten.slice.Tensor(slice_153, dim = 2, start = 0, end = 9223372036854775807);  slice_153 = None
    _to_copy_320 = torch.ops.aten._to_copy.default(slice_151, dtype = torch.float32);  slice_151 = None
    native_layer_norm_default_67 = torch.ops.aten.native_layer_norm.default(_to_copy_320, [64], None, None, 1e-05);  _to_copy_320 = None
    getitem_448 = native_layer_norm_default_67[0];  native_layer_norm_default_67 = None
    view_553 = torch.ops.aten.view.default(slice_154, [1, 4096, 384, 1]);  slice_154 = None
    bitwise_not_41 = torch.ops.aten.bitwise_not.default(view_553);  view_553 = None
    masked_fill_41 = torch.ops.aten.masked_fill.Scalar(getitem_448, bitwise_not_41, 0);  getitem_448 = bitwise_not_41 = None
    unbind_int_30 = torch.ops.aten.unbind.int(msa_module_outer_product_mean_3_weight_ab)
    getitem_451 = unbind_int_30[0]
    getitem_452 = unbind_int_30[1];  unbind_int_30 = None
    _to_copy_321 = torch.ops.aten._to_copy.default(getitem_451, dtype = torch.bfloat16);  getitem_451 = None
    _to_copy_322 = torch.ops.aten._to_copy.default(masked_fill_41, dtype = torch.bfloat16)
    unsqueeze_273 = torch.ops.aten.unsqueeze.default(_to_copy_321, 3);  _to_copy_321 = None
    unsqueeze_274 = torch.ops.aten.unsqueeze.default(unsqueeze_273, 4);  unsqueeze_273 = None
    unsqueeze_275 = torch.ops.aten.unsqueeze.default(unsqueeze_274, 5);  unsqueeze_274 = None
    permute_372 = torch.ops.aten.permute.default(unsqueeze_275, [0, 1, 3, 4, 5, 2]);  unsqueeze_275 = None
    unsqueeze_276 = torch.ops.aten.unsqueeze.default(_to_copy_322, 4);  _to_copy_322 = None
    unsqueeze_277 = torch.ops.aten.unsqueeze.default(unsqueeze_276, 5);  unsqueeze_276 = None
    permute_373 = torch.ops.aten.permute.default(unsqueeze_277, [4, 5, 0, 1, 2, 3]);  unsqueeze_277 = None
    permute_374 = torch.ops.aten.permute.default(permute_372, [0, 1, 5, 2, 3, 4]);  permute_372 = None
    view_554 = torch.ops.aten.view.default(permute_374, [1, 64, 64]);  permute_374 = None
    permute_375 = torch.ops.aten.permute.default(permute_373, [5, 2, 3, 4, 0, 1]);  permute_373 = None
    view_555 = torch.ops.aten.view.default(permute_375, [1, 64, 1572864]);  permute_375 = None
    bmm_63 = torch.ops.aten.bmm.default(view_554, view_555);  view_554 = view_555 = None
    view_556 = torch.ops.aten.view.default(bmm_63, [8, 8, 1, 1, 4096, 384]);  bmm_63 = None
    permute_376 = torch.ops.aten.permute.default(view_556, [0, 1, 3, 4, 5, 2]);  view_556 = None
    view_557 = torch.ops.aten.view.default(permute_376, [8, 8, 1, 4096, 384]);  permute_376 = None
    _to_copy_323 = torch.ops.aten._to_copy.default(getitem_452, dtype = torch.bfloat16);  getitem_452 = None
    _to_copy_324 = torch.ops.aten._to_copy.default(masked_fill_41, dtype = torch.bfloat16);  masked_fill_41 = None
    unsqueeze_278 = torch.ops.aten.unsqueeze.default(_to_copy_323, 3);  _to_copy_323 = None
    unsqueeze_279 = torch.ops.aten.unsqueeze.default(unsqueeze_278, 4);  unsqueeze_278 = None
    unsqueeze_280 = torch.ops.aten.unsqueeze.default(unsqueeze_279, 5);  unsqueeze_279 = None
    permute_377 = torch.ops.aten.permute.default(unsqueeze_280, [0, 1, 3, 4, 5, 2]);  unsqueeze_280 = None
    unsqueeze_281 = torch.ops.aten.unsqueeze.default(_to_copy_324, 4);  _to_copy_324 = None
    unsqueeze_282 = torch.ops.aten.unsqueeze.default(unsqueeze_281, 5);  unsqueeze_281 = None
    permute_378 = torch.ops.aten.permute.default(unsqueeze_282, [4, 5, 0, 1, 2, 3]);  unsqueeze_282 = None
    permute_379 = torch.ops.aten.permute.default(permute_377, [0, 1, 5, 2, 3, 4]);  permute_377 = None
    view_558 = torch.ops.aten.view.default(permute_379, [1, 64, 64]);  permute_379 = None
    permute_380 = torch.ops.aten.permute.default(permute_378, [5, 2, 3, 4, 0, 1]);  permute_378 = None
    view_559 = torch.ops.aten.view.default(permute_380, [1, 64, 1572864]);  permute_380 = None
    bmm_64 = torch.ops.aten.bmm.default(view_558, view_559);  view_558 = view_559 = None
    view_560 = torch.ops.aten.view.default(bmm_64, [8, 8, 1, 1, 4096, 384]);  bmm_64 = None
    permute_381 = torch.ops.aten.permute.default(view_560, [0, 1, 3, 4, 5, 2]);  view_560 = None
    view_561 = torch.ops.aten.view.default(permute_381, [8, 8, 1, 4096, 384]);  permute_381 = None
    unsqueeze_283 = torch.ops.aten.unsqueeze.default(view_557, 5);  view_557 = None
    unsqueeze_284 = torch.ops.aten.unsqueeze.default(unsqueeze_283, 6);  unsqueeze_283 = None
    permute_382 = torch.ops.aten.permute.default(unsqueeze_284, [2, 4, 5, 0, 1, 6, 3]);  unsqueeze_284 = None
    unsqueeze_285 = torch.ops.aten.unsqueeze.default(view_561, 5);  view_561 = None
    unsqueeze_286 = torch.ops.aten.unsqueeze.default(unsqueeze_285, 6);  unsqueeze_285 = None
    permute_383 = torch.ops.aten.permute.default(unsqueeze_286, [2, 5, 4, 0, 6, 1, 3]);  unsqueeze_286 = None
    permute_384 = torch.ops.aten.permute.default(permute_382, [3, 1, 4, 6, 0, 2, 5]);  permute_382 = None
    clone_80 = torch.ops.aten.clone.default(permute_384, memory_format = torch.contiguous_format);  permute_384 = None
    _unsafe_view_74 = torch.ops.aten._unsafe_view.default(clone_80, [8, 3072, 4096]);  clone_80 = None
    permute_385 = torch.ops.aten.permute.default(permute_383, [3, 6, 0, 2, 5, 1, 4]);  permute_383 = None
    clone_81 = torch.ops.aten.clone.default(permute_385, memory_format = torch.contiguous_format);  permute_385 = None
    _unsafe_view_75 = torch.ops.aten._unsafe_view.default(clone_81, [8, 4096, 3072]);  clone_81 = None
    bmm_65 = torch.ops.aten.bmm.default(_unsafe_view_74, _unsafe_view_75);  _unsafe_view_74 = _unsafe_view_75 = None
    view_562 = torch.ops.aten.view.default(bmm_65, [8, 384, 8, 1, 1, 384, 8]);  bmm_65 = None
    permute_386 = torch.ops.aten.permute.default(view_562, [4, 1, 5, 0, 2, 6, 3]);  view_562 = None
    view_563 = torch.ops.aten.view.default(permute_386, [1, 384, 384, 8, 8, 8]);  permute_386 = None
    clone_82 = torch.ops.aten.clone.default(view_563, memory_format = torch.contiguous_format);  view_563 = None
    _unsafe_view_76 = torch.ops.aten._unsafe_view.default(clone_82, [1, 384, 384, 512]);  clone_82 = None
    add_48 = torch.ops.aten.add.Tensor(add_47, _unsafe_view_76);  add_47 = _unsafe_view_76 = None
    slice_155 = torch.ops.aten.slice.Tensor(add_41, dim = 0, start = 0, end = 9223372036854775807);  add_41 = None
    slice_156 = torch.ops.aten.slice.Tensor(slice_155, dim = 1, start = 12288, end = 16384);  slice_155 = None
    slice_157 = torch.ops.aten.slice.Tensor(slice_156, dim = 2, start = 0, end = 9223372036854775807);  slice_156 = None
    slice_158 = torch.ops.aten.slice.Tensor(slice_157, dim = 3, start = 0, end = 9223372036854775807);  slice_157 = None
    slice_159 = torch.ops.aten.slice.Tensor(arg1403_1, dim = 0, start = 0, end = 9223372036854775807);  arg1403_1 = None
    slice_160 = torch.ops.aten.slice.Tensor(slice_159, dim = 1, start = 12288, end = 16384);  slice_159 = None
    slice_161 = torch.ops.aten.slice.Tensor(slice_160, dim = 2, start = 0, end = 9223372036854775807);  slice_160 = None
    _to_copy_325 = torch.ops.aten._to_copy.default(slice_158, dtype = torch.float32);  slice_158 = None
    native_layer_norm_default_68 = torch.ops.aten.native_layer_norm.default(_to_copy_325, [64], None, None, 1e-05);  _to_copy_325 = None
    getitem_453 = native_layer_norm_default_68[0];  native_layer_norm_default_68 = None
    view_564 = torch.ops.aten.view.default(slice_161, [1, 4096, 384, 1]);  slice_161 = None
    bitwise_not_42 = torch.ops.aten.bitwise_not.default(view_564);  view_564 = None
    masked_fill_42 = torch.ops.aten.masked_fill.Scalar(getitem_453, bitwise_not_42, 0);  getitem_453 = bitwise_not_42 = None
    unbind_int_31 = torch.ops.aten.unbind.int(msa_module_outer_product_mean_3_weight_ab);  msa_module_outer_product_mean_3_weight_ab = None
    getitem_456 = unbind_int_31[0]
    getitem_457 = unbind_int_31[1];  unbind_int_31 = None
    _to_copy_326 = torch.ops.aten._to_copy.default(getitem_456, dtype = torch.bfloat16);  getitem_456 = None
    _to_copy_327 = torch.ops.aten._to_copy.default(masked_fill_42, dtype = torch.bfloat16)
    unsqueeze_287 = torch.ops.aten.unsqueeze.default(_to_copy_326, 3);  _to_copy_326 = None
    unsqueeze_288 = torch.ops.aten.unsqueeze.default(unsqueeze_287, 4);  unsqueeze_287 = None
    unsqueeze_289 = torch.ops.aten.unsqueeze.default(unsqueeze_288, 5);  unsqueeze_288 = None
    permute_387 = torch.ops.aten.permute.default(unsqueeze_289, [0, 1, 3, 4, 5, 2]);  unsqueeze_289 = None
    unsqueeze_290 = torch.ops.aten.unsqueeze.default(_to_copy_327, 4);  _to_copy_327 = None
    unsqueeze_291 = torch.ops.aten.unsqueeze.default(unsqueeze_290, 5);  unsqueeze_290 = None
    permute_388 = torch.ops.aten.permute.default(unsqueeze_291, [4, 5, 0, 1, 2, 3]);  unsqueeze_291 = None
    permute_389 = torch.ops.aten.permute.default(permute_387, [0, 1, 5, 2, 3, 4]);  permute_387 = None
    view_565 = torch.ops.aten.view.default(permute_389, [1, 64, 64]);  permute_389 = None
    permute_390 = torch.ops.aten.permute.default(permute_388, [5, 2, 3, 4, 0, 1]);  permute_388 = None
    view_566 = torch.ops.aten.view.default(permute_390, [1, 64, 1572864]);  permute_390 = None
    bmm_66 = torch.ops.aten.bmm.default(view_565, view_566);  view_565 = view_566 = None
    view_567 = torch.ops.aten.view.default(bmm_66, [8, 8, 1, 1, 4096, 384]);  bmm_66 = None
    permute_391 = torch.ops.aten.permute.default(view_567, [0, 1, 3, 4, 5, 2]);  view_567 = None
    view_568 = torch.ops.aten.view.default(permute_391, [8, 8, 1, 4096, 384]);  permute_391 = None
    _to_copy_328 = torch.ops.aten._to_copy.default(getitem_457, dtype = torch.bfloat16);  getitem_457 = None
    _to_copy_329 = torch.ops.aten._to_copy.default(masked_fill_42, dtype = torch.bfloat16);  masked_fill_42 = None
    unsqueeze_292 = torch.ops.aten.unsqueeze.default(_to_copy_328, 3);  _to_copy_328 = None
    unsqueeze_293 = torch.ops.aten.unsqueeze.default(unsqueeze_292, 4);  unsqueeze_292 = None
    unsqueeze_294 = torch.ops.aten.unsqueeze.default(unsqueeze_293, 5);  unsqueeze_293 = None
    permute_392 = torch.ops.aten.permute.default(unsqueeze_294, [0, 1, 3, 4, 5, 2]);  unsqueeze_294 = None
    unsqueeze_295 = torch.ops.aten.unsqueeze.default(_to_copy_329, 4);  _to_copy_329 = None
    unsqueeze_296 = torch.ops.aten.unsqueeze.default(unsqueeze_295, 5);  unsqueeze_295 = None
    permute_393 = torch.ops.aten.permute.default(unsqueeze_296, [4, 5, 0, 1, 2, 3]);  unsqueeze_296 = None
    permute_394 = torch.ops.aten.permute.default(permute_392, [0, 1, 5, 2, 3, 4]);  permute_392 = None
    view_569 = torch.ops.aten.view.default(permute_394, [1, 64, 64]);  permute_394 = None
    permute_395 = torch.ops.aten.permute.default(permute_393, [5, 2, 3, 4, 0, 1]);  permute_393 = None
    view_570 = torch.ops.aten.view.default(permute_395, [1, 64, 1572864]);  permute_395 = None
    bmm_67 = torch.ops.aten.bmm.default(view_569, view_570);  view_569 = view_570 = None
    view_571 = torch.ops.aten.view.default(bmm_67, [8, 8, 1, 1, 4096, 384]);  bmm_67 = None
    permute_396 = torch.ops.aten.permute.default(view_571, [0, 1, 3, 4, 5, 2]);  view_571 = None
    view_572 = torch.ops.aten.view.default(permute_396, [8, 8, 1, 4096, 384]);  permute_396 = None
    unsqueeze_297 = torch.ops.aten.unsqueeze.default(view_568, 5);  view_568 = None
    unsqueeze_298 = torch.ops.aten.unsqueeze.default(unsqueeze_297, 6);  unsqueeze_297 = None
    permute_397 = torch.ops.aten.permute.default(unsqueeze_298, [2, 4, 5, 0, 1, 6, 3]);  unsqueeze_298 = None
    unsqueeze_299 = torch.ops.aten.unsqueeze.default(view_572, 5);  view_572 = None
    unsqueeze_300 = torch.ops.aten.unsqueeze.default(unsqueeze_299, 6);  unsqueeze_299 = None
    permute_398 = torch.ops.aten.permute.default(unsqueeze_300, [2, 5, 4, 0, 6, 1, 3]);  unsqueeze_300 = None
    permute_399 = torch.ops.aten.permute.default(permute_397, [3, 1, 4, 6, 0, 2, 5]);  permute_397 = None
    clone_83 = torch.ops.aten.clone.default(permute_399, memory_format = torch.contiguous_format);  permute_399 = None
    _unsafe_view_77 = torch.ops.aten._unsafe_view.default(clone_83, [8, 3072, 4096]);  clone_83 = None
    permute_400 = torch.ops.aten.permute.default(permute_398, [3, 6, 0, 2, 5, 1, 4]);  permute_398 = None
    clone_84 = torch.ops.aten.clone.default(permute_400, memory_format = torch.contiguous_format);  permute_400 = None
    _unsafe_view_78 = torch.ops.aten._unsafe_view.default(clone_84, [8, 4096, 3072]);  clone_84 = None
    bmm_68 = torch.ops.aten.bmm.default(_unsafe_view_77, _unsafe_view_78);  _unsafe_view_77 = _unsafe_view_78 = None
    view_573 = torch.ops.aten.view.default(bmm_68, [8, 384, 8, 1, 1, 384, 8]);  bmm_68 = None
    permute_401 = torch.ops.aten.permute.default(view_573, [4, 1, 5, 0, 2, 6, 3]);  view_573 = None
    view_574 = torch.ops.aten.view.default(permute_401, [1, 384, 384, 8, 8, 8]);  permute_401 = None
    clone_85 = torch.ops.aten.clone.default(view_574, memory_format = torch.contiguous_format);  view_574 = None
    _unsafe_view_79 = torch.ops.aten._unsafe_view.default(clone_85, [1, 384, 384, 512]);  clone_85 = None
    add_49 = torch.ops.aten.add.Tensor(add_48, _unsafe_view_79);  add_48 = _unsafe_view_79 = None
    _to_copy_330 = torch.ops.aten._to_copy.default(add_49, dtype = torch.float32);  add_49 = None
    native_layer_norm_default_69 = torch.ops.aten.native_layer_norm.default(_to_copy_330, [512], msa_module_outer_product_mean_3_ln_out_weight, msa_module_outer_product_mean_3_ln_out_bias, 0.1);  _to_copy_330 = msa_module_outer_product_mean_3_ln_out_weight = msa_module_outer_product_mean_3_ln_out_bias = None
    getitem_458 = native_layer_norm_default_69[0];  native_layer_norm_default_69 = None
    _to_copy_331 = torch.ops.aten._to_copy.default(msa_module_outer_product_mean_3_linear_out_bias, dtype = torch.bfloat16);  msa_module_outer_product_mean_3_linear_out_bias = None
    _to_copy_332 = torch.ops.aten._to_copy.default(msa_module_outer_product_mean_3_linear_out_weight, dtype = torch.bfloat16);  msa_module_outer_product_mean_3_linear_out_weight = None
    _to_copy_333 = torch.ops.aten._to_copy.default(getitem_458, dtype = torch.bfloat16);  getitem_458 = None
    view_575 = torch.ops.aten.view.default(_to_copy_333, [147456, 512]);  _to_copy_333 = None
    t_104 = torch.ops.aten.t.default(_to_copy_332);  _to_copy_332 = None
    addmm_3 = torch.ops.aten.addmm.default(_to_copy_331, view_575, t_104);  _to_copy_331 = view_575 = t_104 = None
    view_576 = torch.ops.aten.view.default(addmm_3, [1, 384, 384, 256]);  addmm_3 = None
    add_50 = torch.ops.aten.add.Tensor(add_45, view_576);  add_45 = view_576 = None
    _to_copy_334 = torch.ops.aten._to_copy.default(add_50, dtype = torch.float32)
    native_layer_norm_default_70 = torch.ops.aten.native_layer_norm.default(_to_copy_334, [256], msa_module_triangular_multiplication_3_layernorm_z_in_weight, msa_module_triangular_multiplication_3_layernorm_z_in_bias, 1e-05);  _to_copy_334 = msa_module_triangular_multiplication_3_layernorm_z_in_weight = msa_module_triangular_multiplication_3_layernorm_z_in_bias = None
    getitem_461 = native_layer_norm_default_70[0];  native_layer_norm_default_70 = None
    split_with_sizes_default_10 = torch.ops.aten.split_with_sizes.default(msa_module_triangular_multiplication_3_merged_linear_p_weight, [512, 512]);  msa_module_triangular_multiplication_3_merged_linear_p_weight = None
    getitem_464 = split_with_sizes_default_10[0]
    getitem_465 = split_with_sizes_default_10[1];  split_with_sizes_default_10 = None
    split_with_sizes_default_11 = torch.ops.aten.split_with_sizes.default(msa_module_triangular_multiplication_3_merged_linear_g_weight, [512, 512, 256]);  msa_module_triangular_multiplication_3_merged_linear_g_weight = None
    getitem_466 = split_with_sizes_default_11[0]
    getitem_467 = split_with_sizes_default_11[1]
    getitem_468 = split_with_sizes_default_11[2];  split_with_sizes_default_11 = None
    _to_copy_335 = torch.ops.aten._to_copy.default(getitem_464, dtype = torch.bfloat16);  getitem_464 = None
    _to_copy_336 = torch.ops.aten._to_copy.default(getitem_461, dtype = torch.bfloat16)
    t_105 = torch.ops.aten.t.default(_to_copy_335);  _to_copy_335 = None
    view_577 = torch.ops.aten.view.default(_to_copy_336, [147456, 256]);  _to_copy_336 = None
    mm_96 = torch.ops.aten.mm.default(view_577, t_105);  view_577 = t_105 = None
    view_578 = torch.ops.aten.view.default(mm_96, [1, 384, 384, 512]);  mm_96 = None
    _to_copy_337 = torch.ops.aten._to_copy.default(getitem_466, dtype = torch.bfloat16);  getitem_466 = None
    _to_copy_338 = torch.ops.aten._to_copy.default(getitem_461, dtype = torch.bfloat16)
    t_106 = torch.ops.aten.t.default(_to_copy_337);  _to_copy_337 = None
    view_579 = torch.ops.aten.view.default(_to_copy_338, [147456, 256]);  _to_copy_338 = None
    mm_97 = torch.ops.aten.mm.default(view_579, t_106);  view_579 = t_106 = None
    view_580 = torch.ops.aten.view.default(mm_97, [1, 384, 384, 512]);  mm_97 = None
    sigmoid_31 = torch.ops.aten.sigmoid.default(view_580);  view_580 = None
    mul_51 = torch.ops.aten.mul.Tensor(view_578, sigmoid_31);  view_578 = sigmoid_31 = None
    unsqueeze_301 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
    bitwise_not_43 = torch.ops.aten.bitwise_not.default(unsqueeze_301);  unsqueeze_301 = None
    masked_fill_43 = torch.ops.aten.masked_fill.Scalar(mul_51, bitwise_not_43, 0);  mul_51 = bitwise_not_43 = None
    split_tensor_42 = torch.ops.aten.split.Tensor(masked_fill_43, 256, dim = -1)
    getitem_471 = split_tensor_42[0];  split_tensor_42 = None
    unsqueeze_304 = torch.ops.aten.unsqueeze.default(getitem_471, 4);  getitem_471 = None
    permute_406 = torch.ops.aten.permute.default(unsqueeze_304, [0, 1, 4, 3, 2]);  unsqueeze_304 = None
    permute_407 = torch.ops.aten.permute.default(permute_406, [3, 1, 4, 0, 2]);  permute_406 = None
    view_583 = torch.ops.aten.view.default(permute_407, [256, 384, 384]);  permute_407 = None
    split_tensor_43 = torch.ops.aten.split.Tensor(masked_fill_43, 256, dim = -1);  masked_fill_43 = None
    getitem_474 = split_tensor_43[1];  split_tensor_43 = None
    unsqueeze_305 = torch.ops.aten.unsqueeze.default(getitem_474, 4);  getitem_474 = None
    permute_408 = torch.ops.aten.permute.default(unsqueeze_305, [0, 4, 1, 3, 2]);  unsqueeze_305 = None
    permute_409 = torch.ops.aten.permute.default(permute_408, [3, 4, 0, 2, 1]);  permute_408 = None
    view_584 = torch.ops.aten.view.default(permute_409, [256, 384, 384]);  permute_409 = None
    bmm_69 = torch.ops.aten.bmm.default(view_583, view_584);  view_583 = view_584 = None
    view_585 = torch.ops.aten.view.default(bmm_69, [256, 384, 1, 1, 384]);  bmm_69 = None
    permute_410 = torch.ops.aten.permute.default(view_585, [3, 1, 4, 0, 2]);  view_585 = None
    view_586 = torch.ops.aten.view.default(permute_410, [1, 384, 384, 256]);  permute_410 = None
    _to_copy_339 = torch.ops.aten._to_copy.default(getitem_465, dtype = torch.bfloat16);  getitem_465 = None
    _to_copy_340 = torch.ops.aten._to_copy.default(getitem_461, dtype = torch.bfloat16)
    t_107 = torch.ops.aten.t.default(_to_copy_339);  _to_copy_339 = None
    view_587 = torch.ops.aten.view.default(_to_copy_340, [147456, 256]);  _to_copy_340 = None
    mm_98 = torch.ops.aten.mm.default(view_587, t_107);  view_587 = t_107 = None
    view_588 = torch.ops.aten.view.default(mm_98, [1, 384, 384, 512]);  mm_98 = None
    _to_copy_341 = torch.ops.aten._to_copy.default(getitem_467, dtype = torch.bfloat16);  getitem_467 = None
    _to_copy_342 = torch.ops.aten._to_copy.default(getitem_461, dtype = torch.bfloat16)
    t_108 = torch.ops.aten.t.default(_to_copy_341);  _to_copy_341 = None
    view_589 = torch.ops.aten.view.default(_to_copy_342, [147456, 256]);  _to_copy_342 = None
    mm_99 = torch.ops.aten.mm.default(view_589, t_108);  view_589 = t_108 = None
    view_590 = torch.ops.aten.view.default(mm_99, [1, 384, 384, 512]);  mm_99 = None
    sigmoid_32 = torch.ops.aten.sigmoid.default(view_590);  view_590 = None
    mul_52 = torch.ops.aten.mul.Tensor(view_588, sigmoid_32);  view_588 = sigmoid_32 = None
    view_591 = torch.ops.aten.view.default(mul_52, [147456, 512]);  mul_52 = None
    view_592 = torch.ops.aten.view.default(view_591, [1, 384, 384, 512]);  view_591 = None
    transpose_10 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
    unsqueeze_306 = torch.ops.aten.unsqueeze.default(transpose_10, 3);  transpose_10 = None
    clone_86 = torch.ops.aten.clone.default(unsqueeze_306, memory_format = torch.contiguous_format);  unsqueeze_306 = None
    bitwise_not_44 = torch.ops.aten.bitwise_not.default(clone_86);  clone_86 = None
    masked_fill_44 = torch.ops.aten.masked_fill.Scalar(view_592, bitwise_not_44, 0);  view_592 = bitwise_not_44 = None
    view_593 = torch.ops.aten.view.default(masked_fill_44, [147456, 512]);  masked_fill_44 = None
    view_597 = torch.ops.aten.view.default(view_593, [1, 384, 384, 512])
    split_tensor_44 = torch.ops.aten.split.Tensor(view_597, 256, dim = -1);  view_597 = None
    getitem_477 = split_tensor_44[0];  split_tensor_44 = None
    unsqueeze_309 = torch.ops.aten.unsqueeze.default(getitem_477, 4);  getitem_477 = None
    permute_415 = torch.ops.aten.permute.default(unsqueeze_309, [0, 2, 4, 3, 1]);  unsqueeze_309 = None
    permute_416 = torch.ops.aten.permute.default(permute_415, [3, 1, 4, 0, 2]);  permute_415 = None
    view_598 = torch.ops.aten.view.default(permute_416, [256, 384, 384]);  permute_416 = None
    view_599 = torch.ops.aten.view.default(view_593, [1, 384, 384, 512]);  view_593 = None
    split_tensor_45 = torch.ops.aten.split.Tensor(view_599, 256, dim = -1);  view_599 = None
    getitem_480 = split_tensor_45[1];  split_tensor_45 = None
    unsqueeze_310 = torch.ops.aten.unsqueeze.default(getitem_480, 4);  getitem_480 = None
    permute_417 = torch.ops.aten.permute.default(unsqueeze_310, [0, 4, 2, 3, 1]);  unsqueeze_310 = None
    permute_418 = torch.ops.aten.permute.default(permute_417, [3, 4, 0, 2, 1]);  permute_417 = None
    view_600 = torch.ops.aten.view.default(permute_418, [256, 384, 384]);  permute_418 = None
    bmm_70 = torch.ops.aten.bmm.default(view_598, view_600);  view_598 = view_600 = None
    view_601 = torch.ops.aten.view.default(bmm_70, [256, 384, 1, 1, 384]);  bmm_70 = None
    permute_419 = torch.ops.aten.permute.default(view_601, [3, 1, 4, 0, 2]);  view_601 = None
    view_602 = torch.ops.aten.view.default(permute_419, [1, 384, 384, 256]);  permute_419 = None
    _to_copy_343 = torch.ops.aten._to_copy.default(view_586, dtype = torch.float32);  view_586 = None
    native_layer_norm_default_71 = torch.ops.aten.native_layer_norm.default(_to_copy_343, [256], None, None, 1e-05);  _to_copy_343 = None
    getitem_481 = native_layer_norm_default_71[0];  native_layer_norm_default_71 = None
    _to_copy_344 = torch.ops.aten._to_copy.default(view_602, dtype = torch.float32);  view_602 = None
    native_layer_norm_default_72 = torch.ops.aten.native_layer_norm.default(_to_copy_344, [256], None, None, 1e-05);  _to_copy_344 = None
    getitem_484 = native_layer_norm_default_72[0];  native_layer_norm_default_72 = None
    add_51 = torch.ops.aten.add.Tensor(getitem_481, getitem_484);  getitem_481 = getitem_484 = None
    _to_copy_345 = torch.ops.aten._to_copy.default(msa_module_triangular_multiplication_3_linear_z_out_weight, dtype = torch.bfloat16);  msa_module_triangular_multiplication_3_linear_z_out_weight = None
    _to_copy_346 = torch.ops.aten._to_copy.default(add_51, dtype = torch.bfloat16);  add_51 = None
    t_109 = torch.ops.aten.t.default(_to_copy_345);  _to_copy_345 = None
    view_603 = torch.ops.aten.view.default(_to_copy_346, [147456, 256]);  _to_copy_346 = None
    mm_100 = torch.ops.aten.mm.default(view_603, t_109);  view_603 = t_109 = None
    view_604 = torch.ops.aten.view.default(mm_100, [1, 384, 384, 256]);  mm_100 = None
    _to_copy_347 = torch.ops.aten._to_copy.default(getitem_468, dtype = torch.bfloat16);  getitem_468 = None
    _to_copy_348 = torch.ops.aten._to_copy.default(getitem_461, dtype = torch.bfloat16);  getitem_461 = None
    t_110 = torch.ops.aten.t.default(_to_copy_347);  _to_copy_347 = None
    view_605 = torch.ops.aten.view.default(_to_copy_348, [147456, 256]);  _to_copy_348 = None
    mm_101 = torch.ops.aten.mm.default(view_605, t_110);  view_605 = t_110 = None
    view_606 = torch.ops.aten.view.default(mm_101, [1, 384, 384, 256]);  mm_101 = None
    sigmoid_33 = torch.ops.aten.sigmoid.default(view_606);  view_606 = None
    mul_53 = torch.ops.aten.mul.Tensor(view_604, sigmoid_33);  view_604 = sigmoid_33 = None
    add_52 = torch.ops.aten.add.Tensor(add_50, mul_53);  mul_53 = None
    split_tensor_46 = torch.ops.aten.split.Tensor(add_50, 384, dim = -2);  add_50 = None
    getitem_487 = split_tensor_46[0];  split_tensor_46 = None
    _to_copy_349 = torch.ops.aten._to_copy.default(getitem_487, dtype = torch.float32);  getitem_487 = None
    native_layer_norm_default_73 = torch.ops.aten.native_layer_norm.default(_to_copy_349, [256], msa_module_pair_transition_3_layer_norm_weight, msa_module_pair_transition_3_layer_norm_bias, 1e-05);  _to_copy_349 = msa_module_pair_transition_3_layer_norm_weight = msa_module_pair_transition_3_layer_norm_bias = None
    getitem_488 = native_layer_norm_default_73[0];  native_layer_norm_default_73 = None
    _to_copy_350 = torch.ops.aten._to_copy.default(msa_module_pair_transition_3_linear_no_bias_ab_weight, dtype = torch.bfloat16);  msa_module_pair_transition_3_linear_no_bias_ab_weight = None
    _to_copy_351 = torch.ops.aten._to_copy.default(getitem_488, dtype = torch.bfloat16);  getitem_488 = None
    t_111 = torch.ops.aten.t.default(_to_copy_350);  _to_copy_350 = None
    view_607 = torch.ops.aten.view.default(_to_copy_351, [147456, 256]);  _to_copy_351 = None
    mm_102 = torch.ops.aten.mm.default(view_607, t_111);  view_607 = t_111 = None
    view_608 = torch.ops.aten.view.default(mm_102, [1, 384, 384, 2048]);  mm_102 = None
    split_tensor_47 = torch.ops.aten.split.Tensor(view_608, 1024, dim = -1);  view_608 = None
    getitem_491 = split_tensor_47[0]
    getitem_492 = split_tensor_47[1];  split_tensor_47 = None
    silu_14 = torch.ops.aten.silu.default(getitem_491);  getitem_491 = None
    mul_54 = torch.ops.aten.mul.Tensor(silu_14, getitem_492);  silu_14 = getitem_492 = None
    _to_copy_352 = torch.ops.aten._to_copy.default(msa_module_pair_transition_3_linear_out_weight, dtype = torch.bfloat16);  msa_module_pair_transition_3_linear_out_weight = None
    t_112 = torch.ops.aten.t.default(_to_copy_352);  _to_copy_352 = None
    view_610 = torch.ops.aten.view.default(mul_54, [147456, 1024]);  mul_54 = None
    mm_103 = torch.ops.aten.mm.default(view_610, t_112);  view_610 = t_112 = None
    view_611 = torch.ops.aten.view.default(mm_103, [1, 384, 384, 256]);  mm_103 = None
    add_53 = torch.ops.aten.add.Tensor(add_52, view_611);  add_52 = view_611 = None
    _to_copy_353 = torch.ops.aten._to_copy.default(add_53, dtype = torch.float32)
    native_layer_norm_default_74 = torch.ops.aten.native_layer_norm.default(_to_copy_353, [256], None, None, 1e-05);  _to_copy_353 = None
    getitem_493 = native_layer_norm_default_74[0];  native_layer_norm_default_74 = None
    _to_copy_354 = torch.ops.aten._to_copy.default(msa_module_triangular_attention_3_pair2b_weight, dtype = torch.bfloat16);  msa_module_triangular_attention_3_pair2b_weight = None
    _to_copy_355 = torch.ops.aten._to_copy.default(getitem_493, dtype = torch.bfloat16)
    t_113 = torch.ops.aten.t.default(_to_copy_354);  _to_copy_354 = None
    view_612 = torch.ops.aten.view.default(_to_copy_355, [147456, 256]);  _to_copy_355 = None
    mm_104 = torch.ops.aten.mm.default(view_612, t_113);  view_612 = t_113 = None
    view_613 = torch.ops.aten.view.default(mm_104, [1, 384, 384, 8]);  mm_104 = None
    view_614 = torch.ops.aten.view.default(view_613, [1, 384, 384, 2, 4]);  view_613 = None
    permute_420 = torch.ops.aten.permute.default(view_614, [0, 3, 4, 1, 2]);  view_614 = None
    view_615 = torch.ops.aten.view.default(permute_420, [1, 2, 4, 1, 384, 384]);  permute_420 = None
    view_616 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 384, 384])
    bitwise_not_45 = torch.ops.aten.bitwise_not.default(view_616);  view_616 = None
    masked_fill_45 = torch.ops.aten.masked_fill.Scalar(view_615, bitwise_not_45, -10000);  view_615 = bitwise_not_45 = None
    view_617 = torch.ops.aten.view.default(masked_fill_45, [1, 2, 4, 384, 384]);  masked_fill_45 = None
    permute_421 = torch.ops.aten.permute.default(view_617, [1, 0, 2, 3, 4]);  view_617 = None
    view_618 = torch.ops.aten.view.default(permute_421, [2, 4, 1, 384, 384]);  permute_421 = None
    _to_copy_356 = torch.ops.aten._to_copy.default(msa_module_triangular_attention_3_pair2qkvg1_weight, dtype = torch.bfloat16);  msa_module_triangular_attention_3_pair2qkvg1_weight = None
    _to_copy_357 = torch.ops.aten._to_copy.default(getitem_493, dtype = torch.bfloat16)
    t_114 = torch.ops.aten.t.default(_to_copy_356);  _to_copy_356 = None
    view_619 = torch.ops.aten.view.default(_to_copy_357, [147456, 256]);  _to_copy_357 = None
    mm_105 = torch.ops.aten.mm.default(view_619, t_114);  view_619 = t_114 = None
    view_620 = torch.ops.aten.view.default(mm_105, [1, 384, 384, 1024]);  mm_105 = None
    select_11 = torch.ops.aten.select.int(view_618, 0, 0)
    view_621 = torch.ops.aten.view.default(view_620, [1, 384, 384, 4, 4, 64]);  view_620 = None
    permute_422 = torch.ops.aten.permute.default(view_621, [4, 0, 3, 1, 2, 5]);  view_621 = None
    view_622 = torch.ops.aten.view.default(permute_422, [4, 4, 384, 384, 64]);  permute_422 = None
    unbind_int_32 = torch.ops.aten.unbind.int(view_622);  view_622 = None
    getitem_496 = unbind_int_32[0]
    getitem_497 = unbind_int_32[1]
    getitem_498 = unbind_int_32[2]
    getitem_499 = unbind_int_32[3];  unbind_int_32 = None
    expand_23 = torch.ops.aten.expand.default(select_11, [4, 384, 384, 384]);  select_11 = None
    _scaled_dot_product_efficient_attention_default_10 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_496, getitem_497, getitem_498, expand_23, False);  getitem_496 = getitem_497 = getitem_498 = expand_23 = None
    getitem_500 = _scaled_dot_product_efficient_attention_default_10[0];  _scaled_dot_product_efficient_attention_default_10 = None
    sigmoid_34 = torch.ops.aten.sigmoid.default(getitem_499);  getitem_499 = None
    mul_55 = torch.ops.aten.mul.Tensor(getitem_500, sigmoid_34);  getitem_500 = sigmoid_34 = None
    view_623 = torch.ops.aten.view.default(mul_55, [1, 4, 384, 384, 64]);  mul_55 = None
    permute_423 = torch.ops.aten.permute.default(view_623, [0, 2, 3, 1, 4]);  view_623 = None
    clone_87 = torch.ops.aten.clone.default(permute_423, memory_format = torch.contiguous_format);  permute_423 = None
    _unsafe_view_80 = torch.ops.aten._unsafe_view.default(clone_87, [1, 384, 384, 256]);  clone_87 = None
    transpose_11 = torch.ops.aten.transpose.int(getitem_493, 1, 2);  getitem_493 = None
    _to_copy_358 = torch.ops.aten._to_copy.default(msa_module_triangular_attention_3_pair2qkvg2_weight, dtype = torch.bfloat16);  msa_module_triangular_attention_3_pair2qkvg2_weight = None
    _to_copy_359 = torch.ops.aten._to_copy.default(transpose_11, dtype = torch.bfloat16);  transpose_11 = None
    t_115 = torch.ops.aten.t.default(_to_copy_358);  _to_copy_358 = None
    expand_24 = torch.ops.aten.expand.default(_to_copy_359, [1, 384, 384, 256]);  _to_copy_359 = None
    view_624 = torch.ops.aten.view.default(expand_24, [384, 384, 256]);  expand_24 = None
    expand_25 = torch.ops.aten.expand.default(t_115, [1, 384, 256, 1024]);  t_115 = None
    view_625 = torch.ops.aten.view.default(expand_25, [384, 256, 1024]);  expand_25 = None
    bmm_71 = torch.ops.aten.bmm.default(view_624, view_625);  view_624 = view_625 = None
    view_626 = torch.ops.aten.view.default(bmm_71, [1, 384, 384, 1024]);  bmm_71 = None
    select_12 = torch.ops.aten.select.int(view_618, 0, 1);  view_618 = None
    view_627 = torch.ops.aten.view.default(view_626, [1, 384, 384, 4, 4, 64]);  view_626 = None
    permute_424 = torch.ops.aten.permute.default(view_627, [4, 0, 3, 1, 2, 5]);  view_627 = None
    view_628 = torch.ops.aten.view.default(permute_424, [4, 4, 384, 384, 64]);  permute_424 = None
    unbind_int_33 = torch.ops.aten.unbind.int(view_628);  view_628 = None
    getitem_504 = unbind_int_33[0]
    getitem_505 = unbind_int_33[1]
    getitem_506 = unbind_int_33[2]
    getitem_507 = unbind_int_33[3];  unbind_int_33 = None
    expand_26 = torch.ops.aten.expand.default(select_12, [4, 384, 384, 384]);  select_12 = None
    _scaled_dot_product_efficient_attention_default_11 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_504, getitem_505, getitem_506, expand_26, False);  getitem_504 = getitem_505 = getitem_506 = expand_26 = None
    getitem_508 = _scaled_dot_product_efficient_attention_default_11[0];  _scaled_dot_product_efficient_attention_default_11 = None
    sigmoid_35 = torch.ops.aten.sigmoid.default(getitem_507);  getitem_507 = None
    mul_56 = torch.ops.aten.mul.Tensor(getitem_508, sigmoid_35);  getitem_508 = sigmoid_35 = None
    view_629 = torch.ops.aten.view.default(mul_56, [1, 4, 384, 384, 64]);  mul_56 = None
    permute_425 = torch.ops.aten.permute.default(view_629, [0, 2, 3, 1, 4]);  view_629 = None
    clone_88 = torch.ops.aten.clone.default(permute_425, memory_format = torch.contiguous_format);  permute_425 = None
    _unsafe_view_81 = torch.ops.aten._unsafe_view.default(clone_88, [1, 384, 384, 256]);  clone_88 = None
    cat_11 = torch.ops.aten.cat.default([_unsafe_view_80, _unsafe_view_81], dim = -1);  _unsafe_view_80 = _unsafe_view_81 = None
    slice_162 = torch.ops.aten.slice.Tensor(msa_module_triangular_attention_3_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  msa_module_triangular_attention_3_out_scalers = None
    unsqueeze_311 = torch.ops.aten.unsqueeze.default(slice_162, 1);  slice_162 = None
    mul_57 = torch.ops.aten.mul.Tensor(msa_module_triangular_attention_3_linear_out_weight, unsqueeze_311);  msa_module_triangular_attention_3_linear_out_weight = unsqueeze_311 = None
    _to_copy_360 = torch.ops.aten._to_copy.default(mul_57, dtype = torch.bfloat16);  mul_57 = None
    t_116 = torch.ops.aten.t.default(_to_copy_360);  _to_copy_360 = None
    view_630 = torch.ops.aten.view.default(cat_11, [147456, 512]);  cat_11 = None
    mm_106 = torch.ops.aten.mm.default(view_630, t_116);  view_630 = t_116 = None
    view_631 = torch.ops.aten.view.default(mm_106, [1, 384, 384, 256]);  mm_106 = None
    add_54 = torch.ops.aten.add.Tensor(add_53, view_631);  add_53 = view_631 = None
    add_55 = torch.ops.aten.add.Tensor(add_11, add_54);  add_11 = add_54 = None
    _to_copy_361 = torch.ops.aten._to_copy.default(add_55, dtype = torch.float32)
    native_layer_norm_default_75 = torch.ops.aten.native_layer_norm.default(_to_copy_361, [256], pairformer_stack_blocks_0_triangle_multiplication_layernorm_z_in_weight, pairformer_stack_blocks_0_triangle_multiplication_layernorm_z_in_bias, 1e-05);  _to_copy_361 = pairformer_stack_blocks_0_triangle_multiplication_layernorm_z_in_weight = pairformer_stack_blocks_0_triangle_multiplication_layernorm_z_in_bias = None
    getitem_512 = native_layer_norm_default_75[0];  native_layer_norm_default_75 = None
    split_with_sizes_default_12 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_0_triangle_multiplication_merged_linear_p_weight, [512, 512]);  pairformer_stack_blocks_0_triangle_multiplication_merged_linear_p_weight = None
    getitem_515 = split_with_sizes_default_12[0]
    getitem_516 = split_with_sizes_default_12[1];  split_with_sizes_default_12 = None
    split_with_sizes_default_13 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_0_triangle_multiplication_merged_linear_g_weight, [512, 512, 256]);  pairformer_stack_blocks_0_triangle_multiplication_merged_linear_g_weight = None
    getitem_517 = split_with_sizes_default_13[0]
    getitem_518 = split_with_sizes_default_13[1]
    getitem_519 = split_with_sizes_default_13[2];  split_with_sizes_default_13 = None
    _to_copy_362 = torch.ops.aten._to_copy.default(getitem_515, dtype = torch.bfloat16);  getitem_515 = None
    _to_copy_363 = torch.ops.aten._to_copy.default(getitem_512, dtype = torch.bfloat16)
    t_117 = torch.ops.aten.t.default(_to_copy_362);  _to_copy_362 = None
    view_632 = torch.ops.aten.view.default(_to_copy_363, [147456, 256]);  _to_copy_363 = None
    mm_107 = torch.ops.aten.mm.default(view_632, t_117);  view_632 = t_117 = None
    view_633 = torch.ops.aten.view.default(mm_107, [1, 384, 384, 512]);  mm_107 = None
    _to_copy_364 = torch.ops.aten._to_copy.default(getitem_517, dtype = torch.bfloat16);  getitem_517 = None
    _to_copy_365 = torch.ops.aten._to_copy.default(getitem_512, dtype = torch.bfloat16)
    t_118 = torch.ops.aten.t.default(_to_copy_364);  _to_copy_364 = None
    view_634 = torch.ops.aten.view.default(_to_copy_365, [147456, 256]);  _to_copy_365 = None
    mm_108 = torch.ops.aten.mm.default(view_634, t_118);  view_634 = t_118 = None
    view_635 = torch.ops.aten.view.default(mm_108, [1, 384, 384, 512]);  mm_108 = None
    sigmoid_36 = torch.ops.aten.sigmoid.default(view_635);  view_635 = None
    mul_58 = torch.ops.aten.mul.Tensor(view_633, sigmoid_36);  view_633 = sigmoid_36 = None
    unsqueeze_312 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
    bitwise_not_46 = torch.ops.aten.bitwise_not.default(unsqueeze_312);  unsqueeze_312 = None
    masked_fill_46 = torch.ops.aten.masked_fill.Scalar(mul_58, bitwise_not_46, 0);  mul_58 = bitwise_not_46 = None
    split_tensor_48 = torch.ops.aten.split.Tensor(masked_fill_46, 256, dim = -1)
    getitem_522 = split_tensor_48[0];  split_tensor_48 = None
    unsqueeze_315 = torch.ops.aten.unsqueeze.default(getitem_522, 4);  getitem_522 = None
    permute_430 = torch.ops.aten.permute.default(unsqueeze_315, [0, 1, 4, 3, 2]);  unsqueeze_315 = None
    permute_431 = torch.ops.aten.permute.default(permute_430, [3, 1, 4, 0, 2]);  permute_430 = None
    view_638 = torch.ops.aten.view.default(permute_431, [256, 384, 384]);  permute_431 = None
    split_tensor_49 = torch.ops.aten.split.Tensor(masked_fill_46, 256, dim = -1);  masked_fill_46 = None
    getitem_525 = split_tensor_49[1];  split_tensor_49 = None
    unsqueeze_316 = torch.ops.aten.unsqueeze.default(getitem_525, 4);  getitem_525 = None
    permute_432 = torch.ops.aten.permute.default(unsqueeze_316, [0, 4, 1, 3, 2]);  unsqueeze_316 = None
    permute_433 = torch.ops.aten.permute.default(permute_432, [3, 4, 0, 2, 1]);  permute_432 = None
    view_639 = torch.ops.aten.view.default(permute_433, [256, 384, 384]);  permute_433 = None
    bmm_72 = torch.ops.aten.bmm.default(view_638, view_639);  view_638 = view_639 = None
    view_640 = torch.ops.aten.view.default(bmm_72, [256, 384, 1, 1, 384]);  bmm_72 = None
    permute_434 = torch.ops.aten.permute.default(view_640, [3, 1, 4, 0, 2]);  view_640 = None
    view_641 = torch.ops.aten.view.default(permute_434, [1, 384, 384, 256]);  permute_434 = None
    _to_copy_366 = torch.ops.aten._to_copy.default(getitem_516, dtype = torch.bfloat16);  getitem_516 = None
    _to_copy_367 = torch.ops.aten._to_copy.default(getitem_512, dtype = torch.bfloat16)
    t_119 = torch.ops.aten.t.default(_to_copy_366);  _to_copy_366 = None
    view_642 = torch.ops.aten.view.default(_to_copy_367, [147456, 256]);  _to_copy_367 = None
    mm_109 = torch.ops.aten.mm.default(view_642, t_119);  view_642 = t_119 = None
    view_643 = torch.ops.aten.view.default(mm_109, [1, 384, 384, 512]);  mm_109 = None
    _to_copy_368 = torch.ops.aten._to_copy.default(getitem_518, dtype = torch.bfloat16);  getitem_518 = None
    _to_copy_369 = torch.ops.aten._to_copy.default(getitem_512, dtype = torch.bfloat16)
    t_120 = torch.ops.aten.t.default(_to_copy_368);  _to_copy_368 = None
    view_644 = torch.ops.aten.view.default(_to_copy_369, [147456, 256]);  _to_copy_369 = None
    mm_110 = torch.ops.aten.mm.default(view_644, t_120);  view_644 = t_120 = None
    view_645 = torch.ops.aten.view.default(mm_110, [1, 384, 384, 512]);  mm_110 = None
    sigmoid_37 = torch.ops.aten.sigmoid.default(view_645);  view_645 = None
    mul_59 = torch.ops.aten.mul.Tensor(view_643, sigmoid_37);  view_643 = sigmoid_37 = None
    view_646 = torch.ops.aten.view.default(mul_59, [147456, 512]);  mul_59 = None
    view_647 = torch.ops.aten.view.default(view_646, [1, 384, 384, 512]);  view_646 = None
    transpose_12 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
    unsqueeze_317 = torch.ops.aten.unsqueeze.default(transpose_12, 3);  transpose_12 = None
    clone_89 = torch.ops.aten.clone.default(unsqueeze_317, memory_format = torch.contiguous_format);  unsqueeze_317 = None
    bitwise_not_47 = torch.ops.aten.bitwise_not.default(clone_89);  clone_89 = None
    masked_fill_47 = torch.ops.aten.masked_fill.Scalar(view_647, bitwise_not_47, 0);  view_647 = bitwise_not_47 = None
    view_648 = torch.ops.aten.view.default(masked_fill_47, [147456, 512]);  masked_fill_47 = None
    view_652 = torch.ops.aten.view.default(view_648, [1, 384, 384, 512])
    split_tensor_50 = torch.ops.aten.split.Tensor(view_652, 256, dim = -1);  view_652 = None
    getitem_528 = split_tensor_50[0];  split_tensor_50 = None
    unsqueeze_320 = torch.ops.aten.unsqueeze.default(getitem_528, 4);  getitem_528 = None
    permute_439 = torch.ops.aten.permute.default(unsqueeze_320, [0, 2, 4, 3, 1]);  unsqueeze_320 = None
    permute_440 = torch.ops.aten.permute.default(permute_439, [3, 1, 4, 0, 2]);  permute_439 = None
    view_653 = torch.ops.aten.view.default(permute_440, [256, 384, 384]);  permute_440 = None
    view_654 = torch.ops.aten.view.default(view_648, [1, 384, 384, 512]);  view_648 = None
    split_tensor_51 = torch.ops.aten.split.Tensor(view_654, 256, dim = -1);  view_654 = None
    getitem_531 = split_tensor_51[1];  split_tensor_51 = None
    unsqueeze_321 = torch.ops.aten.unsqueeze.default(getitem_531, 4);  getitem_531 = None
    permute_441 = torch.ops.aten.permute.default(unsqueeze_321, [0, 4, 2, 3, 1]);  unsqueeze_321 = None
    permute_442 = torch.ops.aten.permute.default(permute_441, [3, 4, 0, 2, 1]);  permute_441 = None
    view_655 = torch.ops.aten.view.default(permute_442, [256, 384, 384]);  permute_442 = None
    bmm_73 = torch.ops.aten.bmm.default(view_653, view_655);  view_653 = view_655 = None
    view_656 = torch.ops.aten.view.default(bmm_73, [256, 384, 1, 1, 384]);  bmm_73 = None
    permute_443 = torch.ops.aten.permute.default(view_656, [3, 1, 4, 0, 2]);  view_656 = None
    view_657 = torch.ops.aten.view.default(permute_443, [1, 384, 384, 256]);  permute_443 = None
    _to_copy_370 = torch.ops.aten._to_copy.default(view_641, dtype = torch.float32);  view_641 = None
    native_layer_norm_default_76 = torch.ops.aten.native_layer_norm.default(_to_copy_370, [256], None, None, 1e-05);  _to_copy_370 = None
    getitem_532 = native_layer_norm_default_76[0];  native_layer_norm_default_76 = None
    _to_copy_371 = torch.ops.aten._to_copy.default(view_657, dtype = torch.float32);  view_657 = None
    native_layer_norm_default_77 = torch.ops.aten.native_layer_norm.default(_to_copy_371, [256], None, None, 1e-05);  _to_copy_371 = None
    getitem_535 = native_layer_norm_default_77[0];  native_layer_norm_default_77 = None
    add_56 = torch.ops.aten.add.Tensor(getitem_532, getitem_535);  getitem_532 = getitem_535 = None
    _to_copy_372 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_0_triangle_multiplication_linear_z_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_0_triangle_multiplication_linear_z_out_weight = None
    _to_copy_373 = torch.ops.aten._to_copy.default(add_56, dtype = torch.bfloat16);  add_56 = None
    t_121 = torch.ops.aten.t.default(_to_copy_372);  _to_copy_372 = None
    view_658 = torch.ops.aten.view.default(_to_copy_373, [147456, 256]);  _to_copy_373 = None
    mm_111 = torch.ops.aten.mm.default(view_658, t_121);  view_658 = t_121 = None
    view_659 = torch.ops.aten.view.default(mm_111, [1, 384, 384, 256]);  mm_111 = None
    _to_copy_374 = torch.ops.aten._to_copy.default(getitem_519, dtype = torch.bfloat16);  getitem_519 = None
    _to_copy_375 = torch.ops.aten._to_copy.default(getitem_512, dtype = torch.bfloat16);  getitem_512 = None
    t_122 = torch.ops.aten.t.default(_to_copy_374);  _to_copy_374 = None
    view_660 = torch.ops.aten.view.default(_to_copy_375, [147456, 256]);  _to_copy_375 = None
    mm_112 = torch.ops.aten.mm.default(view_660, t_122);  view_660 = t_122 = None
    view_661 = torch.ops.aten.view.default(mm_112, [1, 384, 384, 256]);  mm_112 = None
    sigmoid_38 = torch.ops.aten.sigmoid.default(view_661);  view_661 = None
    mul_60 = torch.ops.aten.mul.Tensor(view_659, sigmoid_38);  view_659 = sigmoid_38 = None
    add_57 = torch.ops.aten.add.Tensor(add_55, mul_60);  mul_60 = None
    _to_copy_376 = torch.ops.aten._to_copy.default(add_55, dtype = torch.float32)
    native_layer_norm_default_78 = torch.ops.aten.native_layer_norm.default(_to_copy_376, [256], None, None, 1e-05);  _to_copy_376 = None
    getitem_538 = native_layer_norm_default_78[0];  native_layer_norm_default_78 = None
    _to_copy_377 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_0_triangle_attention_pair2b_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_0_triangle_attention_pair2b_weight = None
    _to_copy_378 = torch.ops.aten._to_copy.default(getitem_538, dtype = torch.bfloat16)
    t_123 = torch.ops.aten.t.default(_to_copy_377);  _to_copy_377 = None
    view_662 = torch.ops.aten.view.default(_to_copy_378, [147456, 256]);  _to_copy_378 = None
    mm_113 = torch.ops.aten.mm.default(view_662, t_123);  view_662 = t_123 = None
    view_663 = torch.ops.aten.view.default(mm_113, [1, 384, 384, 8]);  mm_113 = None
    view_664 = torch.ops.aten.view.default(view_663, [1, 384, 384, 2, 4]);  view_663 = None
    permute_444 = torch.ops.aten.permute.default(view_664, [0, 3, 4, 1, 2]);  view_664 = None
    view_665 = torch.ops.aten.view.default(permute_444, [1, 2, 4, 1, 384, 384]);  permute_444 = None
    view_666 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 384, 384])
    bitwise_not_48 = torch.ops.aten.bitwise_not.default(view_666);  view_666 = None
    masked_fill_48 = torch.ops.aten.masked_fill.Scalar(view_665, bitwise_not_48, -10000);  view_665 = bitwise_not_48 = None
    view_667 = torch.ops.aten.view.default(masked_fill_48, [1, 2, 4, 384, 384]);  masked_fill_48 = None
    permute_445 = torch.ops.aten.permute.default(view_667, [1, 0, 2, 3, 4]);  view_667 = None
    view_668 = torch.ops.aten.view.default(permute_445, [2, 4, 1, 384, 384]);  permute_445 = None
    _to_copy_379 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_0_triangle_attention_pair2qkvg1_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_0_triangle_attention_pair2qkvg1_weight = None
    _to_copy_380 = torch.ops.aten._to_copy.default(getitem_538, dtype = torch.bfloat16)
    t_124 = torch.ops.aten.t.default(_to_copy_379);  _to_copy_379 = None
    view_669 = torch.ops.aten.view.default(_to_copy_380, [147456, 256]);  _to_copy_380 = None
    mm_114 = torch.ops.aten.mm.default(view_669, t_124);  view_669 = t_124 = None
    view_670 = torch.ops.aten.view.default(mm_114, [1, 384, 384, 1024]);  mm_114 = None
    select_13 = torch.ops.aten.select.int(view_668, 0, 0)
    view_671 = torch.ops.aten.view.default(view_670, [1, 384, 384, 4, 4, 64]);  view_670 = None
    permute_446 = torch.ops.aten.permute.default(view_671, [4, 0, 3, 1, 2, 5]);  view_671 = None
    view_672 = torch.ops.aten.view.default(permute_446, [4, 4, 384, 384, 64]);  permute_446 = None
    unbind_int_34 = torch.ops.aten.unbind.int(view_672);  view_672 = None
    getitem_541 = unbind_int_34[0]
    getitem_542 = unbind_int_34[1]
    getitem_543 = unbind_int_34[2]
    getitem_544 = unbind_int_34[3];  unbind_int_34 = None
    expand_27 = torch.ops.aten.expand.default(select_13, [4, 384, 384, 384]);  select_13 = None
    _scaled_dot_product_efficient_attention_default_12 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_541, getitem_542, getitem_543, expand_27, False);  getitem_541 = getitem_542 = getitem_543 = expand_27 = None
    getitem_545 = _scaled_dot_product_efficient_attention_default_12[0];  _scaled_dot_product_efficient_attention_default_12 = None
    sigmoid_39 = torch.ops.aten.sigmoid.default(getitem_544);  getitem_544 = None
    mul_61 = torch.ops.aten.mul.Tensor(getitem_545, sigmoid_39);  getitem_545 = sigmoid_39 = None
    view_673 = torch.ops.aten.view.default(mul_61, [1, 4, 384, 384, 64]);  mul_61 = None
    permute_447 = torch.ops.aten.permute.default(view_673, [0, 2, 3, 1, 4]);  view_673 = None
    clone_90 = torch.ops.aten.clone.default(permute_447, memory_format = torch.contiguous_format);  permute_447 = None
    _unsafe_view_82 = torch.ops.aten._unsafe_view.default(clone_90, [1, 384, 384, 256]);  clone_90 = None
    transpose_13 = torch.ops.aten.transpose.int(getitem_538, 1, 2);  getitem_538 = None
    _to_copy_381 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_0_triangle_attention_pair2qkvg2_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_0_triangle_attention_pair2qkvg2_weight = None
    _to_copy_382 = torch.ops.aten._to_copy.default(transpose_13, dtype = torch.bfloat16);  transpose_13 = None
    t_125 = torch.ops.aten.t.default(_to_copy_381);  _to_copy_381 = None
    expand_28 = torch.ops.aten.expand.default(_to_copy_382, [1, 384, 384, 256]);  _to_copy_382 = None
    view_674 = torch.ops.aten.view.default(expand_28, [384, 384, 256]);  expand_28 = None
    expand_29 = torch.ops.aten.expand.default(t_125, [1, 384, 256, 1024]);  t_125 = None
    view_675 = torch.ops.aten.view.default(expand_29, [384, 256, 1024]);  expand_29 = None
    bmm_74 = torch.ops.aten.bmm.default(view_674, view_675);  view_674 = view_675 = None
    view_676 = torch.ops.aten.view.default(bmm_74, [1, 384, 384, 1024]);  bmm_74 = None
    select_14 = torch.ops.aten.select.int(view_668, 0, 1);  view_668 = None
    view_677 = torch.ops.aten.view.default(view_676, [1, 384, 384, 4, 4, 64]);  view_676 = None
    permute_448 = torch.ops.aten.permute.default(view_677, [4, 0, 3, 1, 2, 5]);  view_677 = None
    view_678 = torch.ops.aten.view.default(permute_448, [4, 4, 384, 384, 64]);  permute_448 = None
    unbind_int_35 = torch.ops.aten.unbind.int(view_678);  view_678 = None
    getitem_549 = unbind_int_35[0]
    getitem_550 = unbind_int_35[1]
    getitem_551 = unbind_int_35[2]
    getitem_552 = unbind_int_35[3];  unbind_int_35 = None
    expand_30 = torch.ops.aten.expand.default(select_14, [4, 384, 384, 384]);  select_14 = None
    _scaled_dot_product_efficient_attention_default_13 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_549, getitem_550, getitem_551, expand_30, False);  getitem_549 = getitem_550 = getitem_551 = expand_30 = None
    getitem_553 = _scaled_dot_product_efficient_attention_default_13[0];  _scaled_dot_product_efficient_attention_default_13 = None
    sigmoid_40 = torch.ops.aten.sigmoid.default(getitem_552);  getitem_552 = None
    mul_62 = torch.ops.aten.mul.Tensor(getitem_553, sigmoid_40);  getitem_553 = sigmoid_40 = None
    view_679 = torch.ops.aten.view.default(mul_62, [1, 4, 384, 384, 64]);  mul_62 = None
    permute_449 = torch.ops.aten.permute.default(view_679, [0, 2, 3, 1, 4]);  view_679 = None
    clone_91 = torch.ops.aten.clone.default(permute_449, memory_format = torch.contiguous_format);  permute_449 = None
    _unsafe_view_83 = torch.ops.aten._unsafe_view.default(clone_91, [1, 384, 384, 256]);  clone_91 = None
    cat_12 = torch.ops.aten.cat.default([_unsafe_view_82, _unsafe_view_83], dim = -1);  _unsafe_view_82 = _unsafe_view_83 = None
    slice_163 = torch.ops.aten.slice.Tensor(pairformer_stack_blocks_0_triangle_attention_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  pairformer_stack_blocks_0_triangle_attention_out_scalers = None
    unsqueeze_322 = torch.ops.aten.unsqueeze.default(slice_163, 1);  slice_163 = None
    mul_63 = torch.ops.aten.mul.Tensor(pairformer_stack_blocks_0_triangle_attention_linear_out_weight, unsqueeze_322);  pairformer_stack_blocks_0_triangle_attention_linear_out_weight = unsqueeze_322 = None
    _to_copy_383 = torch.ops.aten._to_copy.default(mul_63, dtype = torch.bfloat16);  mul_63 = None
    t_126 = torch.ops.aten.t.default(_to_copy_383);  _to_copy_383 = None
    view_680 = torch.ops.aten.view.default(cat_12, [147456, 512]);  cat_12 = None
    mm_115 = torch.ops.aten.mm.default(view_680, t_126);  view_680 = t_126 = None
    view_681 = torch.ops.aten.view.default(mm_115, [1, 384, 384, 256]);  mm_115 = None
    add_58 = torch.ops.aten.add.Tensor(add_57, view_681);  add_57 = view_681 = None
    split_tensor_52 = torch.ops.aten.split.Tensor(add_55, 384, dim = -2)
    getitem_557 = split_tensor_52[0];  split_tensor_52 = None
    _to_copy_384 = torch.ops.aten._to_copy.default(getitem_557, dtype = torch.float32);  getitem_557 = None
    native_layer_norm_default_79 = torch.ops.aten.native_layer_norm.default(_to_copy_384, [256], pairformer_stack_blocks_0_transition_pair_layer_norm_weight, pairformer_stack_blocks_0_transition_pair_layer_norm_bias, 1e-05);  _to_copy_384 = pairformer_stack_blocks_0_transition_pair_layer_norm_weight = pairformer_stack_blocks_0_transition_pair_layer_norm_bias = None
    getitem_558 = native_layer_norm_default_79[0];  native_layer_norm_default_79 = None
    _to_copy_385 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_0_transition_pair_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_0_transition_pair_linear_no_bias_ab_weight = None
    _to_copy_386 = torch.ops.aten._to_copy.default(getitem_558, dtype = torch.bfloat16);  getitem_558 = None
    t_127 = torch.ops.aten.t.default(_to_copy_385);  _to_copy_385 = None
    view_682 = torch.ops.aten.view.default(_to_copy_386, [147456, 256]);  _to_copy_386 = None
    mm_116 = torch.ops.aten.mm.default(view_682, t_127);  view_682 = t_127 = None
    view_683 = torch.ops.aten.view.default(mm_116, [1, 384, 384, 1024]);  mm_116 = None
    split_tensor_53 = torch.ops.aten.split.Tensor(view_683, 512, dim = -1);  view_683 = None
    getitem_561 = split_tensor_53[0]
    getitem_562 = split_tensor_53[1];  split_tensor_53 = None
    silu_15 = torch.ops.aten.silu.default(getitem_561);  getitem_561 = None
    mul_64 = torch.ops.aten.mul.Tensor(silu_15, getitem_562);  silu_15 = getitem_562 = None
    _to_copy_387 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_0_transition_pair_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_0_transition_pair_linear_out_weight = None
    t_128 = torch.ops.aten.t.default(_to_copy_387);  _to_copy_387 = None
    view_685 = torch.ops.aten.view.default(mul_64, [147456, 512]);  mul_64 = None
    mm_117 = torch.ops.aten.mm.default(view_685, t_128);  view_685 = t_128 = None
    view_686 = torch.ops.aten.view.default(mm_117, [1, 384, 384, 256]);  mm_117 = None
    add_59 = torch.ops.aten.add.Tensor(add_58, view_686);  add_58 = view_686 = None
    _to_copy_388 = torch.ops.aten._to_copy.default(add_1, dtype = torch.float32)
    native_layer_norm_default_80 = torch.ops.aten.native_layer_norm.default(_to_copy_388, [384], pairformer_stack_blocks_0_attention_pair_bias_single_layer_norm_weight, pairformer_stack_blocks_0_attention_pair_bias_single_layer_norm_bias, 1e-05);  _to_copy_388 = pairformer_stack_blocks_0_attention_pair_bias_single_layer_norm_weight = pairformer_stack_blocks_0_attention_pair_bias_single_layer_norm_bias = None
    getitem_563 = native_layer_norm_default_80[0];  native_layer_norm_default_80 = None
    _to_copy_389 = torch.ops.aten._to_copy.default(add_55, dtype = torch.float32);  add_55 = None
    native_layer_norm_default_81 = torch.ops.aten.native_layer_norm.default(_to_copy_389, [256], pairformer_stack_blocks_0_attention_pair_bias_pair_layer_norm_weight, pairformer_stack_blocks_0_attention_pair_bias_pair_layer_norm_bias, 1e-05);  _to_copy_389 = pairformer_stack_blocks_0_attention_pair_bias_pair_layer_norm_weight = pairformer_stack_blocks_0_attention_pair_bias_pair_layer_norm_bias = None
    getitem_566 = native_layer_norm_default_81[0];  native_layer_norm_default_81 = None
    _to_copy_390 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_0_attention_pair_bias_pair_linear_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_0_attention_pair_bias_pair_linear_weight = None
    _to_copy_391 = torch.ops.aten._to_copy.default(getitem_566, dtype = torch.bfloat16);  getitem_566 = None
    t_129 = torch.ops.aten.t.default(_to_copy_390);  _to_copy_390 = None
    view_687 = torch.ops.aten.view.default(_to_copy_391, [147456, 256]);  _to_copy_391 = None
    mm_118 = torch.ops.aten.mm.default(view_687, t_129);  view_687 = t_129 = None
    view_688 = torch.ops.aten.view.default(mm_118, [1, 384, 384, 16]);  mm_118 = None
    permute_450 = torch.ops.aten.permute.default(view_688, [0, 3, 1, 2]);  view_688 = None
    view_689 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384])
    bitwise_not_49 = torch.ops.aten.bitwise_not.default(view_689);  view_689 = None
    masked_fill_49 = torch.ops.aten.masked_fill.Scalar(permute_450, bitwise_not_49, -10000);  permute_450 = bitwise_not_49 = None
    _to_copy_392 = torch.ops.aten._to_copy.default(getitem_563, dtype = torch.bfloat16);  getitem_563 = None
    _to_copy_393 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_0_attention_pair_bias_attention_input2qkvg_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_0_attention_pair_bias_attention_input2qkvg_weight = None
    unsqueeze_323 = torch.ops.aten.unsqueeze.default(_to_copy_392, 3);  _to_copy_392 = None
    unsqueeze_324 = torch.ops.aten.unsqueeze.default(unsqueeze_323, 4);  unsqueeze_323 = None
    unsqueeze_325 = torch.ops.aten.unsqueeze.default(unsqueeze_324, 5);  unsqueeze_324 = None
    permute_451 = torch.ops.aten.permute.default(unsqueeze_325, [3, 0, 4, 1, 5, 2]);  unsqueeze_325 = None
    unsqueeze_326 = torch.ops.aten.unsqueeze.default(_to_copy_393, 4);  _to_copy_393 = None
    unsqueeze_327 = torch.ops.aten.unsqueeze.default(unsqueeze_326, 5);  unsqueeze_326 = None
    permute_452 = torch.ops.aten.permute.default(unsqueeze_327, [1, 4, 2, 5, 3, 0]);  unsqueeze_327 = None
    permute_453 = torch.ops.aten.permute.default(permute_451, [3, 5, 0, 1, 2, 4]);  permute_451 = None
    view_690 = torch.ops.aten.view.default(permute_453, [1, 384, 384]);  permute_453 = None
    permute_454 = torch.ops.aten.permute.default(permute_452, [5, 0, 1, 2, 4, 3]);  permute_452 = None
    view_691 = torch.ops.aten.view.default(permute_454, [1, 384, 1536]);  permute_454 = None
    bmm_75 = torch.ops.aten.bmm.default(view_690, view_691);  view_690 = view_691 = None
    view_692 = torch.ops.aten.view.default(bmm_75, [384, 1, 4, 1, 16, 24]);  bmm_75 = None
    permute_455 = torch.ops.aten.permute.default(view_692, [2, 3, 4, 0, 5, 1]);  view_692 = None
    view_693 = torch.ops.aten.view.default(permute_455, [4, 1, 16, 384, 24]);  permute_455 = None
    unbind_int_36 = torch.ops.aten.unbind.int(view_693);  view_693 = None
    getitem_569 = unbind_int_36[0]
    getitem_570 = unbind_int_36[1]
    getitem_571 = unbind_int_36[2]
    getitem_572 = unbind_int_36[3];  unbind_int_36 = None
    view_694 = torch.ops.aten.view.default(pairformer_stack_blocks_0_attention_pair_bias_attention_query_bias, [1, 16, 1, 24]);  pairformer_stack_blocks_0_attention_pair_bias_attention_query_bias = None
    add_60 = torch.ops.aten.add.Tensor(getitem_569, view_694);  getitem_569 = view_694 = None
    _to_copy_394 = torch.ops.aten._to_copy.default(add_60, dtype = torch.bfloat16);  add_60 = None
    expand_31 = torch.ops.aten.expand.default(masked_fill_49, [1, 16, 384, 384]);  masked_fill_49 = None
    _scaled_dot_product_efficient_attention_default_14 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_394, getitem_570, getitem_571, expand_31, False);  _to_copy_394 = getitem_570 = getitem_571 = expand_31 = None
    getitem_573 = _scaled_dot_product_efficient_attention_default_14[0];  _scaled_dot_product_efficient_attention_default_14 = None
    add_61 = torch.ops.aten.add.Tensor(getitem_572, 1);  getitem_572 = None
    sigmoid_41 = torch.ops.aten.sigmoid.default(add_61);  add_61 = None
    mul_65 = torch.ops.aten.mul.Tensor(getitem_573, sigmoid_41);  getitem_573 = sigmoid_41 = None
    _to_copy_395 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_0_attention_pair_bias_attention_output_proj_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_0_attention_pair_bias_attention_output_proj_weight = None
    unsqueeze_328 = torch.ops.aten.unsqueeze.default(mul_65, 4);  mul_65 = None
    permute_456 = torch.ops.aten.permute.default(unsqueeze_328, [0, 2, 4, 3, 1]);  unsqueeze_328 = None
    unsqueeze_329 = torch.ops.aten.unsqueeze.default(_to_copy_395, 3);  _to_copy_395 = None
    unsqueeze_330 = torch.ops.aten.unsqueeze.default(unsqueeze_329, 4);  unsqueeze_329 = None
    permute_457 = torch.ops.aten.permute.default(unsqueeze_330, [3, 4, 2, 1, 0]);  unsqueeze_330 = None
    permute_458 = torch.ops.aten.permute.default(permute_456, [1, 3, 4, 0, 2]);  permute_456 = None
    clone_92 = torch.ops.aten.clone.default(permute_458, memory_format = torch.contiguous_format);  permute_458 = None
    _unsafe_view_84 = torch.ops.aten._unsafe_view.default(clone_92, [1, 384, 384]);  clone_92 = None
    permute_459 = torch.ops.aten.permute.default(permute_457, [3, 4, 0, 2, 1]);  permute_457 = None
    clone_93 = torch.ops.aten.clone.default(permute_459, memory_format = torch.contiguous_format);  permute_459 = None
    _unsafe_view_85 = torch.ops.aten._unsafe_view.default(clone_93, [1, 384, 384]);  clone_93 = None
    bmm_76 = torch.ops.aten.bmm.default(_unsafe_view_84, _unsafe_view_85);  _unsafe_view_84 = _unsafe_view_85 = None
    view_695 = torch.ops.aten.view.default(bmm_76, [384, 1, 1, 1, 384]);  bmm_76 = None
    permute_460 = torch.ops.aten.permute.default(view_695, [3, 0, 4, 1, 2]);  view_695 = None
    view_696 = torch.ops.aten.view.default(permute_460, [1, 384, 384]);  permute_460 = None
    unsqueeze_331 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
    mul_66 = torch.ops.aten.mul.Tensor(view_696, unsqueeze_331);  view_696 = unsqueeze_331 = None
    add_62 = torch.ops.aten.add.Tensor(add_1, mul_66);  mul_66 = None
    split_tensor_54 = torch.ops.aten.split.Tensor(add_1, 384, dim = -2);  add_1 = None
    getitem_577 = split_tensor_54[0];  split_tensor_54 = None
    _to_copy_396 = torch.ops.aten._to_copy.default(getitem_577, dtype = torch.float32);  getitem_577 = None
    native_layer_norm_default_82 = torch.ops.aten.native_layer_norm.default(_to_copy_396, [384], pairformer_stack_blocks_0_transition_single_layer_norm_weight, pairformer_stack_blocks_0_transition_single_layer_norm_bias, 1e-05);  _to_copy_396 = pairformer_stack_blocks_0_transition_single_layer_norm_weight = pairformer_stack_blocks_0_transition_single_layer_norm_bias = None
    getitem_578 = native_layer_norm_default_82[0];  native_layer_norm_default_82 = None
    _to_copy_397 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_0_transition_single_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_0_transition_single_linear_no_bias_ab_weight = None
    _to_copy_398 = torch.ops.aten._to_copy.default(getitem_578, dtype = torch.bfloat16);  getitem_578 = None
    t_130 = torch.ops.aten.t.default(_to_copy_397);  _to_copy_397 = None
    view_697 = torch.ops.aten.view.default(_to_copy_398, [384, 384]);  _to_copy_398 = None
    mm_119 = torch.ops.aten.mm.default(view_697, t_130);  view_697 = t_130 = None
    view_698 = torch.ops.aten.view.default(mm_119, [1, 384, 1536]);  mm_119 = None
    split_tensor_55 = torch.ops.aten.split.Tensor(view_698, 768, dim = -1);  view_698 = None
    getitem_581 = split_tensor_55[0]
    getitem_582 = split_tensor_55[1];  split_tensor_55 = None
    silu_16 = torch.ops.aten.silu.default(getitem_581);  getitem_581 = None
    mul_67 = torch.ops.aten.mul.Tensor(silu_16, getitem_582);  silu_16 = getitem_582 = None
    _to_copy_399 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_0_transition_single_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_0_transition_single_linear_out_weight = None
    t_131 = torch.ops.aten.t.default(_to_copy_399);  _to_copy_399 = None
    view_700 = torch.ops.aten.view.default(mul_67, [384, 768]);  mul_67 = None
    mm_120 = torch.ops.aten.mm.default(view_700, t_131);  view_700 = t_131 = None
    view_701 = torch.ops.aten.view.default(mm_120, [1, 384, 384]);  mm_120 = None
    add_63 = torch.ops.aten.add.Tensor(add_62, view_701);  add_62 = view_701 = None
    _to_copy_400 = torch.ops.aten._to_copy.default(add_59, dtype = torch.float32)
    native_layer_norm_default_83 = torch.ops.aten.native_layer_norm.default(_to_copy_400, [256], pairformer_stack_blocks_1_triangle_multiplication_layernorm_z_in_weight, pairformer_stack_blocks_1_triangle_multiplication_layernorm_z_in_bias, 1e-05);  _to_copy_400 = pairformer_stack_blocks_1_triangle_multiplication_layernorm_z_in_weight = pairformer_stack_blocks_1_triangle_multiplication_layernorm_z_in_bias = None
    getitem_583 = native_layer_norm_default_83[0];  native_layer_norm_default_83 = None
    split_with_sizes_default_14 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_1_triangle_multiplication_merged_linear_p_weight, [512, 512]);  pairformer_stack_blocks_1_triangle_multiplication_merged_linear_p_weight = None
    getitem_586 = split_with_sizes_default_14[0]
    getitem_587 = split_with_sizes_default_14[1];  split_with_sizes_default_14 = None
    split_with_sizes_default_15 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_1_triangle_multiplication_merged_linear_g_weight, [512, 512, 256]);  pairformer_stack_blocks_1_triangle_multiplication_merged_linear_g_weight = None
    getitem_588 = split_with_sizes_default_15[0]
    getitem_589 = split_with_sizes_default_15[1]
    getitem_590 = split_with_sizes_default_15[2];  split_with_sizes_default_15 = None
    _to_copy_401 = torch.ops.aten._to_copy.default(getitem_586, dtype = torch.bfloat16);  getitem_586 = None
    _to_copy_402 = torch.ops.aten._to_copy.default(getitem_583, dtype = torch.bfloat16)
    t_132 = torch.ops.aten.t.default(_to_copy_401);  _to_copy_401 = None
    view_702 = torch.ops.aten.view.default(_to_copy_402, [147456, 256]);  _to_copy_402 = None
    mm_121 = torch.ops.aten.mm.default(view_702, t_132);  view_702 = t_132 = None
    view_703 = torch.ops.aten.view.default(mm_121, [1, 384, 384, 512]);  mm_121 = None
    _to_copy_403 = torch.ops.aten._to_copy.default(getitem_588, dtype = torch.bfloat16);  getitem_588 = None
    _to_copy_404 = torch.ops.aten._to_copy.default(getitem_583, dtype = torch.bfloat16)
    t_133 = torch.ops.aten.t.default(_to_copy_403);  _to_copy_403 = None
    view_704 = torch.ops.aten.view.default(_to_copy_404, [147456, 256]);  _to_copy_404 = None
    mm_122 = torch.ops.aten.mm.default(view_704, t_133);  view_704 = t_133 = None
    view_705 = torch.ops.aten.view.default(mm_122, [1, 384, 384, 512]);  mm_122 = None
    sigmoid_42 = torch.ops.aten.sigmoid.default(view_705);  view_705 = None
    mul_68 = torch.ops.aten.mul.Tensor(view_703, sigmoid_42);  view_703 = sigmoid_42 = None
    unsqueeze_332 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
    bitwise_not_50 = torch.ops.aten.bitwise_not.default(unsqueeze_332);  unsqueeze_332 = None
    masked_fill_50 = torch.ops.aten.masked_fill.Scalar(mul_68, bitwise_not_50, 0);  mul_68 = bitwise_not_50 = None
    split_tensor_56 = torch.ops.aten.split.Tensor(masked_fill_50, 256, dim = -1)
    getitem_593 = split_tensor_56[0];  split_tensor_56 = None
    unsqueeze_335 = torch.ops.aten.unsqueeze.default(getitem_593, 4);  getitem_593 = None
    permute_465 = torch.ops.aten.permute.default(unsqueeze_335, [0, 1, 4, 3, 2]);  unsqueeze_335 = None
    permute_466 = torch.ops.aten.permute.default(permute_465, [3, 1, 4, 0, 2]);  permute_465 = None
    view_708 = torch.ops.aten.view.default(permute_466, [256, 384, 384]);  permute_466 = None
    split_tensor_57 = torch.ops.aten.split.Tensor(masked_fill_50, 256, dim = -1);  masked_fill_50 = None
    getitem_596 = split_tensor_57[1];  split_tensor_57 = None
    unsqueeze_336 = torch.ops.aten.unsqueeze.default(getitem_596, 4);  getitem_596 = None
    permute_467 = torch.ops.aten.permute.default(unsqueeze_336, [0, 4, 1, 3, 2]);  unsqueeze_336 = None
    permute_468 = torch.ops.aten.permute.default(permute_467, [3, 4, 0, 2, 1]);  permute_467 = None
    view_709 = torch.ops.aten.view.default(permute_468, [256, 384, 384]);  permute_468 = None
    bmm_77 = torch.ops.aten.bmm.default(view_708, view_709);  view_708 = view_709 = None
    view_710 = torch.ops.aten.view.default(bmm_77, [256, 384, 1, 1, 384]);  bmm_77 = None
    permute_469 = torch.ops.aten.permute.default(view_710, [3, 1, 4, 0, 2]);  view_710 = None
    view_711 = torch.ops.aten.view.default(permute_469, [1, 384, 384, 256]);  permute_469 = None
    _to_copy_405 = torch.ops.aten._to_copy.default(getitem_587, dtype = torch.bfloat16);  getitem_587 = None
    _to_copy_406 = torch.ops.aten._to_copy.default(getitem_583, dtype = torch.bfloat16)
    t_134 = torch.ops.aten.t.default(_to_copy_405);  _to_copy_405 = None
    view_712 = torch.ops.aten.view.default(_to_copy_406, [147456, 256]);  _to_copy_406 = None
    mm_123 = torch.ops.aten.mm.default(view_712, t_134);  view_712 = t_134 = None
    view_713 = torch.ops.aten.view.default(mm_123, [1, 384, 384, 512]);  mm_123 = None
    _to_copy_407 = torch.ops.aten._to_copy.default(getitem_589, dtype = torch.bfloat16);  getitem_589 = None
    _to_copy_408 = torch.ops.aten._to_copy.default(getitem_583, dtype = torch.bfloat16)
    t_135 = torch.ops.aten.t.default(_to_copy_407);  _to_copy_407 = None
    view_714 = torch.ops.aten.view.default(_to_copy_408, [147456, 256]);  _to_copy_408 = None
    mm_124 = torch.ops.aten.mm.default(view_714, t_135);  view_714 = t_135 = None
    view_715 = torch.ops.aten.view.default(mm_124, [1, 384, 384, 512]);  mm_124 = None
    sigmoid_43 = torch.ops.aten.sigmoid.default(view_715);  view_715 = None
    mul_69 = torch.ops.aten.mul.Tensor(view_713, sigmoid_43);  view_713 = sigmoid_43 = None
    view_716 = torch.ops.aten.view.default(mul_69, [147456, 512]);  mul_69 = None
    view_717 = torch.ops.aten.view.default(view_716, [1, 384, 384, 512]);  view_716 = None
    transpose_14 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
    unsqueeze_337 = torch.ops.aten.unsqueeze.default(transpose_14, 3);  transpose_14 = None
    clone_94 = torch.ops.aten.clone.default(unsqueeze_337, memory_format = torch.contiguous_format);  unsqueeze_337 = None
    bitwise_not_51 = torch.ops.aten.bitwise_not.default(clone_94);  clone_94 = None
    masked_fill_51 = torch.ops.aten.masked_fill.Scalar(view_717, bitwise_not_51, 0);  view_717 = bitwise_not_51 = None
    view_718 = torch.ops.aten.view.default(masked_fill_51, [147456, 512]);  masked_fill_51 = None
    view_722 = torch.ops.aten.view.default(view_718, [1, 384, 384, 512])
    split_tensor_58 = torch.ops.aten.split.Tensor(view_722, 256, dim = -1);  view_722 = None
    getitem_599 = split_tensor_58[0];  split_tensor_58 = None
    unsqueeze_340 = torch.ops.aten.unsqueeze.default(getitem_599, 4);  getitem_599 = None
    permute_474 = torch.ops.aten.permute.default(unsqueeze_340, [0, 2, 4, 3, 1]);  unsqueeze_340 = None
    permute_475 = torch.ops.aten.permute.default(permute_474, [3, 1, 4, 0, 2]);  permute_474 = None
    view_723 = torch.ops.aten.view.default(permute_475, [256, 384, 384]);  permute_475 = None
    view_724 = torch.ops.aten.view.default(view_718, [1, 384, 384, 512]);  view_718 = None
    split_tensor_59 = torch.ops.aten.split.Tensor(view_724, 256, dim = -1);  view_724 = None
    getitem_602 = split_tensor_59[1];  split_tensor_59 = None
    unsqueeze_341 = torch.ops.aten.unsqueeze.default(getitem_602, 4);  getitem_602 = None
    permute_476 = torch.ops.aten.permute.default(unsqueeze_341, [0, 4, 2, 3, 1]);  unsqueeze_341 = None
    permute_477 = torch.ops.aten.permute.default(permute_476, [3, 4, 0, 2, 1]);  permute_476 = None
    view_725 = torch.ops.aten.view.default(permute_477, [256, 384, 384]);  permute_477 = None
    bmm_78 = torch.ops.aten.bmm.default(view_723, view_725);  view_723 = view_725 = None
    view_726 = torch.ops.aten.view.default(bmm_78, [256, 384, 1, 1, 384]);  bmm_78 = None
    permute_478 = torch.ops.aten.permute.default(view_726, [3, 1, 4, 0, 2]);  view_726 = None
    view_727 = torch.ops.aten.view.default(permute_478, [1, 384, 384, 256]);  permute_478 = None
    _to_copy_409 = torch.ops.aten._to_copy.default(view_711, dtype = torch.float32);  view_711 = None
    native_layer_norm_default_84 = torch.ops.aten.native_layer_norm.default(_to_copy_409, [256], None, None, 1e-05);  _to_copy_409 = None
    getitem_603 = native_layer_norm_default_84[0];  native_layer_norm_default_84 = None
    _to_copy_410 = torch.ops.aten._to_copy.default(view_727, dtype = torch.float32);  view_727 = None
    native_layer_norm_default_85 = torch.ops.aten.native_layer_norm.default(_to_copy_410, [256], None, None, 1e-05);  _to_copy_410 = None
    getitem_606 = native_layer_norm_default_85[0];  native_layer_norm_default_85 = None
    add_64 = torch.ops.aten.add.Tensor(getitem_603, getitem_606);  getitem_603 = getitem_606 = None
    _to_copy_411 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_1_triangle_multiplication_linear_z_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_1_triangle_multiplication_linear_z_out_weight = None
    _to_copy_412 = torch.ops.aten._to_copy.default(add_64, dtype = torch.bfloat16);  add_64 = None
    t_136 = torch.ops.aten.t.default(_to_copy_411);  _to_copy_411 = None
    view_728 = torch.ops.aten.view.default(_to_copy_412, [147456, 256]);  _to_copy_412 = None
    mm_125 = torch.ops.aten.mm.default(view_728, t_136);  view_728 = t_136 = None
    view_729 = torch.ops.aten.view.default(mm_125, [1, 384, 384, 256]);  mm_125 = None
    _to_copy_413 = torch.ops.aten._to_copy.default(getitem_590, dtype = torch.bfloat16);  getitem_590 = None
    _to_copy_414 = torch.ops.aten._to_copy.default(getitem_583, dtype = torch.bfloat16);  getitem_583 = None
    t_137 = torch.ops.aten.t.default(_to_copy_413);  _to_copy_413 = None
    view_730 = torch.ops.aten.view.default(_to_copy_414, [147456, 256]);  _to_copy_414 = None
    mm_126 = torch.ops.aten.mm.default(view_730, t_137);  view_730 = t_137 = None
    view_731 = torch.ops.aten.view.default(mm_126, [1, 384, 384, 256]);  mm_126 = None
    sigmoid_44 = torch.ops.aten.sigmoid.default(view_731);  view_731 = None
    mul_70 = torch.ops.aten.mul.Tensor(view_729, sigmoid_44);  view_729 = sigmoid_44 = None
    add_65 = torch.ops.aten.add.Tensor(add_59, mul_70);  mul_70 = None
    _to_copy_415 = torch.ops.aten._to_copy.default(add_59, dtype = torch.float32)
    native_layer_norm_default_86 = torch.ops.aten.native_layer_norm.default(_to_copy_415, [256], None, None, 1e-05);  _to_copy_415 = None
    getitem_609 = native_layer_norm_default_86[0];  native_layer_norm_default_86 = None
    _to_copy_416 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_1_triangle_attention_pair2b_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_1_triangle_attention_pair2b_weight = None
    _to_copy_417 = torch.ops.aten._to_copy.default(getitem_609, dtype = torch.bfloat16)
    t_138 = torch.ops.aten.t.default(_to_copy_416);  _to_copy_416 = None
    view_732 = torch.ops.aten.view.default(_to_copy_417, [147456, 256]);  _to_copy_417 = None
    mm_127 = torch.ops.aten.mm.default(view_732, t_138);  view_732 = t_138 = None
    view_733 = torch.ops.aten.view.default(mm_127, [1, 384, 384, 8]);  mm_127 = None
    view_734 = torch.ops.aten.view.default(view_733, [1, 384, 384, 2, 4]);  view_733 = None
    permute_479 = torch.ops.aten.permute.default(view_734, [0, 3, 4, 1, 2]);  view_734 = None
    view_735 = torch.ops.aten.view.default(permute_479, [1, 2, 4, 1, 384, 384]);  permute_479 = None
    view_736 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 384, 384])
    bitwise_not_52 = torch.ops.aten.bitwise_not.default(view_736);  view_736 = None
    masked_fill_52 = torch.ops.aten.masked_fill.Scalar(view_735, bitwise_not_52, -10000);  view_735 = bitwise_not_52 = None
    view_737 = torch.ops.aten.view.default(masked_fill_52, [1, 2, 4, 384, 384]);  masked_fill_52 = None
    permute_480 = torch.ops.aten.permute.default(view_737, [1, 0, 2, 3, 4]);  view_737 = None
    view_738 = torch.ops.aten.view.default(permute_480, [2, 4, 1, 384, 384]);  permute_480 = None
    _to_copy_418 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_1_triangle_attention_pair2qkvg1_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_1_triangle_attention_pair2qkvg1_weight = None
    _to_copy_419 = torch.ops.aten._to_copy.default(getitem_609, dtype = torch.bfloat16)
    t_139 = torch.ops.aten.t.default(_to_copy_418);  _to_copy_418 = None
    view_739 = torch.ops.aten.view.default(_to_copy_419, [147456, 256]);  _to_copy_419 = None
    mm_128 = torch.ops.aten.mm.default(view_739, t_139);  view_739 = t_139 = None
    view_740 = torch.ops.aten.view.default(mm_128, [1, 384, 384, 1024]);  mm_128 = None
    select_15 = torch.ops.aten.select.int(view_738, 0, 0)
    view_741 = torch.ops.aten.view.default(view_740, [1, 384, 384, 4, 4, 64]);  view_740 = None
    permute_481 = torch.ops.aten.permute.default(view_741, [4, 0, 3, 1, 2, 5]);  view_741 = None
    view_742 = torch.ops.aten.view.default(permute_481, [4, 4, 384, 384, 64]);  permute_481 = None
    unbind_int_37 = torch.ops.aten.unbind.int(view_742);  view_742 = None
    getitem_612 = unbind_int_37[0]
    getitem_613 = unbind_int_37[1]
    getitem_614 = unbind_int_37[2]
    getitem_615 = unbind_int_37[3];  unbind_int_37 = None
    expand_32 = torch.ops.aten.expand.default(select_15, [4, 384, 384, 384]);  select_15 = None
    _scaled_dot_product_efficient_attention_default_15 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_612, getitem_613, getitem_614, expand_32, False);  getitem_612 = getitem_613 = getitem_614 = expand_32 = None
    getitem_616 = _scaled_dot_product_efficient_attention_default_15[0];  _scaled_dot_product_efficient_attention_default_15 = None
    sigmoid_45 = torch.ops.aten.sigmoid.default(getitem_615);  getitem_615 = None
    mul_71 = torch.ops.aten.mul.Tensor(getitem_616, sigmoid_45);  getitem_616 = sigmoid_45 = None
    view_743 = torch.ops.aten.view.default(mul_71, [1, 4, 384, 384, 64]);  mul_71 = None
    permute_482 = torch.ops.aten.permute.default(view_743, [0, 2, 3, 1, 4]);  view_743 = None
    clone_95 = torch.ops.aten.clone.default(permute_482, memory_format = torch.contiguous_format);  permute_482 = None
    _unsafe_view_86 = torch.ops.aten._unsafe_view.default(clone_95, [1, 384, 384, 256]);  clone_95 = None
    transpose_15 = torch.ops.aten.transpose.int(getitem_609, 1, 2);  getitem_609 = None
    _to_copy_420 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_1_triangle_attention_pair2qkvg2_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_1_triangle_attention_pair2qkvg2_weight = None
    _to_copy_421 = torch.ops.aten._to_copy.default(transpose_15, dtype = torch.bfloat16);  transpose_15 = None
    t_140 = torch.ops.aten.t.default(_to_copy_420);  _to_copy_420 = None
    expand_33 = torch.ops.aten.expand.default(_to_copy_421, [1, 384, 384, 256]);  _to_copy_421 = None
    view_744 = torch.ops.aten.view.default(expand_33, [384, 384, 256]);  expand_33 = None
    expand_34 = torch.ops.aten.expand.default(t_140, [1, 384, 256, 1024]);  t_140 = None
    view_745 = torch.ops.aten.view.default(expand_34, [384, 256, 1024]);  expand_34 = None
    bmm_79 = torch.ops.aten.bmm.default(view_744, view_745);  view_744 = view_745 = None
    view_746 = torch.ops.aten.view.default(bmm_79, [1, 384, 384, 1024]);  bmm_79 = None
    select_16 = torch.ops.aten.select.int(view_738, 0, 1);  view_738 = None
    view_747 = torch.ops.aten.view.default(view_746, [1, 384, 384, 4, 4, 64]);  view_746 = None
    permute_483 = torch.ops.aten.permute.default(view_747, [4, 0, 3, 1, 2, 5]);  view_747 = None
    view_748 = torch.ops.aten.view.default(permute_483, [4, 4, 384, 384, 64]);  permute_483 = None
    unbind_int_38 = torch.ops.aten.unbind.int(view_748);  view_748 = None
    getitem_620 = unbind_int_38[0]
    getitem_621 = unbind_int_38[1]
    getitem_622 = unbind_int_38[2]
    getitem_623 = unbind_int_38[3];  unbind_int_38 = None
    expand_35 = torch.ops.aten.expand.default(select_16, [4, 384, 384, 384]);  select_16 = None
    _scaled_dot_product_efficient_attention_default_16 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_620, getitem_621, getitem_622, expand_35, False);  getitem_620 = getitem_621 = getitem_622 = expand_35 = None
    getitem_624 = _scaled_dot_product_efficient_attention_default_16[0];  _scaled_dot_product_efficient_attention_default_16 = None
    sigmoid_46 = torch.ops.aten.sigmoid.default(getitem_623);  getitem_623 = None
    mul_72 = torch.ops.aten.mul.Tensor(getitem_624, sigmoid_46);  getitem_624 = sigmoid_46 = None
    view_749 = torch.ops.aten.view.default(mul_72, [1, 4, 384, 384, 64]);  mul_72 = None
    permute_484 = torch.ops.aten.permute.default(view_749, [0, 2, 3, 1, 4]);  view_749 = None
    clone_96 = torch.ops.aten.clone.default(permute_484, memory_format = torch.contiguous_format);  permute_484 = None
    _unsafe_view_87 = torch.ops.aten._unsafe_view.default(clone_96, [1, 384, 384, 256]);  clone_96 = None
    cat_13 = torch.ops.aten.cat.default([_unsafe_view_86, _unsafe_view_87], dim = -1);  _unsafe_view_86 = _unsafe_view_87 = None
    slice_164 = torch.ops.aten.slice.Tensor(pairformer_stack_blocks_1_triangle_attention_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  pairformer_stack_blocks_1_triangle_attention_out_scalers = None
    unsqueeze_342 = torch.ops.aten.unsqueeze.default(slice_164, 1);  slice_164 = None
    mul_73 = torch.ops.aten.mul.Tensor(pairformer_stack_blocks_1_triangle_attention_linear_out_weight, unsqueeze_342);  pairformer_stack_blocks_1_triangle_attention_linear_out_weight = unsqueeze_342 = None
    _to_copy_422 = torch.ops.aten._to_copy.default(mul_73, dtype = torch.bfloat16);  mul_73 = None
    t_141 = torch.ops.aten.t.default(_to_copy_422);  _to_copy_422 = None
    view_750 = torch.ops.aten.view.default(cat_13, [147456, 512]);  cat_13 = None
    mm_129 = torch.ops.aten.mm.default(view_750, t_141);  view_750 = t_141 = None
    view_751 = torch.ops.aten.view.default(mm_129, [1, 384, 384, 256]);  mm_129 = None
    add_66 = torch.ops.aten.add.Tensor(add_65, view_751);  add_65 = view_751 = None
    split_tensor_60 = torch.ops.aten.split.Tensor(add_59, 384, dim = -2)
    getitem_628 = split_tensor_60[0];  split_tensor_60 = None
    _to_copy_423 = torch.ops.aten._to_copy.default(getitem_628, dtype = torch.float32);  getitem_628 = None
    native_layer_norm_default_87 = torch.ops.aten.native_layer_norm.default(_to_copy_423, [256], pairformer_stack_blocks_1_transition_pair_layer_norm_weight, pairformer_stack_blocks_1_transition_pair_layer_norm_bias, 1e-05);  _to_copy_423 = pairformer_stack_blocks_1_transition_pair_layer_norm_weight = pairformer_stack_blocks_1_transition_pair_layer_norm_bias = None
    getitem_629 = native_layer_norm_default_87[0];  native_layer_norm_default_87 = None
    _to_copy_424 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_1_transition_pair_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_1_transition_pair_linear_no_bias_ab_weight = None
    _to_copy_425 = torch.ops.aten._to_copy.default(getitem_629, dtype = torch.bfloat16);  getitem_629 = None
    t_142 = torch.ops.aten.t.default(_to_copy_424);  _to_copy_424 = None
    view_752 = torch.ops.aten.view.default(_to_copy_425, [147456, 256]);  _to_copy_425 = None
    mm_130 = torch.ops.aten.mm.default(view_752, t_142);  view_752 = t_142 = None
    view_753 = torch.ops.aten.view.default(mm_130, [1, 384, 384, 1024]);  mm_130 = None
    split_tensor_61 = torch.ops.aten.split.Tensor(view_753, 512, dim = -1);  view_753 = None
    getitem_632 = split_tensor_61[0]
    getitem_633 = split_tensor_61[1];  split_tensor_61 = None
    silu_17 = torch.ops.aten.silu.default(getitem_632);  getitem_632 = None
    mul_74 = torch.ops.aten.mul.Tensor(silu_17, getitem_633);  silu_17 = getitem_633 = None
    _to_copy_426 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_1_transition_pair_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_1_transition_pair_linear_out_weight = None
    t_143 = torch.ops.aten.t.default(_to_copy_426);  _to_copy_426 = None
    view_755 = torch.ops.aten.view.default(mul_74, [147456, 512]);  mul_74 = None
    mm_131 = torch.ops.aten.mm.default(view_755, t_143);  view_755 = t_143 = None
    view_756 = torch.ops.aten.view.default(mm_131, [1, 384, 384, 256]);  mm_131 = None
    add_67 = torch.ops.aten.add.Tensor(add_66, view_756);  add_66 = view_756 = None
    _to_copy_427 = torch.ops.aten._to_copy.default(add_63, dtype = torch.float32)
    native_layer_norm_default_88 = torch.ops.aten.native_layer_norm.default(_to_copy_427, [384], pairformer_stack_blocks_1_attention_pair_bias_single_layer_norm_weight, pairformer_stack_blocks_1_attention_pair_bias_single_layer_norm_bias, 1e-05);  _to_copy_427 = pairformer_stack_blocks_1_attention_pair_bias_single_layer_norm_weight = pairformer_stack_blocks_1_attention_pair_bias_single_layer_norm_bias = None
    getitem_634 = native_layer_norm_default_88[0];  native_layer_norm_default_88 = None
    _to_copy_428 = torch.ops.aten._to_copy.default(add_59, dtype = torch.float32);  add_59 = None
    native_layer_norm_default_89 = torch.ops.aten.native_layer_norm.default(_to_copy_428, [256], pairformer_stack_blocks_1_attention_pair_bias_pair_layer_norm_weight, pairformer_stack_blocks_1_attention_pair_bias_pair_layer_norm_bias, 1e-05);  _to_copy_428 = pairformer_stack_blocks_1_attention_pair_bias_pair_layer_norm_weight = pairformer_stack_blocks_1_attention_pair_bias_pair_layer_norm_bias = None
    getitem_637 = native_layer_norm_default_89[0];  native_layer_norm_default_89 = None
    _to_copy_429 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_1_attention_pair_bias_pair_linear_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_1_attention_pair_bias_pair_linear_weight = None
    _to_copy_430 = torch.ops.aten._to_copy.default(getitem_637, dtype = torch.bfloat16);  getitem_637 = None
    t_144 = torch.ops.aten.t.default(_to_copy_429);  _to_copy_429 = None
    view_757 = torch.ops.aten.view.default(_to_copy_430, [147456, 256]);  _to_copy_430 = None
    mm_132 = torch.ops.aten.mm.default(view_757, t_144);  view_757 = t_144 = None
    view_758 = torch.ops.aten.view.default(mm_132, [1, 384, 384, 16]);  mm_132 = None
    permute_485 = torch.ops.aten.permute.default(view_758, [0, 3, 1, 2]);  view_758 = None
    view_759 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384])
    bitwise_not_53 = torch.ops.aten.bitwise_not.default(view_759);  view_759 = None
    masked_fill_53 = torch.ops.aten.masked_fill.Scalar(permute_485, bitwise_not_53, -10000);  permute_485 = bitwise_not_53 = None
    _to_copy_431 = torch.ops.aten._to_copy.default(getitem_634, dtype = torch.bfloat16);  getitem_634 = None
    _to_copy_432 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_1_attention_pair_bias_attention_input2qkvg_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_1_attention_pair_bias_attention_input2qkvg_weight = None
    unsqueeze_343 = torch.ops.aten.unsqueeze.default(_to_copy_431, 3);  _to_copy_431 = None
    unsqueeze_344 = torch.ops.aten.unsqueeze.default(unsqueeze_343, 4);  unsqueeze_343 = None
    unsqueeze_345 = torch.ops.aten.unsqueeze.default(unsqueeze_344, 5);  unsqueeze_344 = None
    permute_486 = torch.ops.aten.permute.default(unsqueeze_345, [3, 0, 4, 1, 5, 2]);  unsqueeze_345 = None
    unsqueeze_346 = torch.ops.aten.unsqueeze.default(_to_copy_432, 4);  _to_copy_432 = None
    unsqueeze_347 = torch.ops.aten.unsqueeze.default(unsqueeze_346, 5);  unsqueeze_346 = None
    permute_487 = torch.ops.aten.permute.default(unsqueeze_347, [1, 4, 2, 5, 3, 0]);  unsqueeze_347 = None
    permute_488 = torch.ops.aten.permute.default(permute_486, [3, 5, 0, 1, 2, 4]);  permute_486 = None
    view_760 = torch.ops.aten.view.default(permute_488, [1, 384, 384]);  permute_488 = None
    permute_489 = torch.ops.aten.permute.default(permute_487, [5, 0, 1, 2, 4, 3]);  permute_487 = None
    view_761 = torch.ops.aten.view.default(permute_489, [1, 384, 1536]);  permute_489 = None
    bmm_80 = torch.ops.aten.bmm.default(view_760, view_761);  view_760 = view_761 = None
    view_762 = torch.ops.aten.view.default(bmm_80, [384, 1, 4, 1, 16, 24]);  bmm_80 = None
    permute_490 = torch.ops.aten.permute.default(view_762, [2, 3, 4, 0, 5, 1]);  view_762 = None
    view_763 = torch.ops.aten.view.default(permute_490, [4, 1, 16, 384, 24]);  permute_490 = None
    unbind_int_39 = torch.ops.aten.unbind.int(view_763);  view_763 = None
    getitem_640 = unbind_int_39[0]
    getitem_641 = unbind_int_39[1]
    getitem_642 = unbind_int_39[2]
    getitem_643 = unbind_int_39[3];  unbind_int_39 = None
    view_764 = torch.ops.aten.view.default(pairformer_stack_blocks_1_attention_pair_bias_attention_query_bias, [1, 16, 1, 24]);  pairformer_stack_blocks_1_attention_pair_bias_attention_query_bias = None
    add_68 = torch.ops.aten.add.Tensor(getitem_640, view_764);  getitem_640 = view_764 = None
    _to_copy_433 = torch.ops.aten._to_copy.default(add_68, dtype = torch.bfloat16);  add_68 = None
    expand_36 = torch.ops.aten.expand.default(masked_fill_53, [1, 16, 384, 384]);  masked_fill_53 = None
    _scaled_dot_product_efficient_attention_default_17 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_433, getitem_641, getitem_642, expand_36, False);  _to_copy_433 = getitem_641 = getitem_642 = expand_36 = None
    getitem_644 = _scaled_dot_product_efficient_attention_default_17[0];  _scaled_dot_product_efficient_attention_default_17 = None
    add_69 = torch.ops.aten.add.Tensor(getitem_643, 1);  getitem_643 = None
    sigmoid_47 = torch.ops.aten.sigmoid.default(add_69);  add_69 = None
    mul_75 = torch.ops.aten.mul.Tensor(getitem_644, sigmoid_47);  getitem_644 = sigmoid_47 = None
    _to_copy_434 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_1_attention_pair_bias_attention_output_proj_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_1_attention_pair_bias_attention_output_proj_weight = None
    unsqueeze_348 = torch.ops.aten.unsqueeze.default(mul_75, 4);  mul_75 = None
    permute_491 = torch.ops.aten.permute.default(unsqueeze_348, [0, 2, 4, 3, 1]);  unsqueeze_348 = None
    unsqueeze_349 = torch.ops.aten.unsqueeze.default(_to_copy_434, 3);  _to_copy_434 = None
    unsqueeze_350 = torch.ops.aten.unsqueeze.default(unsqueeze_349, 4);  unsqueeze_349 = None
    permute_492 = torch.ops.aten.permute.default(unsqueeze_350, [3, 4, 2, 1, 0]);  unsqueeze_350 = None
    permute_493 = torch.ops.aten.permute.default(permute_491, [1, 3, 4, 0, 2]);  permute_491 = None
    clone_97 = torch.ops.aten.clone.default(permute_493, memory_format = torch.contiguous_format);  permute_493 = None
    _unsafe_view_88 = torch.ops.aten._unsafe_view.default(clone_97, [1, 384, 384]);  clone_97 = None
    permute_494 = torch.ops.aten.permute.default(permute_492, [3, 4, 0, 2, 1]);  permute_492 = None
    clone_98 = torch.ops.aten.clone.default(permute_494, memory_format = torch.contiguous_format);  permute_494 = None
    _unsafe_view_89 = torch.ops.aten._unsafe_view.default(clone_98, [1, 384, 384]);  clone_98 = None
    bmm_81 = torch.ops.aten.bmm.default(_unsafe_view_88, _unsafe_view_89);  _unsafe_view_88 = _unsafe_view_89 = None
    view_765 = torch.ops.aten.view.default(bmm_81, [384, 1, 1, 1, 384]);  bmm_81 = None
    permute_495 = torch.ops.aten.permute.default(view_765, [3, 0, 4, 1, 2]);  view_765 = None
    view_766 = torch.ops.aten.view.default(permute_495, [1, 384, 384]);  permute_495 = None
    unsqueeze_351 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
    mul_76 = torch.ops.aten.mul.Tensor(view_766, unsqueeze_351);  view_766 = unsqueeze_351 = None
    add_70 = torch.ops.aten.add.Tensor(add_63, mul_76);  mul_76 = None
    split_tensor_62 = torch.ops.aten.split.Tensor(add_63, 384, dim = -2);  add_63 = None
    getitem_648 = split_tensor_62[0];  split_tensor_62 = None
    _to_copy_435 = torch.ops.aten._to_copy.default(getitem_648, dtype = torch.float32);  getitem_648 = None
    native_layer_norm_default_90 = torch.ops.aten.native_layer_norm.default(_to_copy_435, [384], pairformer_stack_blocks_1_transition_single_layer_norm_weight, pairformer_stack_blocks_1_transition_single_layer_norm_bias, 1e-05);  _to_copy_435 = pairformer_stack_blocks_1_transition_single_layer_norm_weight = pairformer_stack_blocks_1_transition_single_layer_norm_bias = None
    getitem_649 = native_layer_norm_default_90[0];  native_layer_norm_default_90 = None
    _to_copy_436 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_1_transition_single_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_1_transition_single_linear_no_bias_ab_weight = None
    _to_copy_437 = torch.ops.aten._to_copy.default(getitem_649, dtype = torch.bfloat16);  getitem_649 = None
    t_145 = torch.ops.aten.t.default(_to_copy_436);  _to_copy_436 = None
    view_767 = torch.ops.aten.view.default(_to_copy_437, [384, 384]);  _to_copy_437 = None
    mm_133 = torch.ops.aten.mm.default(view_767, t_145);  view_767 = t_145 = None
    view_768 = torch.ops.aten.view.default(mm_133, [1, 384, 1536]);  mm_133 = None
    split_tensor_63 = torch.ops.aten.split.Tensor(view_768, 768, dim = -1);  view_768 = None
    getitem_652 = split_tensor_63[0]
    getitem_653 = split_tensor_63[1];  split_tensor_63 = None
    silu_18 = torch.ops.aten.silu.default(getitem_652);  getitem_652 = None
    mul_77 = torch.ops.aten.mul.Tensor(silu_18, getitem_653);  silu_18 = getitem_653 = None
    _to_copy_438 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_1_transition_single_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_1_transition_single_linear_out_weight = None
    t_146 = torch.ops.aten.t.default(_to_copy_438);  _to_copy_438 = None
    view_770 = torch.ops.aten.view.default(mul_77, [384, 768]);  mul_77 = None
    mm_134 = torch.ops.aten.mm.default(view_770, t_146);  view_770 = t_146 = None
    view_771 = torch.ops.aten.view.default(mm_134, [1, 384, 384]);  mm_134 = None
    add_71 = torch.ops.aten.add.Tensor(add_70, view_771);  add_70 = view_771 = None
    _to_copy_439 = torch.ops.aten._to_copy.default(add_67, dtype = torch.float32)
    native_layer_norm_default_91 = torch.ops.aten.native_layer_norm.default(_to_copy_439, [256], pairformer_stack_blocks_2_triangle_multiplication_layernorm_z_in_weight, pairformer_stack_blocks_2_triangle_multiplication_layernorm_z_in_bias, 1e-05);  _to_copy_439 = pairformer_stack_blocks_2_triangle_multiplication_layernorm_z_in_weight = pairformer_stack_blocks_2_triangle_multiplication_layernorm_z_in_bias = None
    getitem_654 = native_layer_norm_default_91[0];  native_layer_norm_default_91 = None
    split_with_sizes_default_16 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_2_triangle_multiplication_merged_linear_p_weight, [512, 512]);  pairformer_stack_blocks_2_triangle_multiplication_merged_linear_p_weight = None
    getitem_657 = split_with_sizes_default_16[0]
    getitem_658 = split_with_sizes_default_16[1];  split_with_sizes_default_16 = None
    split_with_sizes_default_17 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_2_triangle_multiplication_merged_linear_g_weight, [512, 512, 256]);  pairformer_stack_blocks_2_triangle_multiplication_merged_linear_g_weight = None
    getitem_659 = split_with_sizes_default_17[0]
    getitem_660 = split_with_sizes_default_17[1]
    getitem_661 = split_with_sizes_default_17[2];  split_with_sizes_default_17 = None
    _to_copy_440 = torch.ops.aten._to_copy.default(getitem_657, dtype = torch.bfloat16);  getitem_657 = None
    _to_copy_441 = torch.ops.aten._to_copy.default(getitem_654, dtype = torch.bfloat16)
    t_147 = torch.ops.aten.t.default(_to_copy_440);  _to_copy_440 = None
    view_772 = torch.ops.aten.view.default(_to_copy_441, [147456, 256]);  _to_copy_441 = None
    mm_135 = torch.ops.aten.mm.default(view_772, t_147);  view_772 = t_147 = None
    view_773 = torch.ops.aten.view.default(mm_135, [1, 384, 384, 512]);  mm_135 = None
    _to_copy_442 = torch.ops.aten._to_copy.default(getitem_659, dtype = torch.bfloat16);  getitem_659 = None
    _to_copy_443 = torch.ops.aten._to_copy.default(getitem_654, dtype = torch.bfloat16)
    t_148 = torch.ops.aten.t.default(_to_copy_442);  _to_copy_442 = None
    view_774 = torch.ops.aten.view.default(_to_copy_443, [147456, 256]);  _to_copy_443 = None
    mm_136 = torch.ops.aten.mm.default(view_774, t_148);  view_774 = t_148 = None
    view_775 = torch.ops.aten.view.default(mm_136, [1, 384, 384, 512]);  mm_136 = None
    sigmoid_48 = torch.ops.aten.sigmoid.default(view_775);  view_775 = None
    mul_78 = torch.ops.aten.mul.Tensor(view_773, sigmoid_48);  view_773 = sigmoid_48 = None
    unsqueeze_352 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
    bitwise_not_54 = torch.ops.aten.bitwise_not.default(unsqueeze_352);  unsqueeze_352 = None
    masked_fill_54 = torch.ops.aten.masked_fill.Scalar(mul_78, bitwise_not_54, 0);  mul_78 = bitwise_not_54 = None
    split_tensor_64 = torch.ops.aten.split.Tensor(masked_fill_54, 256, dim = -1)
    getitem_664 = split_tensor_64[0];  split_tensor_64 = None
    unsqueeze_355 = torch.ops.aten.unsqueeze.default(getitem_664, 4);  getitem_664 = None
    permute_500 = torch.ops.aten.permute.default(unsqueeze_355, [0, 1, 4, 3, 2]);  unsqueeze_355 = None
    permute_501 = torch.ops.aten.permute.default(permute_500, [3, 1, 4, 0, 2]);  permute_500 = None
    view_778 = torch.ops.aten.view.default(permute_501, [256, 384, 384]);  permute_501 = None
    split_tensor_65 = torch.ops.aten.split.Tensor(masked_fill_54, 256, dim = -1);  masked_fill_54 = None
    getitem_667 = split_tensor_65[1];  split_tensor_65 = None
    unsqueeze_356 = torch.ops.aten.unsqueeze.default(getitem_667, 4);  getitem_667 = None
    permute_502 = torch.ops.aten.permute.default(unsqueeze_356, [0, 4, 1, 3, 2]);  unsqueeze_356 = None
    permute_503 = torch.ops.aten.permute.default(permute_502, [3, 4, 0, 2, 1]);  permute_502 = None
    view_779 = torch.ops.aten.view.default(permute_503, [256, 384, 384]);  permute_503 = None
    bmm_82 = torch.ops.aten.bmm.default(view_778, view_779);  view_778 = view_779 = None
    view_780 = torch.ops.aten.view.default(bmm_82, [256, 384, 1, 1, 384]);  bmm_82 = None
    permute_504 = torch.ops.aten.permute.default(view_780, [3, 1, 4, 0, 2]);  view_780 = None
    view_781 = torch.ops.aten.view.default(permute_504, [1, 384, 384, 256]);  permute_504 = None
    _to_copy_444 = torch.ops.aten._to_copy.default(getitem_658, dtype = torch.bfloat16);  getitem_658 = None
    _to_copy_445 = torch.ops.aten._to_copy.default(getitem_654, dtype = torch.bfloat16)
    t_149 = torch.ops.aten.t.default(_to_copy_444);  _to_copy_444 = None
    view_782 = torch.ops.aten.view.default(_to_copy_445, [147456, 256]);  _to_copy_445 = None
    mm_137 = torch.ops.aten.mm.default(view_782, t_149);  view_782 = t_149 = None
    view_783 = torch.ops.aten.view.default(mm_137, [1, 384, 384, 512]);  mm_137 = None
    _to_copy_446 = torch.ops.aten._to_copy.default(getitem_660, dtype = torch.bfloat16);  getitem_660 = None
    _to_copy_447 = torch.ops.aten._to_copy.default(getitem_654, dtype = torch.bfloat16)
    t_150 = torch.ops.aten.t.default(_to_copy_446);  _to_copy_446 = None
    view_784 = torch.ops.aten.view.default(_to_copy_447, [147456, 256]);  _to_copy_447 = None
    mm_138 = torch.ops.aten.mm.default(view_784, t_150);  view_784 = t_150 = None
    view_785 = torch.ops.aten.view.default(mm_138, [1, 384, 384, 512]);  mm_138 = None
    sigmoid_49 = torch.ops.aten.sigmoid.default(view_785);  view_785 = None
    mul_79 = torch.ops.aten.mul.Tensor(view_783, sigmoid_49);  view_783 = sigmoid_49 = None
    view_786 = torch.ops.aten.view.default(mul_79, [147456, 512]);  mul_79 = None
    view_787 = torch.ops.aten.view.default(view_786, [1, 384, 384, 512]);  view_786 = None
    transpose_16 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
    unsqueeze_357 = torch.ops.aten.unsqueeze.default(transpose_16, 3);  transpose_16 = None
    clone_99 = torch.ops.aten.clone.default(unsqueeze_357, memory_format = torch.contiguous_format);  unsqueeze_357 = None
    bitwise_not_55 = torch.ops.aten.bitwise_not.default(clone_99);  clone_99 = None
    masked_fill_55 = torch.ops.aten.masked_fill.Scalar(view_787, bitwise_not_55, 0);  view_787 = bitwise_not_55 = None
    view_788 = torch.ops.aten.view.default(masked_fill_55, [147456, 512]);  masked_fill_55 = None
    view_792 = torch.ops.aten.view.default(view_788, [1, 384, 384, 512])
    split_tensor_66 = torch.ops.aten.split.Tensor(view_792, 256, dim = -1);  view_792 = None
    getitem_670 = split_tensor_66[0];  split_tensor_66 = None
    unsqueeze_360 = torch.ops.aten.unsqueeze.default(getitem_670, 4);  getitem_670 = None
    permute_509 = torch.ops.aten.permute.default(unsqueeze_360, [0, 2, 4, 3, 1]);  unsqueeze_360 = None
    permute_510 = torch.ops.aten.permute.default(permute_509, [3, 1, 4, 0, 2]);  permute_509 = None
    view_793 = torch.ops.aten.view.default(permute_510, [256, 384, 384]);  permute_510 = None
    view_794 = torch.ops.aten.view.default(view_788, [1, 384, 384, 512]);  view_788 = None
    split_tensor_67 = torch.ops.aten.split.Tensor(view_794, 256, dim = -1);  view_794 = None
    getitem_673 = split_tensor_67[1];  split_tensor_67 = None
    unsqueeze_361 = torch.ops.aten.unsqueeze.default(getitem_673, 4);  getitem_673 = None
    permute_511 = torch.ops.aten.permute.default(unsqueeze_361, [0, 4, 2, 3, 1]);  unsqueeze_361 = None
    permute_512 = torch.ops.aten.permute.default(permute_511, [3, 4, 0, 2, 1]);  permute_511 = None
    view_795 = torch.ops.aten.view.default(permute_512, [256, 384, 384]);  permute_512 = None
    bmm_83 = torch.ops.aten.bmm.default(view_793, view_795);  view_793 = view_795 = None
    view_796 = torch.ops.aten.view.default(bmm_83, [256, 384, 1, 1, 384]);  bmm_83 = None
    permute_513 = torch.ops.aten.permute.default(view_796, [3, 1, 4, 0, 2]);  view_796 = None
    view_797 = torch.ops.aten.view.default(permute_513, [1, 384, 384, 256]);  permute_513 = None
    _to_copy_448 = torch.ops.aten._to_copy.default(view_781, dtype = torch.float32);  view_781 = None
    native_layer_norm_default_92 = torch.ops.aten.native_layer_norm.default(_to_copy_448, [256], None, None, 1e-05);  _to_copy_448 = None
    getitem_674 = native_layer_norm_default_92[0];  native_layer_norm_default_92 = None
    _to_copy_449 = torch.ops.aten._to_copy.default(view_797, dtype = torch.float32);  view_797 = None
    native_layer_norm_default_93 = torch.ops.aten.native_layer_norm.default(_to_copy_449, [256], None, None, 1e-05);  _to_copy_449 = None
    getitem_677 = native_layer_norm_default_93[0];  native_layer_norm_default_93 = None
    add_72 = torch.ops.aten.add.Tensor(getitem_674, getitem_677);  getitem_674 = getitem_677 = None
    _to_copy_450 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_2_triangle_multiplication_linear_z_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_2_triangle_multiplication_linear_z_out_weight = None
    _to_copy_451 = torch.ops.aten._to_copy.default(add_72, dtype = torch.bfloat16);  add_72 = None
    t_151 = torch.ops.aten.t.default(_to_copy_450);  _to_copy_450 = None
    view_798 = torch.ops.aten.view.default(_to_copy_451, [147456, 256]);  _to_copy_451 = None
    mm_139 = torch.ops.aten.mm.default(view_798, t_151);  view_798 = t_151 = None
    view_799 = torch.ops.aten.view.default(mm_139, [1, 384, 384, 256]);  mm_139 = None
    _to_copy_452 = torch.ops.aten._to_copy.default(getitem_661, dtype = torch.bfloat16);  getitem_661 = None
    _to_copy_453 = torch.ops.aten._to_copy.default(getitem_654, dtype = torch.bfloat16);  getitem_654 = None
    t_152 = torch.ops.aten.t.default(_to_copy_452);  _to_copy_452 = None
    view_800 = torch.ops.aten.view.default(_to_copy_453, [147456, 256]);  _to_copy_453 = None
    mm_140 = torch.ops.aten.mm.default(view_800, t_152);  view_800 = t_152 = None
    view_801 = torch.ops.aten.view.default(mm_140, [1, 384, 384, 256]);  mm_140 = None
    sigmoid_50 = torch.ops.aten.sigmoid.default(view_801);  view_801 = None
    mul_80 = torch.ops.aten.mul.Tensor(view_799, sigmoid_50);  view_799 = sigmoid_50 = None
    add_73 = torch.ops.aten.add.Tensor(add_67, mul_80);  mul_80 = None
    _to_copy_454 = torch.ops.aten._to_copy.default(add_67, dtype = torch.float32)
    native_layer_norm_default_94 = torch.ops.aten.native_layer_norm.default(_to_copy_454, [256], None, None, 1e-05);  _to_copy_454 = None
    getitem_680 = native_layer_norm_default_94[0];  native_layer_norm_default_94 = None
    _to_copy_455 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_2_triangle_attention_pair2b_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_2_triangle_attention_pair2b_weight = None
    _to_copy_456 = torch.ops.aten._to_copy.default(getitem_680, dtype = torch.bfloat16)
    t_153 = torch.ops.aten.t.default(_to_copy_455);  _to_copy_455 = None
    view_802 = torch.ops.aten.view.default(_to_copy_456, [147456, 256]);  _to_copy_456 = None
    mm_141 = torch.ops.aten.mm.default(view_802, t_153);  view_802 = t_153 = None
    view_803 = torch.ops.aten.view.default(mm_141, [1, 384, 384, 8]);  mm_141 = None
    view_804 = torch.ops.aten.view.default(view_803, [1, 384, 384, 2, 4]);  view_803 = None
    permute_514 = torch.ops.aten.permute.default(view_804, [0, 3, 4, 1, 2]);  view_804 = None
    view_805 = torch.ops.aten.view.default(permute_514, [1, 2, 4, 1, 384, 384]);  permute_514 = None
    view_806 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 384, 384])
    bitwise_not_56 = torch.ops.aten.bitwise_not.default(view_806);  view_806 = None
    masked_fill_56 = torch.ops.aten.masked_fill.Scalar(view_805, bitwise_not_56, -10000);  view_805 = bitwise_not_56 = None
    view_807 = torch.ops.aten.view.default(masked_fill_56, [1, 2, 4, 384, 384]);  masked_fill_56 = None
    permute_515 = torch.ops.aten.permute.default(view_807, [1, 0, 2, 3, 4]);  view_807 = None
    view_808 = torch.ops.aten.view.default(permute_515, [2, 4, 1, 384, 384]);  permute_515 = None
    _to_copy_457 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_2_triangle_attention_pair2qkvg1_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_2_triangle_attention_pair2qkvg1_weight = None
    _to_copy_458 = torch.ops.aten._to_copy.default(getitem_680, dtype = torch.bfloat16)
    t_154 = torch.ops.aten.t.default(_to_copy_457);  _to_copy_457 = None
    view_809 = torch.ops.aten.view.default(_to_copy_458, [147456, 256]);  _to_copy_458 = None
    mm_142 = torch.ops.aten.mm.default(view_809, t_154);  view_809 = t_154 = None
    view_810 = torch.ops.aten.view.default(mm_142, [1, 384, 384, 1024]);  mm_142 = None
    select_17 = torch.ops.aten.select.int(view_808, 0, 0)
    view_811 = torch.ops.aten.view.default(view_810, [1, 384, 384, 4, 4, 64]);  view_810 = None
    permute_516 = torch.ops.aten.permute.default(view_811, [4, 0, 3, 1, 2, 5]);  view_811 = None
    view_812 = torch.ops.aten.view.default(permute_516, [4, 4, 384, 384, 64]);  permute_516 = None
    unbind_int_40 = torch.ops.aten.unbind.int(view_812);  view_812 = None
    getitem_683 = unbind_int_40[0]
    getitem_684 = unbind_int_40[1]
    getitem_685 = unbind_int_40[2]
    getitem_686 = unbind_int_40[3];  unbind_int_40 = None
    expand_37 = torch.ops.aten.expand.default(select_17, [4, 384, 384, 384]);  select_17 = None
    _scaled_dot_product_efficient_attention_default_18 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_683, getitem_684, getitem_685, expand_37, False);  getitem_683 = getitem_684 = getitem_685 = expand_37 = None
    getitem_687 = _scaled_dot_product_efficient_attention_default_18[0];  _scaled_dot_product_efficient_attention_default_18 = None
    sigmoid_51 = torch.ops.aten.sigmoid.default(getitem_686);  getitem_686 = None
    mul_81 = torch.ops.aten.mul.Tensor(getitem_687, sigmoid_51);  getitem_687 = sigmoid_51 = None
    view_813 = torch.ops.aten.view.default(mul_81, [1, 4, 384, 384, 64]);  mul_81 = None
    permute_517 = torch.ops.aten.permute.default(view_813, [0, 2, 3, 1, 4]);  view_813 = None
    clone_100 = torch.ops.aten.clone.default(permute_517, memory_format = torch.contiguous_format);  permute_517 = None
    _unsafe_view_90 = torch.ops.aten._unsafe_view.default(clone_100, [1, 384, 384, 256]);  clone_100 = None
    transpose_17 = torch.ops.aten.transpose.int(getitem_680, 1, 2);  getitem_680 = None
    _to_copy_459 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_2_triangle_attention_pair2qkvg2_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_2_triangle_attention_pair2qkvg2_weight = None
    _to_copy_460 = torch.ops.aten._to_copy.default(transpose_17, dtype = torch.bfloat16);  transpose_17 = None
    t_155 = torch.ops.aten.t.default(_to_copy_459);  _to_copy_459 = None
    expand_38 = torch.ops.aten.expand.default(_to_copy_460, [1, 384, 384, 256]);  _to_copy_460 = None
    view_814 = torch.ops.aten.view.default(expand_38, [384, 384, 256]);  expand_38 = None
    expand_39 = torch.ops.aten.expand.default(t_155, [1, 384, 256, 1024]);  t_155 = None
    view_815 = torch.ops.aten.view.default(expand_39, [384, 256, 1024]);  expand_39 = None
    bmm_84 = torch.ops.aten.bmm.default(view_814, view_815);  view_814 = view_815 = None
    view_816 = torch.ops.aten.view.default(bmm_84, [1, 384, 384, 1024]);  bmm_84 = None
    select_18 = torch.ops.aten.select.int(view_808, 0, 1);  view_808 = None
    view_817 = torch.ops.aten.view.default(view_816, [1, 384, 384, 4, 4, 64]);  view_816 = None
    permute_518 = torch.ops.aten.permute.default(view_817, [4, 0, 3, 1, 2, 5]);  view_817 = None
    view_818 = torch.ops.aten.view.default(permute_518, [4, 4, 384, 384, 64]);  permute_518 = None
    unbind_int_41 = torch.ops.aten.unbind.int(view_818);  view_818 = None
    getitem_691 = unbind_int_41[0]
    getitem_692 = unbind_int_41[1]
    getitem_693 = unbind_int_41[2]
    getitem_694 = unbind_int_41[3];  unbind_int_41 = None
    expand_40 = torch.ops.aten.expand.default(select_18, [4, 384, 384, 384]);  select_18 = None
    _scaled_dot_product_efficient_attention_default_19 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_691, getitem_692, getitem_693, expand_40, False);  getitem_691 = getitem_692 = getitem_693 = expand_40 = None
    getitem_695 = _scaled_dot_product_efficient_attention_default_19[0];  _scaled_dot_product_efficient_attention_default_19 = None
    sigmoid_52 = torch.ops.aten.sigmoid.default(getitem_694);  getitem_694 = None
    mul_82 = torch.ops.aten.mul.Tensor(getitem_695, sigmoid_52);  getitem_695 = sigmoid_52 = None
    view_819 = torch.ops.aten.view.default(mul_82, [1, 4, 384, 384, 64]);  mul_82 = None
    permute_519 = torch.ops.aten.permute.default(view_819, [0, 2, 3, 1, 4]);  view_819 = None
    clone_101 = torch.ops.aten.clone.default(permute_519, memory_format = torch.contiguous_format);  permute_519 = None
    _unsafe_view_91 = torch.ops.aten._unsafe_view.default(clone_101, [1, 384, 384, 256]);  clone_101 = None
    cat_14 = torch.ops.aten.cat.default([_unsafe_view_90, _unsafe_view_91], dim = -1);  _unsafe_view_90 = _unsafe_view_91 = None
    slice_165 = torch.ops.aten.slice.Tensor(pairformer_stack_blocks_2_triangle_attention_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  pairformer_stack_blocks_2_triangle_attention_out_scalers = None
    unsqueeze_362 = torch.ops.aten.unsqueeze.default(slice_165, 1);  slice_165 = None
    mul_83 = torch.ops.aten.mul.Tensor(pairformer_stack_blocks_2_triangle_attention_linear_out_weight, unsqueeze_362);  pairformer_stack_blocks_2_triangle_attention_linear_out_weight = unsqueeze_362 = None
    _to_copy_461 = torch.ops.aten._to_copy.default(mul_83, dtype = torch.bfloat16);  mul_83 = None
    t_156 = torch.ops.aten.t.default(_to_copy_461);  _to_copy_461 = None
    view_820 = torch.ops.aten.view.default(cat_14, [147456, 512]);  cat_14 = None
    mm_143 = torch.ops.aten.mm.default(view_820, t_156);  view_820 = t_156 = None
    view_821 = torch.ops.aten.view.default(mm_143, [1, 384, 384, 256]);  mm_143 = None
    add_74 = torch.ops.aten.add.Tensor(add_73, view_821);  add_73 = view_821 = None
    split_tensor_68 = torch.ops.aten.split.Tensor(add_67, 384, dim = -2)
    getitem_699 = split_tensor_68[0];  split_tensor_68 = None
    _to_copy_462 = torch.ops.aten._to_copy.default(getitem_699, dtype = torch.float32);  getitem_699 = None
    native_layer_norm_default_95 = torch.ops.aten.native_layer_norm.default(_to_copy_462, [256], pairformer_stack_blocks_2_transition_pair_layer_norm_weight, pairformer_stack_blocks_2_transition_pair_layer_norm_bias, 1e-05);  _to_copy_462 = pairformer_stack_blocks_2_transition_pair_layer_norm_weight = pairformer_stack_blocks_2_transition_pair_layer_norm_bias = None
    getitem_700 = native_layer_norm_default_95[0];  native_layer_norm_default_95 = None
    _to_copy_463 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_2_transition_pair_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_2_transition_pair_linear_no_bias_ab_weight = None
    _to_copy_464 = torch.ops.aten._to_copy.default(getitem_700, dtype = torch.bfloat16);  getitem_700 = None
    t_157 = torch.ops.aten.t.default(_to_copy_463);  _to_copy_463 = None
    view_822 = torch.ops.aten.view.default(_to_copy_464, [147456, 256]);  _to_copy_464 = None
    mm_144 = torch.ops.aten.mm.default(view_822, t_157);  view_822 = t_157 = None
    view_823 = torch.ops.aten.view.default(mm_144, [1, 384, 384, 1024]);  mm_144 = None
    split_tensor_69 = torch.ops.aten.split.Tensor(view_823, 512, dim = -1);  view_823 = None
    getitem_703 = split_tensor_69[0]
    getitem_704 = split_tensor_69[1];  split_tensor_69 = None
    silu_19 = torch.ops.aten.silu.default(getitem_703);  getitem_703 = None
    mul_84 = torch.ops.aten.mul.Tensor(silu_19, getitem_704);  silu_19 = getitem_704 = None
    _to_copy_465 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_2_transition_pair_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_2_transition_pair_linear_out_weight = None
    t_158 = torch.ops.aten.t.default(_to_copy_465);  _to_copy_465 = None
    view_825 = torch.ops.aten.view.default(mul_84, [147456, 512]);  mul_84 = None
    mm_145 = torch.ops.aten.mm.default(view_825, t_158);  view_825 = t_158 = None
    view_826 = torch.ops.aten.view.default(mm_145, [1, 384, 384, 256]);  mm_145 = None
    add_75 = torch.ops.aten.add.Tensor(add_74, view_826);  add_74 = view_826 = None
    _to_copy_466 = torch.ops.aten._to_copy.default(add_71, dtype = torch.float32)
    native_layer_norm_default_96 = torch.ops.aten.native_layer_norm.default(_to_copy_466, [384], pairformer_stack_blocks_2_attention_pair_bias_single_layer_norm_weight, pairformer_stack_blocks_2_attention_pair_bias_single_layer_norm_bias, 1e-05);  _to_copy_466 = pairformer_stack_blocks_2_attention_pair_bias_single_layer_norm_weight = pairformer_stack_blocks_2_attention_pair_bias_single_layer_norm_bias = None
    getitem_705 = native_layer_norm_default_96[0];  native_layer_norm_default_96 = None
    _to_copy_467 = torch.ops.aten._to_copy.default(add_67, dtype = torch.float32);  add_67 = None
    native_layer_norm_default_97 = torch.ops.aten.native_layer_norm.default(_to_copy_467, [256], pairformer_stack_blocks_2_attention_pair_bias_pair_layer_norm_weight, pairformer_stack_blocks_2_attention_pair_bias_pair_layer_norm_bias, 1e-05);  _to_copy_467 = pairformer_stack_blocks_2_attention_pair_bias_pair_layer_norm_weight = pairformer_stack_blocks_2_attention_pair_bias_pair_layer_norm_bias = None
    getitem_708 = native_layer_norm_default_97[0];  native_layer_norm_default_97 = None
    _to_copy_468 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_2_attention_pair_bias_pair_linear_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_2_attention_pair_bias_pair_linear_weight = None
    _to_copy_469 = torch.ops.aten._to_copy.default(getitem_708, dtype = torch.bfloat16);  getitem_708 = None
    t_159 = torch.ops.aten.t.default(_to_copy_468);  _to_copy_468 = None
    view_827 = torch.ops.aten.view.default(_to_copy_469, [147456, 256]);  _to_copy_469 = None
    mm_146 = torch.ops.aten.mm.default(view_827, t_159);  view_827 = t_159 = None
    view_828 = torch.ops.aten.view.default(mm_146, [1, 384, 384, 16]);  mm_146 = None
    permute_520 = torch.ops.aten.permute.default(view_828, [0, 3, 1, 2]);  view_828 = None
    view_829 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384])
    bitwise_not_57 = torch.ops.aten.bitwise_not.default(view_829);  view_829 = None
    masked_fill_57 = torch.ops.aten.masked_fill.Scalar(permute_520, bitwise_not_57, -10000);  permute_520 = bitwise_not_57 = None
    _to_copy_470 = torch.ops.aten._to_copy.default(getitem_705, dtype = torch.bfloat16);  getitem_705 = None
    _to_copy_471 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_2_attention_pair_bias_attention_input2qkvg_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_2_attention_pair_bias_attention_input2qkvg_weight = None
    unsqueeze_363 = torch.ops.aten.unsqueeze.default(_to_copy_470, 3);  _to_copy_470 = None
    unsqueeze_364 = torch.ops.aten.unsqueeze.default(unsqueeze_363, 4);  unsqueeze_363 = None
    unsqueeze_365 = torch.ops.aten.unsqueeze.default(unsqueeze_364, 5);  unsqueeze_364 = None
    permute_521 = torch.ops.aten.permute.default(unsqueeze_365, [3, 0, 4, 1, 5, 2]);  unsqueeze_365 = None
    unsqueeze_366 = torch.ops.aten.unsqueeze.default(_to_copy_471, 4);  _to_copy_471 = None
    unsqueeze_367 = torch.ops.aten.unsqueeze.default(unsqueeze_366, 5);  unsqueeze_366 = None
    permute_522 = torch.ops.aten.permute.default(unsqueeze_367, [1, 4, 2, 5, 3, 0]);  unsqueeze_367 = None
    permute_523 = torch.ops.aten.permute.default(permute_521, [3, 5, 0, 1, 2, 4]);  permute_521 = None
    view_830 = torch.ops.aten.view.default(permute_523, [1, 384, 384]);  permute_523 = None
    permute_524 = torch.ops.aten.permute.default(permute_522, [5, 0, 1, 2, 4, 3]);  permute_522 = None
    view_831 = torch.ops.aten.view.default(permute_524, [1, 384, 1536]);  permute_524 = None
    bmm_85 = torch.ops.aten.bmm.default(view_830, view_831);  view_830 = view_831 = None
    view_832 = torch.ops.aten.view.default(bmm_85, [384, 1, 4, 1, 16, 24]);  bmm_85 = None
    permute_525 = torch.ops.aten.permute.default(view_832, [2, 3, 4, 0, 5, 1]);  view_832 = None
    view_833 = torch.ops.aten.view.default(permute_525, [4, 1, 16, 384, 24]);  permute_525 = None
    unbind_int_42 = torch.ops.aten.unbind.int(view_833);  view_833 = None
    getitem_711 = unbind_int_42[0]
    getitem_712 = unbind_int_42[1]
    getitem_713 = unbind_int_42[2]
    getitem_714 = unbind_int_42[3];  unbind_int_42 = None
    view_834 = torch.ops.aten.view.default(pairformer_stack_blocks_2_attention_pair_bias_attention_query_bias, [1, 16, 1, 24]);  pairformer_stack_blocks_2_attention_pair_bias_attention_query_bias = None
    add_76 = torch.ops.aten.add.Tensor(getitem_711, view_834);  getitem_711 = view_834 = None
    _to_copy_472 = torch.ops.aten._to_copy.default(add_76, dtype = torch.bfloat16);  add_76 = None
    expand_41 = torch.ops.aten.expand.default(masked_fill_57, [1, 16, 384, 384]);  masked_fill_57 = None
    _scaled_dot_product_efficient_attention_default_20 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_472, getitem_712, getitem_713, expand_41, False);  _to_copy_472 = getitem_712 = getitem_713 = expand_41 = None
    getitem_715 = _scaled_dot_product_efficient_attention_default_20[0];  _scaled_dot_product_efficient_attention_default_20 = None
    add_77 = torch.ops.aten.add.Tensor(getitem_714, 1);  getitem_714 = None
    sigmoid_53 = torch.ops.aten.sigmoid.default(add_77);  add_77 = None
    mul_85 = torch.ops.aten.mul.Tensor(getitem_715, sigmoid_53);  getitem_715 = sigmoid_53 = None
    _to_copy_473 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_2_attention_pair_bias_attention_output_proj_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_2_attention_pair_bias_attention_output_proj_weight = None
    unsqueeze_368 = torch.ops.aten.unsqueeze.default(mul_85, 4);  mul_85 = None
    permute_526 = torch.ops.aten.permute.default(unsqueeze_368, [0, 2, 4, 3, 1]);  unsqueeze_368 = None
    unsqueeze_369 = torch.ops.aten.unsqueeze.default(_to_copy_473, 3);  _to_copy_473 = None
    unsqueeze_370 = torch.ops.aten.unsqueeze.default(unsqueeze_369, 4);  unsqueeze_369 = None
    permute_527 = torch.ops.aten.permute.default(unsqueeze_370, [3, 4, 2, 1, 0]);  unsqueeze_370 = None
    permute_528 = torch.ops.aten.permute.default(permute_526, [1, 3, 4, 0, 2]);  permute_526 = None
    clone_102 = torch.ops.aten.clone.default(permute_528, memory_format = torch.contiguous_format);  permute_528 = None
    _unsafe_view_92 = torch.ops.aten._unsafe_view.default(clone_102, [1, 384, 384]);  clone_102 = None
    permute_529 = torch.ops.aten.permute.default(permute_527, [3, 4, 0, 2, 1]);  permute_527 = None
    clone_103 = torch.ops.aten.clone.default(permute_529, memory_format = torch.contiguous_format);  permute_529 = None
    _unsafe_view_93 = torch.ops.aten._unsafe_view.default(clone_103, [1, 384, 384]);  clone_103 = None
    bmm_86 = torch.ops.aten.bmm.default(_unsafe_view_92, _unsafe_view_93);  _unsafe_view_92 = _unsafe_view_93 = None
    view_835 = torch.ops.aten.view.default(bmm_86, [384, 1, 1, 1, 384]);  bmm_86 = None
    permute_530 = torch.ops.aten.permute.default(view_835, [3, 0, 4, 1, 2]);  view_835 = None
    view_836 = torch.ops.aten.view.default(permute_530, [1, 384, 384]);  permute_530 = None
    unsqueeze_371 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
    mul_86 = torch.ops.aten.mul.Tensor(view_836, unsqueeze_371);  view_836 = unsqueeze_371 = None
    add_78 = torch.ops.aten.add.Tensor(add_71, mul_86);  mul_86 = None
    split_tensor_70 = torch.ops.aten.split.Tensor(add_71, 384, dim = -2);  add_71 = None
    getitem_719 = split_tensor_70[0];  split_tensor_70 = None
    _to_copy_474 = torch.ops.aten._to_copy.default(getitem_719, dtype = torch.float32);  getitem_719 = None
    native_layer_norm_default_98 = torch.ops.aten.native_layer_norm.default(_to_copy_474, [384], pairformer_stack_blocks_2_transition_single_layer_norm_weight, pairformer_stack_blocks_2_transition_single_layer_norm_bias, 1e-05);  _to_copy_474 = pairformer_stack_blocks_2_transition_single_layer_norm_weight = pairformer_stack_blocks_2_transition_single_layer_norm_bias = None
    getitem_720 = native_layer_norm_default_98[0];  native_layer_norm_default_98 = None
    _to_copy_475 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_2_transition_single_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_2_transition_single_linear_no_bias_ab_weight = None
    _to_copy_476 = torch.ops.aten._to_copy.default(getitem_720, dtype = torch.bfloat16);  getitem_720 = None
    t_160 = torch.ops.aten.t.default(_to_copy_475);  _to_copy_475 = None
    view_837 = torch.ops.aten.view.default(_to_copy_476, [384, 384]);  _to_copy_476 = None
    mm_147 = torch.ops.aten.mm.default(view_837, t_160);  view_837 = t_160 = None
    view_838 = torch.ops.aten.view.default(mm_147, [1, 384, 1536]);  mm_147 = None
    split_tensor_71 = torch.ops.aten.split.Tensor(view_838, 768, dim = -1);  view_838 = None
    getitem_723 = split_tensor_71[0]
    getitem_724 = split_tensor_71[1];  split_tensor_71 = None
    silu_20 = torch.ops.aten.silu.default(getitem_723);  getitem_723 = None
    mul_87 = torch.ops.aten.mul.Tensor(silu_20, getitem_724);  silu_20 = getitem_724 = None
    _to_copy_477 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_2_transition_single_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_2_transition_single_linear_out_weight = None
    t_161 = torch.ops.aten.t.default(_to_copy_477);  _to_copy_477 = None
    view_840 = torch.ops.aten.view.default(mul_87, [384, 768]);  mul_87 = None
    mm_148 = torch.ops.aten.mm.default(view_840, t_161);  view_840 = t_161 = None
    view_841 = torch.ops.aten.view.default(mm_148, [1, 384, 384]);  mm_148 = None
    add_79 = torch.ops.aten.add.Tensor(add_78, view_841);  add_78 = view_841 = None
    _to_copy_478 = torch.ops.aten._to_copy.default(add_75, dtype = torch.float32)
    native_layer_norm_default_99 = torch.ops.aten.native_layer_norm.default(_to_copy_478, [256], pairformer_stack_blocks_3_triangle_multiplication_layernorm_z_in_weight, pairformer_stack_blocks_3_triangle_multiplication_layernorm_z_in_bias, 1e-05);  _to_copy_478 = pairformer_stack_blocks_3_triangle_multiplication_layernorm_z_in_weight = pairformer_stack_blocks_3_triangle_multiplication_layernorm_z_in_bias = None
    getitem_725 = native_layer_norm_default_99[0];  native_layer_norm_default_99 = None
    split_with_sizes_default_18 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_3_triangle_multiplication_merged_linear_p_weight, [512, 512]);  pairformer_stack_blocks_3_triangle_multiplication_merged_linear_p_weight = None
    getitem_728 = split_with_sizes_default_18[0]
    getitem_729 = split_with_sizes_default_18[1];  split_with_sizes_default_18 = None
    split_with_sizes_default_19 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_3_triangle_multiplication_merged_linear_g_weight, [512, 512, 256]);  pairformer_stack_blocks_3_triangle_multiplication_merged_linear_g_weight = None
    getitem_730 = split_with_sizes_default_19[0]
    getitem_731 = split_with_sizes_default_19[1]
    getitem_732 = split_with_sizes_default_19[2];  split_with_sizes_default_19 = None
    _to_copy_479 = torch.ops.aten._to_copy.default(getitem_728, dtype = torch.bfloat16);  getitem_728 = None
    _to_copy_480 = torch.ops.aten._to_copy.default(getitem_725, dtype = torch.bfloat16)
    t_162 = torch.ops.aten.t.default(_to_copy_479);  _to_copy_479 = None
    view_842 = torch.ops.aten.view.default(_to_copy_480, [147456, 256]);  _to_copy_480 = None
    mm_149 = torch.ops.aten.mm.default(view_842, t_162);  view_842 = t_162 = None
    view_843 = torch.ops.aten.view.default(mm_149, [1, 384, 384, 512]);  mm_149 = None
    _to_copy_481 = torch.ops.aten._to_copy.default(getitem_730, dtype = torch.bfloat16);  getitem_730 = None
    _to_copy_482 = torch.ops.aten._to_copy.default(getitem_725, dtype = torch.bfloat16)
    t_163 = torch.ops.aten.t.default(_to_copy_481);  _to_copy_481 = None
    view_844 = torch.ops.aten.view.default(_to_copy_482, [147456, 256]);  _to_copy_482 = None
    mm_150 = torch.ops.aten.mm.default(view_844, t_163);  view_844 = t_163 = None
    view_845 = torch.ops.aten.view.default(mm_150, [1, 384, 384, 512]);  mm_150 = None
    sigmoid_54 = torch.ops.aten.sigmoid.default(view_845);  view_845 = None
    mul_88 = torch.ops.aten.mul.Tensor(view_843, sigmoid_54);  view_843 = sigmoid_54 = None
    unsqueeze_372 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
    bitwise_not_58 = torch.ops.aten.bitwise_not.default(unsqueeze_372);  unsqueeze_372 = None
    masked_fill_58 = torch.ops.aten.masked_fill.Scalar(mul_88, bitwise_not_58, 0);  mul_88 = bitwise_not_58 = None
    split_tensor_72 = torch.ops.aten.split.Tensor(masked_fill_58, 256, dim = -1)
    getitem_735 = split_tensor_72[0];  split_tensor_72 = None
    unsqueeze_375 = torch.ops.aten.unsqueeze.default(getitem_735, 4);  getitem_735 = None
    permute_535 = torch.ops.aten.permute.default(unsqueeze_375, [0, 1, 4, 3, 2]);  unsqueeze_375 = None
    permute_536 = torch.ops.aten.permute.default(permute_535, [3, 1, 4, 0, 2]);  permute_535 = None
    view_848 = torch.ops.aten.view.default(permute_536, [256, 384, 384]);  permute_536 = None
    split_tensor_73 = torch.ops.aten.split.Tensor(masked_fill_58, 256, dim = -1);  masked_fill_58 = None
    getitem_738 = split_tensor_73[1];  split_tensor_73 = None
    unsqueeze_376 = torch.ops.aten.unsqueeze.default(getitem_738, 4);  getitem_738 = None
    permute_537 = torch.ops.aten.permute.default(unsqueeze_376, [0, 4, 1, 3, 2]);  unsqueeze_376 = None
    permute_538 = torch.ops.aten.permute.default(permute_537, [3, 4, 0, 2, 1]);  permute_537 = None
    view_849 = torch.ops.aten.view.default(permute_538, [256, 384, 384]);  permute_538 = None
    bmm_87 = torch.ops.aten.bmm.default(view_848, view_849);  view_848 = view_849 = None
    view_850 = torch.ops.aten.view.default(bmm_87, [256, 384, 1, 1, 384]);  bmm_87 = None
    permute_539 = torch.ops.aten.permute.default(view_850, [3, 1, 4, 0, 2]);  view_850 = None
    view_851 = torch.ops.aten.view.default(permute_539, [1, 384, 384, 256]);  permute_539 = None
    _to_copy_483 = torch.ops.aten._to_copy.default(getitem_729, dtype = torch.bfloat16);  getitem_729 = None
    _to_copy_484 = torch.ops.aten._to_copy.default(getitem_725, dtype = torch.bfloat16)
    t_164 = torch.ops.aten.t.default(_to_copy_483);  _to_copy_483 = None
    view_852 = torch.ops.aten.view.default(_to_copy_484, [147456, 256]);  _to_copy_484 = None
    mm_151 = torch.ops.aten.mm.default(view_852, t_164);  view_852 = t_164 = None
    view_853 = torch.ops.aten.view.default(mm_151, [1, 384, 384, 512]);  mm_151 = None
    _to_copy_485 = torch.ops.aten._to_copy.default(getitem_731, dtype = torch.bfloat16);  getitem_731 = None
    _to_copy_486 = torch.ops.aten._to_copy.default(getitem_725, dtype = torch.bfloat16)
    t_165 = torch.ops.aten.t.default(_to_copy_485);  _to_copy_485 = None
    view_854 = torch.ops.aten.view.default(_to_copy_486, [147456, 256]);  _to_copy_486 = None
    mm_152 = torch.ops.aten.mm.default(view_854, t_165);  view_854 = t_165 = None
    view_855 = torch.ops.aten.view.default(mm_152, [1, 384, 384, 512]);  mm_152 = None
    sigmoid_55 = torch.ops.aten.sigmoid.default(view_855);  view_855 = None
    mul_89 = torch.ops.aten.mul.Tensor(view_853, sigmoid_55);  view_853 = sigmoid_55 = None
    view_856 = torch.ops.aten.view.default(mul_89, [147456, 512]);  mul_89 = None
    view_857 = torch.ops.aten.view.default(view_856, [1, 384, 384, 512]);  view_856 = None
    transpose_18 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
    unsqueeze_377 = torch.ops.aten.unsqueeze.default(transpose_18, 3);  transpose_18 = None
    clone_104 = torch.ops.aten.clone.default(unsqueeze_377, memory_format = torch.contiguous_format);  unsqueeze_377 = None
    bitwise_not_59 = torch.ops.aten.bitwise_not.default(clone_104);  clone_104 = None
    masked_fill_59 = torch.ops.aten.masked_fill.Scalar(view_857, bitwise_not_59, 0);  view_857 = bitwise_not_59 = None
    view_858 = torch.ops.aten.view.default(masked_fill_59, [147456, 512]);  masked_fill_59 = None
    view_862 = torch.ops.aten.view.default(view_858, [1, 384, 384, 512])
    split_tensor_74 = torch.ops.aten.split.Tensor(view_862, 256, dim = -1);  view_862 = None
    getitem_741 = split_tensor_74[0];  split_tensor_74 = None
    unsqueeze_380 = torch.ops.aten.unsqueeze.default(getitem_741, 4);  getitem_741 = None
    permute_544 = torch.ops.aten.permute.default(unsqueeze_380, [0, 2, 4, 3, 1]);  unsqueeze_380 = None
    permute_545 = torch.ops.aten.permute.default(permute_544, [3, 1, 4, 0, 2]);  permute_544 = None
    view_863 = torch.ops.aten.view.default(permute_545, [256, 384, 384]);  permute_545 = None
    view_864 = torch.ops.aten.view.default(view_858, [1, 384, 384, 512]);  view_858 = None
    split_tensor_75 = torch.ops.aten.split.Tensor(view_864, 256, dim = -1);  view_864 = None
    getitem_744 = split_tensor_75[1];  split_tensor_75 = None
    unsqueeze_381 = torch.ops.aten.unsqueeze.default(getitem_744, 4);  getitem_744 = None
    permute_546 = torch.ops.aten.permute.default(unsqueeze_381, [0, 4, 2, 3, 1]);  unsqueeze_381 = None
    permute_547 = torch.ops.aten.permute.default(permute_546, [3, 4, 0, 2, 1]);  permute_546 = None
    view_865 = torch.ops.aten.view.default(permute_547, [256, 384, 384]);  permute_547 = None
    bmm_88 = torch.ops.aten.bmm.default(view_863, view_865);  view_863 = view_865 = None
    view_866 = torch.ops.aten.view.default(bmm_88, [256, 384, 1, 1, 384]);  bmm_88 = None
    permute_548 = torch.ops.aten.permute.default(view_866, [3, 1, 4, 0, 2]);  view_866 = None
    view_867 = torch.ops.aten.view.default(permute_548, [1, 384, 384, 256]);  permute_548 = None
    _to_copy_487 = torch.ops.aten._to_copy.default(view_851, dtype = torch.float32);  view_851 = None
    native_layer_norm_default_100 = torch.ops.aten.native_layer_norm.default(_to_copy_487, [256], None, None, 1e-05);  _to_copy_487 = None
    getitem_745 = native_layer_norm_default_100[0];  native_layer_norm_default_100 = None
    _to_copy_488 = torch.ops.aten._to_copy.default(view_867, dtype = torch.float32);  view_867 = None
    native_layer_norm_default_101 = torch.ops.aten.native_layer_norm.default(_to_copy_488, [256], None, None, 1e-05);  _to_copy_488 = None
    getitem_748 = native_layer_norm_default_101[0];  native_layer_norm_default_101 = None
    add_80 = torch.ops.aten.add.Tensor(getitem_745, getitem_748);  getitem_745 = getitem_748 = None
    _to_copy_489 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_3_triangle_multiplication_linear_z_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_3_triangle_multiplication_linear_z_out_weight = None
    _to_copy_490 = torch.ops.aten._to_copy.default(add_80, dtype = torch.bfloat16);  add_80 = None
    t_166 = torch.ops.aten.t.default(_to_copy_489);  _to_copy_489 = None
    view_868 = torch.ops.aten.view.default(_to_copy_490, [147456, 256]);  _to_copy_490 = None
    mm_153 = torch.ops.aten.mm.default(view_868, t_166);  view_868 = t_166 = None
    view_869 = torch.ops.aten.view.default(mm_153, [1, 384, 384, 256]);  mm_153 = None
    _to_copy_491 = torch.ops.aten._to_copy.default(getitem_732, dtype = torch.bfloat16);  getitem_732 = None
    _to_copy_492 = torch.ops.aten._to_copy.default(getitem_725, dtype = torch.bfloat16);  getitem_725 = None
    t_167 = torch.ops.aten.t.default(_to_copy_491);  _to_copy_491 = None
    view_870 = torch.ops.aten.view.default(_to_copy_492, [147456, 256]);  _to_copy_492 = None
    mm_154 = torch.ops.aten.mm.default(view_870, t_167);  view_870 = t_167 = None
    view_871 = torch.ops.aten.view.default(mm_154, [1, 384, 384, 256]);  mm_154 = None
    sigmoid_56 = torch.ops.aten.sigmoid.default(view_871);  view_871 = None
    mul_90 = torch.ops.aten.mul.Tensor(view_869, sigmoid_56);  view_869 = sigmoid_56 = None
    add_81 = torch.ops.aten.add.Tensor(add_75, mul_90);  mul_90 = None
    _to_copy_493 = torch.ops.aten._to_copy.default(add_75, dtype = torch.float32)
    native_layer_norm_default_102 = torch.ops.aten.native_layer_norm.default(_to_copy_493, [256], None, None, 1e-05);  _to_copy_493 = None
    getitem_751 = native_layer_norm_default_102[0];  native_layer_norm_default_102 = None
    _to_copy_494 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_3_triangle_attention_pair2b_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_3_triangle_attention_pair2b_weight = None
    _to_copy_495 = torch.ops.aten._to_copy.default(getitem_751, dtype = torch.bfloat16)
    t_168 = torch.ops.aten.t.default(_to_copy_494);  _to_copy_494 = None
    view_872 = torch.ops.aten.view.default(_to_copy_495, [147456, 256]);  _to_copy_495 = None
    mm_155 = torch.ops.aten.mm.default(view_872, t_168);  view_872 = t_168 = None
    view_873 = torch.ops.aten.view.default(mm_155, [1, 384, 384, 8]);  mm_155 = None
    view_874 = torch.ops.aten.view.default(view_873, [1, 384, 384, 2, 4]);  view_873 = None
    permute_549 = torch.ops.aten.permute.default(view_874, [0, 3, 4, 1, 2]);  view_874 = None
    view_875 = torch.ops.aten.view.default(permute_549, [1, 2, 4, 1, 384, 384]);  permute_549 = None
    view_876 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 384, 384])
    bitwise_not_60 = torch.ops.aten.bitwise_not.default(view_876);  view_876 = None
    masked_fill_60 = torch.ops.aten.masked_fill.Scalar(view_875, bitwise_not_60, -10000);  view_875 = bitwise_not_60 = None
    view_877 = torch.ops.aten.view.default(masked_fill_60, [1, 2, 4, 384, 384]);  masked_fill_60 = None
    permute_550 = torch.ops.aten.permute.default(view_877, [1, 0, 2, 3, 4]);  view_877 = None
    view_878 = torch.ops.aten.view.default(permute_550, [2, 4, 1, 384, 384]);  permute_550 = None
    _to_copy_496 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_3_triangle_attention_pair2qkvg1_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_3_triangle_attention_pair2qkvg1_weight = None
    _to_copy_497 = torch.ops.aten._to_copy.default(getitem_751, dtype = torch.bfloat16)
    t_169 = torch.ops.aten.t.default(_to_copy_496);  _to_copy_496 = None
    view_879 = torch.ops.aten.view.default(_to_copy_497, [147456, 256]);  _to_copy_497 = None
    mm_156 = torch.ops.aten.mm.default(view_879, t_169);  view_879 = t_169 = None
    view_880 = torch.ops.aten.view.default(mm_156, [1, 384, 384, 1024]);  mm_156 = None
    select_19 = torch.ops.aten.select.int(view_878, 0, 0)
    view_881 = torch.ops.aten.view.default(view_880, [1, 384, 384, 4, 4, 64]);  view_880 = None
    permute_551 = torch.ops.aten.permute.default(view_881, [4, 0, 3, 1, 2, 5]);  view_881 = None
    view_882 = torch.ops.aten.view.default(permute_551, [4, 4, 384, 384, 64]);  permute_551 = None
    unbind_int_43 = torch.ops.aten.unbind.int(view_882);  view_882 = None
    getitem_754 = unbind_int_43[0]
    getitem_755 = unbind_int_43[1]
    getitem_756 = unbind_int_43[2]
    getitem_757 = unbind_int_43[3];  unbind_int_43 = None
    expand_42 = torch.ops.aten.expand.default(select_19, [4, 384, 384, 384]);  select_19 = None
    _scaled_dot_product_efficient_attention_default_21 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_754, getitem_755, getitem_756, expand_42, False);  getitem_754 = getitem_755 = getitem_756 = expand_42 = None
    getitem_758 = _scaled_dot_product_efficient_attention_default_21[0];  _scaled_dot_product_efficient_attention_default_21 = None
    sigmoid_57 = torch.ops.aten.sigmoid.default(getitem_757);  getitem_757 = None
    mul_91 = torch.ops.aten.mul.Tensor(getitem_758, sigmoid_57);  getitem_758 = sigmoid_57 = None
    view_883 = torch.ops.aten.view.default(mul_91, [1, 4, 384, 384, 64]);  mul_91 = None
    permute_552 = torch.ops.aten.permute.default(view_883, [0, 2, 3, 1, 4]);  view_883 = None
    clone_105 = torch.ops.aten.clone.default(permute_552, memory_format = torch.contiguous_format);  permute_552 = None
    _unsafe_view_94 = torch.ops.aten._unsafe_view.default(clone_105, [1, 384, 384, 256]);  clone_105 = None
    transpose_19 = torch.ops.aten.transpose.int(getitem_751, 1, 2);  getitem_751 = None
    _to_copy_498 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_3_triangle_attention_pair2qkvg2_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_3_triangle_attention_pair2qkvg2_weight = None
    _to_copy_499 = torch.ops.aten._to_copy.default(transpose_19, dtype = torch.bfloat16);  transpose_19 = None
    t_170 = torch.ops.aten.t.default(_to_copy_498);  _to_copy_498 = None
    expand_43 = torch.ops.aten.expand.default(_to_copy_499, [1, 384, 384, 256]);  _to_copy_499 = None
    view_884 = torch.ops.aten.view.default(expand_43, [384, 384, 256]);  expand_43 = None
    expand_44 = torch.ops.aten.expand.default(t_170, [1, 384, 256, 1024]);  t_170 = None
    view_885 = torch.ops.aten.view.default(expand_44, [384, 256, 1024]);  expand_44 = None
    bmm_89 = torch.ops.aten.bmm.default(view_884, view_885);  view_884 = view_885 = None
    view_886 = torch.ops.aten.view.default(bmm_89, [1, 384, 384, 1024]);  bmm_89 = None
    select_20 = torch.ops.aten.select.int(view_878, 0, 1);  view_878 = None
    view_887 = torch.ops.aten.view.default(view_886, [1, 384, 384, 4, 4, 64]);  view_886 = None
    permute_553 = torch.ops.aten.permute.default(view_887, [4, 0, 3, 1, 2, 5]);  view_887 = None
    view_888 = torch.ops.aten.view.default(permute_553, [4, 4, 384, 384, 64]);  permute_553 = None
    unbind_int_44 = torch.ops.aten.unbind.int(view_888);  view_888 = None
    getitem_762 = unbind_int_44[0]
    getitem_763 = unbind_int_44[1]
    getitem_764 = unbind_int_44[2]
    getitem_765 = unbind_int_44[3];  unbind_int_44 = None
    expand_45 = torch.ops.aten.expand.default(select_20, [4, 384, 384, 384]);  select_20 = None
    _scaled_dot_product_efficient_attention_default_22 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_762, getitem_763, getitem_764, expand_45, False);  getitem_762 = getitem_763 = getitem_764 = expand_45 = None
    getitem_766 = _scaled_dot_product_efficient_attention_default_22[0];  _scaled_dot_product_efficient_attention_default_22 = None
    sigmoid_58 = torch.ops.aten.sigmoid.default(getitem_765);  getitem_765 = None
    mul_92 = torch.ops.aten.mul.Tensor(getitem_766, sigmoid_58);  getitem_766 = sigmoid_58 = None
    view_889 = torch.ops.aten.view.default(mul_92, [1, 4, 384, 384, 64]);  mul_92 = None
    permute_554 = torch.ops.aten.permute.default(view_889, [0, 2, 3, 1, 4]);  view_889 = None
    clone_106 = torch.ops.aten.clone.default(permute_554, memory_format = torch.contiguous_format);  permute_554 = None
    _unsafe_view_95 = torch.ops.aten._unsafe_view.default(clone_106, [1, 384, 384, 256]);  clone_106 = None
    cat_15 = torch.ops.aten.cat.default([_unsafe_view_94, _unsafe_view_95], dim = -1);  _unsafe_view_94 = _unsafe_view_95 = None
    slice_166 = torch.ops.aten.slice.Tensor(pairformer_stack_blocks_3_triangle_attention_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  pairformer_stack_blocks_3_triangle_attention_out_scalers = None
    unsqueeze_382 = torch.ops.aten.unsqueeze.default(slice_166, 1);  slice_166 = None
    mul_93 = torch.ops.aten.mul.Tensor(pairformer_stack_blocks_3_triangle_attention_linear_out_weight, unsqueeze_382);  pairformer_stack_blocks_3_triangle_attention_linear_out_weight = unsqueeze_382 = None
    _to_copy_500 = torch.ops.aten._to_copy.default(mul_93, dtype = torch.bfloat16);  mul_93 = None
    t_171 = torch.ops.aten.t.default(_to_copy_500);  _to_copy_500 = None
    view_890 = torch.ops.aten.view.default(cat_15, [147456, 512]);  cat_15 = None
    mm_157 = torch.ops.aten.mm.default(view_890, t_171);  view_890 = t_171 = None
    view_891 = torch.ops.aten.view.default(mm_157, [1, 384, 384, 256]);  mm_157 = None
    add_82 = torch.ops.aten.add.Tensor(add_81, view_891);  add_81 = view_891 = None
    split_tensor_76 = torch.ops.aten.split.Tensor(add_75, 384, dim = -2)
    getitem_770 = split_tensor_76[0];  split_tensor_76 = None
    _to_copy_501 = torch.ops.aten._to_copy.default(getitem_770, dtype = torch.float32);  getitem_770 = None
    native_layer_norm_default_103 = torch.ops.aten.native_layer_norm.default(_to_copy_501, [256], pairformer_stack_blocks_3_transition_pair_layer_norm_weight, pairformer_stack_blocks_3_transition_pair_layer_norm_bias, 1e-05);  _to_copy_501 = pairformer_stack_blocks_3_transition_pair_layer_norm_weight = pairformer_stack_blocks_3_transition_pair_layer_norm_bias = None
    getitem_771 = native_layer_norm_default_103[0];  native_layer_norm_default_103 = None
    _to_copy_502 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_3_transition_pair_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_3_transition_pair_linear_no_bias_ab_weight = None
    _to_copy_503 = torch.ops.aten._to_copy.default(getitem_771, dtype = torch.bfloat16);  getitem_771 = None
    t_172 = torch.ops.aten.t.default(_to_copy_502);  _to_copy_502 = None
    view_892 = torch.ops.aten.view.default(_to_copy_503, [147456, 256]);  _to_copy_503 = None
    mm_158 = torch.ops.aten.mm.default(view_892, t_172);  view_892 = t_172 = None
    view_893 = torch.ops.aten.view.default(mm_158, [1, 384, 384, 1024]);  mm_158 = None
    split_tensor_77 = torch.ops.aten.split.Tensor(view_893, 512, dim = -1);  view_893 = None
    getitem_774 = split_tensor_77[0]
    getitem_775 = split_tensor_77[1];  split_tensor_77 = None
    silu_21 = torch.ops.aten.silu.default(getitem_774);  getitem_774 = None
    mul_94 = torch.ops.aten.mul.Tensor(silu_21, getitem_775);  silu_21 = getitem_775 = None
    _to_copy_504 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_3_transition_pair_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_3_transition_pair_linear_out_weight = None
    t_173 = torch.ops.aten.t.default(_to_copy_504);  _to_copy_504 = None
    view_895 = torch.ops.aten.view.default(mul_94, [147456, 512]);  mul_94 = None
    mm_159 = torch.ops.aten.mm.default(view_895, t_173);  view_895 = t_173 = None
    view_896 = torch.ops.aten.view.default(mm_159, [1, 384, 384, 256]);  mm_159 = None
    add_83 = torch.ops.aten.add.Tensor(add_82, view_896);  add_82 = view_896 = None
    _to_copy_505 = torch.ops.aten._to_copy.default(add_79, dtype = torch.float32)
    native_layer_norm_default_104 = torch.ops.aten.native_layer_norm.default(_to_copy_505, [384], pairformer_stack_blocks_3_attention_pair_bias_single_layer_norm_weight, pairformer_stack_blocks_3_attention_pair_bias_single_layer_norm_bias, 1e-05);  _to_copy_505 = pairformer_stack_blocks_3_attention_pair_bias_single_layer_norm_weight = pairformer_stack_blocks_3_attention_pair_bias_single_layer_norm_bias = None
    getitem_776 = native_layer_norm_default_104[0];  native_layer_norm_default_104 = None
    _to_copy_506 = torch.ops.aten._to_copy.default(add_75, dtype = torch.float32);  add_75 = None
    native_layer_norm_default_105 = torch.ops.aten.native_layer_norm.default(_to_copy_506, [256], pairformer_stack_blocks_3_attention_pair_bias_pair_layer_norm_weight, pairformer_stack_blocks_3_attention_pair_bias_pair_layer_norm_bias, 1e-05);  _to_copy_506 = pairformer_stack_blocks_3_attention_pair_bias_pair_layer_norm_weight = pairformer_stack_blocks_3_attention_pair_bias_pair_layer_norm_bias = None
    getitem_779 = native_layer_norm_default_105[0];  native_layer_norm_default_105 = None
    _to_copy_507 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_3_attention_pair_bias_pair_linear_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_3_attention_pair_bias_pair_linear_weight = None
    _to_copy_508 = torch.ops.aten._to_copy.default(getitem_779, dtype = torch.bfloat16);  getitem_779 = None
    t_174 = torch.ops.aten.t.default(_to_copy_507);  _to_copy_507 = None
    view_897 = torch.ops.aten.view.default(_to_copy_508, [147456, 256]);  _to_copy_508 = None
    mm_160 = torch.ops.aten.mm.default(view_897, t_174);  view_897 = t_174 = None
    view_898 = torch.ops.aten.view.default(mm_160, [1, 384, 384, 16]);  mm_160 = None
    permute_555 = torch.ops.aten.permute.default(view_898, [0, 3, 1, 2]);  view_898 = None
    view_899 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384])
    bitwise_not_61 = torch.ops.aten.bitwise_not.default(view_899);  view_899 = None
    masked_fill_61 = torch.ops.aten.masked_fill.Scalar(permute_555, bitwise_not_61, -10000);  permute_555 = bitwise_not_61 = None
    _to_copy_509 = torch.ops.aten._to_copy.default(getitem_776, dtype = torch.bfloat16);  getitem_776 = None
    _to_copy_510 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_3_attention_pair_bias_attention_input2qkvg_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_3_attention_pair_bias_attention_input2qkvg_weight = None
    unsqueeze_383 = torch.ops.aten.unsqueeze.default(_to_copy_509, 3);  _to_copy_509 = None
    unsqueeze_384 = torch.ops.aten.unsqueeze.default(unsqueeze_383, 4);  unsqueeze_383 = None
    unsqueeze_385 = torch.ops.aten.unsqueeze.default(unsqueeze_384, 5);  unsqueeze_384 = None
    permute_556 = torch.ops.aten.permute.default(unsqueeze_385, [3, 0, 4, 1, 5, 2]);  unsqueeze_385 = None
    unsqueeze_386 = torch.ops.aten.unsqueeze.default(_to_copy_510, 4);  _to_copy_510 = None
    unsqueeze_387 = torch.ops.aten.unsqueeze.default(unsqueeze_386, 5);  unsqueeze_386 = None
    permute_557 = torch.ops.aten.permute.default(unsqueeze_387, [1, 4, 2, 5, 3, 0]);  unsqueeze_387 = None
    permute_558 = torch.ops.aten.permute.default(permute_556, [3, 5, 0, 1, 2, 4]);  permute_556 = None
    view_900 = torch.ops.aten.view.default(permute_558, [1, 384, 384]);  permute_558 = None
    permute_559 = torch.ops.aten.permute.default(permute_557, [5, 0, 1, 2, 4, 3]);  permute_557 = None
    view_901 = torch.ops.aten.view.default(permute_559, [1, 384, 1536]);  permute_559 = None
    bmm_90 = torch.ops.aten.bmm.default(view_900, view_901);  view_900 = view_901 = None
    view_902 = torch.ops.aten.view.default(bmm_90, [384, 1, 4, 1, 16, 24]);  bmm_90 = None
    permute_560 = torch.ops.aten.permute.default(view_902, [2, 3, 4, 0, 5, 1]);  view_902 = None
    view_903 = torch.ops.aten.view.default(permute_560, [4, 1, 16, 384, 24]);  permute_560 = None
    unbind_int_45 = torch.ops.aten.unbind.int(view_903);  view_903 = None
    getitem_782 = unbind_int_45[0]
    getitem_783 = unbind_int_45[1]
    getitem_784 = unbind_int_45[2]
    getitem_785 = unbind_int_45[3];  unbind_int_45 = None
    view_904 = torch.ops.aten.view.default(pairformer_stack_blocks_3_attention_pair_bias_attention_query_bias, [1, 16, 1, 24]);  pairformer_stack_blocks_3_attention_pair_bias_attention_query_bias = None
    add_84 = torch.ops.aten.add.Tensor(getitem_782, view_904);  getitem_782 = view_904 = None
    _to_copy_511 = torch.ops.aten._to_copy.default(add_84, dtype = torch.bfloat16);  add_84 = None
    expand_46 = torch.ops.aten.expand.default(masked_fill_61, [1, 16, 384, 384]);  masked_fill_61 = None
    _scaled_dot_product_efficient_attention_default_23 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_511, getitem_783, getitem_784, expand_46, False);  _to_copy_511 = getitem_783 = getitem_784 = expand_46 = None
    getitem_786 = _scaled_dot_product_efficient_attention_default_23[0];  _scaled_dot_product_efficient_attention_default_23 = None
    add_85 = torch.ops.aten.add.Tensor(getitem_785, 1);  getitem_785 = None
    sigmoid_59 = torch.ops.aten.sigmoid.default(add_85);  add_85 = None
    mul_95 = torch.ops.aten.mul.Tensor(getitem_786, sigmoid_59);  getitem_786 = sigmoid_59 = None
    _to_copy_512 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_3_attention_pair_bias_attention_output_proj_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_3_attention_pair_bias_attention_output_proj_weight = None
    unsqueeze_388 = torch.ops.aten.unsqueeze.default(mul_95, 4);  mul_95 = None
    permute_561 = torch.ops.aten.permute.default(unsqueeze_388, [0, 2, 4, 3, 1]);  unsqueeze_388 = None
    unsqueeze_389 = torch.ops.aten.unsqueeze.default(_to_copy_512, 3);  _to_copy_512 = None
    unsqueeze_390 = torch.ops.aten.unsqueeze.default(unsqueeze_389, 4);  unsqueeze_389 = None
    permute_562 = torch.ops.aten.permute.default(unsqueeze_390, [3, 4, 2, 1, 0]);  unsqueeze_390 = None
    permute_563 = torch.ops.aten.permute.default(permute_561, [1, 3, 4, 0, 2]);  permute_561 = None
    clone_107 = torch.ops.aten.clone.default(permute_563, memory_format = torch.contiguous_format);  permute_563 = None
    _unsafe_view_96 = torch.ops.aten._unsafe_view.default(clone_107, [1, 384, 384]);  clone_107 = None
    permute_564 = torch.ops.aten.permute.default(permute_562, [3, 4, 0, 2, 1]);  permute_562 = None
    clone_108 = torch.ops.aten.clone.default(permute_564, memory_format = torch.contiguous_format);  permute_564 = None
    _unsafe_view_97 = torch.ops.aten._unsafe_view.default(clone_108, [1, 384, 384]);  clone_108 = None
    bmm_91 = torch.ops.aten.bmm.default(_unsafe_view_96, _unsafe_view_97);  _unsafe_view_96 = _unsafe_view_97 = None
    view_905 = torch.ops.aten.view.default(bmm_91, [384, 1, 1, 1, 384]);  bmm_91 = None
    permute_565 = torch.ops.aten.permute.default(view_905, [3, 0, 4, 1, 2]);  view_905 = None
    view_906 = torch.ops.aten.view.default(permute_565, [1, 384, 384]);  permute_565 = None
    unsqueeze_391 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
    mul_96 = torch.ops.aten.mul.Tensor(view_906, unsqueeze_391);  view_906 = unsqueeze_391 = None
    add_86 = torch.ops.aten.add.Tensor(add_79, mul_96);  mul_96 = None
    split_tensor_78 = torch.ops.aten.split.Tensor(add_79, 384, dim = -2);  add_79 = None
    getitem_790 = split_tensor_78[0];  split_tensor_78 = None
    _to_copy_513 = torch.ops.aten._to_copy.default(getitem_790, dtype = torch.float32);  getitem_790 = None
    native_layer_norm_default_106 = torch.ops.aten.native_layer_norm.default(_to_copy_513, [384], pairformer_stack_blocks_3_transition_single_layer_norm_weight, pairformer_stack_blocks_3_transition_single_layer_norm_bias, 1e-05);  _to_copy_513 = pairformer_stack_blocks_3_transition_single_layer_norm_weight = pairformer_stack_blocks_3_transition_single_layer_norm_bias = None
    getitem_791 = native_layer_norm_default_106[0];  native_layer_norm_default_106 = None
    _to_copy_514 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_3_transition_single_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_3_transition_single_linear_no_bias_ab_weight = None
    _to_copy_515 = torch.ops.aten._to_copy.default(getitem_791, dtype = torch.bfloat16);  getitem_791 = None
    t_175 = torch.ops.aten.t.default(_to_copy_514);  _to_copy_514 = None
    view_907 = torch.ops.aten.view.default(_to_copy_515, [384, 384]);  _to_copy_515 = None
    mm_161 = torch.ops.aten.mm.default(view_907, t_175);  view_907 = t_175 = None
    view_908 = torch.ops.aten.view.default(mm_161, [1, 384, 1536]);  mm_161 = None
    split_tensor_79 = torch.ops.aten.split.Tensor(view_908, 768, dim = -1);  view_908 = None
    getitem_794 = split_tensor_79[0]
    getitem_795 = split_tensor_79[1];  split_tensor_79 = None
    silu_22 = torch.ops.aten.silu.default(getitem_794);  getitem_794 = None
    mul_97 = torch.ops.aten.mul.Tensor(silu_22, getitem_795);  silu_22 = getitem_795 = None
    _to_copy_516 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_3_transition_single_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_3_transition_single_linear_out_weight = None
    t_176 = torch.ops.aten.t.default(_to_copy_516);  _to_copy_516 = None
    view_910 = torch.ops.aten.view.default(mul_97, [384, 768]);  mul_97 = None
    mm_162 = torch.ops.aten.mm.default(view_910, t_176);  view_910 = t_176 = None
    view_911 = torch.ops.aten.view.default(mm_162, [1, 384, 384]);  mm_162 = None
    add_87 = torch.ops.aten.add.Tensor(add_86, view_911);  add_86 = view_911 = None
    _to_copy_517 = torch.ops.aten._to_copy.default(add_83, dtype = torch.float32)
    native_layer_norm_default_107 = torch.ops.aten.native_layer_norm.default(_to_copy_517, [256], pairformer_stack_blocks_4_triangle_multiplication_layernorm_z_in_weight, pairformer_stack_blocks_4_triangle_multiplication_layernorm_z_in_bias, 1e-05);  _to_copy_517 = pairformer_stack_blocks_4_triangle_multiplication_layernorm_z_in_weight = pairformer_stack_blocks_4_triangle_multiplication_layernorm_z_in_bias = None
    getitem_796 = native_layer_norm_default_107[0];  native_layer_norm_default_107 = None
    split_with_sizes_default_20 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_4_triangle_multiplication_merged_linear_p_weight, [512, 512]);  pairformer_stack_blocks_4_triangle_multiplication_merged_linear_p_weight = None
    getitem_799 = split_with_sizes_default_20[0]
    getitem_800 = split_with_sizes_default_20[1];  split_with_sizes_default_20 = None
    split_with_sizes_default_21 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_4_triangle_multiplication_merged_linear_g_weight, [512, 512, 256]);  pairformer_stack_blocks_4_triangle_multiplication_merged_linear_g_weight = None
    getitem_801 = split_with_sizes_default_21[0]
    getitem_802 = split_with_sizes_default_21[1]
    getitem_803 = split_with_sizes_default_21[2];  split_with_sizes_default_21 = None
    _to_copy_518 = torch.ops.aten._to_copy.default(getitem_799, dtype = torch.bfloat16);  getitem_799 = None
    _to_copy_519 = torch.ops.aten._to_copy.default(getitem_796, dtype = torch.bfloat16)
    t_177 = torch.ops.aten.t.default(_to_copy_518);  _to_copy_518 = None
    view_912 = torch.ops.aten.view.default(_to_copy_519, [147456, 256]);  _to_copy_519 = None
    mm_163 = torch.ops.aten.mm.default(view_912, t_177);  view_912 = t_177 = None
    view_913 = torch.ops.aten.view.default(mm_163, [1, 384, 384, 512]);  mm_163 = None
    _to_copy_520 = torch.ops.aten._to_copy.default(getitem_801, dtype = torch.bfloat16);  getitem_801 = None
    _to_copy_521 = torch.ops.aten._to_copy.default(getitem_796, dtype = torch.bfloat16)
    t_178 = torch.ops.aten.t.default(_to_copy_520);  _to_copy_520 = None
    view_914 = torch.ops.aten.view.default(_to_copy_521, [147456, 256]);  _to_copy_521 = None
    mm_164 = torch.ops.aten.mm.default(view_914, t_178);  view_914 = t_178 = None
    view_915 = torch.ops.aten.view.default(mm_164, [1, 384, 384, 512]);  mm_164 = None
    sigmoid_60 = torch.ops.aten.sigmoid.default(view_915);  view_915 = None
    mul_98 = torch.ops.aten.mul.Tensor(view_913, sigmoid_60);  view_913 = sigmoid_60 = None
    unsqueeze_392 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
    bitwise_not_62 = torch.ops.aten.bitwise_not.default(unsqueeze_392);  unsqueeze_392 = None
    masked_fill_62 = torch.ops.aten.masked_fill.Scalar(mul_98, bitwise_not_62, 0);  mul_98 = bitwise_not_62 = None
    split_tensor_80 = torch.ops.aten.split.Tensor(masked_fill_62, 256, dim = -1)
    getitem_806 = split_tensor_80[0];  split_tensor_80 = None
    unsqueeze_395 = torch.ops.aten.unsqueeze.default(getitem_806, 4);  getitem_806 = None
    permute_570 = torch.ops.aten.permute.default(unsqueeze_395, [0, 1, 4, 3, 2]);  unsqueeze_395 = None
    permute_571 = torch.ops.aten.permute.default(permute_570, [3, 1, 4, 0, 2]);  permute_570 = None
    view_918 = torch.ops.aten.view.default(permute_571, [256, 384, 384]);  permute_571 = None
    split_tensor_81 = torch.ops.aten.split.Tensor(masked_fill_62, 256, dim = -1);  masked_fill_62 = None
    getitem_809 = split_tensor_81[1];  split_tensor_81 = None
    unsqueeze_396 = torch.ops.aten.unsqueeze.default(getitem_809, 4);  getitem_809 = None
    permute_572 = torch.ops.aten.permute.default(unsqueeze_396, [0, 4, 1, 3, 2]);  unsqueeze_396 = None
    permute_573 = torch.ops.aten.permute.default(permute_572, [3, 4, 0, 2, 1]);  permute_572 = None
    view_919 = torch.ops.aten.view.default(permute_573, [256, 384, 384]);  permute_573 = None
    bmm_92 = torch.ops.aten.bmm.default(view_918, view_919);  view_918 = view_919 = None
    view_920 = torch.ops.aten.view.default(bmm_92, [256, 384, 1, 1, 384]);  bmm_92 = None
    permute_574 = torch.ops.aten.permute.default(view_920, [3, 1, 4, 0, 2]);  view_920 = None
    view_921 = torch.ops.aten.view.default(permute_574, [1, 384, 384, 256]);  permute_574 = None
    _to_copy_522 = torch.ops.aten._to_copy.default(getitem_800, dtype = torch.bfloat16);  getitem_800 = None
    _to_copy_523 = torch.ops.aten._to_copy.default(getitem_796, dtype = torch.bfloat16)
    t_179 = torch.ops.aten.t.default(_to_copy_522);  _to_copy_522 = None
    view_922 = torch.ops.aten.view.default(_to_copy_523, [147456, 256]);  _to_copy_523 = None
    mm_165 = torch.ops.aten.mm.default(view_922, t_179);  view_922 = t_179 = None
    view_923 = torch.ops.aten.view.default(mm_165, [1, 384, 384, 512]);  mm_165 = None
    _to_copy_524 = torch.ops.aten._to_copy.default(getitem_802, dtype = torch.bfloat16);  getitem_802 = None
    _to_copy_525 = torch.ops.aten._to_copy.default(getitem_796, dtype = torch.bfloat16)
    t_180 = torch.ops.aten.t.default(_to_copy_524);  _to_copy_524 = None
    view_924 = torch.ops.aten.view.default(_to_copy_525, [147456, 256]);  _to_copy_525 = None
    mm_166 = torch.ops.aten.mm.default(view_924, t_180);  view_924 = t_180 = None
    view_925 = torch.ops.aten.view.default(mm_166, [1, 384, 384, 512]);  mm_166 = None
    sigmoid_61 = torch.ops.aten.sigmoid.default(view_925);  view_925 = None
    mul_99 = torch.ops.aten.mul.Tensor(view_923, sigmoid_61);  view_923 = sigmoid_61 = None
    view_926 = torch.ops.aten.view.default(mul_99, [147456, 512]);  mul_99 = None
    view_927 = torch.ops.aten.view.default(view_926, [1, 384, 384, 512]);  view_926 = None
    transpose_20 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
    unsqueeze_397 = torch.ops.aten.unsqueeze.default(transpose_20, 3);  transpose_20 = None
    clone_109 = torch.ops.aten.clone.default(unsqueeze_397, memory_format = torch.contiguous_format);  unsqueeze_397 = None
    bitwise_not_63 = torch.ops.aten.bitwise_not.default(clone_109);  clone_109 = None
    masked_fill_63 = torch.ops.aten.masked_fill.Scalar(view_927, bitwise_not_63, 0);  view_927 = bitwise_not_63 = None
    view_928 = torch.ops.aten.view.default(masked_fill_63, [147456, 512]);  masked_fill_63 = None
    view_932 = torch.ops.aten.view.default(view_928, [1, 384, 384, 512])
    split_tensor_82 = torch.ops.aten.split.Tensor(view_932, 256, dim = -1);  view_932 = None
    getitem_812 = split_tensor_82[0];  split_tensor_82 = None
    unsqueeze_400 = torch.ops.aten.unsqueeze.default(getitem_812, 4);  getitem_812 = None
    permute_579 = torch.ops.aten.permute.default(unsqueeze_400, [0, 2, 4, 3, 1]);  unsqueeze_400 = None
    permute_580 = torch.ops.aten.permute.default(permute_579, [3, 1, 4, 0, 2]);  permute_579 = None
    view_933 = torch.ops.aten.view.default(permute_580, [256, 384, 384]);  permute_580 = None
    view_934 = torch.ops.aten.view.default(view_928, [1, 384, 384, 512]);  view_928 = None
    split_tensor_83 = torch.ops.aten.split.Tensor(view_934, 256, dim = -1);  view_934 = None
    getitem_815 = split_tensor_83[1];  split_tensor_83 = None
    unsqueeze_401 = torch.ops.aten.unsqueeze.default(getitem_815, 4);  getitem_815 = None
    permute_581 = torch.ops.aten.permute.default(unsqueeze_401, [0, 4, 2, 3, 1]);  unsqueeze_401 = None
    permute_582 = torch.ops.aten.permute.default(permute_581, [3, 4, 0, 2, 1]);  permute_581 = None
    view_935 = torch.ops.aten.view.default(permute_582, [256, 384, 384]);  permute_582 = None
    bmm_93 = torch.ops.aten.bmm.default(view_933, view_935);  view_933 = view_935 = None
    view_936 = torch.ops.aten.view.default(bmm_93, [256, 384, 1, 1, 384]);  bmm_93 = None
    permute_583 = torch.ops.aten.permute.default(view_936, [3, 1, 4, 0, 2]);  view_936 = None
    view_937 = torch.ops.aten.view.default(permute_583, [1, 384, 384, 256]);  permute_583 = None
    _to_copy_526 = torch.ops.aten._to_copy.default(view_921, dtype = torch.float32);  view_921 = None
    native_layer_norm_default_108 = torch.ops.aten.native_layer_norm.default(_to_copy_526, [256], None, None, 1e-05);  _to_copy_526 = None
    getitem_816 = native_layer_norm_default_108[0];  native_layer_norm_default_108 = None
    _to_copy_527 = torch.ops.aten._to_copy.default(view_937, dtype = torch.float32);  view_937 = None
    native_layer_norm_default_109 = torch.ops.aten.native_layer_norm.default(_to_copy_527, [256], None, None, 1e-05);  _to_copy_527 = None
    getitem_819 = native_layer_norm_default_109[0];  native_layer_norm_default_109 = None
    add_88 = torch.ops.aten.add.Tensor(getitem_816, getitem_819);  getitem_816 = getitem_819 = None
    _to_copy_528 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_4_triangle_multiplication_linear_z_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_4_triangle_multiplication_linear_z_out_weight = None
    _to_copy_529 = torch.ops.aten._to_copy.default(add_88, dtype = torch.bfloat16);  add_88 = None
    t_181 = torch.ops.aten.t.default(_to_copy_528);  _to_copy_528 = None
    view_938 = torch.ops.aten.view.default(_to_copy_529, [147456, 256]);  _to_copy_529 = None
    mm_167 = torch.ops.aten.mm.default(view_938, t_181);  view_938 = t_181 = None
    view_939 = torch.ops.aten.view.default(mm_167, [1, 384, 384, 256]);  mm_167 = None
    _to_copy_530 = torch.ops.aten._to_copy.default(getitem_803, dtype = torch.bfloat16);  getitem_803 = None
    _to_copy_531 = torch.ops.aten._to_copy.default(getitem_796, dtype = torch.bfloat16);  getitem_796 = None
    t_182 = torch.ops.aten.t.default(_to_copy_530);  _to_copy_530 = None
    view_940 = torch.ops.aten.view.default(_to_copy_531, [147456, 256]);  _to_copy_531 = None
    mm_168 = torch.ops.aten.mm.default(view_940, t_182);  view_940 = t_182 = None
    view_941 = torch.ops.aten.view.default(mm_168, [1, 384, 384, 256]);  mm_168 = None
    sigmoid_62 = torch.ops.aten.sigmoid.default(view_941);  view_941 = None
    mul_100 = torch.ops.aten.mul.Tensor(view_939, sigmoid_62);  view_939 = sigmoid_62 = None
    add_89 = torch.ops.aten.add.Tensor(add_83, mul_100);  mul_100 = None
    _to_copy_532 = torch.ops.aten._to_copy.default(add_83, dtype = torch.float32)
    native_layer_norm_default_110 = torch.ops.aten.native_layer_norm.default(_to_copy_532, [256], None, None, 1e-05);  _to_copy_532 = None
    getitem_822 = native_layer_norm_default_110[0];  native_layer_norm_default_110 = None
    _to_copy_533 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_4_triangle_attention_pair2b_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_4_triangle_attention_pair2b_weight = None
    _to_copy_534 = torch.ops.aten._to_copy.default(getitem_822, dtype = torch.bfloat16)
    t_183 = torch.ops.aten.t.default(_to_copy_533);  _to_copy_533 = None
    view_942 = torch.ops.aten.view.default(_to_copy_534, [147456, 256]);  _to_copy_534 = None
    mm_169 = torch.ops.aten.mm.default(view_942, t_183);  view_942 = t_183 = None
    view_943 = torch.ops.aten.view.default(mm_169, [1, 384, 384, 8]);  mm_169 = None
    view_944 = torch.ops.aten.view.default(view_943, [1, 384, 384, 2, 4]);  view_943 = None
    permute_584 = torch.ops.aten.permute.default(view_944, [0, 3, 4, 1, 2]);  view_944 = None
    view_945 = torch.ops.aten.view.default(permute_584, [1, 2, 4, 1, 384, 384]);  permute_584 = None
    view_946 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 384, 384])
    bitwise_not_64 = torch.ops.aten.bitwise_not.default(view_946);  view_946 = None
    masked_fill_64 = torch.ops.aten.masked_fill.Scalar(view_945, bitwise_not_64, -10000);  view_945 = bitwise_not_64 = None
    view_947 = torch.ops.aten.view.default(masked_fill_64, [1, 2, 4, 384, 384]);  masked_fill_64 = None
    permute_585 = torch.ops.aten.permute.default(view_947, [1, 0, 2, 3, 4]);  view_947 = None
    view_948 = torch.ops.aten.view.default(permute_585, [2, 4, 1, 384, 384]);  permute_585 = None
    _to_copy_535 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_4_triangle_attention_pair2qkvg1_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_4_triangle_attention_pair2qkvg1_weight = None
    _to_copy_536 = torch.ops.aten._to_copy.default(getitem_822, dtype = torch.bfloat16)
    t_184 = torch.ops.aten.t.default(_to_copy_535);  _to_copy_535 = None
    view_949 = torch.ops.aten.view.default(_to_copy_536, [147456, 256]);  _to_copy_536 = None
    mm_170 = torch.ops.aten.mm.default(view_949, t_184);  view_949 = t_184 = None
    view_950 = torch.ops.aten.view.default(mm_170, [1, 384, 384, 1024]);  mm_170 = None
    select_21 = torch.ops.aten.select.int(view_948, 0, 0)
    view_951 = torch.ops.aten.view.default(view_950, [1, 384, 384, 4, 4, 64]);  view_950 = None
    permute_586 = torch.ops.aten.permute.default(view_951, [4, 0, 3, 1, 2, 5]);  view_951 = None
    view_952 = torch.ops.aten.view.default(permute_586, [4, 4, 384, 384, 64]);  permute_586 = None
    unbind_int_46 = torch.ops.aten.unbind.int(view_952);  view_952 = None
    getitem_825 = unbind_int_46[0]
    getitem_826 = unbind_int_46[1]
    getitem_827 = unbind_int_46[2]
    getitem_828 = unbind_int_46[3];  unbind_int_46 = None
    expand_47 = torch.ops.aten.expand.default(select_21, [4, 384, 384, 384]);  select_21 = None
    _scaled_dot_product_efficient_attention_default_24 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_825, getitem_826, getitem_827, expand_47, False);  getitem_825 = getitem_826 = getitem_827 = expand_47 = None
    getitem_829 = _scaled_dot_product_efficient_attention_default_24[0];  _scaled_dot_product_efficient_attention_default_24 = None
    sigmoid_63 = torch.ops.aten.sigmoid.default(getitem_828);  getitem_828 = None
    mul_101 = torch.ops.aten.mul.Tensor(getitem_829, sigmoid_63);  getitem_829 = sigmoid_63 = None
    view_953 = torch.ops.aten.view.default(mul_101, [1, 4, 384, 384, 64]);  mul_101 = None
    permute_587 = torch.ops.aten.permute.default(view_953, [0, 2, 3, 1, 4]);  view_953 = None
    clone_110 = torch.ops.aten.clone.default(permute_587, memory_format = torch.contiguous_format);  permute_587 = None
    _unsafe_view_98 = torch.ops.aten._unsafe_view.default(clone_110, [1, 384, 384, 256]);  clone_110 = None
    transpose_21 = torch.ops.aten.transpose.int(getitem_822, 1, 2);  getitem_822 = None
    _to_copy_537 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_4_triangle_attention_pair2qkvg2_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_4_triangle_attention_pair2qkvg2_weight = None
    _to_copy_538 = torch.ops.aten._to_copy.default(transpose_21, dtype = torch.bfloat16);  transpose_21 = None
    t_185 = torch.ops.aten.t.default(_to_copy_537);  _to_copy_537 = None
    expand_48 = torch.ops.aten.expand.default(_to_copy_538, [1, 384, 384, 256]);  _to_copy_538 = None
    view_954 = torch.ops.aten.view.default(expand_48, [384, 384, 256]);  expand_48 = None
    expand_49 = torch.ops.aten.expand.default(t_185, [1, 384, 256, 1024]);  t_185 = None
    view_955 = torch.ops.aten.view.default(expand_49, [384, 256, 1024]);  expand_49 = None
    bmm_94 = torch.ops.aten.bmm.default(view_954, view_955);  view_954 = view_955 = None
    view_956 = torch.ops.aten.view.default(bmm_94, [1, 384, 384, 1024]);  bmm_94 = None
    select_22 = torch.ops.aten.select.int(view_948, 0, 1);  view_948 = None
    view_957 = torch.ops.aten.view.default(view_956, [1, 384, 384, 4, 4, 64]);  view_956 = None
    permute_588 = torch.ops.aten.permute.default(view_957, [4, 0, 3, 1, 2, 5]);  view_957 = None
    view_958 = torch.ops.aten.view.default(permute_588, [4, 4, 384, 384, 64]);  permute_588 = None
    unbind_int_47 = torch.ops.aten.unbind.int(view_958);  view_958 = None
    getitem_833 = unbind_int_47[0]
    getitem_834 = unbind_int_47[1]
    getitem_835 = unbind_int_47[2]
    getitem_836 = unbind_int_47[3];  unbind_int_47 = None
    expand_50 = torch.ops.aten.expand.default(select_22, [4, 384, 384, 384]);  select_22 = None
    _scaled_dot_product_efficient_attention_default_25 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_833, getitem_834, getitem_835, expand_50, False);  getitem_833 = getitem_834 = getitem_835 = expand_50 = None
    getitem_837 = _scaled_dot_product_efficient_attention_default_25[0];  _scaled_dot_product_efficient_attention_default_25 = None
    sigmoid_64 = torch.ops.aten.sigmoid.default(getitem_836);  getitem_836 = None
    mul_102 = torch.ops.aten.mul.Tensor(getitem_837, sigmoid_64);  getitem_837 = sigmoid_64 = None
    view_959 = torch.ops.aten.view.default(mul_102, [1, 4, 384, 384, 64]);  mul_102 = None
    permute_589 = torch.ops.aten.permute.default(view_959, [0, 2, 3, 1, 4]);  view_959 = None
    clone_111 = torch.ops.aten.clone.default(permute_589, memory_format = torch.contiguous_format);  permute_589 = None
    _unsafe_view_99 = torch.ops.aten._unsafe_view.default(clone_111, [1, 384, 384, 256]);  clone_111 = None
    cat_16 = torch.ops.aten.cat.default([_unsafe_view_98, _unsafe_view_99], dim = -1);  _unsafe_view_98 = _unsafe_view_99 = None
    slice_167 = torch.ops.aten.slice.Tensor(pairformer_stack_blocks_4_triangle_attention_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  pairformer_stack_blocks_4_triangle_attention_out_scalers = None
    unsqueeze_402 = torch.ops.aten.unsqueeze.default(slice_167, 1);  slice_167 = None
    mul_103 = torch.ops.aten.mul.Tensor(pairformer_stack_blocks_4_triangle_attention_linear_out_weight, unsqueeze_402);  pairformer_stack_blocks_4_triangle_attention_linear_out_weight = unsqueeze_402 = None
    _to_copy_539 = torch.ops.aten._to_copy.default(mul_103, dtype = torch.bfloat16);  mul_103 = None
    t_186 = torch.ops.aten.t.default(_to_copy_539);  _to_copy_539 = None
    view_960 = torch.ops.aten.view.default(cat_16, [147456, 512]);  cat_16 = None
    mm_171 = torch.ops.aten.mm.default(view_960, t_186);  view_960 = t_186 = None
    view_961 = torch.ops.aten.view.default(mm_171, [1, 384, 384, 256]);  mm_171 = None
    add_90 = torch.ops.aten.add.Tensor(add_89, view_961);  add_89 = view_961 = None
    split_tensor_84 = torch.ops.aten.split.Tensor(add_83, 384, dim = -2)
    getitem_841 = split_tensor_84[0];  split_tensor_84 = None
    _to_copy_540 = torch.ops.aten._to_copy.default(getitem_841, dtype = torch.float32);  getitem_841 = None
    native_layer_norm_default_111 = torch.ops.aten.native_layer_norm.default(_to_copy_540, [256], pairformer_stack_blocks_4_transition_pair_layer_norm_weight, pairformer_stack_blocks_4_transition_pair_layer_norm_bias, 1e-05);  _to_copy_540 = pairformer_stack_blocks_4_transition_pair_layer_norm_weight = pairformer_stack_blocks_4_transition_pair_layer_norm_bias = None
    getitem_842 = native_layer_norm_default_111[0];  native_layer_norm_default_111 = None
    _to_copy_541 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_4_transition_pair_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_4_transition_pair_linear_no_bias_ab_weight = None
    _to_copy_542 = torch.ops.aten._to_copy.default(getitem_842, dtype = torch.bfloat16);  getitem_842 = None
    t_187 = torch.ops.aten.t.default(_to_copy_541);  _to_copy_541 = None
    view_962 = torch.ops.aten.view.default(_to_copy_542, [147456, 256]);  _to_copy_542 = None
    mm_172 = torch.ops.aten.mm.default(view_962, t_187);  view_962 = t_187 = None
    view_963 = torch.ops.aten.view.default(mm_172, [1, 384, 384, 1024]);  mm_172 = None
    split_tensor_85 = torch.ops.aten.split.Tensor(view_963, 512, dim = -1);  view_963 = None
    getitem_845 = split_tensor_85[0]
    getitem_846 = split_tensor_85[1];  split_tensor_85 = None
    silu_23 = torch.ops.aten.silu.default(getitem_845);  getitem_845 = None
    mul_104 = torch.ops.aten.mul.Tensor(silu_23, getitem_846);  silu_23 = getitem_846 = None
    _to_copy_543 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_4_transition_pair_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_4_transition_pair_linear_out_weight = None
    t_188 = torch.ops.aten.t.default(_to_copy_543);  _to_copy_543 = None
    view_965 = torch.ops.aten.view.default(mul_104, [147456, 512]);  mul_104 = None
    mm_173 = torch.ops.aten.mm.default(view_965, t_188);  view_965 = t_188 = None
    view_966 = torch.ops.aten.view.default(mm_173, [1, 384, 384, 256]);  mm_173 = None
    add_91 = torch.ops.aten.add.Tensor(add_90, view_966);  add_90 = view_966 = None
    _to_copy_544 = torch.ops.aten._to_copy.default(add_87, dtype = torch.float32)
    native_layer_norm_default_112 = torch.ops.aten.native_layer_norm.default(_to_copy_544, [384], pairformer_stack_blocks_4_attention_pair_bias_single_layer_norm_weight, pairformer_stack_blocks_4_attention_pair_bias_single_layer_norm_bias, 1e-05);  _to_copy_544 = pairformer_stack_blocks_4_attention_pair_bias_single_layer_norm_weight = pairformer_stack_blocks_4_attention_pair_bias_single_layer_norm_bias = None
    getitem_847 = native_layer_norm_default_112[0];  native_layer_norm_default_112 = None
    _to_copy_545 = torch.ops.aten._to_copy.default(add_83, dtype = torch.float32);  add_83 = None
    native_layer_norm_default_113 = torch.ops.aten.native_layer_norm.default(_to_copy_545, [256], pairformer_stack_blocks_4_attention_pair_bias_pair_layer_norm_weight, pairformer_stack_blocks_4_attention_pair_bias_pair_layer_norm_bias, 1e-05);  _to_copy_545 = pairformer_stack_blocks_4_attention_pair_bias_pair_layer_norm_weight = pairformer_stack_blocks_4_attention_pair_bias_pair_layer_norm_bias = None
    getitem_850 = native_layer_norm_default_113[0];  native_layer_norm_default_113 = None
    _to_copy_546 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_4_attention_pair_bias_pair_linear_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_4_attention_pair_bias_pair_linear_weight = None
    _to_copy_547 = torch.ops.aten._to_copy.default(getitem_850, dtype = torch.bfloat16);  getitem_850 = None
    t_189 = torch.ops.aten.t.default(_to_copy_546);  _to_copy_546 = None
    view_967 = torch.ops.aten.view.default(_to_copy_547, [147456, 256]);  _to_copy_547 = None
    mm_174 = torch.ops.aten.mm.default(view_967, t_189);  view_967 = t_189 = None
    view_968 = torch.ops.aten.view.default(mm_174, [1, 384, 384, 16]);  mm_174 = None
    permute_590 = torch.ops.aten.permute.default(view_968, [0, 3, 1, 2]);  view_968 = None
    view_969 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384])
    bitwise_not_65 = torch.ops.aten.bitwise_not.default(view_969);  view_969 = None
    masked_fill_65 = torch.ops.aten.masked_fill.Scalar(permute_590, bitwise_not_65, -10000);  permute_590 = bitwise_not_65 = None
    _to_copy_548 = torch.ops.aten._to_copy.default(getitem_847, dtype = torch.bfloat16);  getitem_847 = None
    _to_copy_549 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_4_attention_pair_bias_attention_input2qkvg_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_4_attention_pair_bias_attention_input2qkvg_weight = None
    unsqueeze_403 = torch.ops.aten.unsqueeze.default(_to_copy_548, 3);  _to_copy_548 = None
    unsqueeze_404 = torch.ops.aten.unsqueeze.default(unsqueeze_403, 4);  unsqueeze_403 = None
    unsqueeze_405 = torch.ops.aten.unsqueeze.default(unsqueeze_404, 5);  unsqueeze_404 = None
    permute_591 = torch.ops.aten.permute.default(unsqueeze_405, [3, 0, 4, 1, 5, 2]);  unsqueeze_405 = None
    unsqueeze_406 = torch.ops.aten.unsqueeze.default(_to_copy_549, 4);  _to_copy_549 = None
    unsqueeze_407 = torch.ops.aten.unsqueeze.default(unsqueeze_406, 5);  unsqueeze_406 = None
    permute_592 = torch.ops.aten.permute.default(unsqueeze_407, [1, 4, 2, 5, 3, 0]);  unsqueeze_407 = None
    permute_593 = torch.ops.aten.permute.default(permute_591, [3, 5, 0, 1, 2, 4]);  permute_591 = None
    view_970 = torch.ops.aten.view.default(permute_593, [1, 384, 384]);  permute_593 = None
    permute_594 = torch.ops.aten.permute.default(permute_592, [5, 0, 1, 2, 4, 3]);  permute_592 = None
    view_971 = torch.ops.aten.view.default(permute_594, [1, 384, 1536]);  permute_594 = None
    bmm_95 = torch.ops.aten.bmm.default(view_970, view_971);  view_970 = view_971 = None
    view_972 = torch.ops.aten.view.default(bmm_95, [384, 1, 4, 1, 16, 24]);  bmm_95 = None
    permute_595 = torch.ops.aten.permute.default(view_972, [2, 3, 4, 0, 5, 1]);  view_972 = None
    view_973 = torch.ops.aten.view.default(permute_595, [4, 1, 16, 384, 24]);  permute_595 = None
    unbind_int_48 = torch.ops.aten.unbind.int(view_973);  view_973 = None
    getitem_853 = unbind_int_48[0]
    getitem_854 = unbind_int_48[1]
    getitem_855 = unbind_int_48[2]
    getitem_856 = unbind_int_48[3];  unbind_int_48 = None
    view_974 = torch.ops.aten.view.default(pairformer_stack_blocks_4_attention_pair_bias_attention_query_bias, [1, 16, 1, 24]);  pairformer_stack_blocks_4_attention_pair_bias_attention_query_bias = None
    add_92 = torch.ops.aten.add.Tensor(getitem_853, view_974);  getitem_853 = view_974 = None
    _to_copy_550 = torch.ops.aten._to_copy.default(add_92, dtype = torch.bfloat16);  add_92 = None
    expand_51 = torch.ops.aten.expand.default(masked_fill_65, [1, 16, 384, 384]);  masked_fill_65 = None
    _scaled_dot_product_efficient_attention_default_26 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_550, getitem_854, getitem_855, expand_51, False);  _to_copy_550 = getitem_854 = getitem_855 = expand_51 = None
    getitem_857 = _scaled_dot_product_efficient_attention_default_26[0];  _scaled_dot_product_efficient_attention_default_26 = None
    add_93 = torch.ops.aten.add.Tensor(getitem_856, 1);  getitem_856 = None
    sigmoid_65 = torch.ops.aten.sigmoid.default(add_93);  add_93 = None
    mul_105 = torch.ops.aten.mul.Tensor(getitem_857, sigmoid_65);  getitem_857 = sigmoid_65 = None
    _to_copy_551 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_4_attention_pair_bias_attention_output_proj_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_4_attention_pair_bias_attention_output_proj_weight = None
    unsqueeze_408 = torch.ops.aten.unsqueeze.default(mul_105, 4);  mul_105 = None
    permute_596 = torch.ops.aten.permute.default(unsqueeze_408, [0, 2, 4, 3, 1]);  unsqueeze_408 = None
    unsqueeze_409 = torch.ops.aten.unsqueeze.default(_to_copy_551, 3);  _to_copy_551 = None
    unsqueeze_410 = torch.ops.aten.unsqueeze.default(unsqueeze_409, 4);  unsqueeze_409 = None
    permute_597 = torch.ops.aten.permute.default(unsqueeze_410, [3, 4, 2, 1, 0]);  unsqueeze_410 = None
    permute_598 = torch.ops.aten.permute.default(permute_596, [1, 3, 4, 0, 2]);  permute_596 = None
    clone_112 = torch.ops.aten.clone.default(permute_598, memory_format = torch.contiguous_format);  permute_598 = None
    _unsafe_view_100 = torch.ops.aten._unsafe_view.default(clone_112, [1, 384, 384]);  clone_112 = None
    permute_599 = torch.ops.aten.permute.default(permute_597, [3, 4, 0, 2, 1]);  permute_597 = None
    clone_113 = torch.ops.aten.clone.default(permute_599, memory_format = torch.contiguous_format);  permute_599 = None
    _unsafe_view_101 = torch.ops.aten._unsafe_view.default(clone_113, [1, 384, 384]);  clone_113 = None
    bmm_96 = torch.ops.aten.bmm.default(_unsafe_view_100, _unsafe_view_101);  _unsafe_view_100 = _unsafe_view_101 = None
    view_975 = torch.ops.aten.view.default(bmm_96, [384, 1, 1, 1, 384]);  bmm_96 = None
    permute_600 = torch.ops.aten.permute.default(view_975, [3, 0, 4, 1, 2]);  view_975 = None
    view_976 = torch.ops.aten.view.default(permute_600, [1, 384, 384]);  permute_600 = None
    unsqueeze_411 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
    mul_106 = torch.ops.aten.mul.Tensor(view_976, unsqueeze_411);  view_976 = unsqueeze_411 = None
    add_94 = torch.ops.aten.add.Tensor(add_87, mul_106);  mul_106 = None
    split_tensor_86 = torch.ops.aten.split.Tensor(add_87, 384, dim = -2);  add_87 = None
    getitem_861 = split_tensor_86[0];  split_tensor_86 = None
    _to_copy_552 = torch.ops.aten._to_copy.default(getitem_861, dtype = torch.float32);  getitem_861 = None
    native_layer_norm_default_114 = torch.ops.aten.native_layer_norm.default(_to_copy_552, [384], pairformer_stack_blocks_4_transition_single_layer_norm_weight, pairformer_stack_blocks_4_transition_single_layer_norm_bias, 1e-05);  _to_copy_552 = pairformer_stack_blocks_4_transition_single_layer_norm_weight = pairformer_stack_blocks_4_transition_single_layer_norm_bias = None
    getitem_862 = native_layer_norm_default_114[0];  native_layer_norm_default_114 = None
    _to_copy_553 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_4_transition_single_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_4_transition_single_linear_no_bias_ab_weight = None
    _to_copy_554 = torch.ops.aten._to_copy.default(getitem_862, dtype = torch.bfloat16);  getitem_862 = None
    t_190 = torch.ops.aten.t.default(_to_copy_553);  _to_copy_553 = None
    view_977 = torch.ops.aten.view.default(_to_copy_554, [384, 384]);  _to_copy_554 = None
    mm_175 = torch.ops.aten.mm.default(view_977, t_190);  view_977 = t_190 = None
    view_978 = torch.ops.aten.view.default(mm_175, [1, 384, 1536]);  mm_175 = None
    split_tensor_87 = torch.ops.aten.split.Tensor(view_978, 768, dim = -1);  view_978 = None
    getitem_865 = split_tensor_87[0]
    getitem_866 = split_tensor_87[1];  split_tensor_87 = None
    silu_24 = torch.ops.aten.silu.default(getitem_865);  getitem_865 = None
    mul_107 = torch.ops.aten.mul.Tensor(silu_24, getitem_866);  silu_24 = getitem_866 = None
    _to_copy_555 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_4_transition_single_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_4_transition_single_linear_out_weight = None
    t_191 = torch.ops.aten.t.default(_to_copy_555);  _to_copy_555 = None
    view_980 = torch.ops.aten.view.default(mul_107, [384, 768]);  mul_107 = None
    mm_176 = torch.ops.aten.mm.default(view_980, t_191);  view_980 = t_191 = None
    view_981 = torch.ops.aten.view.default(mm_176, [1, 384, 384]);  mm_176 = None
    add_95 = torch.ops.aten.add.Tensor(add_94, view_981);  add_94 = view_981 = None
    _to_copy_556 = torch.ops.aten._to_copy.default(add_91, dtype = torch.float32)
    native_layer_norm_default_115 = torch.ops.aten.native_layer_norm.default(_to_copy_556, [256], pairformer_stack_blocks_5_triangle_multiplication_layernorm_z_in_weight, pairformer_stack_blocks_5_triangle_multiplication_layernorm_z_in_bias, 1e-05);  _to_copy_556 = pairformer_stack_blocks_5_triangle_multiplication_layernorm_z_in_weight = pairformer_stack_blocks_5_triangle_multiplication_layernorm_z_in_bias = None
    getitem_867 = native_layer_norm_default_115[0];  native_layer_norm_default_115 = None
    split_with_sizes_default_22 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_5_triangle_multiplication_merged_linear_p_weight, [512, 512]);  pairformer_stack_blocks_5_triangle_multiplication_merged_linear_p_weight = None
    getitem_870 = split_with_sizes_default_22[0]
    getitem_871 = split_with_sizes_default_22[1];  split_with_sizes_default_22 = None
    split_with_sizes_default_23 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_5_triangle_multiplication_merged_linear_g_weight, [512, 512, 256]);  pairformer_stack_blocks_5_triangle_multiplication_merged_linear_g_weight = None
    getitem_872 = split_with_sizes_default_23[0]
    getitem_873 = split_with_sizes_default_23[1]
    getitem_874 = split_with_sizes_default_23[2];  split_with_sizes_default_23 = None
    _to_copy_557 = torch.ops.aten._to_copy.default(getitem_870, dtype = torch.bfloat16);  getitem_870 = None
    _to_copy_558 = torch.ops.aten._to_copy.default(getitem_867, dtype = torch.bfloat16)
    t_192 = torch.ops.aten.t.default(_to_copy_557);  _to_copy_557 = None
    view_982 = torch.ops.aten.view.default(_to_copy_558, [147456, 256]);  _to_copy_558 = None
    mm_177 = torch.ops.aten.mm.default(view_982, t_192);  view_982 = t_192 = None
    view_983 = torch.ops.aten.view.default(mm_177, [1, 384, 384, 512]);  mm_177 = None
    _to_copy_559 = torch.ops.aten._to_copy.default(getitem_872, dtype = torch.bfloat16);  getitem_872 = None
    _to_copy_560 = torch.ops.aten._to_copy.default(getitem_867, dtype = torch.bfloat16)
    t_193 = torch.ops.aten.t.default(_to_copy_559);  _to_copy_559 = None
    view_984 = torch.ops.aten.view.default(_to_copy_560, [147456, 256]);  _to_copy_560 = None
    mm_178 = torch.ops.aten.mm.default(view_984, t_193);  view_984 = t_193 = None
    view_985 = torch.ops.aten.view.default(mm_178, [1, 384, 384, 512]);  mm_178 = None
    sigmoid_66 = torch.ops.aten.sigmoid.default(view_985);  view_985 = None
    mul_108 = torch.ops.aten.mul.Tensor(view_983, sigmoid_66);  view_983 = sigmoid_66 = None
    unsqueeze_412 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
    bitwise_not_66 = torch.ops.aten.bitwise_not.default(unsqueeze_412);  unsqueeze_412 = None
    masked_fill_66 = torch.ops.aten.masked_fill.Scalar(mul_108, bitwise_not_66, 0);  mul_108 = bitwise_not_66 = None
    split_tensor_88 = torch.ops.aten.split.Tensor(masked_fill_66, 256, dim = -1)
    getitem_877 = split_tensor_88[0];  split_tensor_88 = None
    unsqueeze_415 = torch.ops.aten.unsqueeze.default(getitem_877, 4);  getitem_877 = None
    permute_605 = torch.ops.aten.permute.default(unsqueeze_415, [0, 1, 4, 3, 2]);  unsqueeze_415 = None
    permute_606 = torch.ops.aten.permute.default(permute_605, [3, 1, 4, 0, 2]);  permute_605 = None
    view_988 = torch.ops.aten.view.default(permute_606, [256, 384, 384]);  permute_606 = None
    split_tensor_89 = torch.ops.aten.split.Tensor(masked_fill_66, 256, dim = -1);  masked_fill_66 = None
    getitem_880 = split_tensor_89[1];  split_tensor_89 = None
    unsqueeze_416 = torch.ops.aten.unsqueeze.default(getitem_880, 4);  getitem_880 = None
    permute_607 = torch.ops.aten.permute.default(unsqueeze_416, [0, 4, 1, 3, 2]);  unsqueeze_416 = None
    permute_608 = torch.ops.aten.permute.default(permute_607, [3, 4, 0, 2, 1]);  permute_607 = None
    view_989 = torch.ops.aten.view.default(permute_608, [256, 384, 384]);  permute_608 = None
    bmm_97 = torch.ops.aten.bmm.default(view_988, view_989);  view_988 = view_989 = None
    view_990 = torch.ops.aten.view.default(bmm_97, [256, 384, 1, 1, 384]);  bmm_97 = None
    permute_609 = torch.ops.aten.permute.default(view_990, [3, 1, 4, 0, 2]);  view_990 = None
    view_991 = torch.ops.aten.view.default(permute_609, [1, 384, 384, 256]);  permute_609 = None
    _to_copy_561 = torch.ops.aten._to_copy.default(getitem_871, dtype = torch.bfloat16);  getitem_871 = None
    _to_copy_562 = torch.ops.aten._to_copy.default(getitem_867, dtype = torch.bfloat16)
    t_194 = torch.ops.aten.t.default(_to_copy_561);  _to_copy_561 = None
    view_992 = torch.ops.aten.view.default(_to_copy_562, [147456, 256]);  _to_copy_562 = None
    mm_179 = torch.ops.aten.mm.default(view_992, t_194);  view_992 = t_194 = None
    view_993 = torch.ops.aten.view.default(mm_179, [1, 384, 384, 512]);  mm_179 = None
    _to_copy_563 = torch.ops.aten._to_copy.default(getitem_873, dtype = torch.bfloat16);  getitem_873 = None
    _to_copy_564 = torch.ops.aten._to_copy.default(getitem_867, dtype = torch.bfloat16)
    t_195 = torch.ops.aten.t.default(_to_copy_563);  _to_copy_563 = None
    view_994 = torch.ops.aten.view.default(_to_copy_564, [147456, 256]);  _to_copy_564 = None
    mm_180 = torch.ops.aten.mm.default(view_994, t_195);  view_994 = t_195 = None
    view_995 = torch.ops.aten.view.default(mm_180, [1, 384, 384, 512]);  mm_180 = None
    sigmoid_67 = torch.ops.aten.sigmoid.default(view_995);  view_995 = None
    mul_109 = torch.ops.aten.mul.Tensor(view_993, sigmoid_67);  view_993 = sigmoid_67 = None
    view_996 = torch.ops.aten.view.default(mul_109, [147456, 512]);  mul_109 = None
    view_997 = torch.ops.aten.view.default(view_996, [1, 384, 384, 512]);  view_996 = None
    transpose_22 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
    unsqueeze_417 = torch.ops.aten.unsqueeze.default(transpose_22, 3);  transpose_22 = None
    clone_114 = torch.ops.aten.clone.default(unsqueeze_417, memory_format = torch.contiguous_format);  unsqueeze_417 = None
    bitwise_not_67 = torch.ops.aten.bitwise_not.default(clone_114);  clone_114 = None
    masked_fill_67 = torch.ops.aten.masked_fill.Scalar(view_997, bitwise_not_67, 0);  view_997 = bitwise_not_67 = None
    view_998 = torch.ops.aten.view.default(masked_fill_67, [147456, 512]);  masked_fill_67 = None
    view_1002 = torch.ops.aten.view.default(view_998, [1, 384, 384, 512])
    split_tensor_90 = torch.ops.aten.split.Tensor(view_1002, 256, dim = -1);  view_1002 = None
    getitem_883 = split_tensor_90[0];  split_tensor_90 = None
    unsqueeze_420 = torch.ops.aten.unsqueeze.default(getitem_883, 4);  getitem_883 = None
    permute_614 = torch.ops.aten.permute.default(unsqueeze_420, [0, 2, 4, 3, 1]);  unsqueeze_420 = None
    permute_615 = torch.ops.aten.permute.default(permute_614, [3, 1, 4, 0, 2]);  permute_614 = None
    view_1003 = torch.ops.aten.view.default(permute_615, [256, 384, 384]);  permute_615 = None
    view_1004 = torch.ops.aten.view.default(view_998, [1, 384, 384, 512]);  view_998 = None
    split_tensor_91 = torch.ops.aten.split.Tensor(view_1004, 256, dim = -1);  view_1004 = None
    getitem_886 = split_tensor_91[1];  split_tensor_91 = None
    unsqueeze_421 = torch.ops.aten.unsqueeze.default(getitem_886, 4);  getitem_886 = None
    permute_616 = torch.ops.aten.permute.default(unsqueeze_421, [0, 4, 2, 3, 1]);  unsqueeze_421 = None
    permute_617 = torch.ops.aten.permute.default(permute_616, [3, 4, 0, 2, 1]);  permute_616 = None
    view_1005 = torch.ops.aten.view.default(permute_617, [256, 384, 384]);  permute_617 = None
    bmm_98 = torch.ops.aten.bmm.default(view_1003, view_1005);  view_1003 = view_1005 = None
    view_1006 = torch.ops.aten.view.default(bmm_98, [256, 384, 1, 1, 384]);  bmm_98 = None
    permute_618 = torch.ops.aten.permute.default(view_1006, [3, 1, 4, 0, 2]);  view_1006 = None
    view_1007 = torch.ops.aten.view.default(permute_618, [1, 384, 384, 256]);  permute_618 = None
    _to_copy_565 = torch.ops.aten._to_copy.default(view_991, dtype = torch.float32);  view_991 = None
    native_layer_norm_default_116 = torch.ops.aten.native_layer_norm.default(_to_copy_565, [256], None, None, 1e-05);  _to_copy_565 = None
    getitem_887 = native_layer_norm_default_116[0];  native_layer_norm_default_116 = None
    _to_copy_566 = torch.ops.aten._to_copy.default(view_1007, dtype = torch.float32);  view_1007 = None
    native_layer_norm_default_117 = torch.ops.aten.native_layer_norm.default(_to_copy_566, [256], None, None, 1e-05);  _to_copy_566 = None
    getitem_890 = native_layer_norm_default_117[0];  native_layer_norm_default_117 = None
    add_96 = torch.ops.aten.add.Tensor(getitem_887, getitem_890);  getitem_887 = getitem_890 = None
    _to_copy_567 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_5_triangle_multiplication_linear_z_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_5_triangle_multiplication_linear_z_out_weight = None
    _to_copy_568 = torch.ops.aten._to_copy.default(add_96, dtype = torch.bfloat16);  add_96 = None
    t_196 = torch.ops.aten.t.default(_to_copy_567);  _to_copy_567 = None
    view_1008 = torch.ops.aten.view.default(_to_copy_568, [147456, 256]);  _to_copy_568 = None
    mm_181 = torch.ops.aten.mm.default(view_1008, t_196);  view_1008 = t_196 = None
    view_1009 = torch.ops.aten.view.default(mm_181, [1, 384, 384, 256]);  mm_181 = None
    _to_copy_569 = torch.ops.aten._to_copy.default(getitem_874, dtype = torch.bfloat16);  getitem_874 = None
    _to_copy_570 = torch.ops.aten._to_copy.default(getitem_867, dtype = torch.bfloat16);  getitem_867 = None
    t_197 = torch.ops.aten.t.default(_to_copy_569);  _to_copy_569 = None
    view_1010 = torch.ops.aten.view.default(_to_copy_570, [147456, 256]);  _to_copy_570 = None
    mm_182 = torch.ops.aten.mm.default(view_1010, t_197);  view_1010 = t_197 = None
    view_1011 = torch.ops.aten.view.default(mm_182, [1, 384, 384, 256]);  mm_182 = None
    sigmoid_68 = torch.ops.aten.sigmoid.default(view_1011);  view_1011 = None
    mul_110 = torch.ops.aten.mul.Tensor(view_1009, sigmoid_68);  view_1009 = sigmoid_68 = None
    add_97 = torch.ops.aten.add.Tensor(add_91, mul_110);  mul_110 = None
    _to_copy_571 = torch.ops.aten._to_copy.default(add_91, dtype = torch.float32)
    native_layer_norm_default_118 = torch.ops.aten.native_layer_norm.default(_to_copy_571, [256], None, None, 1e-05);  _to_copy_571 = None
    getitem_893 = native_layer_norm_default_118[0];  native_layer_norm_default_118 = None
    _to_copy_572 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_5_triangle_attention_pair2b_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_5_triangle_attention_pair2b_weight = None
    _to_copy_573 = torch.ops.aten._to_copy.default(getitem_893, dtype = torch.bfloat16)
    t_198 = torch.ops.aten.t.default(_to_copy_572);  _to_copy_572 = None
    view_1012 = torch.ops.aten.view.default(_to_copy_573, [147456, 256]);  _to_copy_573 = None
    mm_183 = torch.ops.aten.mm.default(view_1012, t_198);  view_1012 = t_198 = None
    view_1013 = torch.ops.aten.view.default(mm_183, [1, 384, 384, 8]);  mm_183 = None
    view_1014 = torch.ops.aten.view.default(view_1013, [1, 384, 384, 2, 4]);  view_1013 = None
    permute_619 = torch.ops.aten.permute.default(view_1014, [0, 3, 4, 1, 2]);  view_1014 = None
    view_1015 = torch.ops.aten.view.default(permute_619, [1, 2, 4, 1, 384, 384]);  permute_619 = None
    view_1016 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 384, 384])
    bitwise_not_68 = torch.ops.aten.bitwise_not.default(view_1016);  view_1016 = None
    masked_fill_68 = torch.ops.aten.masked_fill.Scalar(view_1015, bitwise_not_68, -10000);  view_1015 = bitwise_not_68 = None
    view_1017 = torch.ops.aten.view.default(masked_fill_68, [1, 2, 4, 384, 384]);  masked_fill_68 = None
    permute_620 = torch.ops.aten.permute.default(view_1017, [1, 0, 2, 3, 4]);  view_1017 = None
    view_1018 = torch.ops.aten.view.default(permute_620, [2, 4, 1, 384, 384]);  permute_620 = None
    _to_copy_574 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_5_triangle_attention_pair2qkvg1_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_5_triangle_attention_pair2qkvg1_weight = None
    _to_copy_575 = torch.ops.aten._to_copy.default(getitem_893, dtype = torch.bfloat16)
    t_199 = torch.ops.aten.t.default(_to_copy_574);  _to_copy_574 = None
    view_1019 = torch.ops.aten.view.default(_to_copy_575, [147456, 256]);  _to_copy_575 = None
    mm_184 = torch.ops.aten.mm.default(view_1019, t_199);  view_1019 = t_199 = None
    view_1020 = torch.ops.aten.view.default(mm_184, [1, 384, 384, 1024]);  mm_184 = None
    select_23 = torch.ops.aten.select.int(view_1018, 0, 0)
    view_1021 = torch.ops.aten.view.default(view_1020, [1, 384, 384, 4, 4, 64]);  view_1020 = None
    permute_621 = torch.ops.aten.permute.default(view_1021, [4, 0, 3, 1, 2, 5]);  view_1021 = None
    view_1022 = torch.ops.aten.view.default(permute_621, [4, 4, 384, 384, 64]);  permute_621 = None
    unbind_int_49 = torch.ops.aten.unbind.int(view_1022);  view_1022 = None
    getitem_896 = unbind_int_49[0]
    getitem_897 = unbind_int_49[1]
    getitem_898 = unbind_int_49[2]
    getitem_899 = unbind_int_49[3];  unbind_int_49 = None
    expand_52 = torch.ops.aten.expand.default(select_23, [4, 384, 384, 384]);  select_23 = None
    _scaled_dot_product_efficient_attention_default_27 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_896, getitem_897, getitem_898, expand_52, False);  getitem_896 = getitem_897 = getitem_898 = expand_52 = None
    getitem_900 = _scaled_dot_product_efficient_attention_default_27[0];  _scaled_dot_product_efficient_attention_default_27 = None
    sigmoid_69 = torch.ops.aten.sigmoid.default(getitem_899);  getitem_899 = None
    mul_111 = torch.ops.aten.mul.Tensor(getitem_900, sigmoid_69);  getitem_900 = sigmoid_69 = None
    view_1023 = torch.ops.aten.view.default(mul_111, [1, 4, 384, 384, 64]);  mul_111 = None
    permute_622 = torch.ops.aten.permute.default(view_1023, [0, 2, 3, 1, 4]);  view_1023 = None
    clone_115 = torch.ops.aten.clone.default(permute_622, memory_format = torch.contiguous_format);  permute_622 = None
    _unsafe_view_102 = torch.ops.aten._unsafe_view.default(clone_115, [1, 384, 384, 256]);  clone_115 = None
    transpose_23 = torch.ops.aten.transpose.int(getitem_893, 1, 2);  getitem_893 = None
    _to_copy_576 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_5_triangle_attention_pair2qkvg2_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_5_triangle_attention_pair2qkvg2_weight = None
    _to_copy_577 = torch.ops.aten._to_copy.default(transpose_23, dtype = torch.bfloat16);  transpose_23 = None
    t_200 = torch.ops.aten.t.default(_to_copy_576);  _to_copy_576 = None
    expand_53 = torch.ops.aten.expand.default(_to_copy_577, [1, 384, 384, 256]);  _to_copy_577 = None
    view_1024 = torch.ops.aten.view.default(expand_53, [384, 384, 256]);  expand_53 = None
    expand_54 = torch.ops.aten.expand.default(t_200, [1, 384, 256, 1024]);  t_200 = None
    view_1025 = torch.ops.aten.view.default(expand_54, [384, 256, 1024]);  expand_54 = None
    bmm_99 = torch.ops.aten.bmm.default(view_1024, view_1025);  view_1024 = view_1025 = None
    view_1026 = torch.ops.aten.view.default(bmm_99, [1, 384, 384, 1024]);  bmm_99 = None
    select_24 = torch.ops.aten.select.int(view_1018, 0, 1);  view_1018 = None
    view_1027 = torch.ops.aten.view.default(view_1026, [1, 384, 384, 4, 4, 64]);  view_1026 = None
    permute_623 = torch.ops.aten.permute.default(view_1027, [4, 0, 3, 1, 2, 5]);  view_1027 = None
    view_1028 = torch.ops.aten.view.default(permute_623, [4, 4, 384, 384, 64]);  permute_623 = None
    unbind_int_50 = torch.ops.aten.unbind.int(view_1028);  view_1028 = None
    getitem_904 = unbind_int_50[0]
    getitem_905 = unbind_int_50[1]
    getitem_906 = unbind_int_50[2]
    getitem_907 = unbind_int_50[3];  unbind_int_50 = None
    expand_55 = torch.ops.aten.expand.default(select_24, [4, 384, 384, 384]);  select_24 = None
    _scaled_dot_product_efficient_attention_default_28 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_904, getitem_905, getitem_906, expand_55, False);  getitem_904 = getitem_905 = getitem_906 = expand_55 = None
    getitem_908 = _scaled_dot_product_efficient_attention_default_28[0];  _scaled_dot_product_efficient_attention_default_28 = None
    sigmoid_70 = torch.ops.aten.sigmoid.default(getitem_907);  getitem_907 = None
    mul_112 = torch.ops.aten.mul.Tensor(getitem_908, sigmoid_70);  getitem_908 = sigmoid_70 = None
    view_1029 = torch.ops.aten.view.default(mul_112, [1, 4, 384, 384, 64]);  mul_112 = None
    permute_624 = torch.ops.aten.permute.default(view_1029, [0, 2, 3, 1, 4]);  view_1029 = None
    clone_116 = torch.ops.aten.clone.default(permute_624, memory_format = torch.contiguous_format);  permute_624 = None
    _unsafe_view_103 = torch.ops.aten._unsafe_view.default(clone_116, [1, 384, 384, 256]);  clone_116 = None
    cat_17 = torch.ops.aten.cat.default([_unsafe_view_102, _unsafe_view_103], dim = -1);  _unsafe_view_102 = _unsafe_view_103 = None
    slice_168 = torch.ops.aten.slice.Tensor(pairformer_stack_blocks_5_triangle_attention_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  pairformer_stack_blocks_5_triangle_attention_out_scalers = None
    unsqueeze_422 = torch.ops.aten.unsqueeze.default(slice_168, 1);  slice_168 = None
    mul_113 = torch.ops.aten.mul.Tensor(pairformer_stack_blocks_5_triangle_attention_linear_out_weight, unsqueeze_422);  pairformer_stack_blocks_5_triangle_attention_linear_out_weight = unsqueeze_422 = None
    _to_copy_578 = torch.ops.aten._to_copy.default(mul_113, dtype = torch.bfloat16);  mul_113 = None
    t_201 = torch.ops.aten.t.default(_to_copy_578);  _to_copy_578 = None
    view_1030 = torch.ops.aten.view.default(cat_17, [147456, 512]);  cat_17 = None
    mm_185 = torch.ops.aten.mm.default(view_1030, t_201);  view_1030 = t_201 = None
    view_1031 = torch.ops.aten.view.default(mm_185, [1, 384, 384, 256]);  mm_185 = None
    add_98 = torch.ops.aten.add.Tensor(add_97, view_1031);  add_97 = view_1031 = None
    split_tensor_92 = torch.ops.aten.split.Tensor(add_91, 384, dim = -2)
    getitem_912 = split_tensor_92[0];  split_tensor_92 = None
    _to_copy_579 = torch.ops.aten._to_copy.default(getitem_912, dtype = torch.float32);  getitem_912 = None
    native_layer_norm_default_119 = torch.ops.aten.native_layer_norm.default(_to_copy_579, [256], pairformer_stack_blocks_5_transition_pair_layer_norm_weight, pairformer_stack_blocks_5_transition_pair_layer_norm_bias, 1e-05);  _to_copy_579 = pairformer_stack_blocks_5_transition_pair_layer_norm_weight = pairformer_stack_blocks_5_transition_pair_layer_norm_bias = None
    getitem_913 = native_layer_norm_default_119[0];  native_layer_norm_default_119 = None
    _to_copy_580 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_5_transition_pair_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_5_transition_pair_linear_no_bias_ab_weight = None
    _to_copy_581 = torch.ops.aten._to_copy.default(getitem_913, dtype = torch.bfloat16);  getitem_913 = None
    t_202 = torch.ops.aten.t.default(_to_copy_580);  _to_copy_580 = None
    view_1032 = torch.ops.aten.view.default(_to_copy_581, [147456, 256]);  _to_copy_581 = None
    mm_186 = torch.ops.aten.mm.default(view_1032, t_202);  view_1032 = t_202 = None
    view_1033 = torch.ops.aten.view.default(mm_186, [1, 384, 384, 1024]);  mm_186 = None
    split_tensor_93 = torch.ops.aten.split.Tensor(view_1033, 512, dim = -1);  view_1033 = None
    getitem_916 = split_tensor_93[0]
    getitem_917 = split_tensor_93[1];  split_tensor_93 = None
    silu_25 = torch.ops.aten.silu.default(getitem_916);  getitem_916 = None
    mul_114 = torch.ops.aten.mul.Tensor(silu_25, getitem_917);  silu_25 = getitem_917 = None
    _to_copy_582 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_5_transition_pair_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_5_transition_pair_linear_out_weight = None
    t_203 = torch.ops.aten.t.default(_to_copy_582);  _to_copy_582 = None
    view_1035 = torch.ops.aten.view.default(mul_114, [147456, 512]);  mul_114 = None
    mm_187 = torch.ops.aten.mm.default(view_1035, t_203);  view_1035 = t_203 = None
    view_1036 = torch.ops.aten.view.default(mm_187, [1, 384, 384, 256]);  mm_187 = None
    add_99 = torch.ops.aten.add.Tensor(add_98, view_1036);  add_98 = view_1036 = None
    _to_copy_583 = torch.ops.aten._to_copy.default(add_95, dtype = torch.float32)
    native_layer_norm_default_120 = torch.ops.aten.native_layer_norm.default(_to_copy_583, [384], pairformer_stack_blocks_5_attention_pair_bias_single_layer_norm_weight, pairformer_stack_blocks_5_attention_pair_bias_single_layer_norm_bias, 1e-05);  _to_copy_583 = pairformer_stack_blocks_5_attention_pair_bias_single_layer_norm_weight = pairformer_stack_blocks_5_attention_pair_bias_single_layer_norm_bias = None
    getitem_918 = native_layer_norm_default_120[0];  native_layer_norm_default_120 = None
    _to_copy_584 = torch.ops.aten._to_copy.default(add_91, dtype = torch.float32);  add_91 = None
    native_layer_norm_default_121 = torch.ops.aten.native_layer_norm.default(_to_copy_584, [256], pairformer_stack_blocks_5_attention_pair_bias_pair_layer_norm_weight, pairformer_stack_blocks_5_attention_pair_bias_pair_layer_norm_bias, 1e-05);  _to_copy_584 = pairformer_stack_blocks_5_attention_pair_bias_pair_layer_norm_weight = pairformer_stack_blocks_5_attention_pair_bias_pair_layer_norm_bias = None
    getitem_921 = native_layer_norm_default_121[0];  native_layer_norm_default_121 = None
    _to_copy_585 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_5_attention_pair_bias_pair_linear_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_5_attention_pair_bias_pair_linear_weight = None
    _to_copy_586 = torch.ops.aten._to_copy.default(getitem_921, dtype = torch.bfloat16);  getitem_921 = None
    t_204 = torch.ops.aten.t.default(_to_copy_585);  _to_copy_585 = None
    view_1037 = torch.ops.aten.view.default(_to_copy_586, [147456, 256]);  _to_copy_586 = None
    mm_188 = torch.ops.aten.mm.default(view_1037, t_204);  view_1037 = t_204 = None
    view_1038 = torch.ops.aten.view.default(mm_188, [1, 384, 384, 16]);  mm_188 = None
    permute_625 = torch.ops.aten.permute.default(view_1038, [0, 3, 1, 2]);  view_1038 = None
    view_1039 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384])
    bitwise_not_69 = torch.ops.aten.bitwise_not.default(view_1039);  view_1039 = None
    masked_fill_69 = torch.ops.aten.masked_fill.Scalar(permute_625, bitwise_not_69, -10000);  permute_625 = bitwise_not_69 = None
    _to_copy_587 = torch.ops.aten._to_copy.default(getitem_918, dtype = torch.bfloat16);  getitem_918 = None
    _to_copy_588 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_5_attention_pair_bias_attention_input2qkvg_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_5_attention_pair_bias_attention_input2qkvg_weight = None
    unsqueeze_423 = torch.ops.aten.unsqueeze.default(_to_copy_587, 3);  _to_copy_587 = None
    unsqueeze_424 = torch.ops.aten.unsqueeze.default(unsqueeze_423, 4);  unsqueeze_423 = None
    unsqueeze_425 = torch.ops.aten.unsqueeze.default(unsqueeze_424, 5);  unsqueeze_424 = None
    permute_626 = torch.ops.aten.permute.default(unsqueeze_425, [3, 0, 4, 1, 5, 2]);  unsqueeze_425 = None
    unsqueeze_426 = torch.ops.aten.unsqueeze.default(_to_copy_588, 4);  _to_copy_588 = None
    unsqueeze_427 = torch.ops.aten.unsqueeze.default(unsqueeze_426, 5);  unsqueeze_426 = None
    permute_627 = torch.ops.aten.permute.default(unsqueeze_427, [1, 4, 2, 5, 3, 0]);  unsqueeze_427 = None
    permute_628 = torch.ops.aten.permute.default(permute_626, [3, 5, 0, 1, 2, 4]);  permute_626 = None
    view_1040 = torch.ops.aten.view.default(permute_628, [1, 384, 384]);  permute_628 = None
    permute_629 = torch.ops.aten.permute.default(permute_627, [5, 0, 1, 2, 4, 3]);  permute_627 = None
    view_1041 = torch.ops.aten.view.default(permute_629, [1, 384, 1536]);  permute_629 = None
    bmm_100 = torch.ops.aten.bmm.default(view_1040, view_1041);  view_1040 = view_1041 = None
    view_1042 = torch.ops.aten.view.default(bmm_100, [384, 1, 4, 1, 16, 24]);  bmm_100 = None
    permute_630 = torch.ops.aten.permute.default(view_1042, [2, 3, 4, 0, 5, 1]);  view_1042 = None
    view_1043 = torch.ops.aten.view.default(permute_630, [4, 1, 16, 384, 24]);  permute_630 = None
    unbind_int_51 = torch.ops.aten.unbind.int(view_1043);  view_1043 = None
    getitem_924 = unbind_int_51[0]
    getitem_925 = unbind_int_51[1]
    getitem_926 = unbind_int_51[2]
    getitem_927 = unbind_int_51[3];  unbind_int_51 = None
    view_1044 = torch.ops.aten.view.default(pairformer_stack_blocks_5_attention_pair_bias_attention_query_bias, [1, 16, 1, 24]);  pairformer_stack_blocks_5_attention_pair_bias_attention_query_bias = None
    add_100 = torch.ops.aten.add.Tensor(getitem_924, view_1044);  getitem_924 = view_1044 = None
    _to_copy_589 = torch.ops.aten._to_copy.default(add_100, dtype = torch.bfloat16);  add_100 = None
    expand_56 = torch.ops.aten.expand.default(masked_fill_69, [1, 16, 384, 384]);  masked_fill_69 = None
    _scaled_dot_product_efficient_attention_default_29 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_589, getitem_925, getitem_926, expand_56, False);  _to_copy_589 = getitem_925 = getitem_926 = expand_56 = None
    getitem_928 = _scaled_dot_product_efficient_attention_default_29[0];  _scaled_dot_product_efficient_attention_default_29 = None
    add_101 = torch.ops.aten.add.Tensor(getitem_927, 1);  getitem_927 = None
    sigmoid_71 = torch.ops.aten.sigmoid.default(add_101);  add_101 = None
    mul_115 = torch.ops.aten.mul.Tensor(getitem_928, sigmoid_71);  getitem_928 = sigmoid_71 = None
    _to_copy_590 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_5_attention_pair_bias_attention_output_proj_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_5_attention_pair_bias_attention_output_proj_weight = None
    unsqueeze_428 = torch.ops.aten.unsqueeze.default(mul_115, 4);  mul_115 = None
    permute_631 = torch.ops.aten.permute.default(unsqueeze_428, [0, 2, 4, 3, 1]);  unsqueeze_428 = None
    unsqueeze_429 = torch.ops.aten.unsqueeze.default(_to_copy_590, 3);  _to_copy_590 = None
    unsqueeze_430 = torch.ops.aten.unsqueeze.default(unsqueeze_429, 4);  unsqueeze_429 = None
    permute_632 = torch.ops.aten.permute.default(unsqueeze_430, [3, 4, 2, 1, 0]);  unsqueeze_430 = None
    permute_633 = torch.ops.aten.permute.default(permute_631, [1, 3, 4, 0, 2]);  permute_631 = None
    clone_117 = torch.ops.aten.clone.default(permute_633, memory_format = torch.contiguous_format);  permute_633 = None
    _unsafe_view_104 = torch.ops.aten._unsafe_view.default(clone_117, [1, 384, 384]);  clone_117 = None
    permute_634 = torch.ops.aten.permute.default(permute_632, [3, 4, 0, 2, 1]);  permute_632 = None
    clone_118 = torch.ops.aten.clone.default(permute_634, memory_format = torch.contiguous_format);  permute_634 = None
    _unsafe_view_105 = torch.ops.aten._unsafe_view.default(clone_118, [1, 384, 384]);  clone_118 = None
    bmm_101 = torch.ops.aten.bmm.default(_unsafe_view_104, _unsafe_view_105);  _unsafe_view_104 = _unsafe_view_105 = None
    view_1045 = torch.ops.aten.view.default(bmm_101, [384, 1, 1, 1, 384]);  bmm_101 = None
    permute_635 = torch.ops.aten.permute.default(view_1045, [3, 0, 4, 1, 2]);  view_1045 = None
    view_1046 = torch.ops.aten.view.default(permute_635, [1, 384, 384]);  permute_635 = None
    unsqueeze_431 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
    mul_116 = torch.ops.aten.mul.Tensor(view_1046, unsqueeze_431);  view_1046 = unsqueeze_431 = None
    add_102 = torch.ops.aten.add.Tensor(add_95, mul_116);  mul_116 = None
    split_tensor_94 = torch.ops.aten.split.Tensor(add_95, 384, dim = -2);  add_95 = None
    getitem_932 = split_tensor_94[0];  split_tensor_94 = None
    _to_copy_591 = torch.ops.aten._to_copy.default(getitem_932, dtype = torch.float32);  getitem_932 = None
    native_layer_norm_default_122 = torch.ops.aten.native_layer_norm.default(_to_copy_591, [384], pairformer_stack_blocks_5_transition_single_layer_norm_weight, pairformer_stack_blocks_5_transition_single_layer_norm_bias, 1e-05);  _to_copy_591 = pairformer_stack_blocks_5_transition_single_layer_norm_weight = pairformer_stack_blocks_5_transition_single_layer_norm_bias = None
    getitem_933 = native_layer_norm_default_122[0];  native_layer_norm_default_122 = None
    _to_copy_592 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_5_transition_single_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_5_transition_single_linear_no_bias_ab_weight = None
    _to_copy_593 = torch.ops.aten._to_copy.default(getitem_933, dtype = torch.bfloat16);  getitem_933 = None
    t_205 = torch.ops.aten.t.default(_to_copy_592);  _to_copy_592 = None
    view_1047 = torch.ops.aten.view.default(_to_copy_593, [384, 384]);  _to_copy_593 = None
    mm_189 = torch.ops.aten.mm.default(view_1047, t_205);  view_1047 = t_205 = None
    view_1048 = torch.ops.aten.view.default(mm_189, [1, 384, 1536]);  mm_189 = None
    split_tensor_95 = torch.ops.aten.split.Tensor(view_1048, 768, dim = -1);  view_1048 = None
    getitem_936 = split_tensor_95[0]
    getitem_937 = split_tensor_95[1];  split_tensor_95 = None
    silu_26 = torch.ops.aten.silu.default(getitem_936);  getitem_936 = None
    mul_117 = torch.ops.aten.mul.Tensor(silu_26, getitem_937);  silu_26 = getitem_937 = None
    _to_copy_594 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_5_transition_single_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_5_transition_single_linear_out_weight = None
    t_206 = torch.ops.aten.t.default(_to_copy_594);  _to_copy_594 = None
    view_1050 = torch.ops.aten.view.default(mul_117, [384, 768]);  mul_117 = None
    mm_190 = torch.ops.aten.mm.default(view_1050, t_206);  view_1050 = t_206 = None
    view_1051 = torch.ops.aten.view.default(mm_190, [1, 384, 384]);  mm_190 = None
    add_103 = torch.ops.aten.add.Tensor(add_102, view_1051);  add_102 = view_1051 = None
    _to_copy_595 = torch.ops.aten._to_copy.default(add_99, dtype = torch.float32)
    native_layer_norm_default_123 = torch.ops.aten.native_layer_norm.default(_to_copy_595, [256], pairformer_stack_blocks_6_triangle_multiplication_layernorm_z_in_weight, pairformer_stack_blocks_6_triangle_multiplication_layernorm_z_in_bias, 1e-05);  _to_copy_595 = pairformer_stack_blocks_6_triangle_multiplication_layernorm_z_in_weight = pairformer_stack_blocks_6_triangle_multiplication_layernorm_z_in_bias = None
    getitem_938 = native_layer_norm_default_123[0];  native_layer_norm_default_123 = None
    split_with_sizes_default_24 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_6_triangle_multiplication_merged_linear_p_weight, [512, 512]);  pairformer_stack_blocks_6_triangle_multiplication_merged_linear_p_weight = None
    getitem_941 = split_with_sizes_default_24[0]
    getitem_942 = split_with_sizes_default_24[1];  split_with_sizes_default_24 = None
    split_with_sizes_default_25 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_6_triangle_multiplication_merged_linear_g_weight, [512, 512, 256]);  pairformer_stack_blocks_6_triangle_multiplication_merged_linear_g_weight = None
    getitem_943 = split_with_sizes_default_25[0]
    getitem_944 = split_with_sizes_default_25[1]
    getitem_945 = split_with_sizes_default_25[2];  split_with_sizes_default_25 = None
    _to_copy_596 = torch.ops.aten._to_copy.default(getitem_941, dtype = torch.bfloat16);  getitem_941 = None
    _to_copy_597 = torch.ops.aten._to_copy.default(getitem_938, dtype = torch.bfloat16)
    t_207 = torch.ops.aten.t.default(_to_copy_596);  _to_copy_596 = None
    view_1052 = torch.ops.aten.view.default(_to_copy_597, [147456, 256]);  _to_copy_597 = None
    mm_191 = torch.ops.aten.mm.default(view_1052, t_207);  view_1052 = t_207 = None
    view_1053 = torch.ops.aten.view.default(mm_191, [1, 384, 384, 512]);  mm_191 = None
    _to_copy_598 = torch.ops.aten._to_copy.default(getitem_943, dtype = torch.bfloat16);  getitem_943 = None
    _to_copy_599 = torch.ops.aten._to_copy.default(getitem_938, dtype = torch.bfloat16)
    t_208 = torch.ops.aten.t.default(_to_copy_598);  _to_copy_598 = None
    view_1054 = torch.ops.aten.view.default(_to_copy_599, [147456, 256]);  _to_copy_599 = None
    mm_192 = torch.ops.aten.mm.default(view_1054, t_208);  view_1054 = t_208 = None
    view_1055 = torch.ops.aten.view.default(mm_192, [1, 384, 384, 512]);  mm_192 = None
    sigmoid_72 = torch.ops.aten.sigmoid.default(view_1055);  view_1055 = None
    mul_118 = torch.ops.aten.mul.Tensor(view_1053, sigmoid_72);  view_1053 = sigmoid_72 = None
    unsqueeze_432 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
    bitwise_not_70 = torch.ops.aten.bitwise_not.default(unsqueeze_432);  unsqueeze_432 = None
    masked_fill_70 = torch.ops.aten.masked_fill.Scalar(mul_118, bitwise_not_70, 0);  mul_118 = bitwise_not_70 = None
    split_tensor_96 = torch.ops.aten.split.Tensor(masked_fill_70, 256, dim = -1)
    getitem_948 = split_tensor_96[0];  split_tensor_96 = None
    unsqueeze_435 = torch.ops.aten.unsqueeze.default(getitem_948, 4);  getitem_948 = None
    permute_640 = torch.ops.aten.permute.default(unsqueeze_435, [0, 1, 4, 3, 2]);  unsqueeze_435 = None
    permute_641 = torch.ops.aten.permute.default(permute_640, [3, 1, 4, 0, 2]);  permute_640 = None
    view_1058 = torch.ops.aten.view.default(permute_641, [256, 384, 384]);  permute_641 = None
    split_tensor_97 = torch.ops.aten.split.Tensor(masked_fill_70, 256, dim = -1);  masked_fill_70 = None
    getitem_951 = split_tensor_97[1];  split_tensor_97 = None
    unsqueeze_436 = torch.ops.aten.unsqueeze.default(getitem_951, 4);  getitem_951 = None
    permute_642 = torch.ops.aten.permute.default(unsqueeze_436, [0, 4, 1, 3, 2]);  unsqueeze_436 = None
    permute_643 = torch.ops.aten.permute.default(permute_642, [3, 4, 0, 2, 1]);  permute_642 = None
    view_1059 = torch.ops.aten.view.default(permute_643, [256, 384, 384]);  permute_643 = None
    bmm_102 = torch.ops.aten.bmm.default(view_1058, view_1059);  view_1058 = view_1059 = None
    view_1060 = torch.ops.aten.view.default(bmm_102, [256, 384, 1, 1, 384]);  bmm_102 = None
    permute_644 = torch.ops.aten.permute.default(view_1060, [3, 1, 4, 0, 2]);  view_1060 = None
    view_1061 = torch.ops.aten.view.default(permute_644, [1, 384, 384, 256]);  permute_644 = None
    _to_copy_600 = torch.ops.aten._to_copy.default(getitem_942, dtype = torch.bfloat16);  getitem_942 = None
    _to_copy_601 = torch.ops.aten._to_copy.default(getitem_938, dtype = torch.bfloat16)
    t_209 = torch.ops.aten.t.default(_to_copy_600);  _to_copy_600 = None
    view_1062 = torch.ops.aten.view.default(_to_copy_601, [147456, 256]);  _to_copy_601 = None
    mm_193 = torch.ops.aten.mm.default(view_1062, t_209);  view_1062 = t_209 = None
    view_1063 = torch.ops.aten.view.default(mm_193, [1, 384, 384, 512]);  mm_193 = None
    _to_copy_602 = torch.ops.aten._to_copy.default(getitem_944, dtype = torch.bfloat16);  getitem_944 = None
    _to_copy_603 = torch.ops.aten._to_copy.default(getitem_938, dtype = torch.bfloat16)
    t_210 = torch.ops.aten.t.default(_to_copy_602);  _to_copy_602 = None
    view_1064 = torch.ops.aten.view.default(_to_copy_603, [147456, 256]);  _to_copy_603 = None
    mm_194 = torch.ops.aten.mm.default(view_1064, t_210);  view_1064 = t_210 = None
    view_1065 = torch.ops.aten.view.default(mm_194, [1, 384, 384, 512]);  mm_194 = None
    sigmoid_73 = torch.ops.aten.sigmoid.default(view_1065);  view_1065 = None
    mul_119 = torch.ops.aten.mul.Tensor(view_1063, sigmoid_73);  view_1063 = sigmoid_73 = None
    view_1066 = torch.ops.aten.view.default(mul_119, [147456, 512]);  mul_119 = None
    view_1067 = torch.ops.aten.view.default(view_1066, [1, 384, 384, 512]);  view_1066 = None
    transpose_24 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
    unsqueeze_437 = torch.ops.aten.unsqueeze.default(transpose_24, 3);  transpose_24 = None
    clone_119 = torch.ops.aten.clone.default(unsqueeze_437, memory_format = torch.contiguous_format);  unsqueeze_437 = None
    bitwise_not_71 = torch.ops.aten.bitwise_not.default(clone_119);  clone_119 = None
    masked_fill_71 = torch.ops.aten.masked_fill.Scalar(view_1067, bitwise_not_71, 0);  view_1067 = bitwise_not_71 = None
    view_1068 = torch.ops.aten.view.default(masked_fill_71, [147456, 512]);  masked_fill_71 = None
    view_1072 = torch.ops.aten.view.default(view_1068, [1, 384, 384, 512])
    split_tensor_98 = torch.ops.aten.split.Tensor(view_1072, 256, dim = -1);  view_1072 = None
    getitem_954 = split_tensor_98[0];  split_tensor_98 = None
    unsqueeze_440 = torch.ops.aten.unsqueeze.default(getitem_954, 4);  getitem_954 = None
    permute_649 = torch.ops.aten.permute.default(unsqueeze_440, [0, 2, 4, 3, 1]);  unsqueeze_440 = None
    permute_650 = torch.ops.aten.permute.default(permute_649, [3, 1, 4, 0, 2]);  permute_649 = None
    view_1073 = torch.ops.aten.view.default(permute_650, [256, 384, 384]);  permute_650 = None
    view_1074 = torch.ops.aten.view.default(view_1068, [1, 384, 384, 512]);  view_1068 = None
    split_tensor_99 = torch.ops.aten.split.Tensor(view_1074, 256, dim = -1);  view_1074 = None
    getitem_957 = split_tensor_99[1];  split_tensor_99 = None
    unsqueeze_441 = torch.ops.aten.unsqueeze.default(getitem_957, 4);  getitem_957 = None
    permute_651 = torch.ops.aten.permute.default(unsqueeze_441, [0, 4, 2, 3, 1]);  unsqueeze_441 = None
    permute_652 = torch.ops.aten.permute.default(permute_651, [3, 4, 0, 2, 1]);  permute_651 = None
    view_1075 = torch.ops.aten.view.default(permute_652, [256, 384, 384]);  permute_652 = None
    bmm_103 = torch.ops.aten.bmm.default(view_1073, view_1075);  view_1073 = view_1075 = None
    view_1076 = torch.ops.aten.view.default(bmm_103, [256, 384, 1, 1, 384]);  bmm_103 = None
    permute_653 = torch.ops.aten.permute.default(view_1076, [3, 1, 4, 0, 2]);  view_1076 = None
    view_1077 = torch.ops.aten.view.default(permute_653, [1, 384, 384, 256]);  permute_653 = None
    _to_copy_604 = torch.ops.aten._to_copy.default(view_1061, dtype = torch.float32);  view_1061 = None
    native_layer_norm_default_124 = torch.ops.aten.native_layer_norm.default(_to_copy_604, [256], None, None, 1e-05);  _to_copy_604 = None
    getitem_958 = native_layer_norm_default_124[0];  native_layer_norm_default_124 = None
    _to_copy_605 = torch.ops.aten._to_copy.default(view_1077, dtype = torch.float32);  view_1077 = None
    native_layer_norm_default_125 = torch.ops.aten.native_layer_norm.default(_to_copy_605, [256], None, None, 1e-05);  _to_copy_605 = None
    getitem_961 = native_layer_norm_default_125[0];  native_layer_norm_default_125 = None
    add_104 = torch.ops.aten.add.Tensor(getitem_958, getitem_961);  getitem_958 = getitem_961 = None
    _to_copy_606 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_6_triangle_multiplication_linear_z_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_6_triangle_multiplication_linear_z_out_weight = None
    _to_copy_607 = torch.ops.aten._to_copy.default(add_104, dtype = torch.bfloat16);  add_104 = None
    t_211 = torch.ops.aten.t.default(_to_copy_606);  _to_copy_606 = None
    view_1078 = torch.ops.aten.view.default(_to_copy_607, [147456, 256]);  _to_copy_607 = None
    mm_195 = torch.ops.aten.mm.default(view_1078, t_211);  view_1078 = t_211 = None
    view_1079 = torch.ops.aten.view.default(mm_195, [1, 384, 384, 256]);  mm_195 = None
    _to_copy_608 = torch.ops.aten._to_copy.default(getitem_945, dtype = torch.bfloat16);  getitem_945 = None
    _to_copy_609 = torch.ops.aten._to_copy.default(getitem_938, dtype = torch.bfloat16);  getitem_938 = None
    t_212 = torch.ops.aten.t.default(_to_copy_608);  _to_copy_608 = None
    view_1080 = torch.ops.aten.view.default(_to_copy_609, [147456, 256]);  _to_copy_609 = None
    mm_196 = torch.ops.aten.mm.default(view_1080, t_212);  view_1080 = t_212 = None
    view_1081 = torch.ops.aten.view.default(mm_196, [1, 384, 384, 256]);  mm_196 = None
    sigmoid_74 = torch.ops.aten.sigmoid.default(view_1081);  view_1081 = None
    mul_120 = torch.ops.aten.mul.Tensor(view_1079, sigmoid_74);  view_1079 = sigmoid_74 = None
    add_105 = torch.ops.aten.add.Tensor(add_99, mul_120);  mul_120 = None
    _to_copy_610 = torch.ops.aten._to_copy.default(add_99, dtype = torch.float32)
    native_layer_norm_default_126 = torch.ops.aten.native_layer_norm.default(_to_copy_610, [256], None, None, 1e-05);  _to_copy_610 = None
    getitem_964 = native_layer_norm_default_126[0];  native_layer_norm_default_126 = None
    _to_copy_611 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_6_triangle_attention_pair2b_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_6_triangle_attention_pair2b_weight = None
    _to_copy_612 = torch.ops.aten._to_copy.default(getitem_964, dtype = torch.bfloat16)
    t_213 = torch.ops.aten.t.default(_to_copy_611);  _to_copy_611 = None
    view_1082 = torch.ops.aten.view.default(_to_copy_612, [147456, 256]);  _to_copy_612 = None
    mm_197 = torch.ops.aten.mm.default(view_1082, t_213);  view_1082 = t_213 = None
    view_1083 = torch.ops.aten.view.default(mm_197, [1, 384, 384, 8]);  mm_197 = None
    view_1084 = torch.ops.aten.view.default(view_1083, [1, 384, 384, 2, 4]);  view_1083 = None
    permute_654 = torch.ops.aten.permute.default(view_1084, [0, 3, 4, 1, 2]);  view_1084 = None
    view_1085 = torch.ops.aten.view.default(permute_654, [1, 2, 4, 1, 384, 384]);  permute_654 = None
    view_1086 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 384, 384])
    bitwise_not_72 = torch.ops.aten.bitwise_not.default(view_1086);  view_1086 = None
    masked_fill_72 = torch.ops.aten.masked_fill.Scalar(view_1085, bitwise_not_72, -10000);  view_1085 = bitwise_not_72 = None
    view_1087 = torch.ops.aten.view.default(masked_fill_72, [1, 2, 4, 384, 384]);  masked_fill_72 = None
    permute_655 = torch.ops.aten.permute.default(view_1087, [1, 0, 2, 3, 4]);  view_1087 = None
    view_1088 = torch.ops.aten.view.default(permute_655, [2, 4, 1, 384, 384]);  permute_655 = None
    _to_copy_613 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_6_triangle_attention_pair2qkvg1_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_6_triangle_attention_pair2qkvg1_weight = None
    _to_copy_614 = torch.ops.aten._to_copy.default(getitem_964, dtype = torch.bfloat16)
    t_214 = torch.ops.aten.t.default(_to_copy_613);  _to_copy_613 = None
    view_1089 = torch.ops.aten.view.default(_to_copy_614, [147456, 256]);  _to_copy_614 = None
    mm_198 = torch.ops.aten.mm.default(view_1089, t_214);  view_1089 = t_214 = None
    view_1090 = torch.ops.aten.view.default(mm_198, [1, 384, 384, 1024]);  mm_198 = None
    select_25 = torch.ops.aten.select.int(view_1088, 0, 0)
    view_1091 = torch.ops.aten.view.default(view_1090, [1, 384, 384, 4, 4, 64]);  view_1090 = None
    permute_656 = torch.ops.aten.permute.default(view_1091, [4, 0, 3, 1, 2, 5]);  view_1091 = None
    view_1092 = torch.ops.aten.view.default(permute_656, [4, 4, 384, 384, 64]);  permute_656 = None
    unbind_int_52 = torch.ops.aten.unbind.int(view_1092);  view_1092 = None
    getitem_967 = unbind_int_52[0]
    getitem_968 = unbind_int_52[1]
    getitem_969 = unbind_int_52[2]
    getitem_970 = unbind_int_52[3];  unbind_int_52 = None
    expand_57 = torch.ops.aten.expand.default(select_25, [4, 384, 384, 384]);  select_25 = None
    _scaled_dot_product_efficient_attention_default_30 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_967, getitem_968, getitem_969, expand_57, False);  getitem_967 = getitem_968 = getitem_969 = expand_57 = None
    getitem_971 = _scaled_dot_product_efficient_attention_default_30[0];  _scaled_dot_product_efficient_attention_default_30 = None
    sigmoid_75 = torch.ops.aten.sigmoid.default(getitem_970);  getitem_970 = None
    mul_121 = torch.ops.aten.mul.Tensor(getitem_971, sigmoid_75);  getitem_971 = sigmoid_75 = None
    view_1093 = torch.ops.aten.view.default(mul_121, [1, 4, 384, 384, 64]);  mul_121 = None
    permute_657 = torch.ops.aten.permute.default(view_1093, [0, 2, 3, 1, 4]);  view_1093 = None
    clone_120 = torch.ops.aten.clone.default(permute_657, memory_format = torch.contiguous_format);  permute_657 = None
    _unsafe_view_106 = torch.ops.aten._unsafe_view.default(clone_120, [1, 384, 384, 256]);  clone_120 = None
    transpose_25 = torch.ops.aten.transpose.int(getitem_964, 1, 2);  getitem_964 = None
    _to_copy_615 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_6_triangle_attention_pair2qkvg2_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_6_triangle_attention_pair2qkvg2_weight = None
    _to_copy_616 = torch.ops.aten._to_copy.default(transpose_25, dtype = torch.bfloat16);  transpose_25 = None
    t_215 = torch.ops.aten.t.default(_to_copy_615);  _to_copy_615 = None
    expand_58 = torch.ops.aten.expand.default(_to_copy_616, [1, 384, 384, 256]);  _to_copy_616 = None
    view_1094 = torch.ops.aten.view.default(expand_58, [384, 384, 256]);  expand_58 = None
    expand_59 = torch.ops.aten.expand.default(t_215, [1, 384, 256, 1024]);  t_215 = None
    view_1095 = torch.ops.aten.view.default(expand_59, [384, 256, 1024]);  expand_59 = None
    bmm_104 = torch.ops.aten.bmm.default(view_1094, view_1095);  view_1094 = view_1095 = None
    view_1096 = torch.ops.aten.view.default(bmm_104, [1, 384, 384, 1024]);  bmm_104 = None
    select_26 = torch.ops.aten.select.int(view_1088, 0, 1);  view_1088 = None
    view_1097 = torch.ops.aten.view.default(view_1096, [1, 384, 384, 4, 4, 64]);  view_1096 = None
    permute_658 = torch.ops.aten.permute.default(view_1097, [4, 0, 3, 1, 2, 5]);  view_1097 = None
    view_1098 = torch.ops.aten.view.default(permute_658, [4, 4, 384, 384, 64]);  permute_658 = None
    unbind_int_53 = torch.ops.aten.unbind.int(view_1098);  view_1098 = None
    getitem_975 = unbind_int_53[0]
    getitem_976 = unbind_int_53[1]
    getitem_977 = unbind_int_53[2]
    getitem_978 = unbind_int_53[3];  unbind_int_53 = None
    expand_60 = torch.ops.aten.expand.default(select_26, [4, 384, 384, 384]);  select_26 = None
    _scaled_dot_product_efficient_attention_default_31 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_975, getitem_976, getitem_977, expand_60, False);  getitem_975 = getitem_976 = getitem_977 = expand_60 = None
    getitem_979 = _scaled_dot_product_efficient_attention_default_31[0];  _scaled_dot_product_efficient_attention_default_31 = None
    sigmoid_76 = torch.ops.aten.sigmoid.default(getitem_978);  getitem_978 = None
    mul_122 = torch.ops.aten.mul.Tensor(getitem_979, sigmoid_76);  getitem_979 = sigmoid_76 = None
    view_1099 = torch.ops.aten.view.default(mul_122, [1, 4, 384, 384, 64]);  mul_122 = None
    permute_659 = torch.ops.aten.permute.default(view_1099, [0, 2, 3, 1, 4]);  view_1099 = None
    clone_121 = torch.ops.aten.clone.default(permute_659, memory_format = torch.contiguous_format);  permute_659 = None
    _unsafe_view_107 = torch.ops.aten._unsafe_view.default(clone_121, [1, 384, 384, 256]);  clone_121 = None
    cat_18 = torch.ops.aten.cat.default([_unsafe_view_106, _unsafe_view_107], dim = -1);  _unsafe_view_106 = _unsafe_view_107 = None
    slice_169 = torch.ops.aten.slice.Tensor(pairformer_stack_blocks_6_triangle_attention_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  pairformer_stack_blocks_6_triangle_attention_out_scalers = None
    unsqueeze_442 = torch.ops.aten.unsqueeze.default(slice_169, 1);  slice_169 = None
    mul_123 = torch.ops.aten.mul.Tensor(pairformer_stack_blocks_6_triangle_attention_linear_out_weight, unsqueeze_442);  pairformer_stack_blocks_6_triangle_attention_linear_out_weight = unsqueeze_442 = None
    _to_copy_617 = torch.ops.aten._to_copy.default(mul_123, dtype = torch.bfloat16);  mul_123 = None
    t_216 = torch.ops.aten.t.default(_to_copy_617);  _to_copy_617 = None
    view_1100 = torch.ops.aten.view.default(cat_18, [147456, 512]);  cat_18 = None
    mm_199 = torch.ops.aten.mm.default(view_1100, t_216);  view_1100 = t_216 = None
    view_1101 = torch.ops.aten.view.default(mm_199, [1, 384, 384, 256]);  mm_199 = None
    add_106 = torch.ops.aten.add.Tensor(add_105, view_1101);  add_105 = view_1101 = None
    split_tensor_100 = torch.ops.aten.split.Tensor(add_99, 384, dim = -2)
    getitem_983 = split_tensor_100[0];  split_tensor_100 = None
    _to_copy_618 = torch.ops.aten._to_copy.default(getitem_983, dtype = torch.float32);  getitem_983 = None
    native_layer_norm_default_127 = torch.ops.aten.native_layer_norm.default(_to_copy_618, [256], pairformer_stack_blocks_6_transition_pair_layer_norm_weight, pairformer_stack_blocks_6_transition_pair_layer_norm_bias, 1e-05);  _to_copy_618 = pairformer_stack_blocks_6_transition_pair_layer_norm_weight = pairformer_stack_blocks_6_transition_pair_layer_norm_bias = None
    getitem_984 = native_layer_norm_default_127[0];  native_layer_norm_default_127 = None
    _to_copy_619 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_6_transition_pair_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_6_transition_pair_linear_no_bias_ab_weight = None
    _to_copy_620 = torch.ops.aten._to_copy.default(getitem_984, dtype = torch.bfloat16);  getitem_984 = None
    t_217 = torch.ops.aten.t.default(_to_copy_619);  _to_copy_619 = None
    view_1102 = torch.ops.aten.view.default(_to_copy_620, [147456, 256]);  _to_copy_620 = None
    mm_200 = torch.ops.aten.mm.default(view_1102, t_217);  view_1102 = t_217 = None
    view_1103 = torch.ops.aten.view.default(mm_200, [1, 384, 384, 1024]);  mm_200 = None
    split_tensor_101 = torch.ops.aten.split.Tensor(view_1103, 512, dim = -1);  view_1103 = None
    getitem_987 = split_tensor_101[0]
    getitem_988 = split_tensor_101[1];  split_tensor_101 = None
    silu_27 = torch.ops.aten.silu.default(getitem_987);  getitem_987 = None
    mul_124 = torch.ops.aten.mul.Tensor(silu_27, getitem_988);  silu_27 = getitem_988 = None
    _to_copy_621 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_6_transition_pair_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_6_transition_pair_linear_out_weight = None
    t_218 = torch.ops.aten.t.default(_to_copy_621);  _to_copy_621 = None
    view_1105 = torch.ops.aten.view.default(mul_124, [147456, 512]);  mul_124 = None
    mm_201 = torch.ops.aten.mm.default(view_1105, t_218);  view_1105 = t_218 = None
    view_1106 = torch.ops.aten.view.default(mm_201, [1, 384, 384, 256]);  mm_201 = None
    add_107 = torch.ops.aten.add.Tensor(add_106, view_1106);  add_106 = view_1106 = None
    _to_copy_622 = torch.ops.aten._to_copy.default(add_103, dtype = torch.float32)
    native_layer_norm_default_128 = torch.ops.aten.native_layer_norm.default(_to_copy_622, [384], pairformer_stack_blocks_6_attention_pair_bias_single_layer_norm_weight, pairformer_stack_blocks_6_attention_pair_bias_single_layer_norm_bias, 1e-05);  _to_copy_622 = pairformer_stack_blocks_6_attention_pair_bias_single_layer_norm_weight = pairformer_stack_blocks_6_attention_pair_bias_single_layer_norm_bias = None
    getitem_989 = native_layer_norm_default_128[0];  native_layer_norm_default_128 = None
    _to_copy_623 = torch.ops.aten._to_copy.default(add_99, dtype = torch.float32);  add_99 = None
    native_layer_norm_default_129 = torch.ops.aten.native_layer_norm.default(_to_copy_623, [256], pairformer_stack_blocks_6_attention_pair_bias_pair_layer_norm_weight, pairformer_stack_blocks_6_attention_pair_bias_pair_layer_norm_bias, 1e-05);  _to_copy_623 = pairformer_stack_blocks_6_attention_pair_bias_pair_layer_norm_weight = pairformer_stack_blocks_6_attention_pair_bias_pair_layer_norm_bias = None
    getitem_992 = native_layer_norm_default_129[0];  native_layer_norm_default_129 = None
    _to_copy_624 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_6_attention_pair_bias_pair_linear_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_6_attention_pair_bias_pair_linear_weight = None
    _to_copy_625 = torch.ops.aten._to_copy.default(getitem_992, dtype = torch.bfloat16);  getitem_992 = None
    t_219 = torch.ops.aten.t.default(_to_copy_624);  _to_copy_624 = None
    view_1107 = torch.ops.aten.view.default(_to_copy_625, [147456, 256]);  _to_copy_625 = None
    mm_202 = torch.ops.aten.mm.default(view_1107, t_219);  view_1107 = t_219 = None
    view_1108 = torch.ops.aten.view.default(mm_202, [1, 384, 384, 16]);  mm_202 = None
    permute_660 = torch.ops.aten.permute.default(view_1108, [0, 3, 1, 2]);  view_1108 = None
    view_1109 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384])
    bitwise_not_73 = torch.ops.aten.bitwise_not.default(view_1109);  view_1109 = None
    masked_fill_73 = torch.ops.aten.masked_fill.Scalar(permute_660, bitwise_not_73, -10000);  permute_660 = bitwise_not_73 = None
    _to_copy_626 = torch.ops.aten._to_copy.default(getitem_989, dtype = torch.bfloat16);  getitem_989 = None
    _to_copy_627 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_6_attention_pair_bias_attention_input2qkvg_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_6_attention_pair_bias_attention_input2qkvg_weight = None
    unsqueeze_443 = torch.ops.aten.unsqueeze.default(_to_copy_626, 3);  _to_copy_626 = None
    unsqueeze_444 = torch.ops.aten.unsqueeze.default(unsqueeze_443, 4);  unsqueeze_443 = None
    unsqueeze_445 = torch.ops.aten.unsqueeze.default(unsqueeze_444, 5);  unsqueeze_444 = None
    permute_661 = torch.ops.aten.permute.default(unsqueeze_445, [3, 0, 4, 1, 5, 2]);  unsqueeze_445 = None
    unsqueeze_446 = torch.ops.aten.unsqueeze.default(_to_copy_627, 4);  _to_copy_627 = None
    unsqueeze_447 = torch.ops.aten.unsqueeze.default(unsqueeze_446, 5);  unsqueeze_446 = None
    permute_662 = torch.ops.aten.permute.default(unsqueeze_447, [1, 4, 2, 5, 3, 0]);  unsqueeze_447 = None
    permute_663 = torch.ops.aten.permute.default(permute_661, [3, 5, 0, 1, 2, 4]);  permute_661 = None
    view_1110 = torch.ops.aten.view.default(permute_663, [1, 384, 384]);  permute_663 = None
    permute_664 = torch.ops.aten.permute.default(permute_662, [5, 0, 1, 2, 4, 3]);  permute_662 = None
    view_1111 = torch.ops.aten.view.default(permute_664, [1, 384, 1536]);  permute_664 = None
    bmm_105 = torch.ops.aten.bmm.default(view_1110, view_1111);  view_1110 = view_1111 = None
    view_1112 = torch.ops.aten.view.default(bmm_105, [384, 1, 4, 1, 16, 24]);  bmm_105 = None
    permute_665 = torch.ops.aten.permute.default(view_1112, [2, 3, 4, 0, 5, 1]);  view_1112 = None
    view_1113 = torch.ops.aten.view.default(permute_665, [4, 1, 16, 384, 24]);  permute_665 = None
    unbind_int_54 = torch.ops.aten.unbind.int(view_1113);  view_1113 = None
    getitem_995 = unbind_int_54[0]
    getitem_996 = unbind_int_54[1]
    getitem_997 = unbind_int_54[2]
    getitem_998 = unbind_int_54[3];  unbind_int_54 = None
    view_1114 = torch.ops.aten.view.default(pairformer_stack_blocks_6_attention_pair_bias_attention_query_bias, [1, 16, 1, 24]);  pairformer_stack_blocks_6_attention_pair_bias_attention_query_bias = None
    add_108 = torch.ops.aten.add.Tensor(getitem_995, view_1114);  getitem_995 = view_1114 = None
    _to_copy_628 = torch.ops.aten._to_copy.default(add_108, dtype = torch.bfloat16);  add_108 = None
    expand_61 = torch.ops.aten.expand.default(masked_fill_73, [1, 16, 384, 384]);  masked_fill_73 = None
    _scaled_dot_product_efficient_attention_default_32 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_628, getitem_996, getitem_997, expand_61, False);  _to_copy_628 = getitem_996 = getitem_997 = expand_61 = None
    getitem_999 = _scaled_dot_product_efficient_attention_default_32[0];  _scaled_dot_product_efficient_attention_default_32 = None
    add_109 = torch.ops.aten.add.Tensor(getitem_998, 1);  getitem_998 = None
    sigmoid_77 = torch.ops.aten.sigmoid.default(add_109);  add_109 = None
    mul_125 = torch.ops.aten.mul.Tensor(getitem_999, sigmoid_77);  getitem_999 = sigmoid_77 = None
    _to_copy_629 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_6_attention_pair_bias_attention_output_proj_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_6_attention_pair_bias_attention_output_proj_weight = None
    unsqueeze_448 = torch.ops.aten.unsqueeze.default(mul_125, 4);  mul_125 = None
    permute_666 = torch.ops.aten.permute.default(unsqueeze_448, [0, 2, 4, 3, 1]);  unsqueeze_448 = None
    unsqueeze_449 = torch.ops.aten.unsqueeze.default(_to_copy_629, 3);  _to_copy_629 = None
    unsqueeze_450 = torch.ops.aten.unsqueeze.default(unsqueeze_449, 4);  unsqueeze_449 = None
    permute_667 = torch.ops.aten.permute.default(unsqueeze_450, [3, 4, 2, 1, 0]);  unsqueeze_450 = None
    permute_668 = torch.ops.aten.permute.default(permute_666, [1, 3, 4, 0, 2]);  permute_666 = None
    clone_122 = torch.ops.aten.clone.default(permute_668, memory_format = torch.contiguous_format);  permute_668 = None
    _unsafe_view_108 = torch.ops.aten._unsafe_view.default(clone_122, [1, 384, 384]);  clone_122 = None
    permute_669 = torch.ops.aten.permute.default(permute_667, [3, 4, 0, 2, 1]);  permute_667 = None
    clone_123 = torch.ops.aten.clone.default(permute_669, memory_format = torch.contiguous_format);  permute_669 = None
    _unsafe_view_109 = torch.ops.aten._unsafe_view.default(clone_123, [1, 384, 384]);  clone_123 = None
    bmm_106 = torch.ops.aten.bmm.default(_unsafe_view_108, _unsafe_view_109);  _unsafe_view_108 = _unsafe_view_109 = None
    view_1115 = torch.ops.aten.view.default(bmm_106, [384, 1, 1, 1, 384]);  bmm_106 = None
    permute_670 = torch.ops.aten.permute.default(view_1115, [3, 0, 4, 1, 2]);  view_1115 = None
    view_1116 = torch.ops.aten.view.default(permute_670, [1, 384, 384]);  permute_670 = None
    unsqueeze_451 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
    mul_126 = torch.ops.aten.mul.Tensor(view_1116, unsqueeze_451);  view_1116 = unsqueeze_451 = None
    add_110 = torch.ops.aten.add.Tensor(add_103, mul_126);  mul_126 = None
    split_tensor_102 = torch.ops.aten.split.Tensor(add_103, 384, dim = -2);  add_103 = None
    getitem_1003 = split_tensor_102[0];  split_tensor_102 = None
    _to_copy_630 = torch.ops.aten._to_copy.default(getitem_1003, dtype = torch.float32);  getitem_1003 = None
    native_layer_norm_default_130 = torch.ops.aten.native_layer_norm.default(_to_copy_630, [384], pairformer_stack_blocks_6_transition_single_layer_norm_weight, pairformer_stack_blocks_6_transition_single_layer_norm_bias, 1e-05);  _to_copy_630 = pairformer_stack_blocks_6_transition_single_layer_norm_weight = pairformer_stack_blocks_6_transition_single_layer_norm_bias = None
    getitem_1004 = native_layer_norm_default_130[0];  native_layer_norm_default_130 = None
    _to_copy_631 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_6_transition_single_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_6_transition_single_linear_no_bias_ab_weight = None
    _to_copy_632 = torch.ops.aten._to_copy.default(getitem_1004, dtype = torch.bfloat16);  getitem_1004 = None
    t_220 = torch.ops.aten.t.default(_to_copy_631);  _to_copy_631 = None
    view_1117 = torch.ops.aten.view.default(_to_copy_632, [384, 384]);  _to_copy_632 = None
    mm_203 = torch.ops.aten.mm.default(view_1117, t_220);  view_1117 = t_220 = None
    view_1118 = torch.ops.aten.view.default(mm_203, [1, 384, 1536]);  mm_203 = None
    split_tensor_103 = torch.ops.aten.split.Tensor(view_1118, 768, dim = -1);  view_1118 = None
    getitem_1007 = split_tensor_103[0]
    getitem_1008 = split_tensor_103[1];  split_tensor_103 = None
    silu_28 = torch.ops.aten.silu.default(getitem_1007);  getitem_1007 = None
    mul_127 = torch.ops.aten.mul.Tensor(silu_28, getitem_1008);  silu_28 = getitem_1008 = None
    _to_copy_633 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_6_transition_single_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_6_transition_single_linear_out_weight = None
    t_221 = torch.ops.aten.t.default(_to_copy_633);  _to_copy_633 = None
    view_1120 = torch.ops.aten.view.default(mul_127, [384, 768]);  mul_127 = None
    mm_204 = torch.ops.aten.mm.default(view_1120, t_221);  view_1120 = t_221 = None
    view_1121 = torch.ops.aten.view.default(mm_204, [1, 384, 384]);  mm_204 = None
    add_111 = torch.ops.aten.add.Tensor(add_110, view_1121);  add_110 = view_1121 = None
    _to_copy_634 = torch.ops.aten._to_copy.default(add_107, dtype = torch.float32)
    native_layer_norm_default_131 = torch.ops.aten.native_layer_norm.default(_to_copy_634, [256], pairformer_stack_blocks_7_triangle_multiplication_layernorm_z_in_weight, pairformer_stack_blocks_7_triangle_multiplication_layernorm_z_in_bias, 1e-05);  _to_copy_634 = pairformer_stack_blocks_7_triangle_multiplication_layernorm_z_in_weight = pairformer_stack_blocks_7_triangle_multiplication_layernorm_z_in_bias = None
    getitem_1009 = native_layer_norm_default_131[0];  native_layer_norm_default_131 = None
    split_with_sizes_default_26 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_7_triangle_multiplication_merged_linear_p_weight, [512, 512]);  pairformer_stack_blocks_7_triangle_multiplication_merged_linear_p_weight = None
    getitem_1012 = split_with_sizes_default_26[0]
    getitem_1013 = split_with_sizes_default_26[1];  split_with_sizes_default_26 = None
    split_with_sizes_default_27 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_7_triangle_multiplication_merged_linear_g_weight, [512, 512, 256]);  pairformer_stack_blocks_7_triangle_multiplication_merged_linear_g_weight = None
    getitem_1014 = split_with_sizes_default_27[0]
    getitem_1015 = split_with_sizes_default_27[1]
    getitem_1016 = split_with_sizes_default_27[2];  split_with_sizes_default_27 = None
    _to_copy_635 = torch.ops.aten._to_copy.default(getitem_1012, dtype = torch.bfloat16);  getitem_1012 = None
    _to_copy_636 = torch.ops.aten._to_copy.default(getitem_1009, dtype = torch.bfloat16)
    t_222 = torch.ops.aten.t.default(_to_copy_635);  _to_copy_635 = None
    view_1122 = torch.ops.aten.view.default(_to_copy_636, [147456, 256]);  _to_copy_636 = None
    mm_205 = torch.ops.aten.mm.default(view_1122, t_222);  view_1122 = t_222 = None
    view_1123 = torch.ops.aten.view.default(mm_205, [1, 384, 384, 512]);  mm_205 = None
    _to_copy_637 = torch.ops.aten._to_copy.default(getitem_1014, dtype = torch.bfloat16);  getitem_1014 = None
    _to_copy_638 = torch.ops.aten._to_copy.default(getitem_1009, dtype = torch.bfloat16)
    t_223 = torch.ops.aten.t.default(_to_copy_637);  _to_copy_637 = None
    view_1124 = torch.ops.aten.view.default(_to_copy_638, [147456, 256]);  _to_copy_638 = None
    mm_206 = torch.ops.aten.mm.default(view_1124, t_223);  view_1124 = t_223 = None
    view_1125 = torch.ops.aten.view.default(mm_206, [1, 384, 384, 512]);  mm_206 = None
    sigmoid_78 = torch.ops.aten.sigmoid.default(view_1125);  view_1125 = None
    mul_128 = torch.ops.aten.mul.Tensor(view_1123, sigmoid_78);  view_1123 = sigmoid_78 = None
    unsqueeze_452 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
    bitwise_not_74 = torch.ops.aten.bitwise_not.default(unsqueeze_452);  unsqueeze_452 = None
    masked_fill_74 = torch.ops.aten.masked_fill.Scalar(mul_128, bitwise_not_74, 0);  mul_128 = bitwise_not_74 = None
    split_tensor_104 = torch.ops.aten.split.Tensor(masked_fill_74, 256, dim = -1)
    getitem_1019 = split_tensor_104[0];  split_tensor_104 = None
    unsqueeze_455 = torch.ops.aten.unsqueeze.default(getitem_1019, 4);  getitem_1019 = None
    permute_675 = torch.ops.aten.permute.default(unsqueeze_455, [0, 1, 4, 3, 2]);  unsqueeze_455 = None
    permute_676 = torch.ops.aten.permute.default(permute_675, [3, 1, 4, 0, 2]);  permute_675 = None
    view_1128 = torch.ops.aten.view.default(permute_676, [256, 384, 384]);  permute_676 = None
    split_tensor_105 = torch.ops.aten.split.Tensor(masked_fill_74, 256, dim = -1);  masked_fill_74 = None
    getitem_1022 = split_tensor_105[1];  split_tensor_105 = None
    unsqueeze_456 = torch.ops.aten.unsqueeze.default(getitem_1022, 4);  getitem_1022 = None
    permute_677 = torch.ops.aten.permute.default(unsqueeze_456, [0, 4, 1, 3, 2]);  unsqueeze_456 = None
    permute_678 = torch.ops.aten.permute.default(permute_677, [3, 4, 0, 2, 1]);  permute_677 = None
    view_1129 = torch.ops.aten.view.default(permute_678, [256, 384, 384]);  permute_678 = None
    bmm_107 = torch.ops.aten.bmm.default(view_1128, view_1129);  view_1128 = view_1129 = None
    view_1130 = torch.ops.aten.view.default(bmm_107, [256, 384, 1, 1, 384]);  bmm_107 = None
    permute_679 = torch.ops.aten.permute.default(view_1130, [3, 1, 4, 0, 2]);  view_1130 = None
    view_1131 = torch.ops.aten.view.default(permute_679, [1, 384, 384, 256]);  permute_679 = None
    _to_copy_639 = torch.ops.aten._to_copy.default(getitem_1013, dtype = torch.bfloat16);  getitem_1013 = None
    _to_copy_640 = torch.ops.aten._to_copy.default(getitem_1009, dtype = torch.bfloat16)
    t_224 = torch.ops.aten.t.default(_to_copy_639);  _to_copy_639 = None
    view_1132 = torch.ops.aten.view.default(_to_copy_640, [147456, 256]);  _to_copy_640 = None
    mm_207 = torch.ops.aten.mm.default(view_1132, t_224);  view_1132 = t_224 = None
    view_1133 = torch.ops.aten.view.default(mm_207, [1, 384, 384, 512]);  mm_207 = None
    _to_copy_641 = torch.ops.aten._to_copy.default(getitem_1015, dtype = torch.bfloat16);  getitem_1015 = None
    _to_copy_642 = torch.ops.aten._to_copy.default(getitem_1009, dtype = torch.bfloat16)
    t_225 = torch.ops.aten.t.default(_to_copy_641);  _to_copy_641 = None
    view_1134 = torch.ops.aten.view.default(_to_copy_642, [147456, 256]);  _to_copy_642 = None
    mm_208 = torch.ops.aten.mm.default(view_1134, t_225);  view_1134 = t_225 = None
    view_1135 = torch.ops.aten.view.default(mm_208, [1, 384, 384, 512]);  mm_208 = None
    sigmoid_79 = torch.ops.aten.sigmoid.default(view_1135);  view_1135 = None
    mul_129 = torch.ops.aten.mul.Tensor(view_1133, sigmoid_79);  view_1133 = sigmoid_79 = None
    view_1136 = torch.ops.aten.view.default(mul_129, [147456, 512]);  mul_129 = None
    view_1137 = torch.ops.aten.view.default(view_1136, [1, 384, 384, 512]);  view_1136 = None
    transpose_26 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
    unsqueeze_457 = torch.ops.aten.unsqueeze.default(transpose_26, 3);  transpose_26 = None
    clone_124 = torch.ops.aten.clone.default(unsqueeze_457, memory_format = torch.contiguous_format);  unsqueeze_457 = None
    bitwise_not_75 = torch.ops.aten.bitwise_not.default(clone_124);  clone_124 = None
    masked_fill_75 = torch.ops.aten.masked_fill.Scalar(view_1137, bitwise_not_75, 0);  view_1137 = bitwise_not_75 = None
    view_1138 = torch.ops.aten.view.default(masked_fill_75, [147456, 512]);  masked_fill_75 = None
    view_1142 = torch.ops.aten.view.default(view_1138, [1, 384, 384, 512])
    split_tensor_106 = torch.ops.aten.split.Tensor(view_1142, 256, dim = -1);  view_1142 = None
    getitem_1025 = split_tensor_106[0];  split_tensor_106 = None
    unsqueeze_460 = torch.ops.aten.unsqueeze.default(getitem_1025, 4);  getitem_1025 = None
    permute_684 = torch.ops.aten.permute.default(unsqueeze_460, [0, 2, 4, 3, 1]);  unsqueeze_460 = None
    permute_685 = torch.ops.aten.permute.default(permute_684, [3, 1, 4, 0, 2]);  permute_684 = None
    view_1143 = torch.ops.aten.view.default(permute_685, [256, 384, 384]);  permute_685 = None
    view_1144 = torch.ops.aten.view.default(view_1138, [1, 384, 384, 512]);  view_1138 = None
    split_tensor_107 = torch.ops.aten.split.Tensor(view_1144, 256, dim = -1);  view_1144 = None
    getitem_1028 = split_tensor_107[1];  split_tensor_107 = None
    unsqueeze_461 = torch.ops.aten.unsqueeze.default(getitem_1028, 4);  getitem_1028 = None
    permute_686 = torch.ops.aten.permute.default(unsqueeze_461, [0, 4, 2, 3, 1]);  unsqueeze_461 = None
    permute_687 = torch.ops.aten.permute.default(permute_686, [3, 4, 0, 2, 1]);  permute_686 = None
    view_1145 = torch.ops.aten.view.default(permute_687, [256, 384, 384]);  permute_687 = None
    bmm_108 = torch.ops.aten.bmm.default(view_1143, view_1145);  view_1143 = view_1145 = None
    view_1146 = torch.ops.aten.view.default(bmm_108, [256, 384, 1, 1, 384]);  bmm_108 = None
    permute_688 = torch.ops.aten.permute.default(view_1146, [3, 1, 4, 0, 2]);  view_1146 = None
    view_1147 = torch.ops.aten.view.default(permute_688, [1, 384, 384, 256]);  permute_688 = None
    _to_copy_643 = torch.ops.aten._to_copy.default(view_1131, dtype = torch.float32);  view_1131 = None
    native_layer_norm_default_132 = torch.ops.aten.native_layer_norm.default(_to_copy_643, [256], None, None, 1e-05);  _to_copy_643 = None
    getitem_1029 = native_layer_norm_default_132[0];  native_layer_norm_default_132 = None
    _to_copy_644 = torch.ops.aten._to_copy.default(view_1147, dtype = torch.float32);  view_1147 = None
    native_layer_norm_default_133 = torch.ops.aten.native_layer_norm.default(_to_copy_644, [256], None, None, 1e-05);  _to_copy_644 = None
    getitem_1032 = native_layer_norm_default_133[0];  native_layer_norm_default_133 = None
    add_112 = torch.ops.aten.add.Tensor(getitem_1029, getitem_1032);  getitem_1029 = getitem_1032 = None
    _to_copy_645 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_7_triangle_multiplication_linear_z_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_7_triangle_multiplication_linear_z_out_weight = None
    _to_copy_646 = torch.ops.aten._to_copy.default(add_112, dtype = torch.bfloat16);  add_112 = None
    t_226 = torch.ops.aten.t.default(_to_copy_645);  _to_copy_645 = None
    view_1148 = torch.ops.aten.view.default(_to_copy_646, [147456, 256]);  _to_copy_646 = None
    mm_209 = torch.ops.aten.mm.default(view_1148, t_226);  view_1148 = t_226 = None
    view_1149 = torch.ops.aten.view.default(mm_209, [1, 384, 384, 256]);  mm_209 = None
    _to_copy_647 = torch.ops.aten._to_copy.default(getitem_1016, dtype = torch.bfloat16);  getitem_1016 = None
    _to_copy_648 = torch.ops.aten._to_copy.default(getitem_1009, dtype = torch.bfloat16);  getitem_1009 = None
    t_227 = torch.ops.aten.t.default(_to_copy_647);  _to_copy_647 = None
    view_1150 = torch.ops.aten.view.default(_to_copy_648, [147456, 256]);  _to_copy_648 = None
    mm_210 = torch.ops.aten.mm.default(view_1150, t_227);  view_1150 = t_227 = None
    view_1151 = torch.ops.aten.view.default(mm_210, [1, 384, 384, 256]);  mm_210 = None
    sigmoid_80 = torch.ops.aten.sigmoid.default(view_1151);  view_1151 = None
    mul_130 = torch.ops.aten.mul.Tensor(view_1149, sigmoid_80);  view_1149 = sigmoid_80 = None
    add_113 = torch.ops.aten.add.Tensor(add_107, mul_130);  mul_130 = None
    _to_copy_649 = torch.ops.aten._to_copy.default(add_107, dtype = torch.float32)
    native_layer_norm_default_134 = torch.ops.aten.native_layer_norm.default(_to_copy_649, [256], None, None, 1e-05);  _to_copy_649 = None
    getitem_1035 = native_layer_norm_default_134[0];  native_layer_norm_default_134 = None
    _to_copy_650 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_7_triangle_attention_pair2b_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_7_triangle_attention_pair2b_weight = None
    _to_copy_651 = torch.ops.aten._to_copy.default(getitem_1035, dtype = torch.bfloat16)
    t_228 = torch.ops.aten.t.default(_to_copy_650);  _to_copy_650 = None
    view_1152 = torch.ops.aten.view.default(_to_copy_651, [147456, 256]);  _to_copy_651 = None
    mm_211 = torch.ops.aten.mm.default(view_1152, t_228);  view_1152 = t_228 = None
    view_1153 = torch.ops.aten.view.default(mm_211, [1, 384, 384, 8]);  mm_211 = None
    view_1154 = torch.ops.aten.view.default(view_1153, [1, 384, 384, 2, 4]);  view_1153 = None
    permute_689 = torch.ops.aten.permute.default(view_1154, [0, 3, 4, 1, 2]);  view_1154 = None
    view_1155 = torch.ops.aten.view.default(permute_689, [1, 2, 4, 1, 384, 384]);  permute_689 = None
    view_1156 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 384, 384])
    bitwise_not_76 = torch.ops.aten.bitwise_not.default(view_1156);  view_1156 = None
    masked_fill_76 = torch.ops.aten.masked_fill.Scalar(view_1155, bitwise_not_76, -10000);  view_1155 = bitwise_not_76 = None
    view_1157 = torch.ops.aten.view.default(masked_fill_76, [1, 2, 4, 384, 384]);  masked_fill_76 = None
    permute_690 = torch.ops.aten.permute.default(view_1157, [1, 0, 2, 3, 4]);  view_1157 = None
    view_1158 = torch.ops.aten.view.default(permute_690, [2, 4, 1, 384, 384]);  permute_690 = None
    _to_copy_652 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_7_triangle_attention_pair2qkvg1_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_7_triangle_attention_pair2qkvg1_weight = None
    _to_copy_653 = torch.ops.aten._to_copy.default(getitem_1035, dtype = torch.bfloat16)
    t_229 = torch.ops.aten.t.default(_to_copy_652);  _to_copy_652 = None
    view_1159 = torch.ops.aten.view.default(_to_copy_653, [147456, 256]);  _to_copy_653 = None
    mm_212 = torch.ops.aten.mm.default(view_1159, t_229);  view_1159 = t_229 = None
    view_1160 = torch.ops.aten.view.default(mm_212, [1, 384, 384, 1024]);  mm_212 = None
    select_27 = torch.ops.aten.select.int(view_1158, 0, 0)
    view_1161 = torch.ops.aten.view.default(view_1160, [1, 384, 384, 4, 4, 64]);  view_1160 = None
    permute_691 = torch.ops.aten.permute.default(view_1161, [4, 0, 3, 1, 2, 5]);  view_1161 = None
    view_1162 = torch.ops.aten.view.default(permute_691, [4, 4, 384, 384, 64]);  permute_691 = None
    unbind_int_55 = torch.ops.aten.unbind.int(view_1162);  view_1162 = None
    getitem_1038 = unbind_int_55[0]
    getitem_1039 = unbind_int_55[1]
    getitem_1040 = unbind_int_55[2]
    getitem_1041 = unbind_int_55[3];  unbind_int_55 = None
    expand_62 = torch.ops.aten.expand.default(select_27, [4, 384, 384, 384]);  select_27 = None
    _scaled_dot_product_efficient_attention_default_33 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1038, getitem_1039, getitem_1040, expand_62, False);  getitem_1038 = getitem_1039 = getitem_1040 = expand_62 = None
    getitem_1042 = _scaled_dot_product_efficient_attention_default_33[0];  _scaled_dot_product_efficient_attention_default_33 = None
    sigmoid_81 = torch.ops.aten.sigmoid.default(getitem_1041);  getitem_1041 = None
    mul_131 = torch.ops.aten.mul.Tensor(getitem_1042, sigmoid_81);  getitem_1042 = sigmoid_81 = None
    view_1163 = torch.ops.aten.view.default(mul_131, [1, 4, 384, 384, 64]);  mul_131 = None
    permute_692 = torch.ops.aten.permute.default(view_1163, [0, 2, 3, 1, 4]);  view_1163 = None
    clone_125 = torch.ops.aten.clone.default(permute_692, memory_format = torch.contiguous_format);  permute_692 = None
    _unsafe_view_110 = torch.ops.aten._unsafe_view.default(clone_125, [1, 384, 384, 256]);  clone_125 = None
    transpose_27 = torch.ops.aten.transpose.int(getitem_1035, 1, 2);  getitem_1035 = None
    _to_copy_654 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_7_triangle_attention_pair2qkvg2_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_7_triangle_attention_pair2qkvg2_weight = None
    _to_copy_655 = torch.ops.aten._to_copy.default(transpose_27, dtype = torch.bfloat16);  transpose_27 = None
    t_230 = torch.ops.aten.t.default(_to_copy_654);  _to_copy_654 = None
    expand_63 = torch.ops.aten.expand.default(_to_copy_655, [1, 384, 384, 256]);  _to_copy_655 = None
    view_1164 = torch.ops.aten.view.default(expand_63, [384, 384, 256]);  expand_63 = None
    expand_64 = torch.ops.aten.expand.default(t_230, [1, 384, 256, 1024]);  t_230 = None
    view_1165 = torch.ops.aten.view.default(expand_64, [384, 256, 1024]);  expand_64 = None
    bmm_109 = torch.ops.aten.bmm.default(view_1164, view_1165);  view_1164 = view_1165 = None
    view_1166 = torch.ops.aten.view.default(bmm_109, [1, 384, 384, 1024]);  bmm_109 = None
    select_28 = torch.ops.aten.select.int(view_1158, 0, 1);  view_1158 = None
    view_1167 = torch.ops.aten.view.default(view_1166, [1, 384, 384, 4, 4, 64]);  view_1166 = None
    permute_693 = torch.ops.aten.permute.default(view_1167, [4, 0, 3, 1, 2, 5]);  view_1167 = None
    view_1168 = torch.ops.aten.view.default(permute_693, [4, 4, 384, 384, 64]);  permute_693 = None
    unbind_int_56 = torch.ops.aten.unbind.int(view_1168);  view_1168 = None
    getitem_1046 = unbind_int_56[0]
    getitem_1047 = unbind_int_56[1]
    getitem_1048 = unbind_int_56[2]
    getitem_1049 = unbind_int_56[3];  unbind_int_56 = None
    expand_65 = torch.ops.aten.expand.default(select_28, [4, 384, 384, 384]);  select_28 = None
    _scaled_dot_product_efficient_attention_default_34 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1046, getitem_1047, getitem_1048, expand_65, False);  getitem_1046 = getitem_1047 = getitem_1048 = expand_65 = None
    getitem_1050 = _scaled_dot_product_efficient_attention_default_34[0];  _scaled_dot_product_efficient_attention_default_34 = None
    sigmoid_82 = torch.ops.aten.sigmoid.default(getitem_1049);  getitem_1049 = None
    mul_132 = torch.ops.aten.mul.Tensor(getitem_1050, sigmoid_82);  getitem_1050 = sigmoid_82 = None
    view_1169 = torch.ops.aten.view.default(mul_132, [1, 4, 384, 384, 64]);  mul_132 = None
    permute_694 = torch.ops.aten.permute.default(view_1169, [0, 2, 3, 1, 4]);  view_1169 = None
    clone_126 = torch.ops.aten.clone.default(permute_694, memory_format = torch.contiguous_format);  permute_694 = None
    _unsafe_view_111 = torch.ops.aten._unsafe_view.default(clone_126, [1, 384, 384, 256]);  clone_126 = None
    cat_19 = torch.ops.aten.cat.default([_unsafe_view_110, _unsafe_view_111], dim = -1);  _unsafe_view_110 = _unsafe_view_111 = None
    slice_170 = torch.ops.aten.slice.Tensor(pairformer_stack_blocks_7_triangle_attention_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  pairformer_stack_blocks_7_triangle_attention_out_scalers = None
    unsqueeze_462 = torch.ops.aten.unsqueeze.default(slice_170, 1);  slice_170 = None
    mul_133 = torch.ops.aten.mul.Tensor(pairformer_stack_blocks_7_triangle_attention_linear_out_weight, unsqueeze_462);  pairformer_stack_blocks_7_triangle_attention_linear_out_weight = unsqueeze_462 = None
    _to_copy_656 = torch.ops.aten._to_copy.default(mul_133, dtype = torch.bfloat16);  mul_133 = None
    t_231 = torch.ops.aten.t.default(_to_copy_656);  _to_copy_656 = None
    view_1170 = torch.ops.aten.view.default(cat_19, [147456, 512]);  cat_19 = None
    mm_213 = torch.ops.aten.mm.default(view_1170, t_231);  view_1170 = t_231 = None
    view_1171 = torch.ops.aten.view.default(mm_213, [1, 384, 384, 256]);  mm_213 = None
    add_114 = torch.ops.aten.add.Tensor(add_113, view_1171);  add_113 = view_1171 = None
    split_tensor_108 = torch.ops.aten.split.Tensor(add_107, 384, dim = -2)
    getitem_1054 = split_tensor_108[0];  split_tensor_108 = None
    _to_copy_657 = torch.ops.aten._to_copy.default(getitem_1054, dtype = torch.float32);  getitem_1054 = None
    native_layer_norm_default_135 = torch.ops.aten.native_layer_norm.default(_to_copy_657, [256], pairformer_stack_blocks_7_transition_pair_layer_norm_weight, pairformer_stack_blocks_7_transition_pair_layer_norm_bias, 1e-05);  _to_copy_657 = pairformer_stack_blocks_7_transition_pair_layer_norm_weight = pairformer_stack_blocks_7_transition_pair_layer_norm_bias = None
    getitem_1055 = native_layer_norm_default_135[0];  native_layer_norm_default_135 = None
    _to_copy_658 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_7_transition_pair_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_7_transition_pair_linear_no_bias_ab_weight = None
    _to_copy_659 = torch.ops.aten._to_copy.default(getitem_1055, dtype = torch.bfloat16);  getitem_1055 = None
    t_232 = torch.ops.aten.t.default(_to_copy_658);  _to_copy_658 = None
    view_1172 = torch.ops.aten.view.default(_to_copy_659, [147456, 256]);  _to_copy_659 = None
    mm_214 = torch.ops.aten.mm.default(view_1172, t_232);  view_1172 = t_232 = None
    view_1173 = torch.ops.aten.view.default(mm_214, [1, 384, 384, 1024]);  mm_214 = None
    split_tensor_109 = torch.ops.aten.split.Tensor(view_1173, 512, dim = -1);  view_1173 = None
    getitem_1058 = split_tensor_109[0]
    getitem_1059 = split_tensor_109[1];  split_tensor_109 = None
    silu_29 = torch.ops.aten.silu.default(getitem_1058);  getitem_1058 = None
    mul_134 = torch.ops.aten.mul.Tensor(silu_29, getitem_1059);  silu_29 = getitem_1059 = None
    _to_copy_660 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_7_transition_pair_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_7_transition_pair_linear_out_weight = None
    t_233 = torch.ops.aten.t.default(_to_copy_660);  _to_copy_660 = None
    view_1175 = torch.ops.aten.view.default(mul_134, [147456, 512]);  mul_134 = None
    mm_215 = torch.ops.aten.mm.default(view_1175, t_233);  view_1175 = t_233 = None
    view_1176 = torch.ops.aten.view.default(mm_215, [1, 384, 384, 256]);  mm_215 = None
    add_115 = torch.ops.aten.add.Tensor(add_114, view_1176);  add_114 = view_1176 = None
    _to_copy_661 = torch.ops.aten._to_copy.default(add_111, dtype = torch.float32)
    native_layer_norm_default_136 = torch.ops.aten.native_layer_norm.default(_to_copy_661, [384], pairformer_stack_blocks_7_attention_pair_bias_single_layer_norm_weight, pairformer_stack_blocks_7_attention_pair_bias_single_layer_norm_bias, 1e-05);  _to_copy_661 = pairformer_stack_blocks_7_attention_pair_bias_single_layer_norm_weight = pairformer_stack_blocks_7_attention_pair_bias_single_layer_norm_bias = None
    getitem_1060 = native_layer_norm_default_136[0];  native_layer_norm_default_136 = None
    _to_copy_662 = torch.ops.aten._to_copy.default(add_107, dtype = torch.float32);  add_107 = None
    native_layer_norm_default_137 = torch.ops.aten.native_layer_norm.default(_to_copy_662, [256], pairformer_stack_blocks_7_attention_pair_bias_pair_layer_norm_weight, pairformer_stack_blocks_7_attention_pair_bias_pair_layer_norm_bias, 1e-05);  _to_copy_662 = pairformer_stack_blocks_7_attention_pair_bias_pair_layer_norm_weight = pairformer_stack_blocks_7_attention_pair_bias_pair_layer_norm_bias = None
    getitem_1063 = native_layer_norm_default_137[0];  native_layer_norm_default_137 = None
    _to_copy_663 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_7_attention_pair_bias_pair_linear_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_7_attention_pair_bias_pair_linear_weight = None
    _to_copy_664 = torch.ops.aten._to_copy.default(getitem_1063, dtype = torch.bfloat16);  getitem_1063 = None
    t_234 = torch.ops.aten.t.default(_to_copy_663);  _to_copy_663 = None
    view_1177 = torch.ops.aten.view.default(_to_copy_664, [147456, 256]);  _to_copy_664 = None
    mm_216 = torch.ops.aten.mm.default(view_1177, t_234);  view_1177 = t_234 = None
    view_1178 = torch.ops.aten.view.default(mm_216, [1, 384, 384, 16]);  mm_216 = None
    permute_695 = torch.ops.aten.permute.default(view_1178, [0, 3, 1, 2]);  view_1178 = None
    view_1179 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384])
    bitwise_not_77 = torch.ops.aten.bitwise_not.default(view_1179);  view_1179 = None
    masked_fill_77 = torch.ops.aten.masked_fill.Scalar(permute_695, bitwise_not_77, -10000);  permute_695 = bitwise_not_77 = None
    _to_copy_665 = torch.ops.aten._to_copy.default(getitem_1060, dtype = torch.bfloat16);  getitem_1060 = None
    _to_copy_666 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_7_attention_pair_bias_attention_input2qkvg_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_7_attention_pair_bias_attention_input2qkvg_weight = None
    unsqueeze_463 = torch.ops.aten.unsqueeze.default(_to_copy_665, 3);  _to_copy_665 = None
    unsqueeze_464 = torch.ops.aten.unsqueeze.default(unsqueeze_463, 4);  unsqueeze_463 = None
    unsqueeze_465 = torch.ops.aten.unsqueeze.default(unsqueeze_464, 5);  unsqueeze_464 = None
    permute_696 = torch.ops.aten.permute.default(unsqueeze_465, [3, 0, 4, 1, 5, 2]);  unsqueeze_465 = None
    unsqueeze_466 = torch.ops.aten.unsqueeze.default(_to_copy_666, 4);  _to_copy_666 = None
    unsqueeze_467 = torch.ops.aten.unsqueeze.default(unsqueeze_466, 5);  unsqueeze_466 = None
    permute_697 = torch.ops.aten.permute.default(unsqueeze_467, [1, 4, 2, 5, 3, 0]);  unsqueeze_467 = None
    permute_698 = torch.ops.aten.permute.default(permute_696, [3, 5, 0, 1, 2, 4]);  permute_696 = None
    view_1180 = torch.ops.aten.view.default(permute_698, [1, 384, 384]);  permute_698 = None
    permute_699 = torch.ops.aten.permute.default(permute_697, [5, 0, 1, 2, 4, 3]);  permute_697 = None
    view_1181 = torch.ops.aten.view.default(permute_699, [1, 384, 1536]);  permute_699 = None
    bmm_110 = torch.ops.aten.bmm.default(view_1180, view_1181);  view_1180 = view_1181 = None
    view_1182 = torch.ops.aten.view.default(bmm_110, [384, 1, 4, 1, 16, 24]);  bmm_110 = None
    permute_700 = torch.ops.aten.permute.default(view_1182, [2, 3, 4, 0, 5, 1]);  view_1182 = None
    view_1183 = torch.ops.aten.view.default(permute_700, [4, 1, 16, 384, 24]);  permute_700 = None
    unbind_int_57 = torch.ops.aten.unbind.int(view_1183);  view_1183 = None
    getitem_1066 = unbind_int_57[0]
    getitem_1067 = unbind_int_57[1]
    getitem_1068 = unbind_int_57[2]
    getitem_1069 = unbind_int_57[3];  unbind_int_57 = None
    view_1184 = torch.ops.aten.view.default(pairformer_stack_blocks_7_attention_pair_bias_attention_query_bias, [1, 16, 1, 24]);  pairformer_stack_blocks_7_attention_pair_bias_attention_query_bias = None
    add_116 = torch.ops.aten.add.Tensor(getitem_1066, view_1184);  getitem_1066 = view_1184 = None
    _to_copy_667 = torch.ops.aten._to_copy.default(add_116, dtype = torch.bfloat16);  add_116 = None
    expand_66 = torch.ops.aten.expand.default(masked_fill_77, [1, 16, 384, 384]);  masked_fill_77 = None
    _scaled_dot_product_efficient_attention_default_35 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_667, getitem_1067, getitem_1068, expand_66, False);  _to_copy_667 = getitem_1067 = getitem_1068 = expand_66 = None
    getitem_1070 = _scaled_dot_product_efficient_attention_default_35[0];  _scaled_dot_product_efficient_attention_default_35 = None
    add_117 = torch.ops.aten.add.Tensor(getitem_1069, 1);  getitem_1069 = None
    sigmoid_83 = torch.ops.aten.sigmoid.default(add_117);  add_117 = None
    mul_135 = torch.ops.aten.mul.Tensor(getitem_1070, sigmoid_83);  getitem_1070 = sigmoid_83 = None
    _to_copy_668 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_7_attention_pair_bias_attention_output_proj_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_7_attention_pair_bias_attention_output_proj_weight = None
    unsqueeze_468 = torch.ops.aten.unsqueeze.default(mul_135, 4);  mul_135 = None
    permute_701 = torch.ops.aten.permute.default(unsqueeze_468, [0, 2, 4, 3, 1]);  unsqueeze_468 = None
    unsqueeze_469 = torch.ops.aten.unsqueeze.default(_to_copy_668, 3);  _to_copy_668 = None
    unsqueeze_470 = torch.ops.aten.unsqueeze.default(unsqueeze_469, 4);  unsqueeze_469 = None
    permute_702 = torch.ops.aten.permute.default(unsqueeze_470, [3, 4, 2, 1, 0]);  unsqueeze_470 = None
    permute_703 = torch.ops.aten.permute.default(permute_701, [1, 3, 4, 0, 2]);  permute_701 = None
    clone_127 = torch.ops.aten.clone.default(permute_703, memory_format = torch.contiguous_format);  permute_703 = None
    _unsafe_view_112 = torch.ops.aten._unsafe_view.default(clone_127, [1, 384, 384]);  clone_127 = None
    permute_704 = torch.ops.aten.permute.default(permute_702, [3, 4, 0, 2, 1]);  permute_702 = None
    clone_128 = torch.ops.aten.clone.default(permute_704, memory_format = torch.contiguous_format);  permute_704 = None
    _unsafe_view_113 = torch.ops.aten._unsafe_view.default(clone_128, [1, 384, 384]);  clone_128 = None
    bmm_111 = torch.ops.aten.bmm.default(_unsafe_view_112, _unsafe_view_113);  _unsafe_view_112 = _unsafe_view_113 = None
    view_1185 = torch.ops.aten.view.default(bmm_111, [384, 1, 1, 1, 384]);  bmm_111 = None
    permute_705 = torch.ops.aten.permute.default(view_1185, [3, 0, 4, 1, 2]);  view_1185 = None
    view_1186 = torch.ops.aten.view.default(permute_705, [1, 384, 384]);  permute_705 = None
    unsqueeze_471 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
    mul_136 = torch.ops.aten.mul.Tensor(view_1186, unsqueeze_471);  view_1186 = unsqueeze_471 = None
    add_118 = torch.ops.aten.add.Tensor(add_111, mul_136);  mul_136 = None
    split_tensor_110 = torch.ops.aten.split.Tensor(add_111, 384, dim = -2);  add_111 = None
    getitem_1074 = split_tensor_110[0];  split_tensor_110 = None
    _to_copy_669 = torch.ops.aten._to_copy.default(getitem_1074, dtype = torch.float32);  getitem_1074 = None
    native_layer_norm_default_138 = torch.ops.aten.native_layer_norm.default(_to_copy_669, [384], pairformer_stack_blocks_7_transition_single_layer_norm_weight, pairformer_stack_blocks_7_transition_single_layer_norm_bias, 1e-05);  _to_copy_669 = pairformer_stack_blocks_7_transition_single_layer_norm_weight = pairformer_stack_blocks_7_transition_single_layer_norm_bias = None
    getitem_1075 = native_layer_norm_default_138[0];  native_layer_norm_default_138 = None
    _to_copy_670 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_7_transition_single_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_7_transition_single_linear_no_bias_ab_weight = None
    _to_copy_671 = torch.ops.aten._to_copy.default(getitem_1075, dtype = torch.bfloat16);  getitem_1075 = None
    t_235 = torch.ops.aten.t.default(_to_copy_670);  _to_copy_670 = None
    view_1187 = torch.ops.aten.view.default(_to_copy_671, [384, 384]);  _to_copy_671 = None
    mm_217 = torch.ops.aten.mm.default(view_1187, t_235);  view_1187 = t_235 = None
    view_1188 = torch.ops.aten.view.default(mm_217, [1, 384, 1536]);  mm_217 = None
    split_tensor_111 = torch.ops.aten.split.Tensor(view_1188, 768, dim = -1);  view_1188 = None
    getitem_1078 = split_tensor_111[0]
    getitem_1079 = split_tensor_111[1];  split_tensor_111 = None
    silu_30 = torch.ops.aten.silu.default(getitem_1078);  getitem_1078 = None
    mul_137 = torch.ops.aten.mul.Tensor(silu_30, getitem_1079);  silu_30 = getitem_1079 = None
    _to_copy_672 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_7_transition_single_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_7_transition_single_linear_out_weight = None
    t_236 = torch.ops.aten.t.default(_to_copy_672);  _to_copy_672 = None
    view_1190 = torch.ops.aten.view.default(mul_137, [384, 768]);  mul_137 = None
    mm_218 = torch.ops.aten.mm.default(view_1190, t_236);  view_1190 = t_236 = None
    view_1191 = torch.ops.aten.view.default(mm_218, [1, 384, 384]);  mm_218 = None
    add_119 = torch.ops.aten.add.Tensor(add_118, view_1191);  add_118 = view_1191 = None
    _to_copy_673 = torch.ops.aten._to_copy.default(add_115, dtype = torch.float32)
    native_layer_norm_default_139 = torch.ops.aten.native_layer_norm.default(_to_copy_673, [256], pairformer_stack_blocks_8_triangle_multiplication_layernorm_z_in_weight, pairformer_stack_blocks_8_triangle_multiplication_layernorm_z_in_bias, 1e-05);  _to_copy_673 = pairformer_stack_blocks_8_triangle_multiplication_layernorm_z_in_weight = pairformer_stack_blocks_8_triangle_multiplication_layernorm_z_in_bias = None
    getitem_1080 = native_layer_norm_default_139[0];  native_layer_norm_default_139 = None
    split_with_sizes_default_28 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_8_triangle_multiplication_merged_linear_p_weight, [512, 512]);  pairformer_stack_blocks_8_triangle_multiplication_merged_linear_p_weight = None
    getitem_1083 = split_with_sizes_default_28[0]
    getitem_1084 = split_with_sizes_default_28[1];  split_with_sizes_default_28 = None
    split_with_sizes_default_29 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_8_triangle_multiplication_merged_linear_g_weight, [512, 512, 256]);  pairformer_stack_blocks_8_triangle_multiplication_merged_linear_g_weight = None
    getitem_1085 = split_with_sizes_default_29[0]
    getitem_1086 = split_with_sizes_default_29[1]
    getitem_1087 = split_with_sizes_default_29[2];  split_with_sizes_default_29 = None
    _to_copy_674 = torch.ops.aten._to_copy.default(getitem_1083, dtype = torch.bfloat16);  getitem_1083 = None
    _to_copy_675 = torch.ops.aten._to_copy.default(getitem_1080, dtype = torch.bfloat16)
    t_237 = torch.ops.aten.t.default(_to_copy_674);  _to_copy_674 = None
    view_1192 = torch.ops.aten.view.default(_to_copy_675, [147456, 256]);  _to_copy_675 = None
    mm_219 = torch.ops.aten.mm.default(view_1192, t_237);  view_1192 = t_237 = None
    view_1193 = torch.ops.aten.view.default(mm_219, [1, 384, 384, 512]);  mm_219 = None
    _to_copy_676 = torch.ops.aten._to_copy.default(getitem_1085, dtype = torch.bfloat16);  getitem_1085 = None
    _to_copy_677 = torch.ops.aten._to_copy.default(getitem_1080, dtype = torch.bfloat16)
    t_238 = torch.ops.aten.t.default(_to_copy_676);  _to_copy_676 = None
    view_1194 = torch.ops.aten.view.default(_to_copy_677, [147456, 256]);  _to_copy_677 = None
    mm_220 = torch.ops.aten.mm.default(view_1194, t_238);  view_1194 = t_238 = None
    view_1195 = torch.ops.aten.view.default(mm_220, [1, 384, 384, 512]);  mm_220 = None
    sigmoid_84 = torch.ops.aten.sigmoid.default(view_1195);  view_1195 = None
    mul_138 = torch.ops.aten.mul.Tensor(view_1193, sigmoid_84);  view_1193 = sigmoid_84 = None
    unsqueeze_472 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
    bitwise_not_78 = torch.ops.aten.bitwise_not.default(unsqueeze_472);  unsqueeze_472 = None
    masked_fill_78 = torch.ops.aten.masked_fill.Scalar(mul_138, bitwise_not_78, 0);  mul_138 = bitwise_not_78 = None
    split_tensor_112 = torch.ops.aten.split.Tensor(masked_fill_78, 256, dim = -1)
    getitem_1090 = split_tensor_112[0];  split_tensor_112 = None
    unsqueeze_475 = torch.ops.aten.unsqueeze.default(getitem_1090, 4);  getitem_1090 = None
    permute_710 = torch.ops.aten.permute.default(unsqueeze_475, [0, 1, 4, 3, 2]);  unsqueeze_475 = None
    permute_711 = torch.ops.aten.permute.default(permute_710, [3, 1, 4, 0, 2]);  permute_710 = None
    view_1198 = torch.ops.aten.view.default(permute_711, [256, 384, 384]);  permute_711 = None
    split_tensor_113 = torch.ops.aten.split.Tensor(masked_fill_78, 256, dim = -1);  masked_fill_78 = None
    getitem_1093 = split_tensor_113[1];  split_tensor_113 = None
    unsqueeze_476 = torch.ops.aten.unsqueeze.default(getitem_1093, 4);  getitem_1093 = None
    permute_712 = torch.ops.aten.permute.default(unsqueeze_476, [0, 4, 1, 3, 2]);  unsqueeze_476 = None
    permute_713 = torch.ops.aten.permute.default(permute_712, [3, 4, 0, 2, 1]);  permute_712 = None
    view_1199 = torch.ops.aten.view.default(permute_713, [256, 384, 384]);  permute_713 = None
    bmm_112 = torch.ops.aten.bmm.default(view_1198, view_1199);  view_1198 = view_1199 = None
    view_1200 = torch.ops.aten.view.default(bmm_112, [256, 384, 1, 1, 384]);  bmm_112 = None
    permute_714 = torch.ops.aten.permute.default(view_1200, [3, 1, 4, 0, 2]);  view_1200 = None
    view_1201 = torch.ops.aten.view.default(permute_714, [1, 384, 384, 256]);  permute_714 = None
    _to_copy_678 = torch.ops.aten._to_copy.default(getitem_1084, dtype = torch.bfloat16);  getitem_1084 = None
    _to_copy_679 = torch.ops.aten._to_copy.default(getitem_1080, dtype = torch.bfloat16)
    t_239 = torch.ops.aten.t.default(_to_copy_678);  _to_copy_678 = None
    view_1202 = torch.ops.aten.view.default(_to_copy_679, [147456, 256]);  _to_copy_679 = None
    mm_221 = torch.ops.aten.mm.default(view_1202, t_239);  view_1202 = t_239 = None
    view_1203 = torch.ops.aten.view.default(mm_221, [1, 384, 384, 512]);  mm_221 = None
    _to_copy_680 = torch.ops.aten._to_copy.default(getitem_1086, dtype = torch.bfloat16);  getitem_1086 = None
    _to_copy_681 = torch.ops.aten._to_copy.default(getitem_1080, dtype = torch.bfloat16)
    t_240 = torch.ops.aten.t.default(_to_copy_680);  _to_copy_680 = None
    view_1204 = torch.ops.aten.view.default(_to_copy_681, [147456, 256]);  _to_copy_681 = None
    mm_222 = torch.ops.aten.mm.default(view_1204, t_240);  view_1204 = t_240 = None
    view_1205 = torch.ops.aten.view.default(mm_222, [1, 384, 384, 512]);  mm_222 = None
    sigmoid_85 = torch.ops.aten.sigmoid.default(view_1205);  view_1205 = None
    mul_139 = torch.ops.aten.mul.Tensor(view_1203, sigmoid_85);  view_1203 = sigmoid_85 = None
    view_1206 = torch.ops.aten.view.default(mul_139, [147456, 512]);  mul_139 = None
    view_1207 = torch.ops.aten.view.default(view_1206, [1, 384, 384, 512]);  view_1206 = None
    transpose_28 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
    unsqueeze_477 = torch.ops.aten.unsqueeze.default(transpose_28, 3);  transpose_28 = None
    clone_129 = torch.ops.aten.clone.default(unsqueeze_477, memory_format = torch.contiguous_format);  unsqueeze_477 = None
    bitwise_not_79 = torch.ops.aten.bitwise_not.default(clone_129);  clone_129 = None
    masked_fill_79 = torch.ops.aten.masked_fill.Scalar(view_1207, bitwise_not_79, 0);  view_1207 = bitwise_not_79 = None
    view_1208 = torch.ops.aten.view.default(masked_fill_79, [147456, 512]);  masked_fill_79 = None
    view_1212 = torch.ops.aten.view.default(view_1208, [1, 384, 384, 512])
    split_tensor_114 = torch.ops.aten.split.Tensor(view_1212, 256, dim = -1);  view_1212 = None
    getitem_1096 = split_tensor_114[0];  split_tensor_114 = None
    unsqueeze_480 = torch.ops.aten.unsqueeze.default(getitem_1096, 4);  getitem_1096 = None
    permute_719 = torch.ops.aten.permute.default(unsqueeze_480, [0, 2, 4, 3, 1]);  unsqueeze_480 = None
    permute_720 = torch.ops.aten.permute.default(permute_719, [3, 1, 4, 0, 2]);  permute_719 = None
    view_1213 = torch.ops.aten.view.default(permute_720, [256, 384, 384]);  permute_720 = None
    view_1214 = torch.ops.aten.view.default(view_1208, [1, 384, 384, 512]);  view_1208 = None
    split_tensor_115 = torch.ops.aten.split.Tensor(view_1214, 256, dim = -1);  view_1214 = None
    getitem_1099 = split_tensor_115[1];  split_tensor_115 = None
    unsqueeze_481 = torch.ops.aten.unsqueeze.default(getitem_1099, 4);  getitem_1099 = None
    permute_721 = torch.ops.aten.permute.default(unsqueeze_481, [0, 4, 2, 3, 1]);  unsqueeze_481 = None
    permute_722 = torch.ops.aten.permute.default(permute_721, [3, 4, 0, 2, 1]);  permute_721 = None
    view_1215 = torch.ops.aten.view.default(permute_722, [256, 384, 384]);  permute_722 = None
    bmm_113 = torch.ops.aten.bmm.default(view_1213, view_1215);  view_1213 = view_1215 = None
    view_1216 = torch.ops.aten.view.default(bmm_113, [256, 384, 1, 1, 384]);  bmm_113 = None
    permute_723 = torch.ops.aten.permute.default(view_1216, [3, 1, 4, 0, 2]);  view_1216 = None
    view_1217 = torch.ops.aten.view.default(permute_723, [1, 384, 384, 256]);  permute_723 = None
    _to_copy_682 = torch.ops.aten._to_copy.default(view_1201, dtype = torch.float32);  view_1201 = None
    native_layer_norm_default_140 = torch.ops.aten.native_layer_norm.default(_to_copy_682, [256], None, None, 1e-05);  _to_copy_682 = None
    getitem_1100 = native_layer_norm_default_140[0];  native_layer_norm_default_140 = None
    _to_copy_683 = torch.ops.aten._to_copy.default(view_1217, dtype = torch.float32);  view_1217 = None
    native_layer_norm_default_141 = torch.ops.aten.native_layer_norm.default(_to_copy_683, [256], None, None, 1e-05);  _to_copy_683 = None
    getitem_1103 = native_layer_norm_default_141[0];  native_layer_norm_default_141 = None
    add_120 = torch.ops.aten.add.Tensor(getitem_1100, getitem_1103);  getitem_1100 = getitem_1103 = None
    _to_copy_684 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_8_triangle_multiplication_linear_z_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_8_triangle_multiplication_linear_z_out_weight = None
    _to_copy_685 = torch.ops.aten._to_copy.default(add_120, dtype = torch.bfloat16);  add_120 = None
    t_241 = torch.ops.aten.t.default(_to_copy_684);  _to_copy_684 = None
    view_1218 = torch.ops.aten.view.default(_to_copy_685, [147456, 256]);  _to_copy_685 = None
    mm_223 = torch.ops.aten.mm.default(view_1218, t_241);  view_1218 = t_241 = None
    view_1219 = torch.ops.aten.view.default(mm_223, [1, 384, 384, 256]);  mm_223 = None
    _to_copy_686 = torch.ops.aten._to_copy.default(getitem_1087, dtype = torch.bfloat16);  getitem_1087 = None
    _to_copy_687 = torch.ops.aten._to_copy.default(getitem_1080, dtype = torch.bfloat16);  getitem_1080 = None
    t_242 = torch.ops.aten.t.default(_to_copy_686);  _to_copy_686 = None
    view_1220 = torch.ops.aten.view.default(_to_copy_687, [147456, 256]);  _to_copy_687 = None
    mm_224 = torch.ops.aten.mm.default(view_1220, t_242);  view_1220 = t_242 = None
    view_1221 = torch.ops.aten.view.default(mm_224, [1, 384, 384, 256]);  mm_224 = None
    sigmoid_86 = torch.ops.aten.sigmoid.default(view_1221);  view_1221 = None
    mul_140 = torch.ops.aten.mul.Tensor(view_1219, sigmoid_86);  view_1219 = sigmoid_86 = None
    add_121 = torch.ops.aten.add.Tensor(add_115, mul_140);  mul_140 = None
    _to_copy_688 = torch.ops.aten._to_copy.default(add_115, dtype = torch.float32)
    native_layer_norm_default_142 = torch.ops.aten.native_layer_norm.default(_to_copy_688, [256], None, None, 1e-05);  _to_copy_688 = None
    getitem_1106 = native_layer_norm_default_142[0];  native_layer_norm_default_142 = None
    _to_copy_689 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_8_triangle_attention_pair2b_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_8_triangle_attention_pair2b_weight = None
    _to_copy_690 = torch.ops.aten._to_copy.default(getitem_1106, dtype = torch.bfloat16)
    t_243 = torch.ops.aten.t.default(_to_copy_689);  _to_copy_689 = None
    view_1222 = torch.ops.aten.view.default(_to_copy_690, [147456, 256]);  _to_copy_690 = None
    mm_225 = torch.ops.aten.mm.default(view_1222, t_243);  view_1222 = t_243 = None
    view_1223 = torch.ops.aten.view.default(mm_225, [1, 384, 384, 8]);  mm_225 = None
    view_1224 = torch.ops.aten.view.default(view_1223, [1, 384, 384, 2, 4]);  view_1223 = None
    permute_724 = torch.ops.aten.permute.default(view_1224, [0, 3, 4, 1, 2]);  view_1224 = None
    view_1225 = torch.ops.aten.view.default(permute_724, [1, 2, 4, 1, 384, 384]);  permute_724 = None
    view_1226 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 384, 384])
    bitwise_not_80 = torch.ops.aten.bitwise_not.default(view_1226);  view_1226 = None
    masked_fill_80 = torch.ops.aten.masked_fill.Scalar(view_1225, bitwise_not_80, -10000);  view_1225 = bitwise_not_80 = None
    view_1227 = torch.ops.aten.view.default(masked_fill_80, [1, 2, 4, 384, 384]);  masked_fill_80 = None
    permute_725 = torch.ops.aten.permute.default(view_1227, [1, 0, 2, 3, 4]);  view_1227 = None
    view_1228 = torch.ops.aten.view.default(permute_725, [2, 4, 1, 384, 384]);  permute_725 = None
    _to_copy_691 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_8_triangle_attention_pair2qkvg1_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_8_triangle_attention_pair2qkvg1_weight = None
    _to_copy_692 = torch.ops.aten._to_copy.default(getitem_1106, dtype = torch.bfloat16)
    t_244 = torch.ops.aten.t.default(_to_copy_691);  _to_copy_691 = None
    view_1229 = torch.ops.aten.view.default(_to_copy_692, [147456, 256]);  _to_copy_692 = None
    mm_226 = torch.ops.aten.mm.default(view_1229, t_244);  view_1229 = t_244 = None
    view_1230 = torch.ops.aten.view.default(mm_226, [1, 384, 384, 1024]);  mm_226 = None
    select_29 = torch.ops.aten.select.int(view_1228, 0, 0)
    view_1231 = torch.ops.aten.view.default(view_1230, [1, 384, 384, 4, 4, 64]);  view_1230 = None
    permute_726 = torch.ops.aten.permute.default(view_1231, [4, 0, 3, 1, 2, 5]);  view_1231 = None
    view_1232 = torch.ops.aten.view.default(permute_726, [4, 4, 384, 384, 64]);  permute_726 = None
    unbind_int_58 = torch.ops.aten.unbind.int(view_1232);  view_1232 = None
    getitem_1109 = unbind_int_58[0]
    getitem_1110 = unbind_int_58[1]
    getitem_1111 = unbind_int_58[2]
    getitem_1112 = unbind_int_58[3];  unbind_int_58 = None
    expand_67 = torch.ops.aten.expand.default(select_29, [4, 384, 384, 384]);  select_29 = None
    _scaled_dot_product_efficient_attention_default_36 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1109, getitem_1110, getitem_1111, expand_67, False);  getitem_1109 = getitem_1110 = getitem_1111 = expand_67 = None
    getitem_1113 = _scaled_dot_product_efficient_attention_default_36[0];  _scaled_dot_product_efficient_attention_default_36 = None
    sigmoid_87 = torch.ops.aten.sigmoid.default(getitem_1112);  getitem_1112 = None
    mul_141 = torch.ops.aten.mul.Tensor(getitem_1113, sigmoid_87);  getitem_1113 = sigmoid_87 = None
    view_1233 = torch.ops.aten.view.default(mul_141, [1, 4, 384, 384, 64]);  mul_141 = None
    permute_727 = torch.ops.aten.permute.default(view_1233, [0, 2, 3, 1, 4]);  view_1233 = None
    clone_130 = torch.ops.aten.clone.default(permute_727, memory_format = torch.contiguous_format);  permute_727 = None
    _unsafe_view_114 = torch.ops.aten._unsafe_view.default(clone_130, [1, 384, 384, 256]);  clone_130 = None
    transpose_29 = torch.ops.aten.transpose.int(getitem_1106, 1, 2);  getitem_1106 = None
    _to_copy_693 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_8_triangle_attention_pair2qkvg2_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_8_triangle_attention_pair2qkvg2_weight = None
    _to_copy_694 = torch.ops.aten._to_copy.default(transpose_29, dtype = torch.bfloat16);  transpose_29 = None
    t_245 = torch.ops.aten.t.default(_to_copy_693);  _to_copy_693 = None
    expand_68 = torch.ops.aten.expand.default(_to_copy_694, [1, 384, 384, 256]);  _to_copy_694 = None
    view_1234 = torch.ops.aten.view.default(expand_68, [384, 384, 256]);  expand_68 = None
    expand_69 = torch.ops.aten.expand.default(t_245, [1, 384, 256, 1024]);  t_245 = None
    view_1235 = torch.ops.aten.view.default(expand_69, [384, 256, 1024]);  expand_69 = None
    bmm_114 = torch.ops.aten.bmm.default(view_1234, view_1235);  view_1234 = view_1235 = None
    view_1236 = torch.ops.aten.view.default(bmm_114, [1, 384, 384, 1024]);  bmm_114 = None
    select_30 = torch.ops.aten.select.int(view_1228, 0, 1);  view_1228 = None
    view_1237 = torch.ops.aten.view.default(view_1236, [1, 384, 384, 4, 4, 64]);  view_1236 = None
    permute_728 = torch.ops.aten.permute.default(view_1237, [4, 0, 3, 1, 2, 5]);  view_1237 = None
    view_1238 = torch.ops.aten.view.default(permute_728, [4, 4, 384, 384, 64]);  permute_728 = None
    unbind_int_59 = torch.ops.aten.unbind.int(view_1238);  view_1238 = None
    getitem_1117 = unbind_int_59[0]
    getitem_1118 = unbind_int_59[1]
    getitem_1119 = unbind_int_59[2]
    getitem_1120 = unbind_int_59[3];  unbind_int_59 = None
    expand_70 = torch.ops.aten.expand.default(select_30, [4, 384, 384, 384]);  select_30 = None
    _scaled_dot_product_efficient_attention_default_37 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1117, getitem_1118, getitem_1119, expand_70, False);  getitem_1117 = getitem_1118 = getitem_1119 = expand_70 = None
    getitem_1121 = _scaled_dot_product_efficient_attention_default_37[0];  _scaled_dot_product_efficient_attention_default_37 = None
    sigmoid_88 = torch.ops.aten.sigmoid.default(getitem_1120);  getitem_1120 = None
    mul_142 = torch.ops.aten.mul.Tensor(getitem_1121, sigmoid_88);  getitem_1121 = sigmoid_88 = None
    view_1239 = torch.ops.aten.view.default(mul_142, [1, 4, 384, 384, 64]);  mul_142 = None
    permute_729 = torch.ops.aten.permute.default(view_1239, [0, 2, 3, 1, 4]);  view_1239 = None
    clone_131 = torch.ops.aten.clone.default(permute_729, memory_format = torch.contiguous_format);  permute_729 = None
    _unsafe_view_115 = torch.ops.aten._unsafe_view.default(clone_131, [1, 384, 384, 256]);  clone_131 = None
    cat_20 = torch.ops.aten.cat.default([_unsafe_view_114, _unsafe_view_115], dim = -1);  _unsafe_view_114 = _unsafe_view_115 = None
    slice_171 = torch.ops.aten.slice.Tensor(pairformer_stack_blocks_8_triangle_attention_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  pairformer_stack_blocks_8_triangle_attention_out_scalers = None
    unsqueeze_482 = torch.ops.aten.unsqueeze.default(slice_171, 1);  slice_171 = None
    mul_143 = torch.ops.aten.mul.Tensor(pairformer_stack_blocks_8_triangle_attention_linear_out_weight, unsqueeze_482);  pairformer_stack_blocks_8_triangle_attention_linear_out_weight = unsqueeze_482 = None
    _to_copy_695 = torch.ops.aten._to_copy.default(mul_143, dtype = torch.bfloat16);  mul_143 = None
    t_246 = torch.ops.aten.t.default(_to_copy_695);  _to_copy_695 = None
    view_1240 = torch.ops.aten.view.default(cat_20, [147456, 512]);  cat_20 = None
    mm_227 = torch.ops.aten.mm.default(view_1240, t_246);  view_1240 = t_246 = None
    view_1241 = torch.ops.aten.view.default(mm_227, [1, 384, 384, 256]);  mm_227 = None
    add_122 = torch.ops.aten.add.Tensor(add_121, view_1241);  add_121 = view_1241 = None
    split_tensor_116 = torch.ops.aten.split.Tensor(add_115, 384, dim = -2)
    getitem_1125 = split_tensor_116[0];  split_tensor_116 = None
    _to_copy_696 = torch.ops.aten._to_copy.default(getitem_1125, dtype = torch.float32);  getitem_1125 = None
    native_layer_norm_default_143 = torch.ops.aten.native_layer_norm.default(_to_copy_696, [256], pairformer_stack_blocks_8_transition_pair_layer_norm_weight, pairformer_stack_blocks_8_transition_pair_layer_norm_bias, 1e-05);  _to_copy_696 = pairformer_stack_blocks_8_transition_pair_layer_norm_weight = pairformer_stack_blocks_8_transition_pair_layer_norm_bias = None
    getitem_1126 = native_layer_norm_default_143[0];  native_layer_norm_default_143 = None
    _to_copy_697 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_8_transition_pair_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_8_transition_pair_linear_no_bias_ab_weight = None
    _to_copy_698 = torch.ops.aten._to_copy.default(getitem_1126, dtype = torch.bfloat16);  getitem_1126 = None
    t_247 = torch.ops.aten.t.default(_to_copy_697);  _to_copy_697 = None
    view_1242 = torch.ops.aten.view.default(_to_copy_698, [147456, 256]);  _to_copy_698 = None
    mm_228 = torch.ops.aten.mm.default(view_1242, t_247);  view_1242 = t_247 = None
    view_1243 = torch.ops.aten.view.default(mm_228, [1, 384, 384, 1024]);  mm_228 = None
    split_tensor_117 = torch.ops.aten.split.Tensor(view_1243, 512, dim = -1);  view_1243 = None
    getitem_1129 = split_tensor_117[0]
    getitem_1130 = split_tensor_117[1];  split_tensor_117 = None
    silu_31 = torch.ops.aten.silu.default(getitem_1129);  getitem_1129 = None
    mul_144 = torch.ops.aten.mul.Tensor(silu_31, getitem_1130);  silu_31 = getitem_1130 = None
    _to_copy_699 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_8_transition_pair_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_8_transition_pair_linear_out_weight = None
    t_248 = torch.ops.aten.t.default(_to_copy_699);  _to_copy_699 = None
    view_1245 = torch.ops.aten.view.default(mul_144, [147456, 512]);  mul_144 = None
    mm_229 = torch.ops.aten.mm.default(view_1245, t_248);  view_1245 = t_248 = None
    view_1246 = torch.ops.aten.view.default(mm_229, [1, 384, 384, 256]);  mm_229 = None
    add_123 = torch.ops.aten.add.Tensor(add_122, view_1246);  add_122 = view_1246 = None
    _to_copy_700 = torch.ops.aten._to_copy.default(add_119, dtype = torch.float32)
    native_layer_norm_default_144 = torch.ops.aten.native_layer_norm.default(_to_copy_700, [384], pairformer_stack_blocks_8_attention_pair_bias_single_layer_norm_weight, pairformer_stack_blocks_8_attention_pair_bias_single_layer_norm_bias, 1e-05);  _to_copy_700 = pairformer_stack_blocks_8_attention_pair_bias_single_layer_norm_weight = pairformer_stack_blocks_8_attention_pair_bias_single_layer_norm_bias = None
    getitem_1131 = native_layer_norm_default_144[0];  native_layer_norm_default_144 = None
    _to_copy_701 = torch.ops.aten._to_copy.default(add_115, dtype = torch.float32);  add_115 = None
    native_layer_norm_default_145 = torch.ops.aten.native_layer_norm.default(_to_copy_701, [256], pairformer_stack_blocks_8_attention_pair_bias_pair_layer_norm_weight, pairformer_stack_blocks_8_attention_pair_bias_pair_layer_norm_bias, 1e-05);  _to_copy_701 = pairformer_stack_blocks_8_attention_pair_bias_pair_layer_norm_weight = pairformer_stack_blocks_8_attention_pair_bias_pair_layer_norm_bias = None
    getitem_1134 = native_layer_norm_default_145[0];  native_layer_norm_default_145 = None
    _to_copy_702 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_8_attention_pair_bias_pair_linear_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_8_attention_pair_bias_pair_linear_weight = None
    _to_copy_703 = torch.ops.aten._to_copy.default(getitem_1134, dtype = torch.bfloat16);  getitem_1134 = None
    t_249 = torch.ops.aten.t.default(_to_copy_702);  _to_copy_702 = None
    view_1247 = torch.ops.aten.view.default(_to_copy_703, [147456, 256]);  _to_copy_703 = None
    mm_230 = torch.ops.aten.mm.default(view_1247, t_249);  view_1247 = t_249 = None
    view_1248 = torch.ops.aten.view.default(mm_230, [1, 384, 384, 16]);  mm_230 = None
    permute_730 = torch.ops.aten.permute.default(view_1248, [0, 3, 1, 2]);  view_1248 = None
    view_1249 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384])
    bitwise_not_81 = torch.ops.aten.bitwise_not.default(view_1249);  view_1249 = None
    masked_fill_81 = torch.ops.aten.masked_fill.Scalar(permute_730, bitwise_not_81, -10000);  permute_730 = bitwise_not_81 = None
    _to_copy_704 = torch.ops.aten._to_copy.default(getitem_1131, dtype = torch.bfloat16);  getitem_1131 = None
    _to_copy_705 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_8_attention_pair_bias_attention_input2qkvg_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_8_attention_pair_bias_attention_input2qkvg_weight = None
    unsqueeze_483 = torch.ops.aten.unsqueeze.default(_to_copy_704, 3);  _to_copy_704 = None
    unsqueeze_484 = torch.ops.aten.unsqueeze.default(unsqueeze_483, 4);  unsqueeze_483 = None
    unsqueeze_485 = torch.ops.aten.unsqueeze.default(unsqueeze_484, 5);  unsqueeze_484 = None
    permute_731 = torch.ops.aten.permute.default(unsqueeze_485, [3, 0, 4, 1, 5, 2]);  unsqueeze_485 = None
    unsqueeze_486 = torch.ops.aten.unsqueeze.default(_to_copy_705, 4);  _to_copy_705 = None
    unsqueeze_487 = torch.ops.aten.unsqueeze.default(unsqueeze_486, 5);  unsqueeze_486 = None
    permute_732 = torch.ops.aten.permute.default(unsqueeze_487, [1, 4, 2, 5, 3, 0]);  unsqueeze_487 = None
    permute_733 = torch.ops.aten.permute.default(permute_731, [3, 5, 0, 1, 2, 4]);  permute_731 = None
    view_1250 = torch.ops.aten.view.default(permute_733, [1, 384, 384]);  permute_733 = None
    permute_734 = torch.ops.aten.permute.default(permute_732, [5, 0, 1, 2, 4, 3]);  permute_732 = None
    view_1251 = torch.ops.aten.view.default(permute_734, [1, 384, 1536]);  permute_734 = None
    bmm_115 = torch.ops.aten.bmm.default(view_1250, view_1251);  view_1250 = view_1251 = None
    view_1252 = torch.ops.aten.view.default(bmm_115, [384, 1, 4, 1, 16, 24]);  bmm_115 = None
    permute_735 = torch.ops.aten.permute.default(view_1252, [2, 3, 4, 0, 5, 1]);  view_1252 = None
    view_1253 = torch.ops.aten.view.default(permute_735, [4, 1, 16, 384, 24]);  permute_735 = None
    unbind_int_60 = torch.ops.aten.unbind.int(view_1253);  view_1253 = None
    getitem_1137 = unbind_int_60[0]
    getitem_1138 = unbind_int_60[1]
    getitem_1139 = unbind_int_60[2]
    getitem_1140 = unbind_int_60[3];  unbind_int_60 = None
    view_1254 = torch.ops.aten.view.default(pairformer_stack_blocks_8_attention_pair_bias_attention_query_bias, [1, 16, 1, 24]);  pairformer_stack_blocks_8_attention_pair_bias_attention_query_bias = None
    add_124 = torch.ops.aten.add.Tensor(getitem_1137, view_1254);  getitem_1137 = view_1254 = None
    _to_copy_706 = torch.ops.aten._to_copy.default(add_124, dtype = torch.bfloat16);  add_124 = None
    expand_71 = torch.ops.aten.expand.default(masked_fill_81, [1, 16, 384, 384]);  masked_fill_81 = None
    _scaled_dot_product_efficient_attention_default_38 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_706, getitem_1138, getitem_1139, expand_71, False);  _to_copy_706 = getitem_1138 = getitem_1139 = expand_71 = None
    getitem_1141 = _scaled_dot_product_efficient_attention_default_38[0];  _scaled_dot_product_efficient_attention_default_38 = None
    add_125 = torch.ops.aten.add.Tensor(getitem_1140, 1);  getitem_1140 = None
    sigmoid_89 = torch.ops.aten.sigmoid.default(add_125);  add_125 = None
    mul_145 = torch.ops.aten.mul.Tensor(getitem_1141, sigmoid_89);  getitem_1141 = sigmoid_89 = None
    _to_copy_707 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_8_attention_pair_bias_attention_output_proj_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_8_attention_pair_bias_attention_output_proj_weight = None
    unsqueeze_488 = torch.ops.aten.unsqueeze.default(mul_145, 4);  mul_145 = None
    permute_736 = torch.ops.aten.permute.default(unsqueeze_488, [0, 2, 4, 3, 1]);  unsqueeze_488 = None
    unsqueeze_489 = torch.ops.aten.unsqueeze.default(_to_copy_707, 3);  _to_copy_707 = None
    unsqueeze_490 = torch.ops.aten.unsqueeze.default(unsqueeze_489, 4);  unsqueeze_489 = None
    permute_737 = torch.ops.aten.permute.default(unsqueeze_490, [3, 4, 2, 1, 0]);  unsqueeze_490 = None
    permute_738 = torch.ops.aten.permute.default(permute_736, [1, 3, 4, 0, 2]);  permute_736 = None
    clone_132 = torch.ops.aten.clone.default(permute_738, memory_format = torch.contiguous_format);  permute_738 = None
    _unsafe_view_116 = torch.ops.aten._unsafe_view.default(clone_132, [1, 384, 384]);  clone_132 = None
    permute_739 = torch.ops.aten.permute.default(permute_737, [3, 4, 0, 2, 1]);  permute_737 = None
    clone_133 = torch.ops.aten.clone.default(permute_739, memory_format = torch.contiguous_format);  permute_739 = None
    _unsafe_view_117 = torch.ops.aten._unsafe_view.default(clone_133, [1, 384, 384]);  clone_133 = None
    bmm_116 = torch.ops.aten.bmm.default(_unsafe_view_116, _unsafe_view_117);  _unsafe_view_116 = _unsafe_view_117 = None
    view_1255 = torch.ops.aten.view.default(bmm_116, [384, 1, 1, 1, 384]);  bmm_116 = None
    permute_740 = torch.ops.aten.permute.default(view_1255, [3, 0, 4, 1, 2]);  view_1255 = None
    view_1256 = torch.ops.aten.view.default(permute_740, [1, 384, 384]);  permute_740 = None
    unsqueeze_491 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
    mul_146 = torch.ops.aten.mul.Tensor(view_1256, unsqueeze_491);  view_1256 = unsqueeze_491 = None
    add_126 = torch.ops.aten.add.Tensor(add_119, mul_146);  mul_146 = None
    split_tensor_118 = torch.ops.aten.split.Tensor(add_119, 384, dim = -2);  add_119 = None
    getitem_1145 = split_tensor_118[0];  split_tensor_118 = None
    _to_copy_708 = torch.ops.aten._to_copy.default(getitem_1145, dtype = torch.float32);  getitem_1145 = None
    native_layer_norm_default_146 = torch.ops.aten.native_layer_norm.default(_to_copy_708, [384], pairformer_stack_blocks_8_transition_single_layer_norm_weight, pairformer_stack_blocks_8_transition_single_layer_norm_bias, 1e-05);  _to_copy_708 = pairformer_stack_blocks_8_transition_single_layer_norm_weight = pairformer_stack_blocks_8_transition_single_layer_norm_bias = None
    getitem_1146 = native_layer_norm_default_146[0];  native_layer_norm_default_146 = None
    _to_copy_709 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_8_transition_single_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_8_transition_single_linear_no_bias_ab_weight = None
    _to_copy_710 = torch.ops.aten._to_copy.default(getitem_1146, dtype = torch.bfloat16);  getitem_1146 = None
    t_250 = torch.ops.aten.t.default(_to_copy_709);  _to_copy_709 = None
    view_1257 = torch.ops.aten.view.default(_to_copy_710, [384, 384]);  _to_copy_710 = None
    mm_231 = torch.ops.aten.mm.default(view_1257, t_250);  view_1257 = t_250 = None
    view_1258 = torch.ops.aten.view.default(mm_231, [1, 384, 1536]);  mm_231 = None
    split_tensor_119 = torch.ops.aten.split.Tensor(view_1258, 768, dim = -1);  view_1258 = None
    getitem_1149 = split_tensor_119[0]
    getitem_1150 = split_tensor_119[1];  split_tensor_119 = None
    silu_32 = torch.ops.aten.silu.default(getitem_1149);  getitem_1149 = None
    mul_147 = torch.ops.aten.mul.Tensor(silu_32, getitem_1150);  silu_32 = getitem_1150 = None
    _to_copy_711 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_8_transition_single_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_8_transition_single_linear_out_weight = None
    t_251 = torch.ops.aten.t.default(_to_copy_711);  _to_copy_711 = None
    view_1260 = torch.ops.aten.view.default(mul_147, [384, 768]);  mul_147 = None
    mm_232 = torch.ops.aten.mm.default(view_1260, t_251);  view_1260 = t_251 = None
    view_1261 = torch.ops.aten.view.default(mm_232, [1, 384, 384]);  mm_232 = None
    add_127 = torch.ops.aten.add.Tensor(add_126, view_1261);  add_126 = view_1261 = None
    _to_copy_712 = torch.ops.aten._to_copy.default(add_123, dtype = torch.float32)
    native_layer_norm_default_147 = torch.ops.aten.native_layer_norm.default(_to_copy_712, [256], pairformer_stack_blocks_9_triangle_multiplication_layernorm_z_in_weight, pairformer_stack_blocks_9_triangle_multiplication_layernorm_z_in_bias, 1e-05);  _to_copy_712 = pairformer_stack_blocks_9_triangle_multiplication_layernorm_z_in_weight = pairformer_stack_blocks_9_triangle_multiplication_layernorm_z_in_bias = None
    getitem_1151 = native_layer_norm_default_147[0];  native_layer_norm_default_147 = None
    split_with_sizes_default_30 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_9_triangle_multiplication_merged_linear_p_weight, [512, 512]);  pairformer_stack_blocks_9_triangle_multiplication_merged_linear_p_weight = None
    getitem_1154 = split_with_sizes_default_30[0]
    getitem_1155 = split_with_sizes_default_30[1];  split_with_sizes_default_30 = None
    split_with_sizes_default_31 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_9_triangle_multiplication_merged_linear_g_weight, [512, 512, 256]);  pairformer_stack_blocks_9_triangle_multiplication_merged_linear_g_weight = None
    getitem_1156 = split_with_sizes_default_31[0]
    getitem_1157 = split_with_sizes_default_31[1]
    getitem_1158 = split_with_sizes_default_31[2];  split_with_sizes_default_31 = None
    _to_copy_713 = torch.ops.aten._to_copy.default(getitem_1154, dtype = torch.bfloat16);  getitem_1154 = None
    _to_copy_714 = torch.ops.aten._to_copy.default(getitem_1151, dtype = torch.bfloat16)
    t_252 = torch.ops.aten.t.default(_to_copy_713);  _to_copy_713 = None
    view_1262 = torch.ops.aten.view.default(_to_copy_714, [147456, 256]);  _to_copy_714 = None
    mm_233 = torch.ops.aten.mm.default(view_1262, t_252);  view_1262 = t_252 = None
    view_1263 = torch.ops.aten.view.default(mm_233, [1, 384, 384, 512]);  mm_233 = None
    _to_copy_715 = torch.ops.aten._to_copy.default(getitem_1156, dtype = torch.bfloat16);  getitem_1156 = None
    _to_copy_716 = torch.ops.aten._to_copy.default(getitem_1151, dtype = torch.bfloat16)
    t_253 = torch.ops.aten.t.default(_to_copy_715);  _to_copy_715 = None
    view_1264 = torch.ops.aten.view.default(_to_copy_716, [147456, 256]);  _to_copy_716 = None
    mm_234 = torch.ops.aten.mm.default(view_1264, t_253);  view_1264 = t_253 = None
    view_1265 = torch.ops.aten.view.default(mm_234, [1, 384, 384, 512]);  mm_234 = None
    sigmoid_90 = torch.ops.aten.sigmoid.default(view_1265);  view_1265 = None
    mul_148 = torch.ops.aten.mul.Tensor(view_1263, sigmoid_90);  view_1263 = sigmoid_90 = None
    unsqueeze_492 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
    bitwise_not_82 = torch.ops.aten.bitwise_not.default(unsqueeze_492);  unsqueeze_492 = None
    masked_fill_82 = torch.ops.aten.masked_fill.Scalar(mul_148, bitwise_not_82, 0);  mul_148 = bitwise_not_82 = None
    split_tensor_120 = torch.ops.aten.split.Tensor(masked_fill_82, 256, dim = -1)
    getitem_1161 = split_tensor_120[0];  split_tensor_120 = None
    unsqueeze_495 = torch.ops.aten.unsqueeze.default(getitem_1161, 4);  getitem_1161 = None
    permute_745 = torch.ops.aten.permute.default(unsqueeze_495, [0, 1, 4, 3, 2]);  unsqueeze_495 = None
    permute_746 = torch.ops.aten.permute.default(permute_745, [3, 1, 4, 0, 2]);  permute_745 = None
    view_1268 = torch.ops.aten.view.default(permute_746, [256, 384, 384]);  permute_746 = None
    split_tensor_121 = torch.ops.aten.split.Tensor(masked_fill_82, 256, dim = -1);  masked_fill_82 = None
    getitem_1164 = split_tensor_121[1];  split_tensor_121 = None
    unsqueeze_496 = torch.ops.aten.unsqueeze.default(getitem_1164, 4);  getitem_1164 = None
    permute_747 = torch.ops.aten.permute.default(unsqueeze_496, [0, 4, 1, 3, 2]);  unsqueeze_496 = None
    permute_748 = torch.ops.aten.permute.default(permute_747, [3, 4, 0, 2, 1]);  permute_747 = None
    view_1269 = torch.ops.aten.view.default(permute_748, [256, 384, 384]);  permute_748 = None
    bmm_117 = torch.ops.aten.bmm.default(view_1268, view_1269);  view_1268 = view_1269 = None
    view_1270 = torch.ops.aten.view.default(bmm_117, [256, 384, 1, 1, 384]);  bmm_117 = None
    permute_749 = torch.ops.aten.permute.default(view_1270, [3, 1, 4, 0, 2]);  view_1270 = None
    view_1271 = torch.ops.aten.view.default(permute_749, [1, 384, 384, 256]);  permute_749 = None
    _to_copy_717 = torch.ops.aten._to_copy.default(getitem_1155, dtype = torch.bfloat16);  getitem_1155 = None
    _to_copy_718 = torch.ops.aten._to_copy.default(getitem_1151, dtype = torch.bfloat16)
    t_254 = torch.ops.aten.t.default(_to_copy_717);  _to_copy_717 = None
    view_1272 = torch.ops.aten.view.default(_to_copy_718, [147456, 256]);  _to_copy_718 = None
    mm_235 = torch.ops.aten.mm.default(view_1272, t_254);  view_1272 = t_254 = None
    view_1273 = torch.ops.aten.view.default(mm_235, [1, 384, 384, 512]);  mm_235 = None
    _to_copy_719 = torch.ops.aten._to_copy.default(getitem_1157, dtype = torch.bfloat16);  getitem_1157 = None
    _to_copy_720 = torch.ops.aten._to_copy.default(getitem_1151, dtype = torch.bfloat16)
    t_255 = torch.ops.aten.t.default(_to_copy_719);  _to_copy_719 = None
    view_1274 = torch.ops.aten.view.default(_to_copy_720, [147456, 256]);  _to_copy_720 = None
    mm_236 = torch.ops.aten.mm.default(view_1274, t_255);  view_1274 = t_255 = None
    view_1275 = torch.ops.aten.view.default(mm_236, [1, 384, 384, 512]);  mm_236 = None
    sigmoid_91 = torch.ops.aten.sigmoid.default(view_1275);  view_1275 = None
    mul_149 = torch.ops.aten.mul.Tensor(view_1273, sigmoid_91);  view_1273 = sigmoid_91 = None
    view_1276 = torch.ops.aten.view.default(mul_149, [147456, 512]);  mul_149 = None
    view_1277 = torch.ops.aten.view.default(view_1276, [1, 384, 384, 512]);  view_1276 = None
    transpose_30 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
    unsqueeze_497 = torch.ops.aten.unsqueeze.default(transpose_30, 3);  transpose_30 = None
    clone_134 = torch.ops.aten.clone.default(unsqueeze_497, memory_format = torch.contiguous_format);  unsqueeze_497 = None
    bitwise_not_83 = torch.ops.aten.bitwise_not.default(clone_134);  clone_134 = None
    masked_fill_83 = torch.ops.aten.masked_fill.Scalar(view_1277, bitwise_not_83, 0);  view_1277 = bitwise_not_83 = None
    view_1278 = torch.ops.aten.view.default(masked_fill_83, [147456, 512]);  masked_fill_83 = None
    view_1282 = torch.ops.aten.view.default(view_1278, [1, 384, 384, 512])
    split_tensor_122 = torch.ops.aten.split.Tensor(view_1282, 256, dim = -1);  view_1282 = None
    getitem_1167 = split_tensor_122[0];  split_tensor_122 = None
    unsqueeze_500 = torch.ops.aten.unsqueeze.default(getitem_1167, 4);  getitem_1167 = None
    permute_754 = torch.ops.aten.permute.default(unsqueeze_500, [0, 2, 4, 3, 1]);  unsqueeze_500 = None
    permute_755 = torch.ops.aten.permute.default(permute_754, [3, 1, 4, 0, 2]);  permute_754 = None
    view_1283 = torch.ops.aten.view.default(permute_755, [256, 384, 384]);  permute_755 = None
    view_1284 = torch.ops.aten.view.default(view_1278, [1, 384, 384, 512]);  view_1278 = None
    split_tensor_123 = torch.ops.aten.split.Tensor(view_1284, 256, dim = -1);  view_1284 = None
    getitem_1170 = split_tensor_123[1];  split_tensor_123 = None
    unsqueeze_501 = torch.ops.aten.unsqueeze.default(getitem_1170, 4);  getitem_1170 = None
    permute_756 = torch.ops.aten.permute.default(unsqueeze_501, [0, 4, 2, 3, 1]);  unsqueeze_501 = None
    permute_757 = torch.ops.aten.permute.default(permute_756, [3, 4, 0, 2, 1]);  permute_756 = None
    view_1285 = torch.ops.aten.view.default(permute_757, [256, 384, 384]);  permute_757 = None
    bmm_118 = torch.ops.aten.bmm.default(view_1283, view_1285);  view_1283 = view_1285 = None
    view_1286 = torch.ops.aten.view.default(bmm_118, [256, 384, 1, 1, 384]);  bmm_118 = None
    permute_758 = torch.ops.aten.permute.default(view_1286, [3, 1, 4, 0, 2]);  view_1286 = None
    view_1287 = torch.ops.aten.view.default(permute_758, [1, 384, 384, 256]);  permute_758 = None
    _to_copy_721 = torch.ops.aten._to_copy.default(view_1271, dtype = torch.float32);  view_1271 = None
    native_layer_norm_default_148 = torch.ops.aten.native_layer_norm.default(_to_copy_721, [256], None, None, 1e-05);  _to_copy_721 = None
    getitem_1171 = native_layer_norm_default_148[0];  native_layer_norm_default_148 = None
    _to_copy_722 = torch.ops.aten._to_copy.default(view_1287, dtype = torch.float32);  view_1287 = None
    native_layer_norm_default_149 = torch.ops.aten.native_layer_norm.default(_to_copy_722, [256], None, None, 1e-05);  _to_copy_722 = None
    getitem_1174 = native_layer_norm_default_149[0];  native_layer_norm_default_149 = None
    add_128 = torch.ops.aten.add.Tensor(getitem_1171, getitem_1174);  getitem_1171 = getitem_1174 = None
    _to_copy_723 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_9_triangle_multiplication_linear_z_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_9_triangle_multiplication_linear_z_out_weight = None
    _to_copy_724 = torch.ops.aten._to_copy.default(add_128, dtype = torch.bfloat16);  add_128 = None
    t_256 = torch.ops.aten.t.default(_to_copy_723);  _to_copy_723 = None
    view_1288 = torch.ops.aten.view.default(_to_copy_724, [147456, 256]);  _to_copy_724 = None
    mm_237 = torch.ops.aten.mm.default(view_1288, t_256);  view_1288 = t_256 = None
    view_1289 = torch.ops.aten.view.default(mm_237, [1, 384, 384, 256]);  mm_237 = None
    _to_copy_725 = torch.ops.aten._to_copy.default(getitem_1158, dtype = torch.bfloat16);  getitem_1158 = None
    _to_copy_726 = torch.ops.aten._to_copy.default(getitem_1151, dtype = torch.bfloat16);  getitem_1151 = None
    t_257 = torch.ops.aten.t.default(_to_copy_725);  _to_copy_725 = None
    view_1290 = torch.ops.aten.view.default(_to_copy_726, [147456, 256]);  _to_copy_726 = None
    mm_238 = torch.ops.aten.mm.default(view_1290, t_257);  view_1290 = t_257 = None
    view_1291 = torch.ops.aten.view.default(mm_238, [1, 384, 384, 256]);  mm_238 = None
    sigmoid_92 = torch.ops.aten.sigmoid.default(view_1291);  view_1291 = None
    mul_150 = torch.ops.aten.mul.Tensor(view_1289, sigmoid_92);  view_1289 = sigmoid_92 = None
    add_129 = torch.ops.aten.add.Tensor(add_123, mul_150);  mul_150 = None
    _to_copy_727 = torch.ops.aten._to_copy.default(add_123, dtype = torch.float32)
    native_layer_norm_default_150 = torch.ops.aten.native_layer_norm.default(_to_copy_727, [256], None, None, 1e-05);  _to_copy_727 = None
    getitem_1177 = native_layer_norm_default_150[0];  native_layer_norm_default_150 = None
    _to_copy_728 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_9_triangle_attention_pair2b_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_9_triangle_attention_pair2b_weight = None
    _to_copy_729 = torch.ops.aten._to_copy.default(getitem_1177, dtype = torch.bfloat16)
    t_258 = torch.ops.aten.t.default(_to_copy_728);  _to_copy_728 = None
    view_1292 = torch.ops.aten.view.default(_to_copy_729, [147456, 256]);  _to_copy_729 = None
    mm_239 = torch.ops.aten.mm.default(view_1292, t_258);  view_1292 = t_258 = None
    view_1293 = torch.ops.aten.view.default(mm_239, [1, 384, 384, 8]);  mm_239 = None
    view_1294 = torch.ops.aten.view.default(view_1293, [1, 384, 384, 2, 4]);  view_1293 = None
    permute_759 = torch.ops.aten.permute.default(view_1294, [0, 3, 4, 1, 2]);  view_1294 = None
    view_1295 = torch.ops.aten.view.default(permute_759, [1, 2, 4, 1, 384, 384]);  permute_759 = None
    view_1296 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 384, 384])
    bitwise_not_84 = torch.ops.aten.bitwise_not.default(view_1296);  view_1296 = None
    masked_fill_84 = torch.ops.aten.masked_fill.Scalar(view_1295, bitwise_not_84, -10000);  view_1295 = bitwise_not_84 = None
    view_1297 = torch.ops.aten.view.default(masked_fill_84, [1, 2, 4, 384, 384]);  masked_fill_84 = None
    permute_760 = torch.ops.aten.permute.default(view_1297, [1, 0, 2, 3, 4]);  view_1297 = None
    view_1298 = torch.ops.aten.view.default(permute_760, [2, 4, 1, 384, 384]);  permute_760 = None
    _to_copy_730 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_9_triangle_attention_pair2qkvg1_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_9_triangle_attention_pair2qkvg1_weight = None
    _to_copy_731 = torch.ops.aten._to_copy.default(getitem_1177, dtype = torch.bfloat16)
    t_259 = torch.ops.aten.t.default(_to_copy_730);  _to_copy_730 = None
    view_1299 = torch.ops.aten.view.default(_to_copy_731, [147456, 256]);  _to_copy_731 = None
    mm_240 = torch.ops.aten.mm.default(view_1299, t_259);  view_1299 = t_259 = None
    view_1300 = torch.ops.aten.view.default(mm_240, [1, 384, 384, 1024]);  mm_240 = None
    select_31 = torch.ops.aten.select.int(view_1298, 0, 0)
    view_1301 = torch.ops.aten.view.default(view_1300, [1, 384, 384, 4, 4, 64]);  view_1300 = None
    permute_761 = torch.ops.aten.permute.default(view_1301, [4, 0, 3, 1, 2, 5]);  view_1301 = None
    view_1302 = torch.ops.aten.view.default(permute_761, [4, 4, 384, 384, 64]);  permute_761 = None
    unbind_int_61 = torch.ops.aten.unbind.int(view_1302);  view_1302 = None
    getitem_1180 = unbind_int_61[0]
    getitem_1181 = unbind_int_61[1]
    getitem_1182 = unbind_int_61[2]
    getitem_1183 = unbind_int_61[3];  unbind_int_61 = None
    expand_72 = torch.ops.aten.expand.default(select_31, [4, 384, 384, 384]);  select_31 = None
    _scaled_dot_product_efficient_attention_default_39 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1180, getitem_1181, getitem_1182, expand_72, False);  getitem_1180 = getitem_1181 = getitem_1182 = expand_72 = None
    getitem_1184 = _scaled_dot_product_efficient_attention_default_39[0];  _scaled_dot_product_efficient_attention_default_39 = None
    sigmoid_93 = torch.ops.aten.sigmoid.default(getitem_1183);  getitem_1183 = None
    mul_151 = torch.ops.aten.mul.Tensor(getitem_1184, sigmoid_93);  getitem_1184 = sigmoid_93 = None
    view_1303 = torch.ops.aten.view.default(mul_151, [1, 4, 384, 384, 64]);  mul_151 = None
    permute_762 = torch.ops.aten.permute.default(view_1303, [0, 2, 3, 1, 4]);  view_1303 = None
    clone_135 = torch.ops.aten.clone.default(permute_762, memory_format = torch.contiguous_format);  permute_762 = None
    _unsafe_view_118 = torch.ops.aten._unsafe_view.default(clone_135, [1, 384, 384, 256]);  clone_135 = None
    transpose_31 = torch.ops.aten.transpose.int(getitem_1177, 1, 2);  getitem_1177 = None
    _to_copy_732 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_9_triangle_attention_pair2qkvg2_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_9_triangle_attention_pair2qkvg2_weight = None
    _to_copy_733 = torch.ops.aten._to_copy.default(transpose_31, dtype = torch.bfloat16);  transpose_31 = None
    t_260 = torch.ops.aten.t.default(_to_copy_732);  _to_copy_732 = None
    expand_73 = torch.ops.aten.expand.default(_to_copy_733, [1, 384, 384, 256]);  _to_copy_733 = None
    view_1304 = torch.ops.aten.view.default(expand_73, [384, 384, 256]);  expand_73 = None
    expand_74 = torch.ops.aten.expand.default(t_260, [1, 384, 256, 1024]);  t_260 = None
    view_1305 = torch.ops.aten.view.default(expand_74, [384, 256, 1024]);  expand_74 = None
    bmm_119 = torch.ops.aten.bmm.default(view_1304, view_1305);  view_1304 = view_1305 = None
    view_1306 = torch.ops.aten.view.default(bmm_119, [1, 384, 384, 1024]);  bmm_119 = None
    select_32 = torch.ops.aten.select.int(view_1298, 0, 1);  view_1298 = None
    view_1307 = torch.ops.aten.view.default(view_1306, [1, 384, 384, 4, 4, 64]);  view_1306 = None
    permute_763 = torch.ops.aten.permute.default(view_1307, [4, 0, 3, 1, 2, 5]);  view_1307 = None
    view_1308 = torch.ops.aten.view.default(permute_763, [4, 4, 384, 384, 64]);  permute_763 = None
    unbind_int_62 = torch.ops.aten.unbind.int(view_1308);  view_1308 = None
    getitem_1188 = unbind_int_62[0]
    getitem_1189 = unbind_int_62[1]
    getitem_1190 = unbind_int_62[2]
    getitem_1191 = unbind_int_62[3];  unbind_int_62 = None
    expand_75 = torch.ops.aten.expand.default(select_32, [4, 384, 384, 384]);  select_32 = None
    _scaled_dot_product_efficient_attention_default_40 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1188, getitem_1189, getitem_1190, expand_75, False);  getitem_1188 = getitem_1189 = getitem_1190 = expand_75 = None
    getitem_1192 = _scaled_dot_product_efficient_attention_default_40[0];  _scaled_dot_product_efficient_attention_default_40 = None
    sigmoid_94 = torch.ops.aten.sigmoid.default(getitem_1191);  getitem_1191 = None
    mul_152 = torch.ops.aten.mul.Tensor(getitem_1192, sigmoid_94);  getitem_1192 = sigmoid_94 = None
    view_1309 = torch.ops.aten.view.default(mul_152, [1, 4, 384, 384, 64]);  mul_152 = None
    permute_764 = torch.ops.aten.permute.default(view_1309, [0, 2, 3, 1, 4]);  view_1309 = None
    clone_136 = torch.ops.aten.clone.default(permute_764, memory_format = torch.contiguous_format);  permute_764 = None
    _unsafe_view_119 = torch.ops.aten._unsafe_view.default(clone_136, [1, 384, 384, 256]);  clone_136 = None
    cat_21 = torch.ops.aten.cat.default([_unsafe_view_118, _unsafe_view_119], dim = -1);  _unsafe_view_118 = _unsafe_view_119 = None
    slice_172 = torch.ops.aten.slice.Tensor(pairformer_stack_blocks_9_triangle_attention_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  pairformer_stack_blocks_9_triangle_attention_out_scalers = None
    unsqueeze_502 = torch.ops.aten.unsqueeze.default(slice_172, 1);  slice_172 = None
    mul_153 = torch.ops.aten.mul.Tensor(pairformer_stack_blocks_9_triangle_attention_linear_out_weight, unsqueeze_502);  pairformer_stack_blocks_9_triangle_attention_linear_out_weight = unsqueeze_502 = None
    _to_copy_734 = torch.ops.aten._to_copy.default(mul_153, dtype = torch.bfloat16);  mul_153 = None
    t_261 = torch.ops.aten.t.default(_to_copy_734);  _to_copy_734 = None
    view_1310 = torch.ops.aten.view.default(cat_21, [147456, 512]);  cat_21 = None
    mm_241 = torch.ops.aten.mm.default(view_1310, t_261);  view_1310 = t_261 = None
    view_1311 = torch.ops.aten.view.default(mm_241, [1, 384, 384, 256]);  mm_241 = None
    add_130 = torch.ops.aten.add.Tensor(add_129, view_1311);  add_129 = view_1311 = None
    split_tensor_124 = torch.ops.aten.split.Tensor(add_123, 384, dim = -2)
    getitem_1196 = split_tensor_124[0];  split_tensor_124 = None
    _to_copy_735 = torch.ops.aten._to_copy.default(getitem_1196, dtype = torch.float32);  getitem_1196 = None
    native_layer_norm_default_151 = torch.ops.aten.native_layer_norm.default(_to_copy_735, [256], pairformer_stack_blocks_9_transition_pair_layer_norm_weight, pairformer_stack_blocks_9_transition_pair_layer_norm_bias, 1e-05);  _to_copy_735 = pairformer_stack_blocks_9_transition_pair_layer_norm_weight = pairformer_stack_blocks_9_transition_pair_layer_norm_bias = None
    getitem_1197 = native_layer_norm_default_151[0];  native_layer_norm_default_151 = None
    _to_copy_736 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_9_transition_pair_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_9_transition_pair_linear_no_bias_ab_weight = None
    _to_copy_737 = torch.ops.aten._to_copy.default(getitem_1197, dtype = torch.bfloat16);  getitem_1197 = None
    t_262 = torch.ops.aten.t.default(_to_copy_736);  _to_copy_736 = None
    view_1312 = torch.ops.aten.view.default(_to_copy_737, [147456, 256]);  _to_copy_737 = None
    mm_242 = torch.ops.aten.mm.default(view_1312, t_262);  view_1312 = t_262 = None
    view_1313 = torch.ops.aten.view.default(mm_242, [1, 384, 384, 1024]);  mm_242 = None
    split_tensor_125 = torch.ops.aten.split.Tensor(view_1313, 512, dim = -1);  view_1313 = None
    getitem_1200 = split_tensor_125[0]
    getitem_1201 = split_tensor_125[1];  split_tensor_125 = None
    silu_33 = torch.ops.aten.silu.default(getitem_1200);  getitem_1200 = None
    mul_154 = torch.ops.aten.mul.Tensor(silu_33, getitem_1201);  silu_33 = getitem_1201 = None
    _to_copy_738 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_9_transition_pair_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_9_transition_pair_linear_out_weight = None
    t_263 = torch.ops.aten.t.default(_to_copy_738);  _to_copy_738 = None
    view_1315 = torch.ops.aten.view.default(mul_154, [147456, 512]);  mul_154 = None
    mm_243 = torch.ops.aten.mm.default(view_1315, t_263);  view_1315 = t_263 = None
    view_1316 = torch.ops.aten.view.default(mm_243, [1, 384, 384, 256]);  mm_243 = None
    add_131 = torch.ops.aten.add.Tensor(add_130, view_1316);  add_130 = view_1316 = None
    _to_copy_739 = torch.ops.aten._to_copy.default(add_127, dtype = torch.float32)
    native_layer_norm_default_152 = torch.ops.aten.native_layer_norm.default(_to_copy_739, [384], pairformer_stack_blocks_9_attention_pair_bias_single_layer_norm_weight, pairformer_stack_blocks_9_attention_pair_bias_single_layer_norm_bias, 1e-05);  _to_copy_739 = pairformer_stack_blocks_9_attention_pair_bias_single_layer_norm_weight = pairformer_stack_blocks_9_attention_pair_bias_single_layer_norm_bias = None
    getitem_1202 = native_layer_norm_default_152[0];  native_layer_norm_default_152 = None
    _to_copy_740 = torch.ops.aten._to_copy.default(add_123, dtype = torch.float32);  add_123 = None
    native_layer_norm_default_153 = torch.ops.aten.native_layer_norm.default(_to_copy_740, [256], pairformer_stack_blocks_9_attention_pair_bias_pair_layer_norm_weight, pairformer_stack_blocks_9_attention_pair_bias_pair_layer_norm_bias, 1e-05);  _to_copy_740 = pairformer_stack_blocks_9_attention_pair_bias_pair_layer_norm_weight = pairformer_stack_blocks_9_attention_pair_bias_pair_layer_norm_bias = None
    getitem_1205 = native_layer_norm_default_153[0];  native_layer_norm_default_153 = None
    _to_copy_741 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_9_attention_pair_bias_pair_linear_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_9_attention_pair_bias_pair_linear_weight = None
    _to_copy_742 = torch.ops.aten._to_copy.default(getitem_1205, dtype = torch.bfloat16);  getitem_1205 = None
    t_264 = torch.ops.aten.t.default(_to_copy_741);  _to_copy_741 = None
    view_1317 = torch.ops.aten.view.default(_to_copy_742, [147456, 256]);  _to_copy_742 = None
    mm_244 = torch.ops.aten.mm.default(view_1317, t_264);  view_1317 = t_264 = None
    view_1318 = torch.ops.aten.view.default(mm_244, [1, 384, 384, 16]);  mm_244 = None
    permute_765 = torch.ops.aten.permute.default(view_1318, [0, 3, 1, 2]);  view_1318 = None
    view_1319 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384])
    bitwise_not_85 = torch.ops.aten.bitwise_not.default(view_1319);  view_1319 = None
    masked_fill_85 = torch.ops.aten.masked_fill.Scalar(permute_765, bitwise_not_85, -10000);  permute_765 = bitwise_not_85 = None
    _to_copy_743 = torch.ops.aten._to_copy.default(getitem_1202, dtype = torch.bfloat16);  getitem_1202 = None
    _to_copy_744 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_9_attention_pair_bias_attention_input2qkvg_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_9_attention_pair_bias_attention_input2qkvg_weight = None
    unsqueeze_503 = torch.ops.aten.unsqueeze.default(_to_copy_743, 3);  _to_copy_743 = None
    unsqueeze_504 = torch.ops.aten.unsqueeze.default(unsqueeze_503, 4);  unsqueeze_503 = None
    unsqueeze_505 = torch.ops.aten.unsqueeze.default(unsqueeze_504, 5);  unsqueeze_504 = None
    permute_766 = torch.ops.aten.permute.default(unsqueeze_505, [3, 0, 4, 1, 5, 2]);  unsqueeze_505 = None
    unsqueeze_506 = torch.ops.aten.unsqueeze.default(_to_copy_744, 4);  _to_copy_744 = None
    unsqueeze_507 = torch.ops.aten.unsqueeze.default(unsqueeze_506, 5);  unsqueeze_506 = None
    permute_767 = torch.ops.aten.permute.default(unsqueeze_507, [1, 4, 2, 5, 3, 0]);  unsqueeze_507 = None
    permute_768 = torch.ops.aten.permute.default(permute_766, [3, 5, 0, 1, 2, 4]);  permute_766 = None
    view_1320 = torch.ops.aten.view.default(permute_768, [1, 384, 384]);  permute_768 = None
    permute_769 = torch.ops.aten.permute.default(permute_767, [5, 0, 1, 2, 4, 3]);  permute_767 = None
    view_1321 = torch.ops.aten.view.default(permute_769, [1, 384, 1536]);  permute_769 = None
    bmm_120 = torch.ops.aten.bmm.default(view_1320, view_1321);  view_1320 = view_1321 = None
    view_1322 = torch.ops.aten.view.default(bmm_120, [384, 1, 4, 1, 16, 24]);  bmm_120 = None
    permute_770 = torch.ops.aten.permute.default(view_1322, [2, 3, 4, 0, 5, 1]);  view_1322 = None
    view_1323 = torch.ops.aten.view.default(permute_770, [4, 1, 16, 384, 24]);  permute_770 = None
    unbind_int_63 = torch.ops.aten.unbind.int(view_1323);  view_1323 = None
    getitem_1208 = unbind_int_63[0]
    getitem_1209 = unbind_int_63[1]
    getitem_1210 = unbind_int_63[2]
    getitem_1211 = unbind_int_63[3];  unbind_int_63 = None
    view_1324 = torch.ops.aten.view.default(pairformer_stack_blocks_9_attention_pair_bias_attention_query_bias, [1, 16, 1, 24]);  pairformer_stack_blocks_9_attention_pair_bias_attention_query_bias = None
    add_132 = torch.ops.aten.add.Tensor(getitem_1208, view_1324);  getitem_1208 = view_1324 = None
    _to_copy_745 = torch.ops.aten._to_copy.default(add_132, dtype = torch.bfloat16);  add_132 = None
    expand_76 = torch.ops.aten.expand.default(masked_fill_85, [1, 16, 384, 384]);  masked_fill_85 = None
    _scaled_dot_product_efficient_attention_default_41 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_745, getitem_1209, getitem_1210, expand_76, False);  _to_copy_745 = getitem_1209 = getitem_1210 = expand_76 = None
    getitem_1212 = _scaled_dot_product_efficient_attention_default_41[0];  _scaled_dot_product_efficient_attention_default_41 = None
    add_133 = torch.ops.aten.add.Tensor(getitem_1211, 1);  getitem_1211 = None
    sigmoid_95 = torch.ops.aten.sigmoid.default(add_133);  add_133 = None
    mul_155 = torch.ops.aten.mul.Tensor(getitem_1212, sigmoid_95);  getitem_1212 = sigmoid_95 = None
    _to_copy_746 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_9_attention_pair_bias_attention_output_proj_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_9_attention_pair_bias_attention_output_proj_weight = None
    unsqueeze_508 = torch.ops.aten.unsqueeze.default(mul_155, 4);  mul_155 = None
    permute_771 = torch.ops.aten.permute.default(unsqueeze_508, [0, 2, 4, 3, 1]);  unsqueeze_508 = None
    unsqueeze_509 = torch.ops.aten.unsqueeze.default(_to_copy_746, 3);  _to_copy_746 = None
    unsqueeze_510 = torch.ops.aten.unsqueeze.default(unsqueeze_509, 4);  unsqueeze_509 = None
    permute_772 = torch.ops.aten.permute.default(unsqueeze_510, [3, 4, 2, 1, 0]);  unsqueeze_510 = None
    permute_773 = torch.ops.aten.permute.default(permute_771, [1, 3, 4, 0, 2]);  permute_771 = None
    clone_137 = torch.ops.aten.clone.default(permute_773, memory_format = torch.contiguous_format);  permute_773 = None
    _unsafe_view_120 = torch.ops.aten._unsafe_view.default(clone_137, [1, 384, 384]);  clone_137 = None
    permute_774 = torch.ops.aten.permute.default(permute_772, [3, 4, 0, 2, 1]);  permute_772 = None
    clone_138 = torch.ops.aten.clone.default(permute_774, memory_format = torch.contiguous_format);  permute_774 = None
    _unsafe_view_121 = torch.ops.aten._unsafe_view.default(clone_138, [1, 384, 384]);  clone_138 = None
    bmm_121 = torch.ops.aten.bmm.default(_unsafe_view_120, _unsafe_view_121);  _unsafe_view_120 = _unsafe_view_121 = None
    view_1325 = torch.ops.aten.view.default(bmm_121, [384, 1, 1, 1, 384]);  bmm_121 = None
    permute_775 = torch.ops.aten.permute.default(view_1325, [3, 0, 4, 1, 2]);  view_1325 = None
    view_1326 = torch.ops.aten.view.default(permute_775, [1, 384, 384]);  permute_775 = None
    unsqueeze_511 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
    mul_156 = torch.ops.aten.mul.Tensor(view_1326, unsqueeze_511);  view_1326 = unsqueeze_511 = None
    add_134 = torch.ops.aten.add.Tensor(add_127, mul_156);  mul_156 = None
    split_tensor_126 = torch.ops.aten.split.Tensor(add_127, 384, dim = -2);  add_127 = None
    getitem_1216 = split_tensor_126[0];  split_tensor_126 = None
    _to_copy_747 = torch.ops.aten._to_copy.default(getitem_1216, dtype = torch.float32);  getitem_1216 = None
    native_layer_norm_default_154 = torch.ops.aten.native_layer_norm.default(_to_copy_747, [384], pairformer_stack_blocks_9_transition_single_layer_norm_weight, pairformer_stack_blocks_9_transition_single_layer_norm_bias, 1e-05);  _to_copy_747 = pairformer_stack_blocks_9_transition_single_layer_norm_weight = pairformer_stack_blocks_9_transition_single_layer_norm_bias = None
    getitem_1217 = native_layer_norm_default_154[0];  native_layer_norm_default_154 = None
    _to_copy_748 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_9_transition_single_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_9_transition_single_linear_no_bias_ab_weight = None
    _to_copy_749 = torch.ops.aten._to_copy.default(getitem_1217, dtype = torch.bfloat16);  getitem_1217 = None
    t_265 = torch.ops.aten.t.default(_to_copy_748);  _to_copy_748 = None
    view_1327 = torch.ops.aten.view.default(_to_copy_749, [384, 384]);  _to_copy_749 = None
    mm_245 = torch.ops.aten.mm.default(view_1327, t_265);  view_1327 = t_265 = None
    view_1328 = torch.ops.aten.view.default(mm_245, [1, 384, 1536]);  mm_245 = None
    split_tensor_127 = torch.ops.aten.split.Tensor(view_1328, 768, dim = -1);  view_1328 = None
    getitem_1220 = split_tensor_127[0]
    getitem_1221 = split_tensor_127[1];  split_tensor_127 = None
    silu_34 = torch.ops.aten.silu.default(getitem_1220);  getitem_1220 = None
    mul_157 = torch.ops.aten.mul.Tensor(silu_34, getitem_1221);  silu_34 = getitem_1221 = None
    _to_copy_750 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_9_transition_single_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_9_transition_single_linear_out_weight = None
    t_266 = torch.ops.aten.t.default(_to_copy_750);  _to_copy_750 = None
    view_1330 = torch.ops.aten.view.default(mul_157, [384, 768]);  mul_157 = None
    mm_246 = torch.ops.aten.mm.default(view_1330, t_266);  view_1330 = t_266 = None
    view_1331 = torch.ops.aten.view.default(mm_246, [1, 384, 384]);  mm_246 = None
    add_135 = torch.ops.aten.add.Tensor(add_134, view_1331);  add_134 = view_1331 = None
    _to_copy_751 = torch.ops.aten._to_copy.default(add_131, dtype = torch.float32)
    native_layer_norm_default_155 = torch.ops.aten.native_layer_norm.default(_to_copy_751, [256], pairformer_stack_blocks_10_triangle_multiplication_layernorm_z_in_weight, pairformer_stack_blocks_10_triangle_multiplication_layernorm_z_in_bias, 1e-05);  _to_copy_751 = pairformer_stack_blocks_10_triangle_multiplication_layernorm_z_in_weight = pairformer_stack_blocks_10_triangle_multiplication_layernorm_z_in_bias = None
    getitem_1222 = native_layer_norm_default_155[0];  native_layer_norm_default_155 = None
    split_with_sizes_default_32 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_10_triangle_multiplication_merged_linear_p_weight, [512, 512]);  pairformer_stack_blocks_10_triangle_multiplication_merged_linear_p_weight = None
    getitem_1225 = split_with_sizes_default_32[0]
    getitem_1226 = split_with_sizes_default_32[1];  split_with_sizes_default_32 = None
    split_with_sizes_default_33 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_10_triangle_multiplication_merged_linear_g_weight, [512, 512, 256]);  pairformer_stack_blocks_10_triangle_multiplication_merged_linear_g_weight = None
    getitem_1227 = split_with_sizes_default_33[0]
    getitem_1228 = split_with_sizes_default_33[1]
    getitem_1229 = split_with_sizes_default_33[2];  split_with_sizes_default_33 = None
    _to_copy_752 = torch.ops.aten._to_copy.default(getitem_1225, dtype = torch.bfloat16);  getitem_1225 = None
    _to_copy_753 = torch.ops.aten._to_copy.default(getitem_1222, dtype = torch.bfloat16)
    t_267 = torch.ops.aten.t.default(_to_copy_752);  _to_copy_752 = None
    view_1332 = torch.ops.aten.view.default(_to_copy_753, [147456, 256]);  _to_copy_753 = None
    mm_247 = torch.ops.aten.mm.default(view_1332, t_267);  view_1332 = t_267 = None
    view_1333 = torch.ops.aten.view.default(mm_247, [1, 384, 384, 512]);  mm_247 = None
    _to_copy_754 = torch.ops.aten._to_copy.default(getitem_1227, dtype = torch.bfloat16);  getitem_1227 = None
    _to_copy_755 = torch.ops.aten._to_copy.default(getitem_1222, dtype = torch.bfloat16)
    t_268 = torch.ops.aten.t.default(_to_copy_754);  _to_copy_754 = None
    view_1334 = torch.ops.aten.view.default(_to_copy_755, [147456, 256]);  _to_copy_755 = None
    mm_248 = torch.ops.aten.mm.default(view_1334, t_268);  view_1334 = t_268 = None
    view_1335 = torch.ops.aten.view.default(mm_248, [1, 384, 384, 512]);  mm_248 = None
    sigmoid_96 = torch.ops.aten.sigmoid.default(view_1335);  view_1335 = None
    mul_158 = torch.ops.aten.mul.Tensor(view_1333, sigmoid_96);  view_1333 = sigmoid_96 = None
    unsqueeze_512 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
    bitwise_not_86 = torch.ops.aten.bitwise_not.default(unsqueeze_512);  unsqueeze_512 = None
    masked_fill_86 = torch.ops.aten.masked_fill.Scalar(mul_158, bitwise_not_86, 0);  mul_158 = bitwise_not_86 = None
    split_tensor_128 = torch.ops.aten.split.Tensor(masked_fill_86, 256, dim = -1)
    getitem_1232 = split_tensor_128[0];  split_tensor_128 = None
    unsqueeze_515 = torch.ops.aten.unsqueeze.default(getitem_1232, 4);  getitem_1232 = None
    permute_780 = torch.ops.aten.permute.default(unsqueeze_515, [0, 1, 4, 3, 2]);  unsqueeze_515 = None
    permute_781 = torch.ops.aten.permute.default(permute_780, [3, 1, 4, 0, 2]);  permute_780 = None
    view_1338 = torch.ops.aten.view.default(permute_781, [256, 384, 384]);  permute_781 = None
    split_tensor_129 = torch.ops.aten.split.Tensor(masked_fill_86, 256, dim = -1);  masked_fill_86 = None
    getitem_1235 = split_tensor_129[1];  split_tensor_129 = None
    unsqueeze_516 = torch.ops.aten.unsqueeze.default(getitem_1235, 4);  getitem_1235 = None
    permute_782 = torch.ops.aten.permute.default(unsqueeze_516, [0, 4, 1, 3, 2]);  unsqueeze_516 = None
    permute_783 = torch.ops.aten.permute.default(permute_782, [3, 4, 0, 2, 1]);  permute_782 = None
    view_1339 = torch.ops.aten.view.default(permute_783, [256, 384, 384]);  permute_783 = None
    bmm_122 = torch.ops.aten.bmm.default(view_1338, view_1339);  view_1338 = view_1339 = None
    view_1340 = torch.ops.aten.view.default(bmm_122, [256, 384, 1, 1, 384]);  bmm_122 = None
    permute_784 = torch.ops.aten.permute.default(view_1340, [3, 1, 4, 0, 2]);  view_1340 = None
    view_1341 = torch.ops.aten.view.default(permute_784, [1, 384, 384, 256]);  permute_784 = None
    _to_copy_756 = torch.ops.aten._to_copy.default(getitem_1226, dtype = torch.bfloat16);  getitem_1226 = None
    _to_copy_757 = torch.ops.aten._to_copy.default(getitem_1222, dtype = torch.bfloat16)
    t_269 = torch.ops.aten.t.default(_to_copy_756);  _to_copy_756 = None
    view_1342 = torch.ops.aten.view.default(_to_copy_757, [147456, 256]);  _to_copy_757 = None
    mm_249 = torch.ops.aten.mm.default(view_1342, t_269);  view_1342 = t_269 = None
    view_1343 = torch.ops.aten.view.default(mm_249, [1, 384, 384, 512]);  mm_249 = None
    _to_copy_758 = torch.ops.aten._to_copy.default(getitem_1228, dtype = torch.bfloat16);  getitem_1228 = None
    _to_copy_759 = torch.ops.aten._to_copy.default(getitem_1222, dtype = torch.bfloat16)
    t_270 = torch.ops.aten.t.default(_to_copy_758);  _to_copy_758 = None
    view_1344 = torch.ops.aten.view.default(_to_copy_759, [147456, 256]);  _to_copy_759 = None
    mm_250 = torch.ops.aten.mm.default(view_1344, t_270);  view_1344 = t_270 = None
    view_1345 = torch.ops.aten.view.default(mm_250, [1, 384, 384, 512]);  mm_250 = None
    sigmoid_97 = torch.ops.aten.sigmoid.default(view_1345);  view_1345 = None
    mul_159 = torch.ops.aten.mul.Tensor(view_1343, sigmoid_97);  view_1343 = sigmoid_97 = None
    view_1346 = torch.ops.aten.view.default(mul_159, [147456, 512]);  mul_159 = None
    view_1347 = torch.ops.aten.view.default(view_1346, [1, 384, 384, 512]);  view_1346 = None
    transpose_32 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
    unsqueeze_517 = torch.ops.aten.unsqueeze.default(transpose_32, 3);  transpose_32 = None
    clone_139 = torch.ops.aten.clone.default(unsqueeze_517, memory_format = torch.contiguous_format);  unsqueeze_517 = None
    bitwise_not_87 = torch.ops.aten.bitwise_not.default(clone_139);  clone_139 = None
    masked_fill_87 = torch.ops.aten.masked_fill.Scalar(view_1347, bitwise_not_87, 0);  view_1347 = bitwise_not_87 = None
    view_1348 = torch.ops.aten.view.default(masked_fill_87, [147456, 512]);  masked_fill_87 = None
    view_1352 = torch.ops.aten.view.default(view_1348, [1, 384, 384, 512])
    split_tensor_130 = torch.ops.aten.split.Tensor(view_1352, 256, dim = -1);  view_1352 = None
    getitem_1238 = split_tensor_130[0];  split_tensor_130 = None
    unsqueeze_520 = torch.ops.aten.unsqueeze.default(getitem_1238, 4);  getitem_1238 = None
    permute_789 = torch.ops.aten.permute.default(unsqueeze_520, [0, 2, 4, 3, 1]);  unsqueeze_520 = None
    permute_790 = torch.ops.aten.permute.default(permute_789, [3, 1, 4, 0, 2]);  permute_789 = None
    view_1353 = torch.ops.aten.view.default(permute_790, [256, 384, 384]);  permute_790 = None
    view_1354 = torch.ops.aten.view.default(view_1348, [1, 384, 384, 512]);  view_1348 = None
    split_tensor_131 = torch.ops.aten.split.Tensor(view_1354, 256, dim = -1);  view_1354 = None
    getitem_1241 = split_tensor_131[1];  split_tensor_131 = None
    unsqueeze_521 = torch.ops.aten.unsqueeze.default(getitem_1241, 4);  getitem_1241 = None
    permute_791 = torch.ops.aten.permute.default(unsqueeze_521, [0, 4, 2, 3, 1]);  unsqueeze_521 = None
    permute_792 = torch.ops.aten.permute.default(permute_791, [3, 4, 0, 2, 1]);  permute_791 = None
    view_1355 = torch.ops.aten.view.default(permute_792, [256, 384, 384]);  permute_792 = None
    bmm_123 = torch.ops.aten.bmm.default(view_1353, view_1355);  view_1353 = view_1355 = None
    view_1356 = torch.ops.aten.view.default(bmm_123, [256, 384, 1, 1, 384]);  bmm_123 = None
    permute_793 = torch.ops.aten.permute.default(view_1356, [3, 1, 4, 0, 2]);  view_1356 = None
    view_1357 = torch.ops.aten.view.default(permute_793, [1, 384, 384, 256]);  permute_793 = None
    _to_copy_760 = torch.ops.aten._to_copy.default(view_1341, dtype = torch.float32);  view_1341 = None
    native_layer_norm_default_156 = torch.ops.aten.native_layer_norm.default(_to_copy_760, [256], None, None, 1e-05);  _to_copy_760 = None
    getitem_1242 = native_layer_norm_default_156[0];  native_layer_norm_default_156 = None
    _to_copy_761 = torch.ops.aten._to_copy.default(view_1357, dtype = torch.float32);  view_1357 = None
    native_layer_norm_default_157 = torch.ops.aten.native_layer_norm.default(_to_copy_761, [256], None, None, 1e-05);  _to_copy_761 = None
    getitem_1245 = native_layer_norm_default_157[0];  native_layer_norm_default_157 = None
    add_136 = torch.ops.aten.add.Tensor(getitem_1242, getitem_1245);  getitem_1242 = getitem_1245 = None
    _to_copy_762 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_10_triangle_multiplication_linear_z_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_10_triangle_multiplication_linear_z_out_weight = None
    _to_copy_763 = torch.ops.aten._to_copy.default(add_136, dtype = torch.bfloat16);  add_136 = None
    t_271 = torch.ops.aten.t.default(_to_copy_762);  _to_copy_762 = None
    view_1358 = torch.ops.aten.view.default(_to_copy_763, [147456, 256]);  _to_copy_763 = None
    mm_251 = torch.ops.aten.mm.default(view_1358, t_271);  view_1358 = t_271 = None
    view_1359 = torch.ops.aten.view.default(mm_251, [1, 384, 384, 256]);  mm_251 = None
    _to_copy_764 = torch.ops.aten._to_copy.default(getitem_1229, dtype = torch.bfloat16);  getitem_1229 = None
    _to_copy_765 = torch.ops.aten._to_copy.default(getitem_1222, dtype = torch.bfloat16);  getitem_1222 = None
    t_272 = torch.ops.aten.t.default(_to_copy_764);  _to_copy_764 = None
    view_1360 = torch.ops.aten.view.default(_to_copy_765, [147456, 256]);  _to_copy_765 = None
    mm_252 = torch.ops.aten.mm.default(view_1360, t_272);  view_1360 = t_272 = None
    view_1361 = torch.ops.aten.view.default(mm_252, [1, 384, 384, 256]);  mm_252 = None
    sigmoid_98 = torch.ops.aten.sigmoid.default(view_1361);  view_1361 = None
    mul_160 = torch.ops.aten.mul.Tensor(view_1359, sigmoid_98);  view_1359 = sigmoid_98 = None
    add_137 = torch.ops.aten.add.Tensor(add_131, mul_160);  mul_160 = None
    _to_copy_766 = torch.ops.aten._to_copy.default(add_131, dtype = torch.float32)
    native_layer_norm_default_158 = torch.ops.aten.native_layer_norm.default(_to_copy_766, [256], None, None, 1e-05);  _to_copy_766 = None
    getitem_1248 = native_layer_norm_default_158[0];  native_layer_norm_default_158 = None
    _to_copy_767 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_10_triangle_attention_pair2b_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_10_triangle_attention_pair2b_weight = None
    _to_copy_768 = torch.ops.aten._to_copy.default(getitem_1248, dtype = torch.bfloat16)
    t_273 = torch.ops.aten.t.default(_to_copy_767);  _to_copy_767 = None
    view_1362 = torch.ops.aten.view.default(_to_copy_768, [147456, 256]);  _to_copy_768 = None
    mm_253 = torch.ops.aten.mm.default(view_1362, t_273);  view_1362 = t_273 = None
    view_1363 = torch.ops.aten.view.default(mm_253, [1, 384, 384, 8]);  mm_253 = None
    view_1364 = torch.ops.aten.view.default(view_1363, [1, 384, 384, 2, 4]);  view_1363 = None
    permute_794 = torch.ops.aten.permute.default(view_1364, [0, 3, 4, 1, 2]);  view_1364 = None
    view_1365 = torch.ops.aten.view.default(permute_794, [1, 2, 4, 1, 384, 384]);  permute_794 = None
    view_1366 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 384, 384])
    bitwise_not_88 = torch.ops.aten.bitwise_not.default(view_1366);  view_1366 = None
    masked_fill_88 = torch.ops.aten.masked_fill.Scalar(view_1365, bitwise_not_88, -10000);  view_1365 = bitwise_not_88 = None
    view_1367 = torch.ops.aten.view.default(masked_fill_88, [1, 2, 4, 384, 384]);  masked_fill_88 = None
    permute_795 = torch.ops.aten.permute.default(view_1367, [1, 0, 2, 3, 4]);  view_1367 = None
    view_1368 = torch.ops.aten.view.default(permute_795, [2, 4, 1, 384, 384]);  permute_795 = None
    _to_copy_769 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_10_triangle_attention_pair2qkvg1_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_10_triangle_attention_pair2qkvg1_weight = None
    _to_copy_770 = torch.ops.aten._to_copy.default(getitem_1248, dtype = torch.bfloat16)
    t_274 = torch.ops.aten.t.default(_to_copy_769);  _to_copy_769 = None
    view_1369 = torch.ops.aten.view.default(_to_copy_770, [147456, 256]);  _to_copy_770 = None
    mm_254 = torch.ops.aten.mm.default(view_1369, t_274);  view_1369 = t_274 = None
    view_1370 = torch.ops.aten.view.default(mm_254, [1, 384, 384, 1024]);  mm_254 = None
    select_33 = torch.ops.aten.select.int(view_1368, 0, 0)
    view_1371 = torch.ops.aten.view.default(view_1370, [1, 384, 384, 4, 4, 64]);  view_1370 = None
    permute_796 = torch.ops.aten.permute.default(view_1371, [4, 0, 3, 1, 2, 5]);  view_1371 = None
    view_1372 = torch.ops.aten.view.default(permute_796, [4, 4, 384, 384, 64]);  permute_796 = None
    unbind_int_64 = torch.ops.aten.unbind.int(view_1372);  view_1372 = None
    getitem_1251 = unbind_int_64[0]
    getitem_1252 = unbind_int_64[1]
    getitem_1253 = unbind_int_64[2]
    getitem_1254 = unbind_int_64[3];  unbind_int_64 = None
    expand_77 = torch.ops.aten.expand.default(select_33, [4, 384, 384, 384]);  select_33 = None
    _scaled_dot_product_efficient_attention_default_42 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1251, getitem_1252, getitem_1253, expand_77, False);  getitem_1251 = getitem_1252 = getitem_1253 = expand_77 = None
    getitem_1255 = _scaled_dot_product_efficient_attention_default_42[0];  _scaled_dot_product_efficient_attention_default_42 = None
    sigmoid_99 = torch.ops.aten.sigmoid.default(getitem_1254);  getitem_1254 = None
    mul_161 = torch.ops.aten.mul.Tensor(getitem_1255, sigmoid_99);  getitem_1255 = sigmoid_99 = None
    view_1373 = torch.ops.aten.view.default(mul_161, [1, 4, 384, 384, 64]);  mul_161 = None
    permute_797 = torch.ops.aten.permute.default(view_1373, [0, 2, 3, 1, 4]);  view_1373 = None
    clone_140 = torch.ops.aten.clone.default(permute_797, memory_format = torch.contiguous_format);  permute_797 = None
    _unsafe_view_122 = torch.ops.aten._unsafe_view.default(clone_140, [1, 384, 384, 256]);  clone_140 = None
    transpose_33 = torch.ops.aten.transpose.int(getitem_1248, 1, 2);  getitem_1248 = None
    _to_copy_771 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_10_triangle_attention_pair2qkvg2_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_10_triangle_attention_pair2qkvg2_weight = None
    _to_copy_772 = torch.ops.aten._to_copy.default(transpose_33, dtype = torch.bfloat16);  transpose_33 = None
    t_275 = torch.ops.aten.t.default(_to_copy_771);  _to_copy_771 = None
    expand_78 = torch.ops.aten.expand.default(_to_copy_772, [1, 384, 384, 256]);  _to_copy_772 = None
    view_1374 = torch.ops.aten.view.default(expand_78, [384, 384, 256]);  expand_78 = None
    expand_79 = torch.ops.aten.expand.default(t_275, [1, 384, 256, 1024]);  t_275 = None
    view_1375 = torch.ops.aten.view.default(expand_79, [384, 256, 1024]);  expand_79 = None
    bmm_124 = torch.ops.aten.bmm.default(view_1374, view_1375);  view_1374 = view_1375 = None
    view_1376 = torch.ops.aten.view.default(bmm_124, [1, 384, 384, 1024]);  bmm_124 = None
    select_34 = torch.ops.aten.select.int(view_1368, 0, 1);  view_1368 = None
    view_1377 = torch.ops.aten.view.default(view_1376, [1, 384, 384, 4, 4, 64]);  view_1376 = None
    permute_798 = torch.ops.aten.permute.default(view_1377, [4, 0, 3, 1, 2, 5]);  view_1377 = None
    view_1378 = torch.ops.aten.view.default(permute_798, [4, 4, 384, 384, 64]);  permute_798 = None
    unbind_int_65 = torch.ops.aten.unbind.int(view_1378);  view_1378 = None
    getitem_1259 = unbind_int_65[0]
    getitem_1260 = unbind_int_65[1]
    getitem_1261 = unbind_int_65[2]
    getitem_1262 = unbind_int_65[3];  unbind_int_65 = None
    expand_80 = torch.ops.aten.expand.default(select_34, [4, 384, 384, 384]);  select_34 = None
    _scaled_dot_product_efficient_attention_default_43 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1259, getitem_1260, getitem_1261, expand_80, False);  getitem_1259 = getitem_1260 = getitem_1261 = expand_80 = None
    getitem_1263 = _scaled_dot_product_efficient_attention_default_43[0];  _scaled_dot_product_efficient_attention_default_43 = None
    sigmoid_100 = torch.ops.aten.sigmoid.default(getitem_1262);  getitem_1262 = None
    mul_162 = torch.ops.aten.mul.Tensor(getitem_1263, sigmoid_100);  getitem_1263 = sigmoid_100 = None
    view_1379 = torch.ops.aten.view.default(mul_162, [1, 4, 384, 384, 64]);  mul_162 = None
    permute_799 = torch.ops.aten.permute.default(view_1379, [0, 2, 3, 1, 4]);  view_1379 = None
    clone_141 = torch.ops.aten.clone.default(permute_799, memory_format = torch.contiguous_format);  permute_799 = None
    _unsafe_view_123 = torch.ops.aten._unsafe_view.default(clone_141, [1, 384, 384, 256]);  clone_141 = None
    cat_22 = torch.ops.aten.cat.default([_unsafe_view_122, _unsafe_view_123], dim = -1);  _unsafe_view_122 = _unsafe_view_123 = None
    slice_173 = torch.ops.aten.slice.Tensor(pairformer_stack_blocks_10_triangle_attention_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  pairformer_stack_blocks_10_triangle_attention_out_scalers = None
    unsqueeze_522 = torch.ops.aten.unsqueeze.default(slice_173, 1);  slice_173 = None
    mul_163 = torch.ops.aten.mul.Tensor(pairformer_stack_blocks_10_triangle_attention_linear_out_weight, unsqueeze_522);  pairformer_stack_blocks_10_triangle_attention_linear_out_weight = unsqueeze_522 = None
    _to_copy_773 = torch.ops.aten._to_copy.default(mul_163, dtype = torch.bfloat16);  mul_163 = None
    t_276 = torch.ops.aten.t.default(_to_copy_773);  _to_copy_773 = None
    view_1380 = torch.ops.aten.view.default(cat_22, [147456, 512]);  cat_22 = None
    mm_255 = torch.ops.aten.mm.default(view_1380, t_276);  view_1380 = t_276 = None
    view_1381 = torch.ops.aten.view.default(mm_255, [1, 384, 384, 256]);  mm_255 = None
    add_138 = torch.ops.aten.add.Tensor(add_137, view_1381);  add_137 = view_1381 = None
    split_tensor_132 = torch.ops.aten.split.Tensor(add_131, 384, dim = -2)
    getitem_1267 = split_tensor_132[0];  split_tensor_132 = None
    _to_copy_774 = torch.ops.aten._to_copy.default(getitem_1267, dtype = torch.float32);  getitem_1267 = None
    native_layer_norm_default_159 = torch.ops.aten.native_layer_norm.default(_to_copy_774, [256], pairformer_stack_blocks_10_transition_pair_layer_norm_weight, pairformer_stack_blocks_10_transition_pair_layer_norm_bias, 1e-05);  _to_copy_774 = pairformer_stack_blocks_10_transition_pair_layer_norm_weight = pairformer_stack_blocks_10_transition_pair_layer_norm_bias = None
    getitem_1268 = native_layer_norm_default_159[0];  native_layer_norm_default_159 = None
    _to_copy_775 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_10_transition_pair_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_10_transition_pair_linear_no_bias_ab_weight = None
    _to_copy_776 = torch.ops.aten._to_copy.default(getitem_1268, dtype = torch.bfloat16);  getitem_1268 = None
    t_277 = torch.ops.aten.t.default(_to_copy_775);  _to_copy_775 = None
    view_1382 = torch.ops.aten.view.default(_to_copy_776, [147456, 256]);  _to_copy_776 = None
    mm_256 = torch.ops.aten.mm.default(view_1382, t_277);  view_1382 = t_277 = None
    view_1383 = torch.ops.aten.view.default(mm_256, [1, 384, 384, 1024]);  mm_256 = None
    split_tensor_133 = torch.ops.aten.split.Tensor(view_1383, 512, dim = -1);  view_1383 = None
    getitem_1271 = split_tensor_133[0]
    getitem_1272 = split_tensor_133[1];  split_tensor_133 = None
    silu_35 = torch.ops.aten.silu.default(getitem_1271);  getitem_1271 = None
    mul_164 = torch.ops.aten.mul.Tensor(silu_35, getitem_1272);  silu_35 = getitem_1272 = None
    _to_copy_777 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_10_transition_pair_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_10_transition_pair_linear_out_weight = None
    t_278 = torch.ops.aten.t.default(_to_copy_777);  _to_copy_777 = None
    view_1385 = torch.ops.aten.view.default(mul_164, [147456, 512]);  mul_164 = None
    mm_257 = torch.ops.aten.mm.default(view_1385, t_278);  view_1385 = t_278 = None
    view_1386 = torch.ops.aten.view.default(mm_257, [1, 384, 384, 256]);  mm_257 = None
    add_139 = torch.ops.aten.add.Tensor(add_138, view_1386);  add_138 = view_1386 = None
    _to_copy_778 = torch.ops.aten._to_copy.default(add_135, dtype = torch.float32)
    native_layer_norm_default_160 = torch.ops.aten.native_layer_norm.default(_to_copy_778, [384], pairformer_stack_blocks_10_attention_pair_bias_single_layer_norm_weight, pairformer_stack_blocks_10_attention_pair_bias_single_layer_norm_bias, 1e-05);  _to_copy_778 = pairformer_stack_blocks_10_attention_pair_bias_single_layer_norm_weight = pairformer_stack_blocks_10_attention_pair_bias_single_layer_norm_bias = None
    getitem_1273 = native_layer_norm_default_160[0];  native_layer_norm_default_160 = None
    _to_copy_779 = torch.ops.aten._to_copy.default(add_131, dtype = torch.float32);  add_131 = None
    native_layer_norm_default_161 = torch.ops.aten.native_layer_norm.default(_to_copy_779, [256], pairformer_stack_blocks_10_attention_pair_bias_pair_layer_norm_weight, pairformer_stack_blocks_10_attention_pair_bias_pair_layer_norm_bias, 1e-05);  _to_copy_779 = pairformer_stack_blocks_10_attention_pair_bias_pair_layer_norm_weight = pairformer_stack_blocks_10_attention_pair_bias_pair_layer_norm_bias = None
    getitem_1276 = native_layer_norm_default_161[0];  native_layer_norm_default_161 = None
    _to_copy_780 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_10_attention_pair_bias_pair_linear_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_10_attention_pair_bias_pair_linear_weight = None
    _to_copy_781 = torch.ops.aten._to_copy.default(getitem_1276, dtype = torch.bfloat16);  getitem_1276 = None
    t_279 = torch.ops.aten.t.default(_to_copy_780);  _to_copy_780 = None
    view_1387 = torch.ops.aten.view.default(_to_copy_781, [147456, 256]);  _to_copy_781 = None
    mm_258 = torch.ops.aten.mm.default(view_1387, t_279);  view_1387 = t_279 = None
    view_1388 = torch.ops.aten.view.default(mm_258, [1, 384, 384, 16]);  mm_258 = None
    permute_800 = torch.ops.aten.permute.default(view_1388, [0, 3, 1, 2]);  view_1388 = None
    view_1389 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384])
    bitwise_not_89 = torch.ops.aten.bitwise_not.default(view_1389);  view_1389 = None
    masked_fill_89 = torch.ops.aten.masked_fill.Scalar(permute_800, bitwise_not_89, -10000);  permute_800 = bitwise_not_89 = None
    _to_copy_782 = torch.ops.aten._to_copy.default(getitem_1273, dtype = torch.bfloat16);  getitem_1273 = None
    _to_copy_783 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_10_attention_pair_bias_attention_input2qkvg_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_10_attention_pair_bias_attention_input2qkvg_weight = None
    unsqueeze_523 = torch.ops.aten.unsqueeze.default(_to_copy_782, 3);  _to_copy_782 = None
    unsqueeze_524 = torch.ops.aten.unsqueeze.default(unsqueeze_523, 4);  unsqueeze_523 = None
    unsqueeze_525 = torch.ops.aten.unsqueeze.default(unsqueeze_524, 5);  unsqueeze_524 = None
    permute_801 = torch.ops.aten.permute.default(unsqueeze_525, [3, 0, 4, 1, 5, 2]);  unsqueeze_525 = None
    unsqueeze_526 = torch.ops.aten.unsqueeze.default(_to_copy_783, 4);  _to_copy_783 = None
    unsqueeze_527 = torch.ops.aten.unsqueeze.default(unsqueeze_526, 5);  unsqueeze_526 = None
    permute_802 = torch.ops.aten.permute.default(unsqueeze_527, [1, 4, 2, 5, 3, 0]);  unsqueeze_527 = None
    permute_803 = torch.ops.aten.permute.default(permute_801, [3, 5, 0, 1, 2, 4]);  permute_801 = None
    view_1390 = torch.ops.aten.view.default(permute_803, [1, 384, 384]);  permute_803 = None
    permute_804 = torch.ops.aten.permute.default(permute_802, [5, 0, 1, 2, 4, 3]);  permute_802 = None
    view_1391 = torch.ops.aten.view.default(permute_804, [1, 384, 1536]);  permute_804 = None
    bmm_125 = torch.ops.aten.bmm.default(view_1390, view_1391);  view_1390 = view_1391 = None
    view_1392 = torch.ops.aten.view.default(bmm_125, [384, 1, 4, 1, 16, 24]);  bmm_125 = None
    permute_805 = torch.ops.aten.permute.default(view_1392, [2, 3, 4, 0, 5, 1]);  view_1392 = None
    view_1393 = torch.ops.aten.view.default(permute_805, [4, 1, 16, 384, 24]);  permute_805 = None
    unbind_int_66 = torch.ops.aten.unbind.int(view_1393);  view_1393 = None
    getitem_1279 = unbind_int_66[0]
    getitem_1280 = unbind_int_66[1]
    getitem_1281 = unbind_int_66[2]
    getitem_1282 = unbind_int_66[3];  unbind_int_66 = None
    view_1394 = torch.ops.aten.view.default(pairformer_stack_blocks_10_attention_pair_bias_attention_query_bias, [1, 16, 1, 24]);  pairformer_stack_blocks_10_attention_pair_bias_attention_query_bias = None
    add_140 = torch.ops.aten.add.Tensor(getitem_1279, view_1394);  getitem_1279 = view_1394 = None
    _to_copy_784 = torch.ops.aten._to_copy.default(add_140, dtype = torch.bfloat16);  add_140 = None
    expand_81 = torch.ops.aten.expand.default(masked_fill_89, [1, 16, 384, 384]);  masked_fill_89 = None
    _scaled_dot_product_efficient_attention_default_44 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_784, getitem_1280, getitem_1281, expand_81, False);  _to_copy_784 = getitem_1280 = getitem_1281 = expand_81 = None
    getitem_1283 = _scaled_dot_product_efficient_attention_default_44[0];  _scaled_dot_product_efficient_attention_default_44 = None
    add_141 = torch.ops.aten.add.Tensor(getitem_1282, 1);  getitem_1282 = None
    sigmoid_101 = torch.ops.aten.sigmoid.default(add_141);  add_141 = None
    mul_165 = torch.ops.aten.mul.Tensor(getitem_1283, sigmoid_101);  getitem_1283 = sigmoid_101 = None
    _to_copy_785 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_10_attention_pair_bias_attention_output_proj_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_10_attention_pair_bias_attention_output_proj_weight = None
    unsqueeze_528 = torch.ops.aten.unsqueeze.default(mul_165, 4);  mul_165 = None
    permute_806 = torch.ops.aten.permute.default(unsqueeze_528, [0, 2, 4, 3, 1]);  unsqueeze_528 = None
    unsqueeze_529 = torch.ops.aten.unsqueeze.default(_to_copy_785, 3);  _to_copy_785 = None
    unsqueeze_530 = torch.ops.aten.unsqueeze.default(unsqueeze_529, 4);  unsqueeze_529 = None
    permute_807 = torch.ops.aten.permute.default(unsqueeze_530, [3, 4, 2, 1, 0]);  unsqueeze_530 = None
    permute_808 = torch.ops.aten.permute.default(permute_806, [1, 3, 4, 0, 2]);  permute_806 = None
    clone_142 = torch.ops.aten.clone.default(permute_808, memory_format = torch.contiguous_format);  permute_808 = None
    _unsafe_view_124 = torch.ops.aten._unsafe_view.default(clone_142, [1, 384, 384]);  clone_142 = None
    permute_809 = torch.ops.aten.permute.default(permute_807, [3, 4, 0, 2, 1]);  permute_807 = None
    clone_143 = torch.ops.aten.clone.default(permute_809, memory_format = torch.contiguous_format);  permute_809 = None
    _unsafe_view_125 = torch.ops.aten._unsafe_view.default(clone_143, [1, 384, 384]);  clone_143 = None
    bmm_126 = torch.ops.aten.bmm.default(_unsafe_view_124, _unsafe_view_125);  _unsafe_view_124 = _unsafe_view_125 = None
    view_1395 = torch.ops.aten.view.default(bmm_126, [384, 1, 1, 1, 384]);  bmm_126 = None
    permute_810 = torch.ops.aten.permute.default(view_1395, [3, 0, 4, 1, 2]);  view_1395 = None
    view_1396 = torch.ops.aten.view.default(permute_810, [1, 384, 384]);  permute_810 = None
    unsqueeze_531 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
    mul_166 = torch.ops.aten.mul.Tensor(view_1396, unsqueeze_531);  view_1396 = unsqueeze_531 = None
    add_142 = torch.ops.aten.add.Tensor(add_135, mul_166);  mul_166 = None
    split_tensor_134 = torch.ops.aten.split.Tensor(add_135, 384, dim = -2);  add_135 = None
    getitem_1287 = split_tensor_134[0];  split_tensor_134 = None
    _to_copy_786 = torch.ops.aten._to_copy.default(getitem_1287, dtype = torch.float32);  getitem_1287 = None
    native_layer_norm_default_162 = torch.ops.aten.native_layer_norm.default(_to_copy_786, [384], pairformer_stack_blocks_10_transition_single_layer_norm_weight, pairformer_stack_blocks_10_transition_single_layer_norm_bias, 1e-05);  _to_copy_786 = pairformer_stack_blocks_10_transition_single_layer_norm_weight = pairformer_stack_blocks_10_transition_single_layer_norm_bias = None
    getitem_1288 = native_layer_norm_default_162[0];  native_layer_norm_default_162 = None
    _to_copy_787 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_10_transition_single_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_10_transition_single_linear_no_bias_ab_weight = None
    _to_copy_788 = torch.ops.aten._to_copy.default(getitem_1288, dtype = torch.bfloat16);  getitem_1288 = None
    t_280 = torch.ops.aten.t.default(_to_copy_787);  _to_copy_787 = None
    view_1397 = torch.ops.aten.view.default(_to_copy_788, [384, 384]);  _to_copy_788 = None
    mm_259 = torch.ops.aten.mm.default(view_1397, t_280);  view_1397 = t_280 = None
    view_1398 = torch.ops.aten.view.default(mm_259, [1, 384, 1536]);  mm_259 = None
    split_tensor_135 = torch.ops.aten.split.Tensor(view_1398, 768, dim = -1);  view_1398 = None
    getitem_1291 = split_tensor_135[0]
    getitem_1292 = split_tensor_135[1];  split_tensor_135 = None
    silu_36 = torch.ops.aten.silu.default(getitem_1291);  getitem_1291 = None
    mul_167 = torch.ops.aten.mul.Tensor(silu_36, getitem_1292);  silu_36 = getitem_1292 = None
    _to_copy_789 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_10_transition_single_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_10_transition_single_linear_out_weight = None
    t_281 = torch.ops.aten.t.default(_to_copy_789);  _to_copy_789 = None
    view_1400 = torch.ops.aten.view.default(mul_167, [384, 768]);  mul_167 = None
    mm_260 = torch.ops.aten.mm.default(view_1400, t_281);  view_1400 = t_281 = None
    view_1401 = torch.ops.aten.view.default(mm_260, [1, 384, 384]);  mm_260 = None
    add_143 = torch.ops.aten.add.Tensor(add_142, view_1401);  add_142 = view_1401 = None
    _to_copy_790 = torch.ops.aten._to_copy.default(add_139, dtype = torch.float32)
    native_layer_norm_default_163 = torch.ops.aten.native_layer_norm.default(_to_copy_790, [256], pairformer_stack_blocks_11_triangle_multiplication_layernorm_z_in_weight, pairformer_stack_blocks_11_triangle_multiplication_layernorm_z_in_bias, 1e-05);  _to_copy_790 = pairformer_stack_blocks_11_triangle_multiplication_layernorm_z_in_weight = pairformer_stack_blocks_11_triangle_multiplication_layernorm_z_in_bias = None
    getitem_1293 = native_layer_norm_default_163[0];  native_layer_norm_default_163 = None
    split_with_sizes_default_34 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_11_triangle_multiplication_merged_linear_p_weight, [512, 512]);  pairformer_stack_blocks_11_triangle_multiplication_merged_linear_p_weight = None
    getitem_1296 = split_with_sizes_default_34[0]
    getitem_1297 = split_with_sizes_default_34[1];  split_with_sizes_default_34 = None
    split_with_sizes_default_35 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_11_triangle_multiplication_merged_linear_g_weight, [512, 512, 256]);  pairformer_stack_blocks_11_triangle_multiplication_merged_linear_g_weight = None
    getitem_1298 = split_with_sizes_default_35[0]
    getitem_1299 = split_with_sizes_default_35[1]
    getitem_1300 = split_with_sizes_default_35[2];  split_with_sizes_default_35 = None
    _to_copy_791 = torch.ops.aten._to_copy.default(getitem_1296, dtype = torch.bfloat16);  getitem_1296 = None
    _to_copy_792 = torch.ops.aten._to_copy.default(getitem_1293, dtype = torch.bfloat16)
    t_282 = torch.ops.aten.t.default(_to_copy_791);  _to_copy_791 = None
    view_1402 = torch.ops.aten.view.default(_to_copy_792, [147456, 256]);  _to_copy_792 = None
    mm_261 = torch.ops.aten.mm.default(view_1402, t_282);  view_1402 = t_282 = None
    view_1403 = torch.ops.aten.view.default(mm_261, [1, 384, 384, 512]);  mm_261 = None
    _to_copy_793 = torch.ops.aten._to_copy.default(getitem_1298, dtype = torch.bfloat16);  getitem_1298 = None
    _to_copy_794 = torch.ops.aten._to_copy.default(getitem_1293, dtype = torch.bfloat16)
    t_283 = torch.ops.aten.t.default(_to_copy_793);  _to_copy_793 = None
    view_1404 = torch.ops.aten.view.default(_to_copy_794, [147456, 256]);  _to_copy_794 = None
    mm_262 = torch.ops.aten.mm.default(view_1404, t_283);  view_1404 = t_283 = None
    view_1405 = torch.ops.aten.view.default(mm_262, [1, 384, 384, 512]);  mm_262 = None
    sigmoid_102 = torch.ops.aten.sigmoid.default(view_1405);  view_1405 = None
    mul_168 = torch.ops.aten.mul.Tensor(view_1403, sigmoid_102);  view_1403 = sigmoid_102 = None
    unsqueeze_532 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
    bitwise_not_90 = torch.ops.aten.bitwise_not.default(unsqueeze_532);  unsqueeze_532 = None
    masked_fill_90 = torch.ops.aten.masked_fill.Scalar(mul_168, bitwise_not_90, 0);  mul_168 = bitwise_not_90 = None
    split_tensor_136 = torch.ops.aten.split.Tensor(masked_fill_90, 256, dim = -1)
    getitem_1303 = split_tensor_136[0];  split_tensor_136 = None
    unsqueeze_535 = torch.ops.aten.unsqueeze.default(getitem_1303, 4);  getitem_1303 = None
    permute_815 = torch.ops.aten.permute.default(unsqueeze_535, [0, 1, 4, 3, 2]);  unsqueeze_535 = None
    permute_816 = torch.ops.aten.permute.default(permute_815, [3, 1, 4, 0, 2]);  permute_815 = None
    view_1408 = torch.ops.aten.view.default(permute_816, [256, 384, 384]);  permute_816 = None
    split_tensor_137 = torch.ops.aten.split.Tensor(masked_fill_90, 256, dim = -1);  masked_fill_90 = None
    getitem_1306 = split_tensor_137[1];  split_tensor_137 = None
    unsqueeze_536 = torch.ops.aten.unsqueeze.default(getitem_1306, 4);  getitem_1306 = None
    permute_817 = torch.ops.aten.permute.default(unsqueeze_536, [0, 4, 1, 3, 2]);  unsqueeze_536 = None
    permute_818 = torch.ops.aten.permute.default(permute_817, [3, 4, 0, 2, 1]);  permute_817 = None
    view_1409 = torch.ops.aten.view.default(permute_818, [256, 384, 384]);  permute_818 = None
    bmm_127 = torch.ops.aten.bmm.default(view_1408, view_1409);  view_1408 = view_1409 = None
    view_1410 = torch.ops.aten.view.default(bmm_127, [256, 384, 1, 1, 384]);  bmm_127 = None
    permute_819 = torch.ops.aten.permute.default(view_1410, [3, 1, 4, 0, 2]);  view_1410 = None
    view_1411 = torch.ops.aten.view.default(permute_819, [1, 384, 384, 256]);  permute_819 = None
    _to_copy_795 = torch.ops.aten._to_copy.default(getitem_1297, dtype = torch.bfloat16);  getitem_1297 = None
    _to_copy_796 = torch.ops.aten._to_copy.default(getitem_1293, dtype = torch.bfloat16)
    t_284 = torch.ops.aten.t.default(_to_copy_795);  _to_copy_795 = None
    view_1412 = torch.ops.aten.view.default(_to_copy_796, [147456, 256]);  _to_copy_796 = None
    mm_263 = torch.ops.aten.mm.default(view_1412, t_284);  view_1412 = t_284 = None
    view_1413 = torch.ops.aten.view.default(mm_263, [1, 384, 384, 512]);  mm_263 = None
    _to_copy_797 = torch.ops.aten._to_copy.default(getitem_1299, dtype = torch.bfloat16);  getitem_1299 = None
    _to_copy_798 = torch.ops.aten._to_copy.default(getitem_1293, dtype = torch.bfloat16)
    t_285 = torch.ops.aten.t.default(_to_copy_797);  _to_copy_797 = None
    view_1414 = torch.ops.aten.view.default(_to_copy_798, [147456, 256]);  _to_copy_798 = None
    mm_264 = torch.ops.aten.mm.default(view_1414, t_285);  view_1414 = t_285 = None
    view_1415 = torch.ops.aten.view.default(mm_264, [1, 384, 384, 512]);  mm_264 = None
    sigmoid_103 = torch.ops.aten.sigmoid.default(view_1415);  view_1415 = None
    mul_169 = torch.ops.aten.mul.Tensor(view_1413, sigmoid_103);  view_1413 = sigmoid_103 = None
    view_1416 = torch.ops.aten.view.default(mul_169, [147456, 512]);  mul_169 = None
    view_1417 = torch.ops.aten.view.default(view_1416, [1, 384, 384, 512]);  view_1416 = None
    transpose_34 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
    unsqueeze_537 = torch.ops.aten.unsqueeze.default(transpose_34, 3);  transpose_34 = None
    clone_144 = torch.ops.aten.clone.default(unsqueeze_537, memory_format = torch.contiguous_format);  unsqueeze_537 = None
    bitwise_not_91 = torch.ops.aten.bitwise_not.default(clone_144);  clone_144 = None
    masked_fill_91 = torch.ops.aten.masked_fill.Scalar(view_1417, bitwise_not_91, 0);  view_1417 = bitwise_not_91 = None
    view_1418 = torch.ops.aten.view.default(masked_fill_91, [147456, 512]);  masked_fill_91 = None
    view_1422 = torch.ops.aten.view.default(view_1418, [1, 384, 384, 512])
    split_tensor_138 = torch.ops.aten.split.Tensor(view_1422, 256, dim = -1);  view_1422 = None
    getitem_1309 = split_tensor_138[0];  split_tensor_138 = None
    unsqueeze_540 = torch.ops.aten.unsqueeze.default(getitem_1309, 4);  getitem_1309 = None
    permute_824 = torch.ops.aten.permute.default(unsqueeze_540, [0, 2, 4, 3, 1]);  unsqueeze_540 = None
    permute_825 = torch.ops.aten.permute.default(permute_824, [3, 1, 4, 0, 2]);  permute_824 = None
    view_1423 = torch.ops.aten.view.default(permute_825, [256, 384, 384]);  permute_825 = None
    view_1424 = torch.ops.aten.view.default(view_1418, [1, 384, 384, 512]);  view_1418 = None
    split_tensor_139 = torch.ops.aten.split.Tensor(view_1424, 256, dim = -1);  view_1424 = None
    getitem_1312 = split_tensor_139[1];  split_tensor_139 = None
    unsqueeze_541 = torch.ops.aten.unsqueeze.default(getitem_1312, 4);  getitem_1312 = None
    permute_826 = torch.ops.aten.permute.default(unsqueeze_541, [0, 4, 2, 3, 1]);  unsqueeze_541 = None
    permute_827 = torch.ops.aten.permute.default(permute_826, [3, 4, 0, 2, 1]);  permute_826 = None
    view_1425 = torch.ops.aten.view.default(permute_827, [256, 384, 384]);  permute_827 = None
    bmm_128 = torch.ops.aten.bmm.default(view_1423, view_1425);  view_1423 = view_1425 = None
    view_1426 = torch.ops.aten.view.default(bmm_128, [256, 384, 1, 1, 384]);  bmm_128 = None
    permute_828 = torch.ops.aten.permute.default(view_1426, [3, 1, 4, 0, 2]);  view_1426 = None
    view_1427 = torch.ops.aten.view.default(permute_828, [1, 384, 384, 256]);  permute_828 = None
    _to_copy_799 = torch.ops.aten._to_copy.default(view_1411, dtype = torch.float32);  view_1411 = None
    native_layer_norm_default_164 = torch.ops.aten.native_layer_norm.default(_to_copy_799, [256], None, None, 1e-05);  _to_copy_799 = None
    getitem_1313 = native_layer_norm_default_164[0];  native_layer_norm_default_164 = None
    _to_copy_800 = torch.ops.aten._to_copy.default(view_1427, dtype = torch.float32);  view_1427 = None
    native_layer_norm_default_165 = torch.ops.aten.native_layer_norm.default(_to_copy_800, [256], None, None, 1e-05);  _to_copy_800 = None
    getitem_1316 = native_layer_norm_default_165[0];  native_layer_norm_default_165 = None
    add_144 = torch.ops.aten.add.Tensor(getitem_1313, getitem_1316);  getitem_1313 = getitem_1316 = None
    _to_copy_801 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_11_triangle_multiplication_linear_z_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_11_triangle_multiplication_linear_z_out_weight = None
    _to_copy_802 = torch.ops.aten._to_copy.default(add_144, dtype = torch.bfloat16);  add_144 = None
    t_286 = torch.ops.aten.t.default(_to_copy_801);  _to_copy_801 = None
    view_1428 = torch.ops.aten.view.default(_to_copy_802, [147456, 256]);  _to_copy_802 = None
    mm_265 = torch.ops.aten.mm.default(view_1428, t_286);  view_1428 = t_286 = None
    view_1429 = torch.ops.aten.view.default(mm_265, [1, 384, 384, 256]);  mm_265 = None
    _to_copy_803 = torch.ops.aten._to_copy.default(getitem_1300, dtype = torch.bfloat16);  getitem_1300 = None
    _to_copy_804 = torch.ops.aten._to_copy.default(getitem_1293, dtype = torch.bfloat16);  getitem_1293 = None
    t_287 = torch.ops.aten.t.default(_to_copy_803);  _to_copy_803 = None
    view_1430 = torch.ops.aten.view.default(_to_copy_804, [147456, 256]);  _to_copy_804 = None
    mm_266 = torch.ops.aten.mm.default(view_1430, t_287);  view_1430 = t_287 = None
    view_1431 = torch.ops.aten.view.default(mm_266, [1, 384, 384, 256]);  mm_266 = None
    sigmoid_104 = torch.ops.aten.sigmoid.default(view_1431);  view_1431 = None
    mul_170 = torch.ops.aten.mul.Tensor(view_1429, sigmoid_104);  view_1429 = sigmoid_104 = None
    add_145 = torch.ops.aten.add.Tensor(add_139, mul_170);  mul_170 = None
    _to_copy_805 = torch.ops.aten._to_copy.default(add_139, dtype = torch.float32)
    native_layer_norm_default_166 = torch.ops.aten.native_layer_norm.default(_to_copy_805, [256], None, None, 1e-05);  _to_copy_805 = None
    getitem_1319 = native_layer_norm_default_166[0];  native_layer_norm_default_166 = None
    _to_copy_806 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_11_triangle_attention_pair2b_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_11_triangle_attention_pair2b_weight = None
    _to_copy_807 = torch.ops.aten._to_copy.default(getitem_1319, dtype = torch.bfloat16)
    t_288 = torch.ops.aten.t.default(_to_copy_806);  _to_copy_806 = None
    view_1432 = torch.ops.aten.view.default(_to_copy_807, [147456, 256]);  _to_copy_807 = None
    mm_267 = torch.ops.aten.mm.default(view_1432, t_288);  view_1432 = t_288 = None
    view_1433 = torch.ops.aten.view.default(mm_267, [1, 384, 384, 8]);  mm_267 = None
    view_1434 = torch.ops.aten.view.default(view_1433, [1, 384, 384, 2, 4]);  view_1433 = None
    permute_829 = torch.ops.aten.permute.default(view_1434, [0, 3, 4, 1, 2]);  view_1434 = None
    view_1435 = torch.ops.aten.view.default(permute_829, [1, 2, 4, 1, 384, 384]);  permute_829 = None
    view_1436 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 384, 384])
    bitwise_not_92 = torch.ops.aten.bitwise_not.default(view_1436);  view_1436 = None
    masked_fill_92 = torch.ops.aten.masked_fill.Scalar(view_1435, bitwise_not_92, -10000);  view_1435 = bitwise_not_92 = None
    view_1437 = torch.ops.aten.view.default(masked_fill_92, [1, 2, 4, 384, 384]);  masked_fill_92 = None
    permute_830 = torch.ops.aten.permute.default(view_1437, [1, 0, 2, 3, 4]);  view_1437 = None
    view_1438 = torch.ops.aten.view.default(permute_830, [2, 4, 1, 384, 384]);  permute_830 = None
    _to_copy_808 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_11_triangle_attention_pair2qkvg1_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_11_triangle_attention_pair2qkvg1_weight = None
    _to_copy_809 = torch.ops.aten._to_copy.default(getitem_1319, dtype = torch.bfloat16)
    t_289 = torch.ops.aten.t.default(_to_copy_808);  _to_copy_808 = None
    view_1439 = torch.ops.aten.view.default(_to_copy_809, [147456, 256]);  _to_copy_809 = None
    mm_268 = torch.ops.aten.mm.default(view_1439, t_289);  view_1439 = t_289 = None
    view_1440 = torch.ops.aten.view.default(mm_268, [1, 384, 384, 1024]);  mm_268 = None
    select_35 = torch.ops.aten.select.int(view_1438, 0, 0)
    view_1441 = torch.ops.aten.view.default(view_1440, [1, 384, 384, 4, 4, 64]);  view_1440 = None
    permute_831 = torch.ops.aten.permute.default(view_1441, [4, 0, 3, 1, 2, 5]);  view_1441 = None
    view_1442 = torch.ops.aten.view.default(permute_831, [4, 4, 384, 384, 64]);  permute_831 = None
    unbind_int_67 = torch.ops.aten.unbind.int(view_1442);  view_1442 = None
    getitem_1322 = unbind_int_67[0]
    getitem_1323 = unbind_int_67[1]
    getitem_1324 = unbind_int_67[2]
    getitem_1325 = unbind_int_67[3];  unbind_int_67 = None
    expand_82 = torch.ops.aten.expand.default(select_35, [4, 384, 384, 384]);  select_35 = None
    _scaled_dot_product_efficient_attention_default_45 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1322, getitem_1323, getitem_1324, expand_82, False);  getitem_1322 = getitem_1323 = getitem_1324 = expand_82 = None
    getitem_1326 = _scaled_dot_product_efficient_attention_default_45[0];  _scaled_dot_product_efficient_attention_default_45 = None
    sigmoid_105 = torch.ops.aten.sigmoid.default(getitem_1325);  getitem_1325 = None
    mul_171 = torch.ops.aten.mul.Tensor(getitem_1326, sigmoid_105);  getitem_1326 = sigmoid_105 = None
    view_1443 = torch.ops.aten.view.default(mul_171, [1, 4, 384, 384, 64]);  mul_171 = None
    permute_832 = torch.ops.aten.permute.default(view_1443, [0, 2, 3, 1, 4]);  view_1443 = None
    clone_145 = torch.ops.aten.clone.default(permute_832, memory_format = torch.contiguous_format);  permute_832 = None
    _unsafe_view_126 = torch.ops.aten._unsafe_view.default(clone_145, [1, 384, 384, 256]);  clone_145 = None
    transpose_35 = torch.ops.aten.transpose.int(getitem_1319, 1, 2);  getitem_1319 = None
    _to_copy_810 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_11_triangle_attention_pair2qkvg2_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_11_triangle_attention_pair2qkvg2_weight = None
    _to_copy_811 = torch.ops.aten._to_copy.default(transpose_35, dtype = torch.bfloat16);  transpose_35 = None
    t_290 = torch.ops.aten.t.default(_to_copy_810);  _to_copy_810 = None
    expand_83 = torch.ops.aten.expand.default(_to_copy_811, [1, 384, 384, 256]);  _to_copy_811 = None
    view_1444 = torch.ops.aten.view.default(expand_83, [384, 384, 256]);  expand_83 = None
    expand_84 = torch.ops.aten.expand.default(t_290, [1, 384, 256, 1024]);  t_290 = None
    view_1445 = torch.ops.aten.view.default(expand_84, [384, 256, 1024]);  expand_84 = None
    bmm_129 = torch.ops.aten.bmm.default(view_1444, view_1445);  view_1444 = view_1445 = None
    view_1446 = torch.ops.aten.view.default(bmm_129, [1, 384, 384, 1024]);  bmm_129 = None
    select_36 = torch.ops.aten.select.int(view_1438, 0, 1);  view_1438 = None
    view_1447 = torch.ops.aten.view.default(view_1446, [1, 384, 384, 4, 4, 64]);  view_1446 = None
    permute_833 = torch.ops.aten.permute.default(view_1447, [4, 0, 3, 1, 2, 5]);  view_1447 = None
    view_1448 = torch.ops.aten.view.default(permute_833, [4, 4, 384, 384, 64]);  permute_833 = None
    unbind_int_68 = torch.ops.aten.unbind.int(view_1448);  view_1448 = None
    getitem_1330 = unbind_int_68[0]
    getitem_1331 = unbind_int_68[1]
    getitem_1332 = unbind_int_68[2]
    getitem_1333 = unbind_int_68[3];  unbind_int_68 = None
    expand_85 = torch.ops.aten.expand.default(select_36, [4, 384, 384, 384]);  select_36 = None
    _scaled_dot_product_efficient_attention_default_46 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1330, getitem_1331, getitem_1332, expand_85, False);  getitem_1330 = getitem_1331 = getitem_1332 = expand_85 = None
    getitem_1334 = _scaled_dot_product_efficient_attention_default_46[0];  _scaled_dot_product_efficient_attention_default_46 = None
    sigmoid_106 = torch.ops.aten.sigmoid.default(getitem_1333);  getitem_1333 = None
    mul_172 = torch.ops.aten.mul.Tensor(getitem_1334, sigmoid_106);  getitem_1334 = sigmoid_106 = None
    view_1449 = torch.ops.aten.view.default(mul_172, [1, 4, 384, 384, 64]);  mul_172 = None
    permute_834 = torch.ops.aten.permute.default(view_1449, [0, 2, 3, 1, 4]);  view_1449 = None
    clone_146 = torch.ops.aten.clone.default(permute_834, memory_format = torch.contiguous_format);  permute_834 = None
    _unsafe_view_127 = torch.ops.aten._unsafe_view.default(clone_146, [1, 384, 384, 256]);  clone_146 = None
    cat_23 = torch.ops.aten.cat.default([_unsafe_view_126, _unsafe_view_127], dim = -1);  _unsafe_view_126 = _unsafe_view_127 = None
    slice_174 = torch.ops.aten.slice.Tensor(pairformer_stack_blocks_11_triangle_attention_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  pairformer_stack_blocks_11_triangle_attention_out_scalers = None
    unsqueeze_542 = torch.ops.aten.unsqueeze.default(slice_174, 1);  slice_174 = None
    mul_173 = torch.ops.aten.mul.Tensor(pairformer_stack_blocks_11_triangle_attention_linear_out_weight, unsqueeze_542);  pairformer_stack_blocks_11_triangle_attention_linear_out_weight = unsqueeze_542 = None
    _to_copy_812 = torch.ops.aten._to_copy.default(mul_173, dtype = torch.bfloat16);  mul_173 = None
    t_291 = torch.ops.aten.t.default(_to_copy_812);  _to_copy_812 = None
    view_1450 = torch.ops.aten.view.default(cat_23, [147456, 512]);  cat_23 = None
    mm_269 = torch.ops.aten.mm.default(view_1450, t_291);  view_1450 = t_291 = None
    view_1451 = torch.ops.aten.view.default(mm_269, [1, 384, 384, 256]);  mm_269 = None
    add_146 = torch.ops.aten.add.Tensor(add_145, view_1451);  add_145 = view_1451 = None
    split_tensor_140 = torch.ops.aten.split.Tensor(add_139, 384, dim = -2)
    getitem_1338 = split_tensor_140[0];  split_tensor_140 = None
    _to_copy_813 = torch.ops.aten._to_copy.default(getitem_1338, dtype = torch.float32);  getitem_1338 = None
    native_layer_norm_default_167 = torch.ops.aten.native_layer_norm.default(_to_copy_813, [256], pairformer_stack_blocks_11_transition_pair_layer_norm_weight, pairformer_stack_blocks_11_transition_pair_layer_norm_bias, 1e-05);  _to_copy_813 = pairformer_stack_blocks_11_transition_pair_layer_norm_weight = pairformer_stack_blocks_11_transition_pair_layer_norm_bias = None
    getitem_1339 = native_layer_norm_default_167[0];  native_layer_norm_default_167 = None
    _to_copy_814 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_11_transition_pair_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_11_transition_pair_linear_no_bias_ab_weight = None
    _to_copy_815 = torch.ops.aten._to_copy.default(getitem_1339, dtype = torch.bfloat16);  getitem_1339 = None
    t_292 = torch.ops.aten.t.default(_to_copy_814);  _to_copy_814 = None
    view_1452 = torch.ops.aten.view.default(_to_copy_815, [147456, 256]);  _to_copy_815 = None
    mm_270 = torch.ops.aten.mm.default(view_1452, t_292);  view_1452 = t_292 = None
    view_1453 = torch.ops.aten.view.default(mm_270, [1, 384, 384, 1024]);  mm_270 = None
    split_tensor_141 = torch.ops.aten.split.Tensor(view_1453, 512, dim = -1);  view_1453 = None
    getitem_1342 = split_tensor_141[0]
    getitem_1343 = split_tensor_141[1];  split_tensor_141 = None
    silu_37 = torch.ops.aten.silu.default(getitem_1342);  getitem_1342 = None
    mul_174 = torch.ops.aten.mul.Tensor(silu_37, getitem_1343);  silu_37 = getitem_1343 = None
    _to_copy_816 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_11_transition_pair_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_11_transition_pair_linear_out_weight = None
    t_293 = torch.ops.aten.t.default(_to_copy_816);  _to_copy_816 = None
    view_1455 = torch.ops.aten.view.default(mul_174, [147456, 512]);  mul_174 = None
    mm_271 = torch.ops.aten.mm.default(view_1455, t_293);  view_1455 = t_293 = None
    view_1456 = torch.ops.aten.view.default(mm_271, [1, 384, 384, 256]);  mm_271 = None
    add_147 = torch.ops.aten.add.Tensor(add_146, view_1456);  add_146 = view_1456 = None
    _to_copy_817 = torch.ops.aten._to_copy.default(add_143, dtype = torch.float32)
    native_layer_norm_default_168 = torch.ops.aten.native_layer_norm.default(_to_copy_817, [384], pairformer_stack_blocks_11_attention_pair_bias_single_layer_norm_weight, pairformer_stack_blocks_11_attention_pair_bias_single_layer_norm_bias, 1e-05);  _to_copy_817 = pairformer_stack_blocks_11_attention_pair_bias_single_layer_norm_weight = pairformer_stack_blocks_11_attention_pair_bias_single_layer_norm_bias = None
    getitem_1344 = native_layer_norm_default_168[0];  native_layer_norm_default_168 = None
    _to_copy_818 = torch.ops.aten._to_copy.default(add_139, dtype = torch.float32);  add_139 = None
    native_layer_norm_default_169 = torch.ops.aten.native_layer_norm.default(_to_copy_818, [256], pairformer_stack_blocks_11_attention_pair_bias_pair_layer_norm_weight, pairformer_stack_blocks_11_attention_pair_bias_pair_layer_norm_bias, 1e-05);  _to_copy_818 = pairformer_stack_blocks_11_attention_pair_bias_pair_layer_norm_weight = pairformer_stack_blocks_11_attention_pair_bias_pair_layer_norm_bias = None
    getitem_1347 = native_layer_norm_default_169[0];  native_layer_norm_default_169 = None
    _to_copy_819 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_11_attention_pair_bias_pair_linear_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_11_attention_pair_bias_pair_linear_weight = None
    _to_copy_820 = torch.ops.aten._to_copy.default(getitem_1347, dtype = torch.bfloat16);  getitem_1347 = None
    t_294 = torch.ops.aten.t.default(_to_copy_819);  _to_copy_819 = None
    view_1457 = torch.ops.aten.view.default(_to_copy_820, [147456, 256]);  _to_copy_820 = None
    mm_272 = torch.ops.aten.mm.default(view_1457, t_294);  view_1457 = t_294 = None
    view_1458 = torch.ops.aten.view.default(mm_272, [1, 384, 384, 16]);  mm_272 = None
    permute_835 = torch.ops.aten.permute.default(view_1458, [0, 3, 1, 2]);  view_1458 = None
    view_1459 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384])
    bitwise_not_93 = torch.ops.aten.bitwise_not.default(view_1459);  view_1459 = None
    masked_fill_93 = torch.ops.aten.masked_fill.Scalar(permute_835, bitwise_not_93, -10000);  permute_835 = bitwise_not_93 = None
    _to_copy_821 = torch.ops.aten._to_copy.default(getitem_1344, dtype = torch.bfloat16);  getitem_1344 = None
    _to_copy_822 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_11_attention_pair_bias_attention_input2qkvg_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_11_attention_pair_bias_attention_input2qkvg_weight = None
    unsqueeze_543 = torch.ops.aten.unsqueeze.default(_to_copy_821, 3);  _to_copy_821 = None
    unsqueeze_544 = torch.ops.aten.unsqueeze.default(unsqueeze_543, 4);  unsqueeze_543 = None
    unsqueeze_545 = torch.ops.aten.unsqueeze.default(unsqueeze_544, 5);  unsqueeze_544 = None
    permute_836 = torch.ops.aten.permute.default(unsqueeze_545, [3, 0, 4, 1, 5, 2]);  unsqueeze_545 = None
    unsqueeze_546 = torch.ops.aten.unsqueeze.default(_to_copy_822, 4);  _to_copy_822 = None
    unsqueeze_547 = torch.ops.aten.unsqueeze.default(unsqueeze_546, 5);  unsqueeze_546 = None
    permute_837 = torch.ops.aten.permute.default(unsqueeze_547, [1, 4, 2, 5, 3, 0]);  unsqueeze_547 = None
    permute_838 = torch.ops.aten.permute.default(permute_836, [3, 5, 0, 1, 2, 4]);  permute_836 = None
    view_1460 = torch.ops.aten.view.default(permute_838, [1, 384, 384]);  permute_838 = None
    permute_839 = torch.ops.aten.permute.default(permute_837, [5, 0, 1, 2, 4, 3]);  permute_837 = None
    view_1461 = torch.ops.aten.view.default(permute_839, [1, 384, 1536]);  permute_839 = None
    bmm_130 = torch.ops.aten.bmm.default(view_1460, view_1461);  view_1460 = view_1461 = None
    view_1462 = torch.ops.aten.view.default(bmm_130, [384, 1, 4, 1, 16, 24]);  bmm_130 = None
    permute_840 = torch.ops.aten.permute.default(view_1462, [2, 3, 4, 0, 5, 1]);  view_1462 = None
    view_1463 = torch.ops.aten.view.default(permute_840, [4, 1, 16, 384, 24]);  permute_840 = None
    unbind_int_69 = torch.ops.aten.unbind.int(view_1463);  view_1463 = None
    getitem_1350 = unbind_int_69[0]
    getitem_1351 = unbind_int_69[1]
    getitem_1352 = unbind_int_69[2]
    getitem_1353 = unbind_int_69[3];  unbind_int_69 = None
    view_1464 = torch.ops.aten.view.default(pairformer_stack_blocks_11_attention_pair_bias_attention_query_bias, [1, 16, 1, 24]);  pairformer_stack_blocks_11_attention_pair_bias_attention_query_bias = None
    add_148 = torch.ops.aten.add.Tensor(getitem_1350, view_1464);  getitem_1350 = view_1464 = None
    _to_copy_823 = torch.ops.aten._to_copy.default(add_148, dtype = torch.bfloat16);  add_148 = None
    expand_86 = torch.ops.aten.expand.default(masked_fill_93, [1, 16, 384, 384]);  masked_fill_93 = None
    _scaled_dot_product_efficient_attention_default_47 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_823, getitem_1351, getitem_1352, expand_86, False);  _to_copy_823 = getitem_1351 = getitem_1352 = expand_86 = None
    getitem_1354 = _scaled_dot_product_efficient_attention_default_47[0];  _scaled_dot_product_efficient_attention_default_47 = None
    add_149 = torch.ops.aten.add.Tensor(getitem_1353, 1);  getitem_1353 = None
    sigmoid_107 = torch.ops.aten.sigmoid.default(add_149);  add_149 = None
    mul_175 = torch.ops.aten.mul.Tensor(getitem_1354, sigmoid_107);  getitem_1354 = sigmoid_107 = None
    _to_copy_824 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_11_attention_pair_bias_attention_output_proj_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_11_attention_pair_bias_attention_output_proj_weight = None
    unsqueeze_548 = torch.ops.aten.unsqueeze.default(mul_175, 4);  mul_175 = None
    permute_841 = torch.ops.aten.permute.default(unsqueeze_548, [0, 2, 4, 3, 1]);  unsqueeze_548 = None
    unsqueeze_549 = torch.ops.aten.unsqueeze.default(_to_copy_824, 3);  _to_copy_824 = None
    unsqueeze_550 = torch.ops.aten.unsqueeze.default(unsqueeze_549, 4);  unsqueeze_549 = None
    permute_842 = torch.ops.aten.permute.default(unsqueeze_550, [3, 4, 2, 1, 0]);  unsqueeze_550 = None
    permute_843 = torch.ops.aten.permute.default(permute_841, [1, 3, 4, 0, 2]);  permute_841 = None
    clone_147 = torch.ops.aten.clone.default(permute_843, memory_format = torch.contiguous_format);  permute_843 = None
    _unsafe_view_128 = torch.ops.aten._unsafe_view.default(clone_147, [1, 384, 384]);  clone_147 = None
    permute_844 = torch.ops.aten.permute.default(permute_842, [3, 4, 0, 2, 1]);  permute_842 = None
    clone_148 = torch.ops.aten.clone.default(permute_844, memory_format = torch.contiguous_format);  permute_844 = None
    _unsafe_view_129 = torch.ops.aten._unsafe_view.default(clone_148, [1, 384, 384]);  clone_148 = None
    bmm_131 = torch.ops.aten.bmm.default(_unsafe_view_128, _unsafe_view_129);  _unsafe_view_128 = _unsafe_view_129 = None
    view_1465 = torch.ops.aten.view.default(bmm_131, [384, 1, 1, 1, 384]);  bmm_131 = None
    permute_845 = torch.ops.aten.permute.default(view_1465, [3, 0, 4, 1, 2]);  view_1465 = None
    view_1466 = torch.ops.aten.view.default(permute_845, [1, 384, 384]);  permute_845 = None
    unsqueeze_551 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
    mul_176 = torch.ops.aten.mul.Tensor(view_1466, unsqueeze_551);  view_1466 = unsqueeze_551 = None
    add_150 = torch.ops.aten.add.Tensor(add_143, mul_176);  mul_176 = None
    split_tensor_142 = torch.ops.aten.split.Tensor(add_143, 384, dim = -2);  add_143 = None
    getitem_1358 = split_tensor_142[0];  split_tensor_142 = None
    _to_copy_825 = torch.ops.aten._to_copy.default(getitem_1358, dtype = torch.float32);  getitem_1358 = None
    native_layer_norm_default_170 = torch.ops.aten.native_layer_norm.default(_to_copy_825, [384], pairformer_stack_blocks_11_transition_single_layer_norm_weight, pairformer_stack_blocks_11_transition_single_layer_norm_bias, 1e-05);  _to_copy_825 = pairformer_stack_blocks_11_transition_single_layer_norm_weight = pairformer_stack_blocks_11_transition_single_layer_norm_bias = None
    getitem_1359 = native_layer_norm_default_170[0];  native_layer_norm_default_170 = None
    _to_copy_826 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_11_transition_single_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_11_transition_single_linear_no_bias_ab_weight = None
    _to_copy_827 = torch.ops.aten._to_copy.default(getitem_1359, dtype = torch.bfloat16);  getitem_1359 = None
    t_295 = torch.ops.aten.t.default(_to_copy_826);  _to_copy_826 = None
    view_1467 = torch.ops.aten.view.default(_to_copy_827, [384, 384]);  _to_copy_827 = None
    mm_273 = torch.ops.aten.mm.default(view_1467, t_295);  view_1467 = t_295 = None
    view_1468 = torch.ops.aten.view.default(mm_273, [1, 384, 1536]);  mm_273 = None
    split_tensor_143 = torch.ops.aten.split.Tensor(view_1468, 768, dim = -1);  view_1468 = None
    getitem_1362 = split_tensor_143[0]
    getitem_1363 = split_tensor_143[1];  split_tensor_143 = None
    silu_38 = torch.ops.aten.silu.default(getitem_1362);  getitem_1362 = None
    mul_177 = torch.ops.aten.mul.Tensor(silu_38, getitem_1363);  silu_38 = getitem_1363 = None
    _to_copy_828 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_11_transition_single_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_11_transition_single_linear_out_weight = None
    t_296 = torch.ops.aten.t.default(_to_copy_828);  _to_copy_828 = None
    view_1470 = torch.ops.aten.view.default(mul_177, [384, 768]);  mul_177 = None
    mm_274 = torch.ops.aten.mm.default(view_1470, t_296);  view_1470 = t_296 = None
    view_1471 = torch.ops.aten.view.default(mm_274, [1, 384, 384]);  mm_274 = None
    add_151 = torch.ops.aten.add.Tensor(add_150, view_1471);  add_150 = view_1471 = None
    _to_copy_829 = torch.ops.aten._to_copy.default(add_147, dtype = torch.float32)
    native_layer_norm_default_171 = torch.ops.aten.native_layer_norm.default(_to_copy_829, [256], pairformer_stack_blocks_12_triangle_multiplication_layernorm_z_in_weight, pairformer_stack_blocks_12_triangle_multiplication_layernorm_z_in_bias, 1e-05);  _to_copy_829 = pairformer_stack_blocks_12_triangle_multiplication_layernorm_z_in_weight = pairformer_stack_blocks_12_triangle_multiplication_layernorm_z_in_bias = None
    getitem_1364 = native_layer_norm_default_171[0];  native_layer_norm_default_171 = None
    split_with_sizes_default_36 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_12_triangle_multiplication_merged_linear_p_weight, [512, 512]);  pairformer_stack_blocks_12_triangle_multiplication_merged_linear_p_weight = None
    getitem_1367 = split_with_sizes_default_36[0]
    getitem_1368 = split_with_sizes_default_36[1];  split_with_sizes_default_36 = None
    split_with_sizes_default_37 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_12_triangle_multiplication_merged_linear_g_weight, [512, 512, 256]);  pairformer_stack_blocks_12_triangle_multiplication_merged_linear_g_weight = None
    getitem_1369 = split_with_sizes_default_37[0]
    getitem_1370 = split_with_sizes_default_37[1]
    getitem_1371 = split_with_sizes_default_37[2];  split_with_sizes_default_37 = None
    _to_copy_830 = torch.ops.aten._to_copy.default(getitem_1367, dtype = torch.bfloat16);  getitem_1367 = None
    _to_copy_831 = torch.ops.aten._to_copy.default(getitem_1364, dtype = torch.bfloat16)
    t_297 = torch.ops.aten.t.default(_to_copy_830);  _to_copy_830 = None
    view_1472 = torch.ops.aten.view.default(_to_copy_831, [147456, 256]);  _to_copy_831 = None
    mm_275 = torch.ops.aten.mm.default(view_1472, t_297);  view_1472 = t_297 = None
    view_1473 = torch.ops.aten.view.default(mm_275, [1, 384, 384, 512]);  mm_275 = None
    _to_copy_832 = torch.ops.aten._to_copy.default(getitem_1369, dtype = torch.bfloat16);  getitem_1369 = None
    _to_copy_833 = torch.ops.aten._to_copy.default(getitem_1364, dtype = torch.bfloat16)
    t_298 = torch.ops.aten.t.default(_to_copy_832);  _to_copy_832 = None
    view_1474 = torch.ops.aten.view.default(_to_copy_833, [147456, 256]);  _to_copy_833 = None
    mm_276 = torch.ops.aten.mm.default(view_1474, t_298);  view_1474 = t_298 = None
    view_1475 = torch.ops.aten.view.default(mm_276, [1, 384, 384, 512]);  mm_276 = None
    sigmoid_108 = torch.ops.aten.sigmoid.default(view_1475);  view_1475 = None
    mul_178 = torch.ops.aten.mul.Tensor(view_1473, sigmoid_108);  view_1473 = sigmoid_108 = None
    unsqueeze_552 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
    bitwise_not_94 = torch.ops.aten.bitwise_not.default(unsqueeze_552);  unsqueeze_552 = None
    masked_fill_94 = torch.ops.aten.masked_fill.Scalar(mul_178, bitwise_not_94, 0);  mul_178 = bitwise_not_94 = None
    split_tensor_144 = torch.ops.aten.split.Tensor(masked_fill_94, 256, dim = -1)
    getitem_1374 = split_tensor_144[0];  split_tensor_144 = None
    unsqueeze_555 = torch.ops.aten.unsqueeze.default(getitem_1374, 4);  getitem_1374 = None
    permute_850 = torch.ops.aten.permute.default(unsqueeze_555, [0, 1, 4, 3, 2]);  unsqueeze_555 = None
    permute_851 = torch.ops.aten.permute.default(permute_850, [3, 1, 4, 0, 2]);  permute_850 = None
    view_1478 = torch.ops.aten.view.default(permute_851, [256, 384, 384]);  permute_851 = None
    split_tensor_145 = torch.ops.aten.split.Tensor(masked_fill_94, 256, dim = -1);  masked_fill_94 = None
    getitem_1377 = split_tensor_145[1];  split_tensor_145 = None
    unsqueeze_556 = torch.ops.aten.unsqueeze.default(getitem_1377, 4);  getitem_1377 = None
    permute_852 = torch.ops.aten.permute.default(unsqueeze_556, [0, 4, 1, 3, 2]);  unsqueeze_556 = None
    permute_853 = torch.ops.aten.permute.default(permute_852, [3, 4, 0, 2, 1]);  permute_852 = None
    view_1479 = torch.ops.aten.view.default(permute_853, [256, 384, 384]);  permute_853 = None
    bmm_132 = torch.ops.aten.bmm.default(view_1478, view_1479);  view_1478 = view_1479 = None
    view_1480 = torch.ops.aten.view.default(bmm_132, [256, 384, 1, 1, 384]);  bmm_132 = None
    permute_854 = torch.ops.aten.permute.default(view_1480, [3, 1, 4, 0, 2]);  view_1480 = None
    view_1481 = torch.ops.aten.view.default(permute_854, [1, 384, 384, 256]);  permute_854 = None
    _to_copy_834 = torch.ops.aten._to_copy.default(getitem_1368, dtype = torch.bfloat16);  getitem_1368 = None
    _to_copy_835 = torch.ops.aten._to_copy.default(getitem_1364, dtype = torch.bfloat16)
    t_299 = torch.ops.aten.t.default(_to_copy_834);  _to_copy_834 = None
    view_1482 = torch.ops.aten.view.default(_to_copy_835, [147456, 256]);  _to_copy_835 = None
    mm_277 = torch.ops.aten.mm.default(view_1482, t_299);  view_1482 = t_299 = None
    view_1483 = torch.ops.aten.view.default(mm_277, [1, 384, 384, 512]);  mm_277 = None
    _to_copy_836 = torch.ops.aten._to_copy.default(getitem_1370, dtype = torch.bfloat16);  getitem_1370 = None
    _to_copy_837 = torch.ops.aten._to_copy.default(getitem_1364, dtype = torch.bfloat16)
    t_300 = torch.ops.aten.t.default(_to_copy_836);  _to_copy_836 = None
    view_1484 = torch.ops.aten.view.default(_to_copy_837, [147456, 256]);  _to_copy_837 = None
    mm_278 = torch.ops.aten.mm.default(view_1484, t_300);  view_1484 = t_300 = None
    view_1485 = torch.ops.aten.view.default(mm_278, [1, 384, 384, 512]);  mm_278 = None
    sigmoid_109 = torch.ops.aten.sigmoid.default(view_1485);  view_1485 = None
    mul_179 = torch.ops.aten.mul.Tensor(view_1483, sigmoid_109);  view_1483 = sigmoid_109 = None
    view_1486 = torch.ops.aten.view.default(mul_179, [147456, 512]);  mul_179 = None
    view_1487 = torch.ops.aten.view.default(view_1486, [1, 384, 384, 512]);  view_1486 = None
    transpose_36 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
    unsqueeze_557 = torch.ops.aten.unsqueeze.default(transpose_36, 3);  transpose_36 = None
    clone_149 = torch.ops.aten.clone.default(unsqueeze_557, memory_format = torch.contiguous_format);  unsqueeze_557 = None
    bitwise_not_95 = torch.ops.aten.bitwise_not.default(clone_149);  clone_149 = None
    masked_fill_95 = torch.ops.aten.masked_fill.Scalar(view_1487, bitwise_not_95, 0);  view_1487 = bitwise_not_95 = None
    view_1488 = torch.ops.aten.view.default(masked_fill_95, [147456, 512]);  masked_fill_95 = None
    view_1492 = torch.ops.aten.view.default(view_1488, [1, 384, 384, 512])
    split_tensor_146 = torch.ops.aten.split.Tensor(view_1492, 256, dim = -1);  view_1492 = None
    getitem_1380 = split_tensor_146[0];  split_tensor_146 = None
    unsqueeze_560 = torch.ops.aten.unsqueeze.default(getitem_1380, 4);  getitem_1380 = None
    permute_859 = torch.ops.aten.permute.default(unsqueeze_560, [0, 2, 4, 3, 1]);  unsqueeze_560 = None
    permute_860 = torch.ops.aten.permute.default(permute_859, [3, 1, 4, 0, 2]);  permute_859 = None
    view_1493 = torch.ops.aten.view.default(permute_860, [256, 384, 384]);  permute_860 = None
    view_1494 = torch.ops.aten.view.default(view_1488, [1, 384, 384, 512]);  view_1488 = None
    split_tensor_147 = torch.ops.aten.split.Tensor(view_1494, 256, dim = -1);  view_1494 = None
    getitem_1383 = split_tensor_147[1];  split_tensor_147 = None
    unsqueeze_561 = torch.ops.aten.unsqueeze.default(getitem_1383, 4);  getitem_1383 = None
    permute_861 = torch.ops.aten.permute.default(unsqueeze_561, [0, 4, 2, 3, 1]);  unsqueeze_561 = None
    permute_862 = torch.ops.aten.permute.default(permute_861, [3, 4, 0, 2, 1]);  permute_861 = None
    view_1495 = torch.ops.aten.view.default(permute_862, [256, 384, 384]);  permute_862 = None
    bmm_133 = torch.ops.aten.bmm.default(view_1493, view_1495);  view_1493 = view_1495 = None
    view_1496 = torch.ops.aten.view.default(bmm_133, [256, 384, 1, 1, 384]);  bmm_133 = None
    permute_863 = torch.ops.aten.permute.default(view_1496, [3, 1, 4, 0, 2]);  view_1496 = None
    view_1497 = torch.ops.aten.view.default(permute_863, [1, 384, 384, 256]);  permute_863 = None
    _to_copy_838 = torch.ops.aten._to_copy.default(view_1481, dtype = torch.float32);  view_1481 = None
    native_layer_norm_default_172 = torch.ops.aten.native_layer_norm.default(_to_copy_838, [256], None, None, 1e-05);  _to_copy_838 = None
    getitem_1384 = native_layer_norm_default_172[0];  native_layer_norm_default_172 = None
    _to_copy_839 = torch.ops.aten._to_copy.default(view_1497, dtype = torch.float32);  view_1497 = None
    native_layer_norm_default_173 = torch.ops.aten.native_layer_norm.default(_to_copy_839, [256], None, None, 1e-05);  _to_copy_839 = None
    getitem_1387 = native_layer_norm_default_173[0];  native_layer_norm_default_173 = None
    add_152 = torch.ops.aten.add.Tensor(getitem_1384, getitem_1387);  getitem_1384 = getitem_1387 = None
    _to_copy_840 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_12_triangle_multiplication_linear_z_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_12_triangle_multiplication_linear_z_out_weight = None
    _to_copy_841 = torch.ops.aten._to_copy.default(add_152, dtype = torch.bfloat16);  add_152 = None
    t_301 = torch.ops.aten.t.default(_to_copy_840);  _to_copy_840 = None
    view_1498 = torch.ops.aten.view.default(_to_copy_841, [147456, 256]);  _to_copy_841 = None
    mm_279 = torch.ops.aten.mm.default(view_1498, t_301);  view_1498 = t_301 = None
    view_1499 = torch.ops.aten.view.default(mm_279, [1, 384, 384, 256]);  mm_279 = None
    _to_copy_842 = torch.ops.aten._to_copy.default(getitem_1371, dtype = torch.bfloat16);  getitem_1371 = None
    _to_copy_843 = torch.ops.aten._to_copy.default(getitem_1364, dtype = torch.bfloat16);  getitem_1364 = None
    t_302 = torch.ops.aten.t.default(_to_copy_842);  _to_copy_842 = None
    view_1500 = torch.ops.aten.view.default(_to_copy_843, [147456, 256]);  _to_copy_843 = None
    mm_280 = torch.ops.aten.mm.default(view_1500, t_302);  view_1500 = t_302 = None
    view_1501 = torch.ops.aten.view.default(mm_280, [1, 384, 384, 256]);  mm_280 = None
    sigmoid_110 = torch.ops.aten.sigmoid.default(view_1501);  view_1501 = None
    mul_180 = torch.ops.aten.mul.Tensor(view_1499, sigmoid_110);  view_1499 = sigmoid_110 = None
    add_153 = torch.ops.aten.add.Tensor(add_147, mul_180);  mul_180 = None
    _to_copy_844 = torch.ops.aten._to_copy.default(add_147, dtype = torch.float32)
    native_layer_norm_default_174 = torch.ops.aten.native_layer_norm.default(_to_copy_844, [256], None, None, 1e-05);  _to_copy_844 = None
    getitem_1390 = native_layer_norm_default_174[0];  native_layer_norm_default_174 = None
    _to_copy_845 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_12_triangle_attention_pair2b_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_12_triangle_attention_pair2b_weight = None
    _to_copy_846 = torch.ops.aten._to_copy.default(getitem_1390, dtype = torch.bfloat16)
    t_303 = torch.ops.aten.t.default(_to_copy_845);  _to_copy_845 = None
    view_1502 = torch.ops.aten.view.default(_to_copy_846, [147456, 256]);  _to_copy_846 = None
    mm_281 = torch.ops.aten.mm.default(view_1502, t_303);  view_1502 = t_303 = None
    view_1503 = torch.ops.aten.view.default(mm_281, [1, 384, 384, 8]);  mm_281 = None
    view_1504 = torch.ops.aten.view.default(view_1503, [1, 384, 384, 2, 4]);  view_1503 = None
    permute_864 = torch.ops.aten.permute.default(view_1504, [0, 3, 4, 1, 2]);  view_1504 = None
    view_1505 = torch.ops.aten.view.default(permute_864, [1, 2, 4, 1, 384, 384]);  permute_864 = None
    view_1506 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 384, 384])
    bitwise_not_96 = torch.ops.aten.bitwise_not.default(view_1506);  view_1506 = None
    masked_fill_96 = torch.ops.aten.masked_fill.Scalar(view_1505, bitwise_not_96, -10000);  view_1505 = bitwise_not_96 = None
    view_1507 = torch.ops.aten.view.default(masked_fill_96, [1, 2, 4, 384, 384]);  masked_fill_96 = None
    permute_865 = torch.ops.aten.permute.default(view_1507, [1, 0, 2, 3, 4]);  view_1507 = None
    view_1508 = torch.ops.aten.view.default(permute_865, [2, 4, 1, 384, 384]);  permute_865 = None
    _to_copy_847 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_12_triangle_attention_pair2qkvg1_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_12_triangle_attention_pair2qkvg1_weight = None
    _to_copy_848 = torch.ops.aten._to_copy.default(getitem_1390, dtype = torch.bfloat16)
    t_304 = torch.ops.aten.t.default(_to_copy_847);  _to_copy_847 = None
    view_1509 = torch.ops.aten.view.default(_to_copy_848, [147456, 256]);  _to_copy_848 = None
    mm_282 = torch.ops.aten.mm.default(view_1509, t_304);  view_1509 = t_304 = None
    view_1510 = torch.ops.aten.view.default(mm_282, [1, 384, 384, 1024]);  mm_282 = None
    select_37 = torch.ops.aten.select.int(view_1508, 0, 0)
    view_1511 = torch.ops.aten.view.default(view_1510, [1, 384, 384, 4, 4, 64]);  view_1510 = None
    permute_866 = torch.ops.aten.permute.default(view_1511, [4, 0, 3, 1, 2, 5]);  view_1511 = None
    view_1512 = torch.ops.aten.view.default(permute_866, [4, 4, 384, 384, 64]);  permute_866 = None
    unbind_int_70 = torch.ops.aten.unbind.int(view_1512);  view_1512 = None
    getitem_1393 = unbind_int_70[0]
    getitem_1394 = unbind_int_70[1]
    getitem_1395 = unbind_int_70[2]
    getitem_1396 = unbind_int_70[3];  unbind_int_70 = None
    expand_87 = torch.ops.aten.expand.default(select_37, [4, 384, 384, 384]);  select_37 = None
    _scaled_dot_product_efficient_attention_default_48 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1393, getitem_1394, getitem_1395, expand_87, False);  getitem_1393 = getitem_1394 = getitem_1395 = expand_87 = None
    getitem_1397 = _scaled_dot_product_efficient_attention_default_48[0];  _scaled_dot_product_efficient_attention_default_48 = None
    sigmoid_111 = torch.ops.aten.sigmoid.default(getitem_1396);  getitem_1396 = None
    mul_181 = torch.ops.aten.mul.Tensor(getitem_1397, sigmoid_111);  getitem_1397 = sigmoid_111 = None
    view_1513 = torch.ops.aten.view.default(mul_181, [1, 4, 384, 384, 64]);  mul_181 = None
    permute_867 = torch.ops.aten.permute.default(view_1513, [0, 2, 3, 1, 4]);  view_1513 = None
    clone_150 = torch.ops.aten.clone.default(permute_867, memory_format = torch.contiguous_format);  permute_867 = None
    _unsafe_view_130 = torch.ops.aten._unsafe_view.default(clone_150, [1, 384, 384, 256]);  clone_150 = None
    transpose_37 = torch.ops.aten.transpose.int(getitem_1390, 1, 2);  getitem_1390 = None
    _to_copy_849 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_12_triangle_attention_pair2qkvg2_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_12_triangle_attention_pair2qkvg2_weight = None
    _to_copy_850 = torch.ops.aten._to_copy.default(transpose_37, dtype = torch.bfloat16);  transpose_37 = None
    t_305 = torch.ops.aten.t.default(_to_copy_849);  _to_copy_849 = None
    expand_88 = torch.ops.aten.expand.default(_to_copy_850, [1, 384, 384, 256]);  _to_copy_850 = None
    view_1514 = torch.ops.aten.view.default(expand_88, [384, 384, 256]);  expand_88 = None
    expand_89 = torch.ops.aten.expand.default(t_305, [1, 384, 256, 1024]);  t_305 = None
    view_1515 = torch.ops.aten.view.default(expand_89, [384, 256, 1024]);  expand_89 = None
    bmm_134 = torch.ops.aten.bmm.default(view_1514, view_1515);  view_1514 = view_1515 = None
    view_1516 = torch.ops.aten.view.default(bmm_134, [1, 384, 384, 1024]);  bmm_134 = None
    select_38 = torch.ops.aten.select.int(view_1508, 0, 1);  view_1508 = None
    view_1517 = torch.ops.aten.view.default(view_1516, [1, 384, 384, 4, 4, 64]);  view_1516 = None
    permute_868 = torch.ops.aten.permute.default(view_1517, [4, 0, 3, 1, 2, 5]);  view_1517 = None
    view_1518 = torch.ops.aten.view.default(permute_868, [4, 4, 384, 384, 64]);  permute_868 = None
    unbind_int_71 = torch.ops.aten.unbind.int(view_1518);  view_1518 = None
    getitem_1401 = unbind_int_71[0]
    getitem_1402 = unbind_int_71[1]
    getitem_1403 = unbind_int_71[2]
    getitem_1404 = unbind_int_71[3];  unbind_int_71 = None
    expand_90 = torch.ops.aten.expand.default(select_38, [4, 384, 384, 384]);  select_38 = None
    _scaled_dot_product_efficient_attention_default_49 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1401, getitem_1402, getitem_1403, expand_90, False);  getitem_1401 = getitem_1402 = getitem_1403 = expand_90 = None
    getitem_1405 = _scaled_dot_product_efficient_attention_default_49[0];  _scaled_dot_product_efficient_attention_default_49 = None
    sigmoid_112 = torch.ops.aten.sigmoid.default(getitem_1404);  getitem_1404 = None
    mul_182 = torch.ops.aten.mul.Tensor(getitem_1405, sigmoid_112);  getitem_1405 = sigmoid_112 = None
    view_1519 = torch.ops.aten.view.default(mul_182, [1, 4, 384, 384, 64]);  mul_182 = None
    permute_869 = torch.ops.aten.permute.default(view_1519, [0, 2, 3, 1, 4]);  view_1519 = None
    clone_151 = torch.ops.aten.clone.default(permute_869, memory_format = torch.contiguous_format);  permute_869 = None
    _unsafe_view_131 = torch.ops.aten._unsafe_view.default(clone_151, [1, 384, 384, 256]);  clone_151 = None
    cat_24 = torch.ops.aten.cat.default([_unsafe_view_130, _unsafe_view_131], dim = -1);  _unsafe_view_130 = _unsafe_view_131 = None
    slice_175 = torch.ops.aten.slice.Tensor(pairformer_stack_blocks_12_triangle_attention_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  pairformer_stack_blocks_12_triangle_attention_out_scalers = None
    unsqueeze_562 = torch.ops.aten.unsqueeze.default(slice_175, 1);  slice_175 = None
    mul_183 = torch.ops.aten.mul.Tensor(pairformer_stack_blocks_12_triangle_attention_linear_out_weight, unsqueeze_562);  pairformer_stack_blocks_12_triangle_attention_linear_out_weight = unsqueeze_562 = None
    _to_copy_851 = torch.ops.aten._to_copy.default(mul_183, dtype = torch.bfloat16);  mul_183 = None
    t_306 = torch.ops.aten.t.default(_to_copy_851);  _to_copy_851 = None
    view_1520 = torch.ops.aten.view.default(cat_24, [147456, 512]);  cat_24 = None
    mm_283 = torch.ops.aten.mm.default(view_1520, t_306);  view_1520 = t_306 = None
    view_1521 = torch.ops.aten.view.default(mm_283, [1, 384, 384, 256]);  mm_283 = None
    add_154 = torch.ops.aten.add.Tensor(add_153, view_1521);  add_153 = view_1521 = None
    split_tensor_148 = torch.ops.aten.split.Tensor(add_147, 384, dim = -2)
    getitem_1409 = split_tensor_148[0];  split_tensor_148 = None
    _to_copy_852 = torch.ops.aten._to_copy.default(getitem_1409, dtype = torch.float32);  getitem_1409 = None
    native_layer_norm_default_175 = torch.ops.aten.native_layer_norm.default(_to_copy_852, [256], pairformer_stack_blocks_12_transition_pair_layer_norm_weight, pairformer_stack_blocks_12_transition_pair_layer_norm_bias, 1e-05);  _to_copy_852 = pairformer_stack_blocks_12_transition_pair_layer_norm_weight = pairformer_stack_blocks_12_transition_pair_layer_norm_bias = None
    getitem_1410 = native_layer_norm_default_175[0];  native_layer_norm_default_175 = None
    _to_copy_853 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_12_transition_pair_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_12_transition_pair_linear_no_bias_ab_weight = None
    _to_copy_854 = torch.ops.aten._to_copy.default(getitem_1410, dtype = torch.bfloat16);  getitem_1410 = None
    t_307 = torch.ops.aten.t.default(_to_copy_853);  _to_copy_853 = None
    view_1522 = torch.ops.aten.view.default(_to_copy_854, [147456, 256]);  _to_copy_854 = None
    mm_284 = torch.ops.aten.mm.default(view_1522, t_307);  view_1522 = t_307 = None
    view_1523 = torch.ops.aten.view.default(mm_284, [1, 384, 384, 1024]);  mm_284 = None
    split_tensor_149 = torch.ops.aten.split.Tensor(view_1523, 512, dim = -1);  view_1523 = None
    getitem_1413 = split_tensor_149[0]
    getitem_1414 = split_tensor_149[1];  split_tensor_149 = None
    silu_39 = torch.ops.aten.silu.default(getitem_1413);  getitem_1413 = None
    mul_184 = torch.ops.aten.mul.Tensor(silu_39, getitem_1414);  silu_39 = getitem_1414 = None
    _to_copy_855 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_12_transition_pair_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_12_transition_pair_linear_out_weight = None
    t_308 = torch.ops.aten.t.default(_to_copy_855);  _to_copy_855 = None
    view_1525 = torch.ops.aten.view.default(mul_184, [147456, 512]);  mul_184 = None
    mm_285 = torch.ops.aten.mm.default(view_1525, t_308);  view_1525 = t_308 = None
    view_1526 = torch.ops.aten.view.default(mm_285, [1, 384, 384, 256]);  mm_285 = None
    add_155 = torch.ops.aten.add.Tensor(add_154, view_1526);  add_154 = view_1526 = None
    _to_copy_856 = torch.ops.aten._to_copy.default(add_151, dtype = torch.float32)
    native_layer_norm_default_176 = torch.ops.aten.native_layer_norm.default(_to_copy_856, [384], pairformer_stack_blocks_12_attention_pair_bias_single_layer_norm_weight, pairformer_stack_blocks_12_attention_pair_bias_single_layer_norm_bias, 1e-05);  _to_copy_856 = pairformer_stack_blocks_12_attention_pair_bias_single_layer_norm_weight = pairformer_stack_blocks_12_attention_pair_bias_single_layer_norm_bias = None
    getitem_1415 = native_layer_norm_default_176[0];  native_layer_norm_default_176 = None
    _to_copy_857 = torch.ops.aten._to_copy.default(add_147, dtype = torch.float32);  add_147 = None
    native_layer_norm_default_177 = torch.ops.aten.native_layer_norm.default(_to_copy_857, [256], pairformer_stack_blocks_12_attention_pair_bias_pair_layer_norm_weight, pairformer_stack_blocks_12_attention_pair_bias_pair_layer_norm_bias, 1e-05);  _to_copy_857 = pairformer_stack_blocks_12_attention_pair_bias_pair_layer_norm_weight = pairformer_stack_blocks_12_attention_pair_bias_pair_layer_norm_bias = None
    getitem_1418 = native_layer_norm_default_177[0];  native_layer_norm_default_177 = None
    _to_copy_858 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_12_attention_pair_bias_pair_linear_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_12_attention_pair_bias_pair_linear_weight = None
    _to_copy_859 = torch.ops.aten._to_copy.default(getitem_1418, dtype = torch.bfloat16);  getitem_1418 = None
    t_309 = torch.ops.aten.t.default(_to_copy_858);  _to_copy_858 = None
    view_1527 = torch.ops.aten.view.default(_to_copy_859, [147456, 256]);  _to_copy_859 = None
    mm_286 = torch.ops.aten.mm.default(view_1527, t_309);  view_1527 = t_309 = None
    view_1528 = torch.ops.aten.view.default(mm_286, [1, 384, 384, 16]);  mm_286 = None
    permute_870 = torch.ops.aten.permute.default(view_1528, [0, 3, 1, 2]);  view_1528 = None
    view_1529 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384])
    bitwise_not_97 = torch.ops.aten.bitwise_not.default(view_1529);  view_1529 = None
    masked_fill_97 = torch.ops.aten.masked_fill.Scalar(permute_870, bitwise_not_97, -10000);  permute_870 = bitwise_not_97 = None
    _to_copy_860 = torch.ops.aten._to_copy.default(getitem_1415, dtype = torch.bfloat16);  getitem_1415 = None
    _to_copy_861 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_12_attention_pair_bias_attention_input2qkvg_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_12_attention_pair_bias_attention_input2qkvg_weight = None
    unsqueeze_563 = torch.ops.aten.unsqueeze.default(_to_copy_860, 3);  _to_copy_860 = None
    unsqueeze_564 = torch.ops.aten.unsqueeze.default(unsqueeze_563, 4);  unsqueeze_563 = None
    unsqueeze_565 = torch.ops.aten.unsqueeze.default(unsqueeze_564, 5);  unsqueeze_564 = None
    permute_871 = torch.ops.aten.permute.default(unsqueeze_565, [3, 0, 4, 1, 5, 2]);  unsqueeze_565 = None
    unsqueeze_566 = torch.ops.aten.unsqueeze.default(_to_copy_861, 4);  _to_copy_861 = None
    unsqueeze_567 = torch.ops.aten.unsqueeze.default(unsqueeze_566, 5);  unsqueeze_566 = None
    permute_872 = torch.ops.aten.permute.default(unsqueeze_567, [1, 4, 2, 5, 3, 0]);  unsqueeze_567 = None
    permute_873 = torch.ops.aten.permute.default(permute_871, [3, 5, 0, 1, 2, 4]);  permute_871 = None
    view_1530 = torch.ops.aten.view.default(permute_873, [1, 384, 384]);  permute_873 = None
    permute_874 = torch.ops.aten.permute.default(permute_872, [5, 0, 1, 2, 4, 3]);  permute_872 = None
    view_1531 = torch.ops.aten.view.default(permute_874, [1, 384, 1536]);  permute_874 = None
    bmm_135 = torch.ops.aten.bmm.default(view_1530, view_1531);  view_1530 = view_1531 = None
    view_1532 = torch.ops.aten.view.default(bmm_135, [384, 1, 4, 1, 16, 24]);  bmm_135 = None
    permute_875 = torch.ops.aten.permute.default(view_1532, [2, 3, 4, 0, 5, 1]);  view_1532 = None
    view_1533 = torch.ops.aten.view.default(permute_875, [4, 1, 16, 384, 24]);  permute_875 = None
    unbind_int_72 = torch.ops.aten.unbind.int(view_1533);  view_1533 = None
    getitem_1421 = unbind_int_72[0]
    getitem_1422 = unbind_int_72[1]
    getitem_1423 = unbind_int_72[2]
    getitem_1424 = unbind_int_72[3];  unbind_int_72 = None
    view_1534 = torch.ops.aten.view.default(pairformer_stack_blocks_12_attention_pair_bias_attention_query_bias, [1, 16, 1, 24]);  pairformer_stack_blocks_12_attention_pair_bias_attention_query_bias = None
    add_156 = torch.ops.aten.add.Tensor(getitem_1421, view_1534);  getitem_1421 = view_1534 = None
    _to_copy_862 = torch.ops.aten._to_copy.default(add_156, dtype = torch.bfloat16);  add_156 = None
    expand_91 = torch.ops.aten.expand.default(masked_fill_97, [1, 16, 384, 384]);  masked_fill_97 = None
    _scaled_dot_product_efficient_attention_default_50 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_862, getitem_1422, getitem_1423, expand_91, False);  _to_copy_862 = getitem_1422 = getitem_1423 = expand_91 = None
    getitem_1425 = _scaled_dot_product_efficient_attention_default_50[0];  _scaled_dot_product_efficient_attention_default_50 = None
    add_157 = torch.ops.aten.add.Tensor(getitem_1424, 1);  getitem_1424 = None
    sigmoid_113 = torch.ops.aten.sigmoid.default(add_157);  add_157 = None
    mul_185 = torch.ops.aten.mul.Tensor(getitem_1425, sigmoid_113);  getitem_1425 = sigmoid_113 = None
    _to_copy_863 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_12_attention_pair_bias_attention_output_proj_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_12_attention_pair_bias_attention_output_proj_weight = None
    unsqueeze_568 = torch.ops.aten.unsqueeze.default(mul_185, 4);  mul_185 = None
    permute_876 = torch.ops.aten.permute.default(unsqueeze_568, [0, 2, 4, 3, 1]);  unsqueeze_568 = None
    unsqueeze_569 = torch.ops.aten.unsqueeze.default(_to_copy_863, 3);  _to_copy_863 = None
    unsqueeze_570 = torch.ops.aten.unsqueeze.default(unsqueeze_569, 4);  unsqueeze_569 = None
    permute_877 = torch.ops.aten.permute.default(unsqueeze_570, [3, 4, 2, 1, 0]);  unsqueeze_570 = None
    permute_878 = torch.ops.aten.permute.default(permute_876, [1, 3, 4, 0, 2]);  permute_876 = None
    clone_152 = torch.ops.aten.clone.default(permute_878, memory_format = torch.contiguous_format);  permute_878 = None
    _unsafe_view_132 = torch.ops.aten._unsafe_view.default(clone_152, [1, 384, 384]);  clone_152 = None
    permute_879 = torch.ops.aten.permute.default(permute_877, [3, 4, 0, 2, 1]);  permute_877 = None
    clone_153 = torch.ops.aten.clone.default(permute_879, memory_format = torch.contiguous_format);  permute_879 = None
    _unsafe_view_133 = torch.ops.aten._unsafe_view.default(clone_153, [1, 384, 384]);  clone_153 = None
    bmm_136 = torch.ops.aten.bmm.default(_unsafe_view_132, _unsafe_view_133);  _unsafe_view_132 = _unsafe_view_133 = None
    view_1535 = torch.ops.aten.view.default(bmm_136, [384, 1, 1, 1, 384]);  bmm_136 = None
    permute_880 = torch.ops.aten.permute.default(view_1535, [3, 0, 4, 1, 2]);  view_1535 = None
    view_1536 = torch.ops.aten.view.default(permute_880, [1, 384, 384]);  permute_880 = None
    unsqueeze_571 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
    mul_186 = torch.ops.aten.mul.Tensor(view_1536, unsqueeze_571);  view_1536 = unsqueeze_571 = None
    add_158 = torch.ops.aten.add.Tensor(add_151, mul_186);  mul_186 = None
    split_tensor_150 = torch.ops.aten.split.Tensor(add_151, 384, dim = -2);  add_151 = None
    getitem_1429 = split_tensor_150[0];  split_tensor_150 = None
    _to_copy_864 = torch.ops.aten._to_copy.default(getitem_1429, dtype = torch.float32);  getitem_1429 = None
    native_layer_norm_default_178 = torch.ops.aten.native_layer_norm.default(_to_copy_864, [384], pairformer_stack_blocks_12_transition_single_layer_norm_weight, pairformer_stack_blocks_12_transition_single_layer_norm_bias, 1e-05);  _to_copy_864 = pairformer_stack_blocks_12_transition_single_layer_norm_weight = pairformer_stack_blocks_12_transition_single_layer_norm_bias = None
    getitem_1430 = native_layer_norm_default_178[0];  native_layer_norm_default_178 = None
    _to_copy_865 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_12_transition_single_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_12_transition_single_linear_no_bias_ab_weight = None
    _to_copy_866 = torch.ops.aten._to_copy.default(getitem_1430, dtype = torch.bfloat16);  getitem_1430 = None
    t_310 = torch.ops.aten.t.default(_to_copy_865);  _to_copy_865 = None
    view_1537 = torch.ops.aten.view.default(_to_copy_866, [384, 384]);  _to_copy_866 = None
    mm_287 = torch.ops.aten.mm.default(view_1537, t_310);  view_1537 = t_310 = None
    view_1538 = torch.ops.aten.view.default(mm_287, [1, 384, 1536]);  mm_287 = None
    split_tensor_151 = torch.ops.aten.split.Tensor(view_1538, 768, dim = -1);  view_1538 = None
    getitem_1433 = split_tensor_151[0]
    getitem_1434 = split_tensor_151[1];  split_tensor_151 = None
    silu_40 = torch.ops.aten.silu.default(getitem_1433);  getitem_1433 = None
    mul_187 = torch.ops.aten.mul.Tensor(silu_40, getitem_1434);  silu_40 = getitem_1434 = None
    _to_copy_867 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_12_transition_single_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_12_transition_single_linear_out_weight = None
    t_311 = torch.ops.aten.t.default(_to_copy_867);  _to_copy_867 = None
    view_1540 = torch.ops.aten.view.default(mul_187, [384, 768]);  mul_187 = None
    mm_288 = torch.ops.aten.mm.default(view_1540, t_311);  view_1540 = t_311 = None
    view_1541 = torch.ops.aten.view.default(mm_288, [1, 384, 384]);  mm_288 = None
    add_159 = torch.ops.aten.add.Tensor(add_158, view_1541);  add_158 = view_1541 = None
    _to_copy_868 = torch.ops.aten._to_copy.default(add_155, dtype = torch.float32)
    native_layer_norm_default_179 = torch.ops.aten.native_layer_norm.default(_to_copy_868, [256], pairformer_stack_blocks_13_triangle_multiplication_layernorm_z_in_weight, pairformer_stack_blocks_13_triangle_multiplication_layernorm_z_in_bias, 1e-05);  _to_copy_868 = pairformer_stack_blocks_13_triangle_multiplication_layernorm_z_in_weight = pairformer_stack_blocks_13_triangle_multiplication_layernorm_z_in_bias = None
    getitem_1435 = native_layer_norm_default_179[0];  native_layer_norm_default_179 = None
    split_with_sizes_default_38 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_13_triangle_multiplication_merged_linear_p_weight, [512, 512]);  pairformer_stack_blocks_13_triangle_multiplication_merged_linear_p_weight = None
    getitem_1438 = split_with_sizes_default_38[0]
    getitem_1439 = split_with_sizes_default_38[1];  split_with_sizes_default_38 = None
    split_with_sizes_default_39 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_13_triangle_multiplication_merged_linear_g_weight, [512, 512, 256]);  pairformer_stack_blocks_13_triangle_multiplication_merged_linear_g_weight = None
    getitem_1440 = split_with_sizes_default_39[0]
    getitem_1441 = split_with_sizes_default_39[1]
    getitem_1442 = split_with_sizes_default_39[2];  split_with_sizes_default_39 = None
    _to_copy_869 = torch.ops.aten._to_copy.default(getitem_1438, dtype = torch.bfloat16);  getitem_1438 = None
    _to_copy_870 = torch.ops.aten._to_copy.default(getitem_1435, dtype = torch.bfloat16)
    t_312 = torch.ops.aten.t.default(_to_copy_869);  _to_copy_869 = None
    view_1542 = torch.ops.aten.view.default(_to_copy_870, [147456, 256]);  _to_copy_870 = None
    mm_289 = torch.ops.aten.mm.default(view_1542, t_312);  view_1542 = t_312 = None
    view_1543 = torch.ops.aten.view.default(mm_289, [1, 384, 384, 512]);  mm_289 = None
    _to_copy_871 = torch.ops.aten._to_copy.default(getitem_1440, dtype = torch.bfloat16);  getitem_1440 = None
    _to_copy_872 = torch.ops.aten._to_copy.default(getitem_1435, dtype = torch.bfloat16)
    t_313 = torch.ops.aten.t.default(_to_copy_871);  _to_copy_871 = None
    view_1544 = torch.ops.aten.view.default(_to_copy_872, [147456, 256]);  _to_copy_872 = None
    mm_290 = torch.ops.aten.mm.default(view_1544, t_313);  view_1544 = t_313 = None
    view_1545 = torch.ops.aten.view.default(mm_290, [1, 384, 384, 512]);  mm_290 = None
    sigmoid_114 = torch.ops.aten.sigmoid.default(view_1545);  view_1545 = None
    mul_188 = torch.ops.aten.mul.Tensor(view_1543, sigmoid_114);  view_1543 = sigmoid_114 = None
    unsqueeze_572 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
    bitwise_not_98 = torch.ops.aten.bitwise_not.default(unsqueeze_572);  unsqueeze_572 = None
    masked_fill_98 = torch.ops.aten.masked_fill.Scalar(mul_188, bitwise_not_98, 0);  mul_188 = bitwise_not_98 = None
    split_tensor_152 = torch.ops.aten.split.Tensor(masked_fill_98, 256, dim = -1)
    getitem_1445 = split_tensor_152[0];  split_tensor_152 = None
    unsqueeze_575 = torch.ops.aten.unsqueeze.default(getitem_1445, 4);  getitem_1445 = None
    permute_885 = torch.ops.aten.permute.default(unsqueeze_575, [0, 1, 4, 3, 2]);  unsqueeze_575 = None
    permute_886 = torch.ops.aten.permute.default(permute_885, [3, 1, 4, 0, 2]);  permute_885 = None
    view_1548 = torch.ops.aten.view.default(permute_886, [256, 384, 384]);  permute_886 = None
    split_tensor_153 = torch.ops.aten.split.Tensor(masked_fill_98, 256, dim = -1);  masked_fill_98 = None
    getitem_1448 = split_tensor_153[1];  split_tensor_153 = None
    unsqueeze_576 = torch.ops.aten.unsqueeze.default(getitem_1448, 4);  getitem_1448 = None
    permute_887 = torch.ops.aten.permute.default(unsqueeze_576, [0, 4, 1, 3, 2]);  unsqueeze_576 = None
    permute_888 = torch.ops.aten.permute.default(permute_887, [3, 4, 0, 2, 1]);  permute_887 = None
    view_1549 = torch.ops.aten.view.default(permute_888, [256, 384, 384]);  permute_888 = None
    bmm_137 = torch.ops.aten.bmm.default(view_1548, view_1549);  view_1548 = view_1549 = None
    view_1550 = torch.ops.aten.view.default(bmm_137, [256, 384, 1, 1, 384]);  bmm_137 = None
    permute_889 = torch.ops.aten.permute.default(view_1550, [3, 1, 4, 0, 2]);  view_1550 = None
    view_1551 = torch.ops.aten.view.default(permute_889, [1, 384, 384, 256]);  permute_889 = None
    _to_copy_873 = torch.ops.aten._to_copy.default(getitem_1439, dtype = torch.bfloat16);  getitem_1439 = None
    _to_copy_874 = torch.ops.aten._to_copy.default(getitem_1435, dtype = torch.bfloat16)
    t_314 = torch.ops.aten.t.default(_to_copy_873);  _to_copy_873 = None
    view_1552 = torch.ops.aten.view.default(_to_copy_874, [147456, 256]);  _to_copy_874 = None
    mm_291 = torch.ops.aten.mm.default(view_1552, t_314);  view_1552 = t_314 = None
    view_1553 = torch.ops.aten.view.default(mm_291, [1, 384, 384, 512]);  mm_291 = None
    _to_copy_875 = torch.ops.aten._to_copy.default(getitem_1441, dtype = torch.bfloat16);  getitem_1441 = None
    _to_copy_876 = torch.ops.aten._to_copy.default(getitem_1435, dtype = torch.bfloat16)
    t_315 = torch.ops.aten.t.default(_to_copy_875);  _to_copy_875 = None
    view_1554 = torch.ops.aten.view.default(_to_copy_876, [147456, 256]);  _to_copy_876 = None
    mm_292 = torch.ops.aten.mm.default(view_1554, t_315);  view_1554 = t_315 = None
    view_1555 = torch.ops.aten.view.default(mm_292, [1, 384, 384, 512]);  mm_292 = None
    sigmoid_115 = torch.ops.aten.sigmoid.default(view_1555);  view_1555 = None
    mul_189 = torch.ops.aten.mul.Tensor(view_1553, sigmoid_115);  view_1553 = sigmoid_115 = None
    view_1556 = torch.ops.aten.view.default(mul_189, [147456, 512]);  mul_189 = None
    view_1557 = torch.ops.aten.view.default(view_1556, [1, 384, 384, 512]);  view_1556 = None
    transpose_38 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
    unsqueeze_577 = torch.ops.aten.unsqueeze.default(transpose_38, 3);  transpose_38 = None
    clone_154 = torch.ops.aten.clone.default(unsqueeze_577, memory_format = torch.contiguous_format);  unsqueeze_577 = None
    bitwise_not_99 = torch.ops.aten.bitwise_not.default(clone_154);  clone_154 = None
    masked_fill_99 = torch.ops.aten.masked_fill.Scalar(view_1557, bitwise_not_99, 0);  view_1557 = bitwise_not_99 = None
    view_1558 = torch.ops.aten.view.default(masked_fill_99, [147456, 512]);  masked_fill_99 = None
    view_1562 = torch.ops.aten.view.default(view_1558, [1, 384, 384, 512])
    split_tensor_154 = torch.ops.aten.split.Tensor(view_1562, 256, dim = -1);  view_1562 = None
    getitem_1451 = split_tensor_154[0];  split_tensor_154 = None
    unsqueeze_580 = torch.ops.aten.unsqueeze.default(getitem_1451, 4);  getitem_1451 = None
    permute_894 = torch.ops.aten.permute.default(unsqueeze_580, [0, 2, 4, 3, 1]);  unsqueeze_580 = None
    permute_895 = torch.ops.aten.permute.default(permute_894, [3, 1, 4, 0, 2]);  permute_894 = None
    view_1563 = torch.ops.aten.view.default(permute_895, [256, 384, 384]);  permute_895 = None
    view_1564 = torch.ops.aten.view.default(view_1558, [1, 384, 384, 512]);  view_1558 = None
    split_tensor_155 = torch.ops.aten.split.Tensor(view_1564, 256, dim = -1);  view_1564 = None
    getitem_1454 = split_tensor_155[1];  split_tensor_155 = None
    unsqueeze_581 = torch.ops.aten.unsqueeze.default(getitem_1454, 4);  getitem_1454 = None
    permute_896 = torch.ops.aten.permute.default(unsqueeze_581, [0, 4, 2, 3, 1]);  unsqueeze_581 = None
    permute_897 = torch.ops.aten.permute.default(permute_896, [3, 4, 0, 2, 1]);  permute_896 = None
    view_1565 = torch.ops.aten.view.default(permute_897, [256, 384, 384]);  permute_897 = None
    bmm_138 = torch.ops.aten.bmm.default(view_1563, view_1565);  view_1563 = view_1565 = None
    view_1566 = torch.ops.aten.view.default(bmm_138, [256, 384, 1, 1, 384]);  bmm_138 = None
    permute_898 = torch.ops.aten.permute.default(view_1566, [3, 1, 4, 0, 2]);  view_1566 = None
    view_1567 = torch.ops.aten.view.default(permute_898, [1, 384, 384, 256]);  permute_898 = None
    _to_copy_877 = torch.ops.aten._to_copy.default(view_1551, dtype = torch.float32);  view_1551 = None
    native_layer_norm_default_180 = torch.ops.aten.native_layer_norm.default(_to_copy_877, [256], None, None, 1e-05);  _to_copy_877 = None
    getitem_1455 = native_layer_norm_default_180[0];  native_layer_norm_default_180 = None
    _to_copy_878 = torch.ops.aten._to_copy.default(view_1567, dtype = torch.float32);  view_1567 = None
    native_layer_norm_default_181 = torch.ops.aten.native_layer_norm.default(_to_copy_878, [256], None, None, 1e-05);  _to_copy_878 = None
    getitem_1458 = native_layer_norm_default_181[0];  native_layer_norm_default_181 = None
    add_160 = torch.ops.aten.add.Tensor(getitem_1455, getitem_1458);  getitem_1455 = getitem_1458 = None
    _to_copy_879 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_13_triangle_multiplication_linear_z_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_13_triangle_multiplication_linear_z_out_weight = None
    _to_copy_880 = torch.ops.aten._to_copy.default(add_160, dtype = torch.bfloat16);  add_160 = None
    t_316 = torch.ops.aten.t.default(_to_copy_879);  _to_copy_879 = None
    view_1568 = torch.ops.aten.view.default(_to_copy_880, [147456, 256]);  _to_copy_880 = None
    mm_293 = torch.ops.aten.mm.default(view_1568, t_316);  view_1568 = t_316 = None
    view_1569 = torch.ops.aten.view.default(mm_293, [1, 384, 384, 256]);  mm_293 = None
    _to_copy_881 = torch.ops.aten._to_copy.default(getitem_1442, dtype = torch.bfloat16);  getitem_1442 = None
    _to_copy_882 = torch.ops.aten._to_copy.default(getitem_1435, dtype = torch.bfloat16);  getitem_1435 = None
    t_317 = torch.ops.aten.t.default(_to_copy_881);  _to_copy_881 = None
    view_1570 = torch.ops.aten.view.default(_to_copy_882, [147456, 256]);  _to_copy_882 = None
    mm_294 = torch.ops.aten.mm.default(view_1570, t_317);  view_1570 = t_317 = None
    view_1571 = torch.ops.aten.view.default(mm_294, [1, 384, 384, 256]);  mm_294 = None
    sigmoid_116 = torch.ops.aten.sigmoid.default(view_1571);  view_1571 = None
    mul_190 = torch.ops.aten.mul.Tensor(view_1569, sigmoid_116);  view_1569 = sigmoid_116 = None
    add_161 = torch.ops.aten.add.Tensor(add_155, mul_190);  mul_190 = None
    _to_copy_883 = torch.ops.aten._to_copy.default(add_155, dtype = torch.float32)
    native_layer_norm_default_182 = torch.ops.aten.native_layer_norm.default(_to_copy_883, [256], None, None, 1e-05);  _to_copy_883 = None
    getitem_1461 = native_layer_norm_default_182[0];  native_layer_norm_default_182 = None
    _to_copy_884 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_13_triangle_attention_pair2b_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_13_triangle_attention_pair2b_weight = None
    _to_copy_885 = torch.ops.aten._to_copy.default(getitem_1461, dtype = torch.bfloat16)
    t_318 = torch.ops.aten.t.default(_to_copy_884);  _to_copy_884 = None
    view_1572 = torch.ops.aten.view.default(_to_copy_885, [147456, 256]);  _to_copy_885 = None
    mm_295 = torch.ops.aten.mm.default(view_1572, t_318);  view_1572 = t_318 = None
    view_1573 = torch.ops.aten.view.default(mm_295, [1, 384, 384, 8]);  mm_295 = None
    view_1574 = torch.ops.aten.view.default(view_1573, [1, 384, 384, 2, 4]);  view_1573 = None
    permute_899 = torch.ops.aten.permute.default(view_1574, [0, 3, 4, 1, 2]);  view_1574 = None
    view_1575 = torch.ops.aten.view.default(permute_899, [1, 2, 4, 1, 384, 384]);  permute_899 = None
    view_1576 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 384, 384])
    bitwise_not_100 = torch.ops.aten.bitwise_not.default(view_1576);  view_1576 = None
    masked_fill_100 = torch.ops.aten.masked_fill.Scalar(view_1575, bitwise_not_100, -10000);  view_1575 = bitwise_not_100 = None
    view_1577 = torch.ops.aten.view.default(masked_fill_100, [1, 2, 4, 384, 384]);  masked_fill_100 = None
    permute_900 = torch.ops.aten.permute.default(view_1577, [1, 0, 2, 3, 4]);  view_1577 = None
    view_1578 = torch.ops.aten.view.default(permute_900, [2, 4, 1, 384, 384]);  permute_900 = None
    _to_copy_886 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_13_triangle_attention_pair2qkvg1_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_13_triangle_attention_pair2qkvg1_weight = None
    _to_copy_887 = torch.ops.aten._to_copy.default(getitem_1461, dtype = torch.bfloat16)
    t_319 = torch.ops.aten.t.default(_to_copy_886);  _to_copy_886 = None
    view_1579 = torch.ops.aten.view.default(_to_copy_887, [147456, 256]);  _to_copy_887 = None
    mm_296 = torch.ops.aten.mm.default(view_1579, t_319);  view_1579 = t_319 = None
    view_1580 = torch.ops.aten.view.default(mm_296, [1, 384, 384, 1024]);  mm_296 = None
    select_39 = torch.ops.aten.select.int(view_1578, 0, 0)
    view_1581 = torch.ops.aten.view.default(view_1580, [1, 384, 384, 4, 4, 64]);  view_1580 = None
    permute_901 = torch.ops.aten.permute.default(view_1581, [4, 0, 3, 1, 2, 5]);  view_1581 = None
    view_1582 = torch.ops.aten.view.default(permute_901, [4, 4, 384, 384, 64]);  permute_901 = None
    unbind_int_73 = torch.ops.aten.unbind.int(view_1582);  view_1582 = None
    getitem_1464 = unbind_int_73[0]
    getitem_1465 = unbind_int_73[1]
    getitem_1466 = unbind_int_73[2]
    getitem_1467 = unbind_int_73[3];  unbind_int_73 = None
    expand_92 = torch.ops.aten.expand.default(select_39, [4, 384, 384, 384]);  select_39 = None
    _scaled_dot_product_efficient_attention_default_51 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1464, getitem_1465, getitem_1466, expand_92, False);  getitem_1464 = getitem_1465 = getitem_1466 = expand_92 = None
    getitem_1468 = _scaled_dot_product_efficient_attention_default_51[0];  _scaled_dot_product_efficient_attention_default_51 = None
    sigmoid_117 = torch.ops.aten.sigmoid.default(getitem_1467);  getitem_1467 = None
    mul_191 = torch.ops.aten.mul.Tensor(getitem_1468, sigmoid_117);  getitem_1468 = sigmoid_117 = None
    view_1583 = torch.ops.aten.view.default(mul_191, [1, 4, 384, 384, 64]);  mul_191 = None
    permute_902 = torch.ops.aten.permute.default(view_1583, [0, 2, 3, 1, 4]);  view_1583 = None
    clone_155 = torch.ops.aten.clone.default(permute_902, memory_format = torch.contiguous_format);  permute_902 = None
    _unsafe_view_134 = torch.ops.aten._unsafe_view.default(clone_155, [1, 384, 384, 256]);  clone_155 = None
    transpose_39 = torch.ops.aten.transpose.int(getitem_1461, 1, 2);  getitem_1461 = None
    _to_copy_888 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_13_triangle_attention_pair2qkvg2_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_13_triangle_attention_pair2qkvg2_weight = None
    _to_copy_889 = torch.ops.aten._to_copy.default(transpose_39, dtype = torch.bfloat16);  transpose_39 = None
    t_320 = torch.ops.aten.t.default(_to_copy_888);  _to_copy_888 = None
    expand_93 = torch.ops.aten.expand.default(_to_copy_889, [1, 384, 384, 256]);  _to_copy_889 = None
    view_1584 = torch.ops.aten.view.default(expand_93, [384, 384, 256]);  expand_93 = None
    expand_94 = torch.ops.aten.expand.default(t_320, [1, 384, 256, 1024]);  t_320 = None
    view_1585 = torch.ops.aten.view.default(expand_94, [384, 256, 1024]);  expand_94 = None
    bmm_139 = torch.ops.aten.bmm.default(view_1584, view_1585);  view_1584 = view_1585 = None
    view_1586 = torch.ops.aten.view.default(bmm_139, [1, 384, 384, 1024]);  bmm_139 = None
    select_40 = torch.ops.aten.select.int(view_1578, 0, 1);  view_1578 = None
    view_1587 = torch.ops.aten.view.default(view_1586, [1, 384, 384, 4, 4, 64]);  view_1586 = None
    permute_903 = torch.ops.aten.permute.default(view_1587, [4, 0, 3, 1, 2, 5]);  view_1587 = None
    view_1588 = torch.ops.aten.view.default(permute_903, [4, 4, 384, 384, 64]);  permute_903 = None
    unbind_int_74 = torch.ops.aten.unbind.int(view_1588);  view_1588 = None
    getitem_1472 = unbind_int_74[0]
    getitem_1473 = unbind_int_74[1]
    getitem_1474 = unbind_int_74[2]
    getitem_1475 = unbind_int_74[3];  unbind_int_74 = None
    expand_95 = torch.ops.aten.expand.default(select_40, [4, 384, 384, 384]);  select_40 = None
    _scaled_dot_product_efficient_attention_default_52 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1472, getitem_1473, getitem_1474, expand_95, False);  getitem_1472 = getitem_1473 = getitem_1474 = expand_95 = None
    getitem_1476 = _scaled_dot_product_efficient_attention_default_52[0];  _scaled_dot_product_efficient_attention_default_52 = None
    sigmoid_118 = torch.ops.aten.sigmoid.default(getitem_1475);  getitem_1475 = None
    mul_192 = torch.ops.aten.mul.Tensor(getitem_1476, sigmoid_118);  getitem_1476 = sigmoid_118 = None
    view_1589 = torch.ops.aten.view.default(mul_192, [1, 4, 384, 384, 64]);  mul_192 = None
    permute_904 = torch.ops.aten.permute.default(view_1589, [0, 2, 3, 1, 4]);  view_1589 = None
    clone_156 = torch.ops.aten.clone.default(permute_904, memory_format = torch.contiguous_format);  permute_904 = None
    _unsafe_view_135 = torch.ops.aten._unsafe_view.default(clone_156, [1, 384, 384, 256]);  clone_156 = None
    cat_25 = torch.ops.aten.cat.default([_unsafe_view_134, _unsafe_view_135], dim = -1);  _unsafe_view_134 = _unsafe_view_135 = None
    slice_176 = torch.ops.aten.slice.Tensor(pairformer_stack_blocks_13_triangle_attention_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  pairformer_stack_blocks_13_triangle_attention_out_scalers = None
    unsqueeze_582 = torch.ops.aten.unsqueeze.default(slice_176, 1);  slice_176 = None
    mul_193 = torch.ops.aten.mul.Tensor(pairformer_stack_blocks_13_triangle_attention_linear_out_weight, unsqueeze_582);  pairformer_stack_blocks_13_triangle_attention_linear_out_weight = unsqueeze_582 = None
    _to_copy_890 = torch.ops.aten._to_copy.default(mul_193, dtype = torch.bfloat16);  mul_193 = None
    t_321 = torch.ops.aten.t.default(_to_copy_890);  _to_copy_890 = None
    view_1590 = torch.ops.aten.view.default(cat_25, [147456, 512]);  cat_25 = None
    mm_297 = torch.ops.aten.mm.default(view_1590, t_321);  view_1590 = t_321 = None
    view_1591 = torch.ops.aten.view.default(mm_297, [1, 384, 384, 256]);  mm_297 = None
    add_162 = torch.ops.aten.add.Tensor(add_161, view_1591);  add_161 = view_1591 = None
    split_tensor_156 = torch.ops.aten.split.Tensor(add_155, 384, dim = -2)
    getitem_1480 = split_tensor_156[0];  split_tensor_156 = None
    _to_copy_891 = torch.ops.aten._to_copy.default(getitem_1480, dtype = torch.float32);  getitem_1480 = None
    native_layer_norm_default_183 = torch.ops.aten.native_layer_norm.default(_to_copy_891, [256], pairformer_stack_blocks_13_transition_pair_layer_norm_weight, pairformer_stack_blocks_13_transition_pair_layer_norm_bias, 1e-05);  _to_copy_891 = pairformer_stack_blocks_13_transition_pair_layer_norm_weight = pairformer_stack_blocks_13_transition_pair_layer_norm_bias = None
    getitem_1481 = native_layer_norm_default_183[0];  native_layer_norm_default_183 = None
    _to_copy_892 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_13_transition_pair_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_13_transition_pair_linear_no_bias_ab_weight = None
    _to_copy_893 = torch.ops.aten._to_copy.default(getitem_1481, dtype = torch.bfloat16);  getitem_1481 = None
    t_322 = torch.ops.aten.t.default(_to_copy_892);  _to_copy_892 = None
    view_1592 = torch.ops.aten.view.default(_to_copy_893, [147456, 256]);  _to_copy_893 = None
    mm_298 = torch.ops.aten.mm.default(view_1592, t_322);  view_1592 = t_322 = None
    view_1593 = torch.ops.aten.view.default(mm_298, [1, 384, 384, 1024]);  mm_298 = None
    split_tensor_157 = torch.ops.aten.split.Tensor(view_1593, 512, dim = -1);  view_1593 = None
    getitem_1484 = split_tensor_157[0]
    getitem_1485 = split_tensor_157[1];  split_tensor_157 = None
    silu_41 = torch.ops.aten.silu.default(getitem_1484);  getitem_1484 = None
    mul_194 = torch.ops.aten.mul.Tensor(silu_41, getitem_1485);  silu_41 = getitem_1485 = None
    _to_copy_894 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_13_transition_pair_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_13_transition_pair_linear_out_weight = None
    t_323 = torch.ops.aten.t.default(_to_copy_894);  _to_copy_894 = None
    view_1595 = torch.ops.aten.view.default(mul_194, [147456, 512]);  mul_194 = None
    mm_299 = torch.ops.aten.mm.default(view_1595, t_323);  view_1595 = t_323 = None
    view_1596 = torch.ops.aten.view.default(mm_299, [1, 384, 384, 256]);  mm_299 = None
    add_163 = torch.ops.aten.add.Tensor(add_162, view_1596);  add_162 = view_1596 = None
    _to_copy_895 = torch.ops.aten._to_copy.default(add_159, dtype = torch.float32)
    native_layer_norm_default_184 = torch.ops.aten.native_layer_norm.default(_to_copy_895, [384], pairformer_stack_blocks_13_attention_pair_bias_single_layer_norm_weight, pairformer_stack_blocks_13_attention_pair_bias_single_layer_norm_bias, 1e-05);  _to_copy_895 = pairformer_stack_blocks_13_attention_pair_bias_single_layer_norm_weight = pairformer_stack_blocks_13_attention_pair_bias_single_layer_norm_bias = None
    getitem_1486 = native_layer_norm_default_184[0];  native_layer_norm_default_184 = None
    _to_copy_896 = torch.ops.aten._to_copy.default(add_155, dtype = torch.float32);  add_155 = None
    native_layer_norm_default_185 = torch.ops.aten.native_layer_norm.default(_to_copy_896, [256], pairformer_stack_blocks_13_attention_pair_bias_pair_layer_norm_weight, pairformer_stack_blocks_13_attention_pair_bias_pair_layer_norm_bias, 1e-05);  _to_copy_896 = pairformer_stack_blocks_13_attention_pair_bias_pair_layer_norm_weight = pairformer_stack_blocks_13_attention_pair_bias_pair_layer_norm_bias = None
    getitem_1489 = native_layer_norm_default_185[0];  native_layer_norm_default_185 = None
    _to_copy_897 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_13_attention_pair_bias_pair_linear_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_13_attention_pair_bias_pair_linear_weight = None
    _to_copy_898 = torch.ops.aten._to_copy.default(getitem_1489, dtype = torch.bfloat16);  getitem_1489 = None
    t_324 = torch.ops.aten.t.default(_to_copy_897);  _to_copy_897 = None
    view_1597 = torch.ops.aten.view.default(_to_copy_898, [147456, 256]);  _to_copy_898 = None
    mm_300 = torch.ops.aten.mm.default(view_1597, t_324);  view_1597 = t_324 = None
    view_1598 = torch.ops.aten.view.default(mm_300, [1, 384, 384, 16]);  mm_300 = None
    permute_905 = torch.ops.aten.permute.default(view_1598, [0, 3, 1, 2]);  view_1598 = None
    view_1599 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384])
    bitwise_not_101 = torch.ops.aten.bitwise_not.default(view_1599);  view_1599 = None
    masked_fill_101 = torch.ops.aten.masked_fill.Scalar(permute_905, bitwise_not_101, -10000);  permute_905 = bitwise_not_101 = None
    _to_copy_899 = torch.ops.aten._to_copy.default(getitem_1486, dtype = torch.bfloat16);  getitem_1486 = None
    _to_copy_900 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_13_attention_pair_bias_attention_input2qkvg_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_13_attention_pair_bias_attention_input2qkvg_weight = None
    unsqueeze_583 = torch.ops.aten.unsqueeze.default(_to_copy_899, 3);  _to_copy_899 = None
    unsqueeze_584 = torch.ops.aten.unsqueeze.default(unsqueeze_583, 4);  unsqueeze_583 = None
    unsqueeze_585 = torch.ops.aten.unsqueeze.default(unsqueeze_584, 5);  unsqueeze_584 = None
    permute_906 = torch.ops.aten.permute.default(unsqueeze_585, [3, 0, 4, 1, 5, 2]);  unsqueeze_585 = None
    unsqueeze_586 = torch.ops.aten.unsqueeze.default(_to_copy_900, 4);  _to_copy_900 = None
    unsqueeze_587 = torch.ops.aten.unsqueeze.default(unsqueeze_586, 5);  unsqueeze_586 = None
    permute_907 = torch.ops.aten.permute.default(unsqueeze_587, [1, 4, 2, 5, 3, 0]);  unsqueeze_587 = None
    permute_908 = torch.ops.aten.permute.default(permute_906, [3, 5, 0, 1, 2, 4]);  permute_906 = None
    view_1600 = torch.ops.aten.view.default(permute_908, [1, 384, 384]);  permute_908 = None
    permute_909 = torch.ops.aten.permute.default(permute_907, [5, 0, 1, 2, 4, 3]);  permute_907 = None
    view_1601 = torch.ops.aten.view.default(permute_909, [1, 384, 1536]);  permute_909 = None
    bmm_140 = torch.ops.aten.bmm.default(view_1600, view_1601);  view_1600 = view_1601 = None
    view_1602 = torch.ops.aten.view.default(bmm_140, [384, 1, 4, 1, 16, 24]);  bmm_140 = None
    permute_910 = torch.ops.aten.permute.default(view_1602, [2, 3, 4, 0, 5, 1]);  view_1602 = None
    view_1603 = torch.ops.aten.view.default(permute_910, [4, 1, 16, 384, 24]);  permute_910 = None
    unbind_int_75 = torch.ops.aten.unbind.int(view_1603);  view_1603 = None
    getitem_1492 = unbind_int_75[0]
    getitem_1493 = unbind_int_75[1]
    getitem_1494 = unbind_int_75[2]
    getitem_1495 = unbind_int_75[3];  unbind_int_75 = None
    view_1604 = torch.ops.aten.view.default(pairformer_stack_blocks_13_attention_pair_bias_attention_query_bias, [1, 16, 1, 24]);  pairformer_stack_blocks_13_attention_pair_bias_attention_query_bias = None
    add_164 = torch.ops.aten.add.Tensor(getitem_1492, view_1604);  getitem_1492 = view_1604 = None
    _to_copy_901 = torch.ops.aten._to_copy.default(add_164, dtype = torch.bfloat16);  add_164 = None
    expand_96 = torch.ops.aten.expand.default(masked_fill_101, [1, 16, 384, 384]);  masked_fill_101 = None
    _scaled_dot_product_efficient_attention_default_53 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_901, getitem_1493, getitem_1494, expand_96, False);  _to_copy_901 = getitem_1493 = getitem_1494 = expand_96 = None
    getitem_1496 = _scaled_dot_product_efficient_attention_default_53[0];  _scaled_dot_product_efficient_attention_default_53 = None
    add_165 = torch.ops.aten.add.Tensor(getitem_1495, 1);  getitem_1495 = None
    sigmoid_119 = torch.ops.aten.sigmoid.default(add_165);  add_165 = None
    mul_195 = torch.ops.aten.mul.Tensor(getitem_1496, sigmoid_119);  getitem_1496 = sigmoid_119 = None
    _to_copy_902 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_13_attention_pair_bias_attention_output_proj_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_13_attention_pair_bias_attention_output_proj_weight = None
    unsqueeze_588 = torch.ops.aten.unsqueeze.default(mul_195, 4);  mul_195 = None
    permute_911 = torch.ops.aten.permute.default(unsqueeze_588, [0, 2, 4, 3, 1]);  unsqueeze_588 = None
    unsqueeze_589 = torch.ops.aten.unsqueeze.default(_to_copy_902, 3);  _to_copy_902 = None
    unsqueeze_590 = torch.ops.aten.unsqueeze.default(unsqueeze_589, 4);  unsqueeze_589 = None
    permute_912 = torch.ops.aten.permute.default(unsqueeze_590, [3, 4, 2, 1, 0]);  unsqueeze_590 = None
    permute_913 = torch.ops.aten.permute.default(permute_911, [1, 3, 4, 0, 2]);  permute_911 = None
    clone_157 = torch.ops.aten.clone.default(permute_913, memory_format = torch.contiguous_format);  permute_913 = None
    _unsafe_view_136 = torch.ops.aten._unsafe_view.default(clone_157, [1, 384, 384]);  clone_157 = None
    permute_914 = torch.ops.aten.permute.default(permute_912, [3, 4, 0, 2, 1]);  permute_912 = None
    clone_158 = torch.ops.aten.clone.default(permute_914, memory_format = torch.contiguous_format);  permute_914 = None
    _unsafe_view_137 = torch.ops.aten._unsafe_view.default(clone_158, [1, 384, 384]);  clone_158 = None
    bmm_141 = torch.ops.aten.bmm.default(_unsafe_view_136, _unsafe_view_137);  _unsafe_view_136 = _unsafe_view_137 = None
    view_1605 = torch.ops.aten.view.default(bmm_141, [384, 1, 1, 1, 384]);  bmm_141 = None
    permute_915 = torch.ops.aten.permute.default(view_1605, [3, 0, 4, 1, 2]);  view_1605 = None
    view_1606 = torch.ops.aten.view.default(permute_915, [1, 384, 384]);  permute_915 = None
    unsqueeze_591 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
    mul_196 = torch.ops.aten.mul.Tensor(view_1606, unsqueeze_591);  view_1606 = unsqueeze_591 = None
    add_166 = torch.ops.aten.add.Tensor(add_159, mul_196);  mul_196 = None
    split_tensor_158 = torch.ops.aten.split.Tensor(add_159, 384, dim = -2);  add_159 = None
    getitem_1500 = split_tensor_158[0];  split_tensor_158 = None
    _to_copy_903 = torch.ops.aten._to_copy.default(getitem_1500, dtype = torch.float32);  getitem_1500 = None
    native_layer_norm_default_186 = torch.ops.aten.native_layer_norm.default(_to_copy_903, [384], pairformer_stack_blocks_13_transition_single_layer_norm_weight, pairformer_stack_blocks_13_transition_single_layer_norm_bias, 1e-05);  _to_copy_903 = pairformer_stack_blocks_13_transition_single_layer_norm_weight = pairformer_stack_blocks_13_transition_single_layer_norm_bias = None
    getitem_1501 = native_layer_norm_default_186[0];  native_layer_norm_default_186 = None
    _to_copy_904 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_13_transition_single_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_13_transition_single_linear_no_bias_ab_weight = None
    _to_copy_905 = torch.ops.aten._to_copy.default(getitem_1501, dtype = torch.bfloat16);  getitem_1501 = None
    t_325 = torch.ops.aten.t.default(_to_copy_904);  _to_copy_904 = None
    view_1607 = torch.ops.aten.view.default(_to_copy_905, [384, 384]);  _to_copy_905 = None
    mm_301 = torch.ops.aten.mm.default(view_1607, t_325);  view_1607 = t_325 = None
    view_1608 = torch.ops.aten.view.default(mm_301, [1, 384, 1536]);  mm_301 = None
    split_tensor_159 = torch.ops.aten.split.Tensor(view_1608, 768, dim = -1);  view_1608 = None
    getitem_1504 = split_tensor_159[0]
    getitem_1505 = split_tensor_159[1];  split_tensor_159 = None
    silu_42 = torch.ops.aten.silu.default(getitem_1504);  getitem_1504 = None
    mul_197 = torch.ops.aten.mul.Tensor(silu_42, getitem_1505);  silu_42 = getitem_1505 = None
    _to_copy_906 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_13_transition_single_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_13_transition_single_linear_out_weight = None
    t_326 = torch.ops.aten.t.default(_to_copy_906);  _to_copy_906 = None
    view_1610 = torch.ops.aten.view.default(mul_197, [384, 768]);  mul_197 = None
    mm_302 = torch.ops.aten.mm.default(view_1610, t_326);  view_1610 = t_326 = None
    view_1611 = torch.ops.aten.view.default(mm_302, [1, 384, 384]);  mm_302 = None
    add_167 = torch.ops.aten.add.Tensor(add_166, view_1611);  add_166 = view_1611 = None
    _to_copy_907 = torch.ops.aten._to_copy.default(add_163, dtype = torch.float32)
    native_layer_norm_default_187 = torch.ops.aten.native_layer_norm.default(_to_copy_907, [256], pairformer_stack_blocks_14_triangle_multiplication_layernorm_z_in_weight, pairformer_stack_blocks_14_triangle_multiplication_layernorm_z_in_bias, 1e-05);  _to_copy_907 = pairformer_stack_blocks_14_triangle_multiplication_layernorm_z_in_weight = pairformer_stack_blocks_14_triangle_multiplication_layernorm_z_in_bias = None
    getitem_1506 = native_layer_norm_default_187[0];  native_layer_norm_default_187 = None
    split_with_sizes_default_40 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_14_triangle_multiplication_merged_linear_p_weight, [512, 512]);  pairformer_stack_blocks_14_triangle_multiplication_merged_linear_p_weight = None
    getitem_1509 = split_with_sizes_default_40[0]
    getitem_1510 = split_with_sizes_default_40[1];  split_with_sizes_default_40 = None
    split_with_sizes_default_41 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_14_triangle_multiplication_merged_linear_g_weight, [512, 512, 256]);  pairformer_stack_blocks_14_triangle_multiplication_merged_linear_g_weight = None
    getitem_1511 = split_with_sizes_default_41[0]
    getitem_1512 = split_with_sizes_default_41[1]
    getitem_1513 = split_with_sizes_default_41[2];  split_with_sizes_default_41 = None
    _to_copy_908 = torch.ops.aten._to_copy.default(getitem_1509, dtype = torch.bfloat16);  getitem_1509 = None
    _to_copy_909 = torch.ops.aten._to_copy.default(getitem_1506, dtype = torch.bfloat16)
    t_327 = torch.ops.aten.t.default(_to_copy_908);  _to_copy_908 = None
    view_1612 = torch.ops.aten.view.default(_to_copy_909, [147456, 256]);  _to_copy_909 = None
    mm_303 = torch.ops.aten.mm.default(view_1612, t_327);  view_1612 = t_327 = None
    view_1613 = torch.ops.aten.view.default(mm_303, [1, 384, 384, 512]);  mm_303 = None
    _to_copy_910 = torch.ops.aten._to_copy.default(getitem_1511, dtype = torch.bfloat16);  getitem_1511 = None
    _to_copy_911 = torch.ops.aten._to_copy.default(getitem_1506, dtype = torch.bfloat16)
    t_328 = torch.ops.aten.t.default(_to_copy_910);  _to_copy_910 = None
    view_1614 = torch.ops.aten.view.default(_to_copy_911, [147456, 256]);  _to_copy_911 = None
    mm_304 = torch.ops.aten.mm.default(view_1614, t_328);  view_1614 = t_328 = None
    view_1615 = torch.ops.aten.view.default(mm_304, [1, 384, 384, 512]);  mm_304 = None
    sigmoid_120 = torch.ops.aten.sigmoid.default(view_1615);  view_1615 = None
    mul_198 = torch.ops.aten.mul.Tensor(view_1613, sigmoid_120);  view_1613 = sigmoid_120 = None
    unsqueeze_592 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
    bitwise_not_102 = torch.ops.aten.bitwise_not.default(unsqueeze_592);  unsqueeze_592 = None
    masked_fill_102 = torch.ops.aten.masked_fill.Scalar(mul_198, bitwise_not_102, 0);  mul_198 = bitwise_not_102 = None
    split_tensor_160 = torch.ops.aten.split.Tensor(masked_fill_102, 256, dim = -1)
    getitem_1516 = split_tensor_160[0];  split_tensor_160 = None
    unsqueeze_595 = torch.ops.aten.unsqueeze.default(getitem_1516, 4);  getitem_1516 = None
    permute_920 = torch.ops.aten.permute.default(unsqueeze_595, [0, 1, 4, 3, 2]);  unsqueeze_595 = None
    permute_921 = torch.ops.aten.permute.default(permute_920, [3, 1, 4, 0, 2]);  permute_920 = None
    view_1618 = torch.ops.aten.view.default(permute_921, [256, 384, 384]);  permute_921 = None
    split_tensor_161 = torch.ops.aten.split.Tensor(masked_fill_102, 256, dim = -1);  masked_fill_102 = None
    getitem_1519 = split_tensor_161[1];  split_tensor_161 = None
    unsqueeze_596 = torch.ops.aten.unsqueeze.default(getitem_1519, 4);  getitem_1519 = None
    permute_922 = torch.ops.aten.permute.default(unsqueeze_596, [0, 4, 1, 3, 2]);  unsqueeze_596 = None
    permute_923 = torch.ops.aten.permute.default(permute_922, [3, 4, 0, 2, 1]);  permute_922 = None
    view_1619 = torch.ops.aten.view.default(permute_923, [256, 384, 384]);  permute_923 = None
    bmm_142 = torch.ops.aten.bmm.default(view_1618, view_1619);  view_1618 = view_1619 = None
    view_1620 = torch.ops.aten.view.default(bmm_142, [256, 384, 1, 1, 384]);  bmm_142 = None
    permute_924 = torch.ops.aten.permute.default(view_1620, [3, 1, 4, 0, 2]);  view_1620 = None
    view_1621 = torch.ops.aten.view.default(permute_924, [1, 384, 384, 256]);  permute_924 = None
    _to_copy_912 = torch.ops.aten._to_copy.default(getitem_1510, dtype = torch.bfloat16);  getitem_1510 = None
    _to_copy_913 = torch.ops.aten._to_copy.default(getitem_1506, dtype = torch.bfloat16)
    t_329 = torch.ops.aten.t.default(_to_copy_912);  _to_copy_912 = None
    view_1622 = torch.ops.aten.view.default(_to_copy_913, [147456, 256]);  _to_copy_913 = None
    mm_305 = torch.ops.aten.mm.default(view_1622, t_329);  view_1622 = t_329 = None
    view_1623 = torch.ops.aten.view.default(mm_305, [1, 384, 384, 512]);  mm_305 = None
    _to_copy_914 = torch.ops.aten._to_copy.default(getitem_1512, dtype = torch.bfloat16);  getitem_1512 = None
    _to_copy_915 = torch.ops.aten._to_copy.default(getitem_1506, dtype = torch.bfloat16)
    t_330 = torch.ops.aten.t.default(_to_copy_914);  _to_copy_914 = None
    view_1624 = torch.ops.aten.view.default(_to_copy_915, [147456, 256]);  _to_copy_915 = None
    mm_306 = torch.ops.aten.mm.default(view_1624, t_330);  view_1624 = t_330 = None
    view_1625 = torch.ops.aten.view.default(mm_306, [1, 384, 384, 512]);  mm_306 = None
    sigmoid_121 = torch.ops.aten.sigmoid.default(view_1625);  view_1625 = None
    mul_199 = torch.ops.aten.mul.Tensor(view_1623, sigmoid_121);  view_1623 = sigmoid_121 = None
    view_1626 = torch.ops.aten.view.default(mul_199, [147456, 512]);  mul_199 = None
    view_1627 = torch.ops.aten.view.default(view_1626, [1, 384, 384, 512]);  view_1626 = None
    transpose_40 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
    unsqueeze_597 = torch.ops.aten.unsqueeze.default(transpose_40, 3);  transpose_40 = None
    clone_159 = torch.ops.aten.clone.default(unsqueeze_597, memory_format = torch.contiguous_format);  unsqueeze_597 = None
    bitwise_not_103 = torch.ops.aten.bitwise_not.default(clone_159);  clone_159 = None
    masked_fill_103 = torch.ops.aten.masked_fill.Scalar(view_1627, bitwise_not_103, 0);  view_1627 = bitwise_not_103 = None
    view_1628 = torch.ops.aten.view.default(masked_fill_103, [147456, 512]);  masked_fill_103 = None
    view_1632 = torch.ops.aten.view.default(view_1628, [1, 384, 384, 512])
    split_tensor_162 = torch.ops.aten.split.Tensor(view_1632, 256, dim = -1);  view_1632 = None
    getitem_1522 = split_tensor_162[0];  split_tensor_162 = None
    unsqueeze_600 = torch.ops.aten.unsqueeze.default(getitem_1522, 4);  getitem_1522 = None
    permute_929 = torch.ops.aten.permute.default(unsqueeze_600, [0, 2, 4, 3, 1]);  unsqueeze_600 = None
    permute_930 = torch.ops.aten.permute.default(permute_929, [3, 1, 4, 0, 2]);  permute_929 = None
    view_1633 = torch.ops.aten.view.default(permute_930, [256, 384, 384]);  permute_930 = None
    view_1634 = torch.ops.aten.view.default(view_1628, [1, 384, 384, 512]);  view_1628 = None
    split_tensor_163 = torch.ops.aten.split.Tensor(view_1634, 256, dim = -1);  view_1634 = None
    getitem_1525 = split_tensor_163[1];  split_tensor_163 = None
    unsqueeze_601 = torch.ops.aten.unsqueeze.default(getitem_1525, 4);  getitem_1525 = None
    permute_931 = torch.ops.aten.permute.default(unsqueeze_601, [0, 4, 2, 3, 1]);  unsqueeze_601 = None
    permute_932 = torch.ops.aten.permute.default(permute_931, [3, 4, 0, 2, 1]);  permute_931 = None
    view_1635 = torch.ops.aten.view.default(permute_932, [256, 384, 384]);  permute_932 = None
    bmm_143 = torch.ops.aten.bmm.default(view_1633, view_1635);  view_1633 = view_1635 = None
    view_1636 = torch.ops.aten.view.default(bmm_143, [256, 384, 1, 1, 384]);  bmm_143 = None
    permute_933 = torch.ops.aten.permute.default(view_1636, [3, 1, 4, 0, 2]);  view_1636 = None
    view_1637 = torch.ops.aten.view.default(permute_933, [1, 384, 384, 256]);  permute_933 = None
    _to_copy_916 = torch.ops.aten._to_copy.default(view_1621, dtype = torch.float32);  view_1621 = None
    native_layer_norm_default_188 = torch.ops.aten.native_layer_norm.default(_to_copy_916, [256], None, None, 1e-05);  _to_copy_916 = None
    getitem_1526 = native_layer_norm_default_188[0];  native_layer_norm_default_188 = None
    _to_copy_917 = torch.ops.aten._to_copy.default(view_1637, dtype = torch.float32);  view_1637 = None
    native_layer_norm_default_189 = torch.ops.aten.native_layer_norm.default(_to_copy_917, [256], None, None, 1e-05);  _to_copy_917 = None
    getitem_1529 = native_layer_norm_default_189[0];  native_layer_norm_default_189 = None
    add_168 = torch.ops.aten.add.Tensor(getitem_1526, getitem_1529);  getitem_1526 = getitem_1529 = None
    _to_copy_918 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_14_triangle_multiplication_linear_z_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_14_triangle_multiplication_linear_z_out_weight = None
    _to_copy_919 = torch.ops.aten._to_copy.default(add_168, dtype = torch.bfloat16);  add_168 = None
    t_331 = torch.ops.aten.t.default(_to_copy_918);  _to_copy_918 = None
    view_1638 = torch.ops.aten.view.default(_to_copy_919, [147456, 256]);  _to_copy_919 = None
    mm_307 = torch.ops.aten.mm.default(view_1638, t_331);  view_1638 = t_331 = None
    view_1639 = torch.ops.aten.view.default(mm_307, [1, 384, 384, 256]);  mm_307 = None
    _to_copy_920 = torch.ops.aten._to_copy.default(getitem_1513, dtype = torch.bfloat16);  getitem_1513 = None
    _to_copy_921 = torch.ops.aten._to_copy.default(getitem_1506, dtype = torch.bfloat16);  getitem_1506 = None
    t_332 = torch.ops.aten.t.default(_to_copy_920);  _to_copy_920 = None
    view_1640 = torch.ops.aten.view.default(_to_copy_921, [147456, 256]);  _to_copy_921 = None
    mm_308 = torch.ops.aten.mm.default(view_1640, t_332);  view_1640 = t_332 = None
    view_1641 = torch.ops.aten.view.default(mm_308, [1, 384, 384, 256]);  mm_308 = None
    sigmoid_122 = torch.ops.aten.sigmoid.default(view_1641);  view_1641 = None
    mul_200 = torch.ops.aten.mul.Tensor(view_1639, sigmoid_122);  view_1639 = sigmoid_122 = None
    add_169 = torch.ops.aten.add.Tensor(add_163, mul_200);  mul_200 = None
    _to_copy_922 = torch.ops.aten._to_copy.default(add_163, dtype = torch.float32)
    native_layer_norm_default_190 = torch.ops.aten.native_layer_norm.default(_to_copy_922, [256], None, None, 1e-05);  _to_copy_922 = None
    getitem_1532 = native_layer_norm_default_190[0];  native_layer_norm_default_190 = None
    _to_copy_923 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_14_triangle_attention_pair2b_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_14_triangle_attention_pair2b_weight = None
    _to_copy_924 = torch.ops.aten._to_copy.default(getitem_1532, dtype = torch.bfloat16)
    t_333 = torch.ops.aten.t.default(_to_copy_923);  _to_copy_923 = None
    view_1642 = torch.ops.aten.view.default(_to_copy_924, [147456, 256]);  _to_copy_924 = None
    mm_309 = torch.ops.aten.mm.default(view_1642, t_333);  view_1642 = t_333 = None
    view_1643 = torch.ops.aten.view.default(mm_309, [1, 384, 384, 8]);  mm_309 = None
    view_1644 = torch.ops.aten.view.default(view_1643, [1, 384, 384, 2, 4]);  view_1643 = None
    permute_934 = torch.ops.aten.permute.default(view_1644, [0, 3, 4, 1, 2]);  view_1644 = None
    view_1645 = torch.ops.aten.view.default(permute_934, [1, 2, 4, 1, 384, 384]);  permute_934 = None
    view_1646 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 384, 384])
    bitwise_not_104 = torch.ops.aten.bitwise_not.default(view_1646);  view_1646 = None
    masked_fill_104 = torch.ops.aten.masked_fill.Scalar(view_1645, bitwise_not_104, -10000);  view_1645 = bitwise_not_104 = None
    view_1647 = torch.ops.aten.view.default(masked_fill_104, [1, 2, 4, 384, 384]);  masked_fill_104 = None
    permute_935 = torch.ops.aten.permute.default(view_1647, [1, 0, 2, 3, 4]);  view_1647 = None
    view_1648 = torch.ops.aten.view.default(permute_935, [2, 4, 1, 384, 384]);  permute_935 = None
    _to_copy_925 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_14_triangle_attention_pair2qkvg1_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_14_triangle_attention_pair2qkvg1_weight = None
    _to_copy_926 = torch.ops.aten._to_copy.default(getitem_1532, dtype = torch.bfloat16)
    t_334 = torch.ops.aten.t.default(_to_copy_925);  _to_copy_925 = None
    view_1649 = torch.ops.aten.view.default(_to_copy_926, [147456, 256]);  _to_copy_926 = None
    mm_310 = torch.ops.aten.mm.default(view_1649, t_334);  view_1649 = t_334 = None
    view_1650 = torch.ops.aten.view.default(mm_310, [1, 384, 384, 1024]);  mm_310 = None
    select_41 = torch.ops.aten.select.int(view_1648, 0, 0)
    view_1651 = torch.ops.aten.view.default(view_1650, [1, 384, 384, 4, 4, 64]);  view_1650 = None
    permute_936 = torch.ops.aten.permute.default(view_1651, [4, 0, 3, 1, 2, 5]);  view_1651 = None
    view_1652 = torch.ops.aten.view.default(permute_936, [4, 4, 384, 384, 64]);  permute_936 = None
    unbind_int_76 = torch.ops.aten.unbind.int(view_1652);  view_1652 = None
    getitem_1535 = unbind_int_76[0]
    getitem_1536 = unbind_int_76[1]
    getitem_1537 = unbind_int_76[2]
    getitem_1538 = unbind_int_76[3];  unbind_int_76 = None
    expand_97 = torch.ops.aten.expand.default(select_41, [4, 384, 384, 384]);  select_41 = None
    _scaled_dot_product_efficient_attention_default_54 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1535, getitem_1536, getitem_1537, expand_97, False);  getitem_1535 = getitem_1536 = getitem_1537 = expand_97 = None
    getitem_1539 = _scaled_dot_product_efficient_attention_default_54[0];  _scaled_dot_product_efficient_attention_default_54 = None
    sigmoid_123 = torch.ops.aten.sigmoid.default(getitem_1538);  getitem_1538 = None
    mul_201 = torch.ops.aten.mul.Tensor(getitem_1539, sigmoid_123);  getitem_1539 = sigmoid_123 = None
    view_1653 = torch.ops.aten.view.default(mul_201, [1, 4, 384, 384, 64]);  mul_201 = None
    permute_937 = torch.ops.aten.permute.default(view_1653, [0, 2, 3, 1, 4]);  view_1653 = None
    clone_160 = torch.ops.aten.clone.default(permute_937, memory_format = torch.contiguous_format);  permute_937 = None
    _unsafe_view_138 = torch.ops.aten._unsafe_view.default(clone_160, [1, 384, 384, 256]);  clone_160 = None
    transpose_41 = torch.ops.aten.transpose.int(getitem_1532, 1, 2);  getitem_1532 = None
    _to_copy_927 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_14_triangle_attention_pair2qkvg2_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_14_triangle_attention_pair2qkvg2_weight = None
    _to_copy_928 = torch.ops.aten._to_copy.default(transpose_41, dtype = torch.bfloat16);  transpose_41 = None
    t_335 = torch.ops.aten.t.default(_to_copy_927);  _to_copy_927 = None
    expand_98 = torch.ops.aten.expand.default(_to_copy_928, [1, 384, 384, 256]);  _to_copy_928 = None
    view_1654 = torch.ops.aten.view.default(expand_98, [384, 384, 256]);  expand_98 = None
    expand_99 = torch.ops.aten.expand.default(t_335, [1, 384, 256, 1024]);  t_335 = None
    view_1655 = torch.ops.aten.view.default(expand_99, [384, 256, 1024]);  expand_99 = None
    bmm_144 = torch.ops.aten.bmm.default(view_1654, view_1655);  view_1654 = view_1655 = None
    view_1656 = torch.ops.aten.view.default(bmm_144, [1, 384, 384, 1024]);  bmm_144 = None
    select_42 = torch.ops.aten.select.int(view_1648, 0, 1);  view_1648 = None
    view_1657 = torch.ops.aten.view.default(view_1656, [1, 384, 384, 4, 4, 64]);  view_1656 = None
    permute_938 = torch.ops.aten.permute.default(view_1657, [4, 0, 3, 1, 2, 5]);  view_1657 = None
    view_1658 = torch.ops.aten.view.default(permute_938, [4, 4, 384, 384, 64]);  permute_938 = None
    unbind_int_77 = torch.ops.aten.unbind.int(view_1658);  view_1658 = None
    getitem_1543 = unbind_int_77[0]
    getitem_1544 = unbind_int_77[1]
    getitem_1545 = unbind_int_77[2]
    getitem_1546 = unbind_int_77[3];  unbind_int_77 = None
    expand_100 = torch.ops.aten.expand.default(select_42, [4, 384, 384, 384]);  select_42 = None
    _scaled_dot_product_efficient_attention_default_55 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1543, getitem_1544, getitem_1545, expand_100, False);  getitem_1543 = getitem_1544 = getitem_1545 = expand_100 = None
    getitem_1547 = _scaled_dot_product_efficient_attention_default_55[0];  _scaled_dot_product_efficient_attention_default_55 = None
    sigmoid_124 = torch.ops.aten.sigmoid.default(getitem_1546);  getitem_1546 = None
    mul_202 = torch.ops.aten.mul.Tensor(getitem_1547, sigmoid_124);  getitem_1547 = sigmoid_124 = None
    view_1659 = torch.ops.aten.view.default(mul_202, [1, 4, 384, 384, 64]);  mul_202 = None
    permute_939 = torch.ops.aten.permute.default(view_1659, [0, 2, 3, 1, 4]);  view_1659 = None
    clone_161 = torch.ops.aten.clone.default(permute_939, memory_format = torch.contiguous_format);  permute_939 = None
    _unsafe_view_139 = torch.ops.aten._unsafe_view.default(clone_161, [1, 384, 384, 256]);  clone_161 = None
    cat_26 = torch.ops.aten.cat.default([_unsafe_view_138, _unsafe_view_139], dim = -1);  _unsafe_view_138 = _unsafe_view_139 = None
    slice_177 = torch.ops.aten.slice.Tensor(pairformer_stack_blocks_14_triangle_attention_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  pairformer_stack_blocks_14_triangle_attention_out_scalers = None
    unsqueeze_602 = torch.ops.aten.unsqueeze.default(slice_177, 1);  slice_177 = None
    mul_203 = torch.ops.aten.mul.Tensor(pairformer_stack_blocks_14_triangle_attention_linear_out_weight, unsqueeze_602);  pairformer_stack_blocks_14_triangle_attention_linear_out_weight = unsqueeze_602 = None
    _to_copy_929 = torch.ops.aten._to_copy.default(mul_203, dtype = torch.bfloat16);  mul_203 = None
    t_336 = torch.ops.aten.t.default(_to_copy_929);  _to_copy_929 = None
    view_1660 = torch.ops.aten.view.default(cat_26, [147456, 512]);  cat_26 = None
    mm_311 = torch.ops.aten.mm.default(view_1660, t_336);  view_1660 = t_336 = None
    view_1661 = torch.ops.aten.view.default(mm_311, [1, 384, 384, 256]);  mm_311 = None
    add_170 = torch.ops.aten.add.Tensor(add_169, view_1661);  add_169 = view_1661 = None
    split_tensor_164 = torch.ops.aten.split.Tensor(add_163, 384, dim = -2)
    getitem_1551 = split_tensor_164[0];  split_tensor_164 = None
    _to_copy_930 = torch.ops.aten._to_copy.default(getitem_1551, dtype = torch.float32);  getitem_1551 = None
    native_layer_norm_default_191 = torch.ops.aten.native_layer_norm.default(_to_copy_930, [256], pairformer_stack_blocks_14_transition_pair_layer_norm_weight, pairformer_stack_blocks_14_transition_pair_layer_norm_bias, 1e-05);  _to_copy_930 = pairformer_stack_blocks_14_transition_pair_layer_norm_weight = pairformer_stack_blocks_14_transition_pair_layer_norm_bias = None
    getitem_1552 = native_layer_norm_default_191[0];  native_layer_norm_default_191 = None
    _to_copy_931 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_14_transition_pair_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_14_transition_pair_linear_no_bias_ab_weight = None
    _to_copy_932 = torch.ops.aten._to_copy.default(getitem_1552, dtype = torch.bfloat16);  getitem_1552 = None
    t_337 = torch.ops.aten.t.default(_to_copy_931);  _to_copy_931 = None
    view_1662 = torch.ops.aten.view.default(_to_copy_932, [147456, 256]);  _to_copy_932 = None
    mm_312 = torch.ops.aten.mm.default(view_1662, t_337);  view_1662 = t_337 = None
    view_1663 = torch.ops.aten.view.default(mm_312, [1, 384, 384, 1024]);  mm_312 = None
    split_tensor_165 = torch.ops.aten.split.Tensor(view_1663, 512, dim = -1);  view_1663 = None
    getitem_1555 = split_tensor_165[0]
    getitem_1556 = split_tensor_165[1];  split_tensor_165 = None
    silu_43 = torch.ops.aten.silu.default(getitem_1555);  getitem_1555 = None
    mul_204 = torch.ops.aten.mul.Tensor(silu_43, getitem_1556);  silu_43 = getitem_1556 = None
    _to_copy_933 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_14_transition_pair_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_14_transition_pair_linear_out_weight = None
    t_338 = torch.ops.aten.t.default(_to_copy_933);  _to_copy_933 = None
    view_1665 = torch.ops.aten.view.default(mul_204, [147456, 512]);  mul_204 = None
    mm_313 = torch.ops.aten.mm.default(view_1665, t_338);  view_1665 = t_338 = None
    view_1666 = torch.ops.aten.view.default(mm_313, [1, 384, 384, 256]);  mm_313 = None
    add_171 = torch.ops.aten.add.Tensor(add_170, view_1666);  add_170 = view_1666 = None
    _to_copy_934 = torch.ops.aten._to_copy.default(add_167, dtype = torch.float32)
    native_layer_norm_default_192 = torch.ops.aten.native_layer_norm.default(_to_copy_934, [384], pairformer_stack_blocks_14_attention_pair_bias_single_layer_norm_weight, pairformer_stack_blocks_14_attention_pair_bias_single_layer_norm_bias, 1e-05);  _to_copy_934 = pairformer_stack_blocks_14_attention_pair_bias_single_layer_norm_weight = pairformer_stack_blocks_14_attention_pair_bias_single_layer_norm_bias = None
    getitem_1557 = native_layer_norm_default_192[0];  native_layer_norm_default_192 = None
    _to_copy_935 = torch.ops.aten._to_copy.default(add_163, dtype = torch.float32);  add_163 = None
    native_layer_norm_default_193 = torch.ops.aten.native_layer_norm.default(_to_copy_935, [256], pairformer_stack_blocks_14_attention_pair_bias_pair_layer_norm_weight, pairformer_stack_blocks_14_attention_pair_bias_pair_layer_norm_bias, 1e-05);  _to_copy_935 = pairformer_stack_blocks_14_attention_pair_bias_pair_layer_norm_weight = pairformer_stack_blocks_14_attention_pair_bias_pair_layer_norm_bias = None
    getitem_1560 = native_layer_norm_default_193[0];  native_layer_norm_default_193 = None
    _to_copy_936 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_14_attention_pair_bias_pair_linear_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_14_attention_pair_bias_pair_linear_weight = None
    _to_copy_937 = torch.ops.aten._to_copy.default(getitem_1560, dtype = torch.bfloat16);  getitem_1560 = None
    t_339 = torch.ops.aten.t.default(_to_copy_936);  _to_copy_936 = None
    view_1667 = torch.ops.aten.view.default(_to_copy_937, [147456, 256]);  _to_copy_937 = None
    mm_314 = torch.ops.aten.mm.default(view_1667, t_339);  view_1667 = t_339 = None
    view_1668 = torch.ops.aten.view.default(mm_314, [1, 384, 384, 16]);  mm_314 = None
    permute_940 = torch.ops.aten.permute.default(view_1668, [0, 3, 1, 2]);  view_1668 = None
    view_1669 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384])
    bitwise_not_105 = torch.ops.aten.bitwise_not.default(view_1669);  view_1669 = None
    masked_fill_105 = torch.ops.aten.masked_fill.Scalar(permute_940, bitwise_not_105, -10000);  permute_940 = bitwise_not_105 = None
    _to_copy_938 = torch.ops.aten._to_copy.default(getitem_1557, dtype = torch.bfloat16);  getitem_1557 = None
    _to_copy_939 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_14_attention_pair_bias_attention_input2qkvg_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_14_attention_pair_bias_attention_input2qkvg_weight = None
    unsqueeze_603 = torch.ops.aten.unsqueeze.default(_to_copy_938, 3);  _to_copy_938 = None
    unsqueeze_604 = torch.ops.aten.unsqueeze.default(unsqueeze_603, 4);  unsqueeze_603 = None
    unsqueeze_605 = torch.ops.aten.unsqueeze.default(unsqueeze_604, 5);  unsqueeze_604 = None
    permute_941 = torch.ops.aten.permute.default(unsqueeze_605, [3, 0, 4, 1, 5, 2]);  unsqueeze_605 = None
    unsqueeze_606 = torch.ops.aten.unsqueeze.default(_to_copy_939, 4);  _to_copy_939 = None
    unsqueeze_607 = torch.ops.aten.unsqueeze.default(unsqueeze_606, 5);  unsqueeze_606 = None
    permute_942 = torch.ops.aten.permute.default(unsqueeze_607, [1, 4, 2, 5, 3, 0]);  unsqueeze_607 = None
    permute_943 = torch.ops.aten.permute.default(permute_941, [3, 5, 0, 1, 2, 4]);  permute_941 = None
    view_1670 = torch.ops.aten.view.default(permute_943, [1, 384, 384]);  permute_943 = None
    permute_944 = torch.ops.aten.permute.default(permute_942, [5, 0, 1, 2, 4, 3]);  permute_942 = None
    view_1671 = torch.ops.aten.view.default(permute_944, [1, 384, 1536]);  permute_944 = None
    bmm_145 = torch.ops.aten.bmm.default(view_1670, view_1671);  view_1670 = view_1671 = None
    view_1672 = torch.ops.aten.view.default(bmm_145, [384, 1, 4, 1, 16, 24]);  bmm_145 = None
    permute_945 = torch.ops.aten.permute.default(view_1672, [2, 3, 4, 0, 5, 1]);  view_1672 = None
    view_1673 = torch.ops.aten.view.default(permute_945, [4, 1, 16, 384, 24]);  permute_945 = None
    unbind_int_78 = torch.ops.aten.unbind.int(view_1673);  view_1673 = None
    getitem_1563 = unbind_int_78[0]
    getitem_1564 = unbind_int_78[1]
    getitem_1565 = unbind_int_78[2]
    getitem_1566 = unbind_int_78[3];  unbind_int_78 = None
    view_1674 = torch.ops.aten.view.default(pairformer_stack_blocks_14_attention_pair_bias_attention_query_bias, [1, 16, 1, 24]);  pairformer_stack_blocks_14_attention_pair_bias_attention_query_bias = None
    add_172 = torch.ops.aten.add.Tensor(getitem_1563, view_1674);  getitem_1563 = view_1674 = None
    _to_copy_940 = torch.ops.aten._to_copy.default(add_172, dtype = torch.bfloat16);  add_172 = None
    expand_101 = torch.ops.aten.expand.default(masked_fill_105, [1, 16, 384, 384]);  masked_fill_105 = None
    _scaled_dot_product_efficient_attention_default_56 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_940, getitem_1564, getitem_1565, expand_101, False);  _to_copy_940 = getitem_1564 = getitem_1565 = expand_101 = None
    getitem_1567 = _scaled_dot_product_efficient_attention_default_56[0];  _scaled_dot_product_efficient_attention_default_56 = None
    add_173 = torch.ops.aten.add.Tensor(getitem_1566, 1);  getitem_1566 = None
    sigmoid_125 = torch.ops.aten.sigmoid.default(add_173);  add_173 = None
    mul_205 = torch.ops.aten.mul.Tensor(getitem_1567, sigmoid_125);  getitem_1567 = sigmoid_125 = None
    _to_copy_941 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_14_attention_pair_bias_attention_output_proj_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_14_attention_pair_bias_attention_output_proj_weight = None
    unsqueeze_608 = torch.ops.aten.unsqueeze.default(mul_205, 4);  mul_205 = None
    permute_946 = torch.ops.aten.permute.default(unsqueeze_608, [0, 2, 4, 3, 1]);  unsqueeze_608 = None
    unsqueeze_609 = torch.ops.aten.unsqueeze.default(_to_copy_941, 3);  _to_copy_941 = None
    unsqueeze_610 = torch.ops.aten.unsqueeze.default(unsqueeze_609, 4);  unsqueeze_609 = None
    permute_947 = torch.ops.aten.permute.default(unsqueeze_610, [3, 4, 2, 1, 0]);  unsqueeze_610 = None
    permute_948 = torch.ops.aten.permute.default(permute_946, [1, 3, 4, 0, 2]);  permute_946 = None
    clone_162 = torch.ops.aten.clone.default(permute_948, memory_format = torch.contiguous_format);  permute_948 = None
    _unsafe_view_140 = torch.ops.aten._unsafe_view.default(clone_162, [1, 384, 384]);  clone_162 = None
    permute_949 = torch.ops.aten.permute.default(permute_947, [3, 4, 0, 2, 1]);  permute_947 = None
    clone_163 = torch.ops.aten.clone.default(permute_949, memory_format = torch.contiguous_format);  permute_949 = None
    _unsafe_view_141 = torch.ops.aten._unsafe_view.default(clone_163, [1, 384, 384]);  clone_163 = None
    bmm_146 = torch.ops.aten.bmm.default(_unsafe_view_140, _unsafe_view_141);  _unsafe_view_140 = _unsafe_view_141 = None
    view_1675 = torch.ops.aten.view.default(bmm_146, [384, 1, 1, 1, 384]);  bmm_146 = None
    permute_950 = torch.ops.aten.permute.default(view_1675, [3, 0, 4, 1, 2]);  view_1675 = None
    view_1676 = torch.ops.aten.view.default(permute_950, [1, 384, 384]);  permute_950 = None
    unsqueeze_611 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
    mul_206 = torch.ops.aten.mul.Tensor(view_1676, unsqueeze_611);  view_1676 = unsqueeze_611 = None
    add_174 = torch.ops.aten.add.Tensor(add_167, mul_206);  mul_206 = None
    split_tensor_166 = torch.ops.aten.split.Tensor(add_167, 384, dim = -2);  add_167 = None
    getitem_1571 = split_tensor_166[0];  split_tensor_166 = None
    _to_copy_942 = torch.ops.aten._to_copy.default(getitem_1571, dtype = torch.float32);  getitem_1571 = None
    native_layer_norm_default_194 = torch.ops.aten.native_layer_norm.default(_to_copy_942, [384], pairformer_stack_blocks_14_transition_single_layer_norm_weight, pairformer_stack_blocks_14_transition_single_layer_norm_bias, 1e-05);  _to_copy_942 = pairformer_stack_blocks_14_transition_single_layer_norm_weight = pairformer_stack_blocks_14_transition_single_layer_norm_bias = None
    getitem_1572 = native_layer_norm_default_194[0];  native_layer_norm_default_194 = None
    _to_copy_943 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_14_transition_single_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_14_transition_single_linear_no_bias_ab_weight = None
    _to_copy_944 = torch.ops.aten._to_copy.default(getitem_1572, dtype = torch.bfloat16);  getitem_1572 = None
    t_340 = torch.ops.aten.t.default(_to_copy_943);  _to_copy_943 = None
    view_1677 = torch.ops.aten.view.default(_to_copy_944, [384, 384]);  _to_copy_944 = None
    mm_315 = torch.ops.aten.mm.default(view_1677, t_340);  view_1677 = t_340 = None
    view_1678 = torch.ops.aten.view.default(mm_315, [1, 384, 1536]);  mm_315 = None
    split_tensor_167 = torch.ops.aten.split.Tensor(view_1678, 768, dim = -1);  view_1678 = None
    getitem_1575 = split_tensor_167[0]
    getitem_1576 = split_tensor_167[1];  split_tensor_167 = None
    silu_44 = torch.ops.aten.silu.default(getitem_1575);  getitem_1575 = None
    mul_207 = torch.ops.aten.mul.Tensor(silu_44, getitem_1576);  silu_44 = getitem_1576 = None
    _to_copy_945 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_14_transition_single_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_14_transition_single_linear_out_weight = None
    t_341 = torch.ops.aten.t.default(_to_copy_945);  _to_copy_945 = None
    view_1680 = torch.ops.aten.view.default(mul_207, [384, 768]);  mul_207 = None
    mm_316 = torch.ops.aten.mm.default(view_1680, t_341);  view_1680 = t_341 = None
    view_1681 = torch.ops.aten.view.default(mm_316, [1, 384, 384]);  mm_316 = None
    add_175 = torch.ops.aten.add.Tensor(add_174, view_1681);  add_174 = view_1681 = None
    _to_copy_946 = torch.ops.aten._to_copy.default(add_171, dtype = torch.float32)
    native_layer_norm_default_195 = torch.ops.aten.native_layer_norm.default(_to_copy_946, [256], pairformer_stack_blocks_15_triangle_multiplication_layernorm_z_in_weight, pairformer_stack_blocks_15_triangle_multiplication_layernorm_z_in_bias, 1e-05);  _to_copy_946 = pairformer_stack_blocks_15_triangle_multiplication_layernorm_z_in_weight = pairformer_stack_blocks_15_triangle_multiplication_layernorm_z_in_bias = None
    getitem_1577 = native_layer_norm_default_195[0];  native_layer_norm_default_195 = None
    split_with_sizes_default_42 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_15_triangle_multiplication_merged_linear_p_weight, [512, 512]);  pairformer_stack_blocks_15_triangle_multiplication_merged_linear_p_weight = None
    getitem_1580 = split_with_sizes_default_42[0]
    getitem_1581 = split_with_sizes_default_42[1];  split_with_sizes_default_42 = None
    split_with_sizes_default_43 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_15_triangle_multiplication_merged_linear_g_weight, [512, 512, 256]);  pairformer_stack_blocks_15_triangle_multiplication_merged_linear_g_weight = None
    getitem_1582 = split_with_sizes_default_43[0]
    getitem_1583 = split_with_sizes_default_43[1]
    getitem_1584 = split_with_sizes_default_43[2];  split_with_sizes_default_43 = None
    _to_copy_947 = torch.ops.aten._to_copy.default(getitem_1580, dtype = torch.bfloat16);  getitem_1580 = None
    _to_copy_948 = torch.ops.aten._to_copy.default(getitem_1577, dtype = torch.bfloat16)
    t_342 = torch.ops.aten.t.default(_to_copy_947);  _to_copy_947 = None
    view_1682 = torch.ops.aten.view.default(_to_copy_948, [147456, 256]);  _to_copy_948 = None
    mm_317 = torch.ops.aten.mm.default(view_1682, t_342);  view_1682 = t_342 = None
    view_1683 = torch.ops.aten.view.default(mm_317, [1, 384, 384, 512]);  mm_317 = None
    _to_copy_949 = torch.ops.aten._to_copy.default(getitem_1582, dtype = torch.bfloat16);  getitem_1582 = None
    _to_copy_950 = torch.ops.aten._to_copy.default(getitem_1577, dtype = torch.bfloat16)
    t_343 = torch.ops.aten.t.default(_to_copy_949);  _to_copy_949 = None
    view_1684 = torch.ops.aten.view.default(_to_copy_950, [147456, 256]);  _to_copy_950 = None
    mm_318 = torch.ops.aten.mm.default(view_1684, t_343);  view_1684 = t_343 = None
    view_1685 = torch.ops.aten.view.default(mm_318, [1, 384, 384, 512]);  mm_318 = None
    sigmoid_126 = torch.ops.aten.sigmoid.default(view_1685);  view_1685 = None
    mul_208 = torch.ops.aten.mul.Tensor(view_1683, sigmoid_126);  view_1683 = sigmoid_126 = None
    unsqueeze_612 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
    bitwise_not_106 = torch.ops.aten.bitwise_not.default(unsqueeze_612);  unsqueeze_612 = None
    masked_fill_106 = torch.ops.aten.masked_fill.Scalar(mul_208, bitwise_not_106, 0);  mul_208 = bitwise_not_106 = None
    split_tensor_168 = torch.ops.aten.split.Tensor(masked_fill_106, 256, dim = -1)
    getitem_1587 = split_tensor_168[0];  split_tensor_168 = None
    unsqueeze_615 = torch.ops.aten.unsqueeze.default(getitem_1587, 4);  getitem_1587 = None
    permute_955 = torch.ops.aten.permute.default(unsqueeze_615, [0, 1, 4, 3, 2]);  unsqueeze_615 = None
    permute_956 = torch.ops.aten.permute.default(permute_955, [3, 1, 4, 0, 2]);  permute_955 = None
    view_1688 = torch.ops.aten.view.default(permute_956, [256, 384, 384]);  permute_956 = None
    split_tensor_169 = torch.ops.aten.split.Tensor(masked_fill_106, 256, dim = -1);  masked_fill_106 = None
    getitem_1590 = split_tensor_169[1];  split_tensor_169 = None
    unsqueeze_616 = torch.ops.aten.unsqueeze.default(getitem_1590, 4);  getitem_1590 = None
    permute_957 = torch.ops.aten.permute.default(unsqueeze_616, [0, 4, 1, 3, 2]);  unsqueeze_616 = None
    permute_958 = torch.ops.aten.permute.default(permute_957, [3, 4, 0, 2, 1]);  permute_957 = None
    view_1689 = torch.ops.aten.view.default(permute_958, [256, 384, 384]);  permute_958 = None
    bmm_147 = torch.ops.aten.bmm.default(view_1688, view_1689);  view_1688 = view_1689 = None
    view_1690 = torch.ops.aten.view.default(bmm_147, [256, 384, 1, 1, 384]);  bmm_147 = None
    permute_959 = torch.ops.aten.permute.default(view_1690, [3, 1, 4, 0, 2]);  view_1690 = None
    view_1691 = torch.ops.aten.view.default(permute_959, [1, 384, 384, 256]);  permute_959 = None
    _to_copy_951 = torch.ops.aten._to_copy.default(getitem_1581, dtype = torch.bfloat16);  getitem_1581 = None
    _to_copy_952 = torch.ops.aten._to_copy.default(getitem_1577, dtype = torch.bfloat16)
    t_344 = torch.ops.aten.t.default(_to_copy_951);  _to_copy_951 = None
    view_1692 = torch.ops.aten.view.default(_to_copy_952, [147456, 256]);  _to_copy_952 = None
    mm_319 = torch.ops.aten.mm.default(view_1692, t_344);  view_1692 = t_344 = None
    view_1693 = torch.ops.aten.view.default(mm_319, [1, 384, 384, 512]);  mm_319 = None
    _to_copy_953 = torch.ops.aten._to_copy.default(getitem_1583, dtype = torch.bfloat16);  getitem_1583 = None
    _to_copy_954 = torch.ops.aten._to_copy.default(getitem_1577, dtype = torch.bfloat16)
    t_345 = torch.ops.aten.t.default(_to_copy_953);  _to_copy_953 = None
    view_1694 = torch.ops.aten.view.default(_to_copy_954, [147456, 256]);  _to_copy_954 = None
    mm_320 = torch.ops.aten.mm.default(view_1694, t_345);  view_1694 = t_345 = None
    view_1695 = torch.ops.aten.view.default(mm_320, [1, 384, 384, 512]);  mm_320 = None
    sigmoid_127 = torch.ops.aten.sigmoid.default(view_1695);  view_1695 = None
    mul_209 = torch.ops.aten.mul.Tensor(view_1693, sigmoid_127);  view_1693 = sigmoid_127 = None
    view_1696 = torch.ops.aten.view.default(mul_209, [147456, 512]);  mul_209 = None
    view_1697 = torch.ops.aten.view.default(view_1696, [1, 384, 384, 512]);  view_1696 = None
    transpose_42 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
    unsqueeze_617 = torch.ops.aten.unsqueeze.default(transpose_42, 3);  transpose_42 = None
    clone_164 = torch.ops.aten.clone.default(unsqueeze_617, memory_format = torch.contiguous_format);  unsqueeze_617 = None
    bitwise_not_107 = torch.ops.aten.bitwise_not.default(clone_164);  clone_164 = None
    masked_fill_107 = torch.ops.aten.masked_fill.Scalar(view_1697, bitwise_not_107, 0);  view_1697 = bitwise_not_107 = None
    view_1698 = torch.ops.aten.view.default(masked_fill_107, [147456, 512]);  masked_fill_107 = None
    view_1702 = torch.ops.aten.view.default(view_1698, [1, 384, 384, 512])
    split_tensor_170 = torch.ops.aten.split.Tensor(view_1702, 256, dim = -1);  view_1702 = None
    getitem_1593 = split_tensor_170[0];  split_tensor_170 = None
    unsqueeze_620 = torch.ops.aten.unsqueeze.default(getitem_1593, 4);  getitem_1593 = None
    permute_964 = torch.ops.aten.permute.default(unsqueeze_620, [0, 2, 4, 3, 1]);  unsqueeze_620 = None
    permute_965 = torch.ops.aten.permute.default(permute_964, [3, 1, 4, 0, 2]);  permute_964 = None
    view_1703 = torch.ops.aten.view.default(permute_965, [256, 384, 384]);  permute_965 = None
    view_1704 = torch.ops.aten.view.default(view_1698, [1, 384, 384, 512]);  view_1698 = None
    split_tensor_171 = torch.ops.aten.split.Tensor(view_1704, 256, dim = -1);  view_1704 = None
    getitem_1596 = split_tensor_171[1];  split_tensor_171 = None
    unsqueeze_621 = torch.ops.aten.unsqueeze.default(getitem_1596, 4);  getitem_1596 = None
    permute_966 = torch.ops.aten.permute.default(unsqueeze_621, [0, 4, 2, 3, 1]);  unsqueeze_621 = None
    permute_967 = torch.ops.aten.permute.default(permute_966, [3, 4, 0, 2, 1]);  permute_966 = None
    view_1705 = torch.ops.aten.view.default(permute_967, [256, 384, 384]);  permute_967 = None
    bmm_148 = torch.ops.aten.bmm.default(view_1703, view_1705);  view_1703 = view_1705 = None
    view_1706 = torch.ops.aten.view.default(bmm_148, [256, 384, 1, 1, 384]);  bmm_148 = None
    permute_968 = torch.ops.aten.permute.default(view_1706, [3, 1, 4, 0, 2]);  view_1706 = None
    view_1707 = torch.ops.aten.view.default(permute_968, [1, 384, 384, 256]);  permute_968 = None
    _to_copy_955 = torch.ops.aten._to_copy.default(view_1691, dtype = torch.float32);  view_1691 = None
    native_layer_norm_default_196 = torch.ops.aten.native_layer_norm.default(_to_copy_955, [256], None, None, 1e-05);  _to_copy_955 = None
    getitem_1597 = native_layer_norm_default_196[0];  native_layer_norm_default_196 = None
    _to_copy_956 = torch.ops.aten._to_copy.default(view_1707, dtype = torch.float32);  view_1707 = None
    native_layer_norm_default_197 = torch.ops.aten.native_layer_norm.default(_to_copy_956, [256], None, None, 1e-05);  _to_copy_956 = None
    getitem_1600 = native_layer_norm_default_197[0];  native_layer_norm_default_197 = None
    add_176 = torch.ops.aten.add.Tensor(getitem_1597, getitem_1600);  getitem_1597 = getitem_1600 = None
    _to_copy_957 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_15_triangle_multiplication_linear_z_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_15_triangle_multiplication_linear_z_out_weight = None
    _to_copy_958 = torch.ops.aten._to_copy.default(add_176, dtype = torch.bfloat16);  add_176 = None
    t_346 = torch.ops.aten.t.default(_to_copy_957);  _to_copy_957 = None
    view_1708 = torch.ops.aten.view.default(_to_copy_958, [147456, 256]);  _to_copy_958 = None
    mm_321 = torch.ops.aten.mm.default(view_1708, t_346);  view_1708 = t_346 = None
    view_1709 = torch.ops.aten.view.default(mm_321, [1, 384, 384, 256]);  mm_321 = None
    _to_copy_959 = torch.ops.aten._to_copy.default(getitem_1584, dtype = torch.bfloat16);  getitem_1584 = None
    _to_copy_960 = torch.ops.aten._to_copy.default(getitem_1577, dtype = torch.bfloat16);  getitem_1577 = None
    t_347 = torch.ops.aten.t.default(_to_copy_959);  _to_copy_959 = None
    view_1710 = torch.ops.aten.view.default(_to_copy_960, [147456, 256]);  _to_copy_960 = None
    mm_322 = torch.ops.aten.mm.default(view_1710, t_347);  view_1710 = t_347 = None
    view_1711 = torch.ops.aten.view.default(mm_322, [1, 384, 384, 256]);  mm_322 = None
    sigmoid_128 = torch.ops.aten.sigmoid.default(view_1711);  view_1711 = None
    mul_210 = torch.ops.aten.mul.Tensor(view_1709, sigmoid_128);  view_1709 = sigmoid_128 = None
    add_177 = torch.ops.aten.add.Tensor(add_171, mul_210);  mul_210 = None
    _to_copy_961 = torch.ops.aten._to_copy.default(add_171, dtype = torch.float32)
    native_layer_norm_default_198 = torch.ops.aten.native_layer_norm.default(_to_copy_961, [256], None, None, 1e-05);  _to_copy_961 = None
    getitem_1603 = native_layer_norm_default_198[0];  native_layer_norm_default_198 = None
    _to_copy_962 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_15_triangle_attention_pair2b_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_15_triangle_attention_pair2b_weight = None
    _to_copy_963 = torch.ops.aten._to_copy.default(getitem_1603, dtype = torch.bfloat16)
    t_348 = torch.ops.aten.t.default(_to_copy_962);  _to_copy_962 = None
    view_1712 = torch.ops.aten.view.default(_to_copy_963, [147456, 256]);  _to_copy_963 = None
    mm_323 = torch.ops.aten.mm.default(view_1712, t_348);  view_1712 = t_348 = None
    view_1713 = torch.ops.aten.view.default(mm_323, [1, 384, 384, 8]);  mm_323 = None
    view_1714 = torch.ops.aten.view.default(view_1713, [1, 384, 384, 2, 4]);  view_1713 = None
    permute_969 = torch.ops.aten.permute.default(view_1714, [0, 3, 4, 1, 2]);  view_1714 = None
    view_1715 = torch.ops.aten.view.default(permute_969, [1, 2, 4, 1, 384, 384]);  permute_969 = None
    view_1716 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 384, 384])
    bitwise_not_108 = torch.ops.aten.bitwise_not.default(view_1716);  view_1716 = None
    masked_fill_108 = torch.ops.aten.masked_fill.Scalar(view_1715, bitwise_not_108, -10000);  view_1715 = bitwise_not_108 = None
    view_1717 = torch.ops.aten.view.default(masked_fill_108, [1, 2, 4, 384, 384]);  masked_fill_108 = None
    permute_970 = torch.ops.aten.permute.default(view_1717, [1, 0, 2, 3, 4]);  view_1717 = None
    view_1718 = torch.ops.aten.view.default(permute_970, [2, 4, 1, 384, 384]);  permute_970 = None
    _to_copy_964 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_15_triangle_attention_pair2qkvg1_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_15_triangle_attention_pair2qkvg1_weight = None
    _to_copy_965 = torch.ops.aten._to_copy.default(getitem_1603, dtype = torch.bfloat16)
    t_349 = torch.ops.aten.t.default(_to_copy_964);  _to_copy_964 = None
    view_1719 = torch.ops.aten.view.default(_to_copy_965, [147456, 256]);  _to_copy_965 = None
    mm_324 = torch.ops.aten.mm.default(view_1719, t_349);  view_1719 = t_349 = None
    view_1720 = torch.ops.aten.view.default(mm_324, [1, 384, 384, 1024]);  mm_324 = None
    select_43 = torch.ops.aten.select.int(view_1718, 0, 0)
    view_1721 = torch.ops.aten.view.default(view_1720, [1, 384, 384, 4, 4, 64]);  view_1720 = None
    permute_971 = torch.ops.aten.permute.default(view_1721, [4, 0, 3, 1, 2, 5]);  view_1721 = None
    view_1722 = torch.ops.aten.view.default(permute_971, [4, 4, 384, 384, 64]);  permute_971 = None
    unbind_int_79 = torch.ops.aten.unbind.int(view_1722);  view_1722 = None
    getitem_1606 = unbind_int_79[0]
    getitem_1607 = unbind_int_79[1]
    getitem_1608 = unbind_int_79[2]
    getitem_1609 = unbind_int_79[3];  unbind_int_79 = None
    expand_102 = torch.ops.aten.expand.default(select_43, [4, 384, 384, 384]);  select_43 = None
    _scaled_dot_product_efficient_attention_default_57 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1606, getitem_1607, getitem_1608, expand_102, False);  getitem_1606 = getitem_1607 = getitem_1608 = expand_102 = None
    getitem_1610 = _scaled_dot_product_efficient_attention_default_57[0];  _scaled_dot_product_efficient_attention_default_57 = None
    sigmoid_129 = torch.ops.aten.sigmoid.default(getitem_1609);  getitem_1609 = None
    mul_211 = torch.ops.aten.mul.Tensor(getitem_1610, sigmoid_129);  getitem_1610 = sigmoid_129 = None
    view_1723 = torch.ops.aten.view.default(mul_211, [1, 4, 384, 384, 64]);  mul_211 = None
    permute_972 = torch.ops.aten.permute.default(view_1723, [0, 2, 3, 1, 4]);  view_1723 = None
    clone_165 = torch.ops.aten.clone.default(permute_972, memory_format = torch.contiguous_format);  permute_972 = None
    _unsafe_view_142 = torch.ops.aten._unsafe_view.default(clone_165, [1, 384, 384, 256]);  clone_165 = None
    transpose_43 = torch.ops.aten.transpose.int(getitem_1603, 1, 2);  getitem_1603 = None
    _to_copy_966 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_15_triangle_attention_pair2qkvg2_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_15_triangle_attention_pair2qkvg2_weight = None
    _to_copy_967 = torch.ops.aten._to_copy.default(transpose_43, dtype = torch.bfloat16);  transpose_43 = None
    t_350 = torch.ops.aten.t.default(_to_copy_966);  _to_copy_966 = None
    expand_103 = torch.ops.aten.expand.default(_to_copy_967, [1, 384, 384, 256]);  _to_copy_967 = None
    view_1724 = torch.ops.aten.view.default(expand_103, [384, 384, 256]);  expand_103 = None
    expand_104 = torch.ops.aten.expand.default(t_350, [1, 384, 256, 1024]);  t_350 = None
    view_1725 = torch.ops.aten.view.default(expand_104, [384, 256, 1024]);  expand_104 = None
    bmm_149 = torch.ops.aten.bmm.default(view_1724, view_1725);  view_1724 = view_1725 = None
    view_1726 = torch.ops.aten.view.default(bmm_149, [1, 384, 384, 1024]);  bmm_149 = None
    select_44 = torch.ops.aten.select.int(view_1718, 0, 1);  view_1718 = None
    view_1727 = torch.ops.aten.view.default(view_1726, [1, 384, 384, 4, 4, 64]);  view_1726 = None
    permute_973 = torch.ops.aten.permute.default(view_1727, [4, 0, 3, 1, 2, 5]);  view_1727 = None
    view_1728 = torch.ops.aten.view.default(permute_973, [4, 4, 384, 384, 64]);  permute_973 = None
    unbind_int_80 = torch.ops.aten.unbind.int(view_1728);  view_1728 = None
    getitem_1614 = unbind_int_80[0]
    getitem_1615 = unbind_int_80[1]
    getitem_1616 = unbind_int_80[2]
    getitem_1617 = unbind_int_80[3];  unbind_int_80 = None
    expand_105 = torch.ops.aten.expand.default(select_44, [4, 384, 384, 384]);  select_44 = None
    _scaled_dot_product_efficient_attention_default_58 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1614, getitem_1615, getitem_1616, expand_105, False);  getitem_1614 = getitem_1615 = getitem_1616 = expand_105 = None
    getitem_1618 = _scaled_dot_product_efficient_attention_default_58[0];  _scaled_dot_product_efficient_attention_default_58 = None
    sigmoid_130 = torch.ops.aten.sigmoid.default(getitem_1617);  getitem_1617 = None
    mul_212 = torch.ops.aten.mul.Tensor(getitem_1618, sigmoid_130);  getitem_1618 = sigmoid_130 = None
    view_1729 = torch.ops.aten.view.default(mul_212, [1, 4, 384, 384, 64]);  mul_212 = None
    permute_974 = torch.ops.aten.permute.default(view_1729, [0, 2, 3, 1, 4]);  view_1729 = None
    clone_166 = torch.ops.aten.clone.default(permute_974, memory_format = torch.contiguous_format);  permute_974 = None
    _unsafe_view_143 = torch.ops.aten._unsafe_view.default(clone_166, [1, 384, 384, 256]);  clone_166 = None
    cat_27 = torch.ops.aten.cat.default([_unsafe_view_142, _unsafe_view_143], dim = -1);  _unsafe_view_142 = _unsafe_view_143 = None
    slice_178 = torch.ops.aten.slice.Tensor(pairformer_stack_blocks_15_triangle_attention_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  pairformer_stack_blocks_15_triangle_attention_out_scalers = None
    unsqueeze_622 = torch.ops.aten.unsqueeze.default(slice_178, 1);  slice_178 = None
    mul_213 = torch.ops.aten.mul.Tensor(pairformer_stack_blocks_15_triangle_attention_linear_out_weight, unsqueeze_622);  pairformer_stack_blocks_15_triangle_attention_linear_out_weight = unsqueeze_622 = None
    _to_copy_968 = torch.ops.aten._to_copy.default(mul_213, dtype = torch.bfloat16);  mul_213 = None
    t_351 = torch.ops.aten.t.default(_to_copy_968);  _to_copy_968 = None
    view_1730 = torch.ops.aten.view.default(cat_27, [147456, 512]);  cat_27 = None
    mm_325 = torch.ops.aten.mm.default(view_1730, t_351);  view_1730 = t_351 = None
    view_1731 = torch.ops.aten.view.default(mm_325, [1, 384, 384, 256]);  mm_325 = None
    add_178 = torch.ops.aten.add.Tensor(add_177, view_1731);  add_177 = view_1731 = None
    split_tensor_172 = torch.ops.aten.split.Tensor(add_171, 384, dim = -2)
    getitem_1622 = split_tensor_172[0];  split_tensor_172 = None
    _to_copy_969 = torch.ops.aten._to_copy.default(getitem_1622, dtype = torch.float32);  getitem_1622 = None
    native_layer_norm_default_199 = torch.ops.aten.native_layer_norm.default(_to_copy_969, [256], pairformer_stack_blocks_15_transition_pair_layer_norm_weight, pairformer_stack_blocks_15_transition_pair_layer_norm_bias, 1e-05);  _to_copy_969 = pairformer_stack_blocks_15_transition_pair_layer_norm_weight = pairformer_stack_blocks_15_transition_pair_layer_norm_bias = None
    getitem_1623 = native_layer_norm_default_199[0];  native_layer_norm_default_199 = None
    _to_copy_970 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_15_transition_pair_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_15_transition_pair_linear_no_bias_ab_weight = None
    _to_copy_971 = torch.ops.aten._to_copy.default(getitem_1623, dtype = torch.bfloat16);  getitem_1623 = None
    t_352 = torch.ops.aten.t.default(_to_copy_970);  _to_copy_970 = None
    view_1732 = torch.ops.aten.view.default(_to_copy_971, [147456, 256]);  _to_copy_971 = None
    mm_326 = torch.ops.aten.mm.default(view_1732, t_352);  view_1732 = t_352 = None
    view_1733 = torch.ops.aten.view.default(mm_326, [1, 384, 384, 1024]);  mm_326 = None
    split_tensor_173 = torch.ops.aten.split.Tensor(view_1733, 512, dim = -1);  view_1733 = None
    getitem_1626 = split_tensor_173[0]
    getitem_1627 = split_tensor_173[1];  split_tensor_173 = None
    silu_45 = torch.ops.aten.silu.default(getitem_1626);  getitem_1626 = None
    mul_214 = torch.ops.aten.mul.Tensor(silu_45, getitem_1627);  silu_45 = getitem_1627 = None
    _to_copy_972 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_15_transition_pair_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_15_transition_pair_linear_out_weight = None
    t_353 = torch.ops.aten.t.default(_to_copy_972);  _to_copy_972 = None
    view_1735 = torch.ops.aten.view.default(mul_214, [147456, 512]);  mul_214 = None
    mm_327 = torch.ops.aten.mm.default(view_1735, t_353);  view_1735 = t_353 = None
    view_1736 = torch.ops.aten.view.default(mm_327, [1, 384, 384, 256]);  mm_327 = None
    add_179 = torch.ops.aten.add.Tensor(add_178, view_1736);  add_178 = view_1736 = None
    _to_copy_973 = torch.ops.aten._to_copy.default(add_175, dtype = torch.float32)
    native_layer_norm_default_200 = torch.ops.aten.native_layer_norm.default(_to_copy_973, [384], pairformer_stack_blocks_15_attention_pair_bias_single_layer_norm_weight, pairformer_stack_blocks_15_attention_pair_bias_single_layer_norm_bias, 1e-05);  _to_copy_973 = pairformer_stack_blocks_15_attention_pair_bias_single_layer_norm_weight = pairformer_stack_blocks_15_attention_pair_bias_single_layer_norm_bias = None
    getitem_1628 = native_layer_norm_default_200[0];  native_layer_norm_default_200 = None
    _to_copy_974 = torch.ops.aten._to_copy.default(add_171, dtype = torch.float32);  add_171 = None
    native_layer_norm_default_201 = torch.ops.aten.native_layer_norm.default(_to_copy_974, [256], pairformer_stack_blocks_15_attention_pair_bias_pair_layer_norm_weight, pairformer_stack_blocks_15_attention_pair_bias_pair_layer_norm_bias, 1e-05);  _to_copy_974 = pairformer_stack_blocks_15_attention_pair_bias_pair_layer_norm_weight = pairformer_stack_blocks_15_attention_pair_bias_pair_layer_norm_bias = None
    getitem_1631 = native_layer_norm_default_201[0];  native_layer_norm_default_201 = None
    _to_copy_975 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_15_attention_pair_bias_pair_linear_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_15_attention_pair_bias_pair_linear_weight = None
    _to_copy_976 = torch.ops.aten._to_copy.default(getitem_1631, dtype = torch.bfloat16);  getitem_1631 = None
    t_354 = torch.ops.aten.t.default(_to_copy_975);  _to_copy_975 = None
    view_1737 = torch.ops.aten.view.default(_to_copy_976, [147456, 256]);  _to_copy_976 = None
    mm_328 = torch.ops.aten.mm.default(view_1737, t_354);  view_1737 = t_354 = None
    view_1738 = torch.ops.aten.view.default(mm_328, [1, 384, 384, 16]);  mm_328 = None
    permute_975 = torch.ops.aten.permute.default(view_1738, [0, 3, 1, 2]);  view_1738 = None
    view_1739 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384])
    bitwise_not_109 = torch.ops.aten.bitwise_not.default(view_1739);  view_1739 = None
    masked_fill_109 = torch.ops.aten.masked_fill.Scalar(permute_975, bitwise_not_109, -10000);  permute_975 = bitwise_not_109 = None
    _to_copy_977 = torch.ops.aten._to_copy.default(getitem_1628, dtype = torch.bfloat16);  getitem_1628 = None
    _to_copy_978 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_15_attention_pair_bias_attention_input2qkvg_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_15_attention_pair_bias_attention_input2qkvg_weight = None
    unsqueeze_623 = torch.ops.aten.unsqueeze.default(_to_copy_977, 3);  _to_copy_977 = None
    unsqueeze_624 = torch.ops.aten.unsqueeze.default(unsqueeze_623, 4);  unsqueeze_623 = None
    unsqueeze_625 = torch.ops.aten.unsqueeze.default(unsqueeze_624, 5);  unsqueeze_624 = None
    permute_976 = torch.ops.aten.permute.default(unsqueeze_625, [3, 0, 4, 1, 5, 2]);  unsqueeze_625 = None
    unsqueeze_626 = torch.ops.aten.unsqueeze.default(_to_copy_978, 4);  _to_copy_978 = None
    unsqueeze_627 = torch.ops.aten.unsqueeze.default(unsqueeze_626, 5);  unsqueeze_626 = None
    permute_977 = torch.ops.aten.permute.default(unsqueeze_627, [1, 4, 2, 5, 3, 0]);  unsqueeze_627 = None
    permute_978 = torch.ops.aten.permute.default(permute_976, [3, 5, 0, 1, 2, 4]);  permute_976 = None
    view_1740 = torch.ops.aten.view.default(permute_978, [1, 384, 384]);  permute_978 = None
    permute_979 = torch.ops.aten.permute.default(permute_977, [5, 0, 1, 2, 4, 3]);  permute_977 = None
    view_1741 = torch.ops.aten.view.default(permute_979, [1, 384, 1536]);  permute_979 = None
    bmm_150 = torch.ops.aten.bmm.default(view_1740, view_1741);  view_1740 = view_1741 = None
    view_1742 = torch.ops.aten.view.default(bmm_150, [384, 1, 4, 1, 16, 24]);  bmm_150 = None
    permute_980 = torch.ops.aten.permute.default(view_1742, [2, 3, 4, 0, 5, 1]);  view_1742 = None
    view_1743 = torch.ops.aten.view.default(permute_980, [4, 1, 16, 384, 24]);  permute_980 = None
    unbind_int_81 = torch.ops.aten.unbind.int(view_1743);  view_1743 = None
    getitem_1634 = unbind_int_81[0]
    getitem_1635 = unbind_int_81[1]
    getitem_1636 = unbind_int_81[2]
    getitem_1637 = unbind_int_81[3];  unbind_int_81 = None
    view_1744 = torch.ops.aten.view.default(pairformer_stack_blocks_15_attention_pair_bias_attention_query_bias, [1, 16, 1, 24]);  pairformer_stack_blocks_15_attention_pair_bias_attention_query_bias = None
    add_180 = torch.ops.aten.add.Tensor(getitem_1634, view_1744);  getitem_1634 = view_1744 = None
    _to_copy_979 = torch.ops.aten._to_copy.default(add_180, dtype = torch.bfloat16);  add_180 = None
    expand_106 = torch.ops.aten.expand.default(masked_fill_109, [1, 16, 384, 384]);  masked_fill_109 = None
    _scaled_dot_product_efficient_attention_default_59 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_979, getitem_1635, getitem_1636, expand_106, False);  _to_copy_979 = getitem_1635 = getitem_1636 = expand_106 = None
    getitem_1638 = _scaled_dot_product_efficient_attention_default_59[0];  _scaled_dot_product_efficient_attention_default_59 = None
    add_181 = torch.ops.aten.add.Tensor(getitem_1637, 1);  getitem_1637 = None
    sigmoid_131 = torch.ops.aten.sigmoid.default(add_181);  add_181 = None
    mul_215 = torch.ops.aten.mul.Tensor(getitem_1638, sigmoid_131);  getitem_1638 = sigmoid_131 = None
    _to_copy_980 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_15_attention_pair_bias_attention_output_proj_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_15_attention_pair_bias_attention_output_proj_weight = None
    unsqueeze_628 = torch.ops.aten.unsqueeze.default(mul_215, 4);  mul_215 = None
    permute_981 = torch.ops.aten.permute.default(unsqueeze_628, [0, 2, 4, 3, 1]);  unsqueeze_628 = None
    unsqueeze_629 = torch.ops.aten.unsqueeze.default(_to_copy_980, 3);  _to_copy_980 = None
    unsqueeze_630 = torch.ops.aten.unsqueeze.default(unsqueeze_629, 4);  unsqueeze_629 = None
    permute_982 = torch.ops.aten.permute.default(unsqueeze_630, [3, 4, 2, 1, 0]);  unsqueeze_630 = None
    permute_983 = torch.ops.aten.permute.default(permute_981, [1, 3, 4, 0, 2]);  permute_981 = None
    clone_167 = torch.ops.aten.clone.default(permute_983, memory_format = torch.contiguous_format);  permute_983 = None
    _unsafe_view_144 = torch.ops.aten._unsafe_view.default(clone_167, [1, 384, 384]);  clone_167 = None
    permute_984 = torch.ops.aten.permute.default(permute_982, [3, 4, 0, 2, 1]);  permute_982 = None
    clone_168 = torch.ops.aten.clone.default(permute_984, memory_format = torch.contiguous_format);  permute_984 = None
    _unsafe_view_145 = torch.ops.aten._unsafe_view.default(clone_168, [1, 384, 384]);  clone_168 = None
    bmm_151 = torch.ops.aten.bmm.default(_unsafe_view_144, _unsafe_view_145);  _unsafe_view_144 = _unsafe_view_145 = None
    view_1745 = torch.ops.aten.view.default(bmm_151, [384, 1, 1, 1, 384]);  bmm_151 = None
    permute_985 = torch.ops.aten.permute.default(view_1745, [3, 0, 4, 1, 2]);  view_1745 = None
    view_1746 = torch.ops.aten.view.default(permute_985, [1, 384, 384]);  permute_985 = None
    unsqueeze_631 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
    mul_216 = torch.ops.aten.mul.Tensor(view_1746, unsqueeze_631);  view_1746 = unsqueeze_631 = None
    add_182 = torch.ops.aten.add.Tensor(add_175, mul_216);  mul_216 = None
    split_tensor_174 = torch.ops.aten.split.Tensor(add_175, 384, dim = -2);  add_175 = None
    getitem_1642 = split_tensor_174[0];  split_tensor_174 = None
    _to_copy_981 = torch.ops.aten._to_copy.default(getitem_1642, dtype = torch.float32);  getitem_1642 = None
    native_layer_norm_default_202 = torch.ops.aten.native_layer_norm.default(_to_copy_981, [384], pairformer_stack_blocks_15_transition_single_layer_norm_weight, pairformer_stack_blocks_15_transition_single_layer_norm_bias, 1e-05);  _to_copy_981 = pairformer_stack_blocks_15_transition_single_layer_norm_weight = pairformer_stack_blocks_15_transition_single_layer_norm_bias = None
    getitem_1643 = native_layer_norm_default_202[0];  native_layer_norm_default_202 = None
    _to_copy_982 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_15_transition_single_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_15_transition_single_linear_no_bias_ab_weight = None
    _to_copy_983 = torch.ops.aten._to_copy.default(getitem_1643, dtype = torch.bfloat16);  getitem_1643 = None
    t_355 = torch.ops.aten.t.default(_to_copy_982);  _to_copy_982 = None
    view_1747 = torch.ops.aten.view.default(_to_copy_983, [384, 384]);  _to_copy_983 = None
    mm_329 = torch.ops.aten.mm.default(view_1747, t_355);  view_1747 = t_355 = None
    view_1748 = torch.ops.aten.view.default(mm_329, [1, 384, 1536]);  mm_329 = None
    split_tensor_175 = torch.ops.aten.split.Tensor(view_1748, 768, dim = -1);  view_1748 = None
    getitem_1646 = split_tensor_175[0]
    getitem_1647 = split_tensor_175[1];  split_tensor_175 = None
    silu_46 = torch.ops.aten.silu.default(getitem_1646);  getitem_1646 = None
    mul_217 = torch.ops.aten.mul.Tensor(silu_46, getitem_1647);  silu_46 = getitem_1647 = None
    _to_copy_984 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_15_transition_single_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_15_transition_single_linear_out_weight = None
    t_356 = torch.ops.aten.t.default(_to_copy_984);  _to_copy_984 = None
    view_1750 = torch.ops.aten.view.default(mul_217, [384, 768]);  mul_217 = None
    mm_330 = torch.ops.aten.mm.default(view_1750, t_356);  view_1750 = t_356 = None
    view_1751 = torch.ops.aten.view.default(mm_330, [1, 384, 384]);  mm_330 = None
    add_183 = torch.ops.aten.add.Tensor(add_182, view_1751);  add_182 = view_1751 = None
    _to_copy_985 = torch.ops.aten._to_copy.default(add_179, dtype = torch.float32)
    native_layer_norm_default_203 = torch.ops.aten.native_layer_norm.default(_to_copy_985, [256], pairformer_stack_blocks_16_triangle_multiplication_layernorm_z_in_weight, pairformer_stack_blocks_16_triangle_multiplication_layernorm_z_in_bias, 1e-05);  _to_copy_985 = pairformer_stack_blocks_16_triangle_multiplication_layernorm_z_in_weight = pairformer_stack_blocks_16_triangle_multiplication_layernorm_z_in_bias = None
    getitem_1648 = native_layer_norm_default_203[0];  native_layer_norm_default_203 = None
    split_with_sizes_default_44 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_16_triangle_multiplication_merged_linear_p_weight, [512, 512]);  pairformer_stack_blocks_16_triangle_multiplication_merged_linear_p_weight = None
    getitem_1651 = split_with_sizes_default_44[0]
    getitem_1652 = split_with_sizes_default_44[1];  split_with_sizes_default_44 = None
    split_with_sizes_default_45 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_16_triangle_multiplication_merged_linear_g_weight, [512, 512, 256]);  pairformer_stack_blocks_16_triangle_multiplication_merged_linear_g_weight = None
    getitem_1653 = split_with_sizes_default_45[0]
    getitem_1654 = split_with_sizes_default_45[1]
    getitem_1655 = split_with_sizes_default_45[2];  split_with_sizes_default_45 = None
    _to_copy_986 = torch.ops.aten._to_copy.default(getitem_1651, dtype = torch.bfloat16);  getitem_1651 = None
    _to_copy_987 = torch.ops.aten._to_copy.default(getitem_1648, dtype = torch.bfloat16)
    t_357 = torch.ops.aten.t.default(_to_copy_986);  _to_copy_986 = None
    view_1752 = torch.ops.aten.view.default(_to_copy_987, [147456, 256]);  _to_copy_987 = None
    mm_331 = torch.ops.aten.mm.default(view_1752, t_357);  view_1752 = t_357 = None
    view_1753 = torch.ops.aten.view.default(mm_331, [1, 384, 384, 512]);  mm_331 = None
    _to_copy_988 = torch.ops.aten._to_copy.default(getitem_1653, dtype = torch.bfloat16);  getitem_1653 = None
    _to_copy_989 = torch.ops.aten._to_copy.default(getitem_1648, dtype = torch.bfloat16)
    t_358 = torch.ops.aten.t.default(_to_copy_988);  _to_copy_988 = None
    view_1754 = torch.ops.aten.view.default(_to_copy_989, [147456, 256]);  _to_copy_989 = None
    mm_332 = torch.ops.aten.mm.default(view_1754, t_358);  view_1754 = t_358 = None
    view_1755 = torch.ops.aten.view.default(mm_332, [1, 384, 384, 512]);  mm_332 = None
    sigmoid_132 = torch.ops.aten.sigmoid.default(view_1755);  view_1755 = None
    mul_218 = torch.ops.aten.mul.Tensor(view_1753, sigmoid_132);  view_1753 = sigmoid_132 = None
    unsqueeze_632 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
    bitwise_not_110 = torch.ops.aten.bitwise_not.default(unsqueeze_632);  unsqueeze_632 = None
    masked_fill_110 = torch.ops.aten.masked_fill.Scalar(mul_218, bitwise_not_110, 0);  mul_218 = bitwise_not_110 = None
    split_tensor_176 = torch.ops.aten.split.Tensor(masked_fill_110, 256, dim = -1)
    getitem_1658 = split_tensor_176[0];  split_tensor_176 = None
    unsqueeze_635 = torch.ops.aten.unsqueeze.default(getitem_1658, 4);  getitem_1658 = None
    permute_990 = torch.ops.aten.permute.default(unsqueeze_635, [0, 1, 4, 3, 2]);  unsqueeze_635 = None
    permute_991 = torch.ops.aten.permute.default(permute_990, [3, 1, 4, 0, 2]);  permute_990 = None
    view_1758 = torch.ops.aten.view.default(permute_991, [256, 384, 384]);  permute_991 = None
    split_tensor_177 = torch.ops.aten.split.Tensor(masked_fill_110, 256, dim = -1);  masked_fill_110 = None
    getitem_1661 = split_tensor_177[1];  split_tensor_177 = None
    unsqueeze_636 = torch.ops.aten.unsqueeze.default(getitem_1661, 4);  getitem_1661 = None
    permute_992 = torch.ops.aten.permute.default(unsqueeze_636, [0, 4, 1, 3, 2]);  unsqueeze_636 = None
    permute_993 = torch.ops.aten.permute.default(permute_992, [3, 4, 0, 2, 1]);  permute_992 = None
    view_1759 = torch.ops.aten.view.default(permute_993, [256, 384, 384]);  permute_993 = None
    bmm_152 = torch.ops.aten.bmm.default(view_1758, view_1759);  view_1758 = view_1759 = None
    view_1760 = torch.ops.aten.view.default(bmm_152, [256, 384, 1, 1, 384]);  bmm_152 = None
    permute_994 = torch.ops.aten.permute.default(view_1760, [3, 1, 4, 0, 2]);  view_1760 = None
    view_1761 = torch.ops.aten.view.default(permute_994, [1, 384, 384, 256]);  permute_994 = None
    _to_copy_990 = torch.ops.aten._to_copy.default(getitem_1652, dtype = torch.bfloat16);  getitem_1652 = None
    _to_copy_991 = torch.ops.aten._to_copy.default(getitem_1648, dtype = torch.bfloat16)
    t_359 = torch.ops.aten.t.default(_to_copy_990);  _to_copy_990 = None
    view_1762 = torch.ops.aten.view.default(_to_copy_991, [147456, 256]);  _to_copy_991 = None
    mm_333 = torch.ops.aten.mm.default(view_1762, t_359);  view_1762 = t_359 = None
    view_1763 = torch.ops.aten.view.default(mm_333, [1, 384, 384, 512]);  mm_333 = None
    _to_copy_992 = torch.ops.aten._to_copy.default(getitem_1654, dtype = torch.bfloat16);  getitem_1654 = None
    _to_copy_993 = torch.ops.aten._to_copy.default(getitem_1648, dtype = torch.bfloat16)
    t_360 = torch.ops.aten.t.default(_to_copy_992);  _to_copy_992 = None
    view_1764 = torch.ops.aten.view.default(_to_copy_993, [147456, 256]);  _to_copy_993 = None
    mm_334 = torch.ops.aten.mm.default(view_1764, t_360);  view_1764 = t_360 = None
    view_1765 = torch.ops.aten.view.default(mm_334, [1, 384, 384, 512]);  mm_334 = None
    sigmoid_133 = torch.ops.aten.sigmoid.default(view_1765);  view_1765 = None
    mul_219 = torch.ops.aten.mul.Tensor(view_1763, sigmoid_133);  view_1763 = sigmoid_133 = None
    view_1766 = torch.ops.aten.view.default(mul_219, [147456, 512]);  mul_219 = None
    view_1767 = torch.ops.aten.view.default(view_1766, [1, 384, 384, 512]);  view_1766 = None
    transpose_44 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
    unsqueeze_637 = torch.ops.aten.unsqueeze.default(transpose_44, 3);  transpose_44 = None
    clone_169 = torch.ops.aten.clone.default(unsqueeze_637, memory_format = torch.contiguous_format);  unsqueeze_637 = None
    bitwise_not_111 = torch.ops.aten.bitwise_not.default(clone_169);  clone_169 = None
    masked_fill_111 = torch.ops.aten.masked_fill.Scalar(view_1767, bitwise_not_111, 0);  view_1767 = bitwise_not_111 = None
    view_1768 = torch.ops.aten.view.default(masked_fill_111, [147456, 512]);  masked_fill_111 = None
    view_1772 = torch.ops.aten.view.default(view_1768, [1, 384, 384, 512])
    split_tensor_178 = torch.ops.aten.split.Tensor(view_1772, 256, dim = -1);  view_1772 = None
    getitem_1664 = split_tensor_178[0];  split_tensor_178 = None
    unsqueeze_640 = torch.ops.aten.unsqueeze.default(getitem_1664, 4);  getitem_1664 = None
    permute_999 = torch.ops.aten.permute.default(unsqueeze_640, [0, 2, 4, 3, 1]);  unsqueeze_640 = None
    permute_1000 = torch.ops.aten.permute.default(permute_999, [3, 1, 4, 0, 2]);  permute_999 = None
    view_1773 = torch.ops.aten.view.default(permute_1000, [256, 384, 384]);  permute_1000 = None
    view_1774 = torch.ops.aten.view.default(view_1768, [1, 384, 384, 512]);  view_1768 = None
    split_tensor_179 = torch.ops.aten.split.Tensor(view_1774, 256, dim = -1);  view_1774 = None
    getitem_1667 = split_tensor_179[1];  split_tensor_179 = None
    unsqueeze_641 = torch.ops.aten.unsqueeze.default(getitem_1667, 4);  getitem_1667 = None
    permute_1001 = torch.ops.aten.permute.default(unsqueeze_641, [0, 4, 2, 3, 1]);  unsqueeze_641 = None
    permute_1002 = torch.ops.aten.permute.default(permute_1001, [3, 4, 0, 2, 1]);  permute_1001 = None
    view_1775 = torch.ops.aten.view.default(permute_1002, [256, 384, 384]);  permute_1002 = None
    bmm_153 = torch.ops.aten.bmm.default(view_1773, view_1775);  view_1773 = view_1775 = None
    view_1776 = torch.ops.aten.view.default(bmm_153, [256, 384, 1, 1, 384]);  bmm_153 = None
    permute_1003 = torch.ops.aten.permute.default(view_1776, [3, 1, 4, 0, 2]);  view_1776 = None
    view_1777 = torch.ops.aten.view.default(permute_1003, [1, 384, 384, 256]);  permute_1003 = None
    _to_copy_994 = torch.ops.aten._to_copy.default(view_1761, dtype = torch.float32);  view_1761 = None
    native_layer_norm_default_204 = torch.ops.aten.native_layer_norm.default(_to_copy_994, [256], None, None, 1e-05);  _to_copy_994 = None
    getitem_1668 = native_layer_norm_default_204[0];  native_layer_norm_default_204 = None
    _to_copy_995 = torch.ops.aten._to_copy.default(view_1777, dtype = torch.float32);  view_1777 = None
    native_layer_norm_default_205 = torch.ops.aten.native_layer_norm.default(_to_copy_995, [256], None, None, 1e-05);  _to_copy_995 = None
    getitem_1671 = native_layer_norm_default_205[0];  native_layer_norm_default_205 = None
    add_184 = torch.ops.aten.add.Tensor(getitem_1668, getitem_1671);  getitem_1668 = getitem_1671 = None
    _to_copy_996 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_16_triangle_multiplication_linear_z_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_16_triangle_multiplication_linear_z_out_weight = None
    _to_copy_997 = torch.ops.aten._to_copy.default(add_184, dtype = torch.bfloat16);  add_184 = None
    t_361 = torch.ops.aten.t.default(_to_copy_996);  _to_copy_996 = None
    view_1778 = torch.ops.aten.view.default(_to_copy_997, [147456, 256]);  _to_copy_997 = None
    mm_335 = torch.ops.aten.mm.default(view_1778, t_361);  view_1778 = t_361 = None
    view_1779 = torch.ops.aten.view.default(mm_335, [1, 384, 384, 256]);  mm_335 = None
    _to_copy_998 = torch.ops.aten._to_copy.default(getitem_1655, dtype = torch.bfloat16);  getitem_1655 = None
    _to_copy_999 = torch.ops.aten._to_copy.default(getitem_1648, dtype = torch.bfloat16);  getitem_1648 = None
    t_362 = torch.ops.aten.t.default(_to_copy_998);  _to_copy_998 = None
    view_1780 = torch.ops.aten.view.default(_to_copy_999, [147456, 256]);  _to_copy_999 = None
    mm_336 = torch.ops.aten.mm.default(view_1780, t_362);  view_1780 = t_362 = None
    view_1781 = torch.ops.aten.view.default(mm_336, [1, 384, 384, 256]);  mm_336 = None
    sigmoid_134 = torch.ops.aten.sigmoid.default(view_1781);  view_1781 = None
    mul_220 = torch.ops.aten.mul.Tensor(view_1779, sigmoid_134);  view_1779 = sigmoid_134 = None
    add_185 = torch.ops.aten.add.Tensor(add_179, mul_220);  mul_220 = None
    _to_copy_1000 = torch.ops.aten._to_copy.default(add_179, dtype = torch.float32)
    native_layer_norm_default_206 = torch.ops.aten.native_layer_norm.default(_to_copy_1000, [256], None, None, 1e-05);  _to_copy_1000 = None
    getitem_1674 = native_layer_norm_default_206[0];  native_layer_norm_default_206 = None
    _to_copy_1001 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_16_triangle_attention_pair2b_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_16_triangle_attention_pair2b_weight = None
    _to_copy_1002 = torch.ops.aten._to_copy.default(getitem_1674, dtype = torch.bfloat16)
    t_363 = torch.ops.aten.t.default(_to_copy_1001);  _to_copy_1001 = None
    view_1782 = torch.ops.aten.view.default(_to_copy_1002, [147456, 256]);  _to_copy_1002 = None
    mm_337 = torch.ops.aten.mm.default(view_1782, t_363);  view_1782 = t_363 = None
    view_1783 = torch.ops.aten.view.default(mm_337, [1, 384, 384, 8]);  mm_337 = None
    view_1784 = torch.ops.aten.view.default(view_1783, [1, 384, 384, 2, 4]);  view_1783 = None
    permute_1004 = torch.ops.aten.permute.default(view_1784, [0, 3, 4, 1, 2]);  view_1784 = None
    view_1785 = torch.ops.aten.view.default(permute_1004, [1, 2, 4, 1, 384, 384]);  permute_1004 = None
    view_1786 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 384, 384])
    bitwise_not_112 = torch.ops.aten.bitwise_not.default(view_1786);  view_1786 = None
    masked_fill_112 = torch.ops.aten.masked_fill.Scalar(view_1785, bitwise_not_112, -10000);  view_1785 = bitwise_not_112 = None
    view_1787 = torch.ops.aten.view.default(masked_fill_112, [1, 2, 4, 384, 384]);  masked_fill_112 = None
    permute_1005 = torch.ops.aten.permute.default(view_1787, [1, 0, 2, 3, 4]);  view_1787 = None
    view_1788 = torch.ops.aten.view.default(permute_1005, [2, 4, 1, 384, 384]);  permute_1005 = None
    _to_copy_1003 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_16_triangle_attention_pair2qkvg1_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_16_triangle_attention_pair2qkvg1_weight = None
    _to_copy_1004 = torch.ops.aten._to_copy.default(getitem_1674, dtype = torch.bfloat16)
    t_364 = torch.ops.aten.t.default(_to_copy_1003);  _to_copy_1003 = None
    view_1789 = torch.ops.aten.view.default(_to_copy_1004, [147456, 256]);  _to_copy_1004 = None
    mm_338 = torch.ops.aten.mm.default(view_1789, t_364);  view_1789 = t_364 = None
    view_1790 = torch.ops.aten.view.default(mm_338, [1, 384, 384, 1024]);  mm_338 = None
    select_45 = torch.ops.aten.select.int(view_1788, 0, 0)
    view_1791 = torch.ops.aten.view.default(view_1790, [1, 384, 384, 4, 4, 64]);  view_1790 = None
    permute_1006 = torch.ops.aten.permute.default(view_1791, [4, 0, 3, 1, 2, 5]);  view_1791 = None
    view_1792 = torch.ops.aten.view.default(permute_1006, [4, 4, 384, 384, 64]);  permute_1006 = None
    unbind_int_82 = torch.ops.aten.unbind.int(view_1792);  view_1792 = None
    getitem_1677 = unbind_int_82[0]
    getitem_1678 = unbind_int_82[1]
    getitem_1679 = unbind_int_82[2]
    getitem_1680 = unbind_int_82[3];  unbind_int_82 = None
    expand_107 = torch.ops.aten.expand.default(select_45, [4, 384, 384, 384]);  select_45 = None
    _scaled_dot_product_efficient_attention_default_60 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1677, getitem_1678, getitem_1679, expand_107, False);  getitem_1677 = getitem_1678 = getitem_1679 = expand_107 = None
    getitem_1681 = _scaled_dot_product_efficient_attention_default_60[0];  _scaled_dot_product_efficient_attention_default_60 = None
    sigmoid_135 = torch.ops.aten.sigmoid.default(getitem_1680);  getitem_1680 = None
    mul_221 = torch.ops.aten.mul.Tensor(getitem_1681, sigmoid_135);  getitem_1681 = sigmoid_135 = None
    view_1793 = torch.ops.aten.view.default(mul_221, [1, 4, 384, 384, 64]);  mul_221 = None
    permute_1007 = torch.ops.aten.permute.default(view_1793, [0, 2, 3, 1, 4]);  view_1793 = None
    clone_170 = torch.ops.aten.clone.default(permute_1007, memory_format = torch.contiguous_format);  permute_1007 = None
    _unsafe_view_146 = torch.ops.aten._unsafe_view.default(clone_170, [1, 384, 384, 256]);  clone_170 = None
    transpose_45 = torch.ops.aten.transpose.int(getitem_1674, 1, 2);  getitem_1674 = None
    _to_copy_1005 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_16_triangle_attention_pair2qkvg2_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_16_triangle_attention_pair2qkvg2_weight = None
    _to_copy_1006 = torch.ops.aten._to_copy.default(transpose_45, dtype = torch.bfloat16);  transpose_45 = None
    t_365 = torch.ops.aten.t.default(_to_copy_1005);  _to_copy_1005 = None
    expand_108 = torch.ops.aten.expand.default(_to_copy_1006, [1, 384, 384, 256]);  _to_copy_1006 = None
    view_1794 = torch.ops.aten.view.default(expand_108, [384, 384, 256]);  expand_108 = None
    expand_109 = torch.ops.aten.expand.default(t_365, [1, 384, 256, 1024]);  t_365 = None
    view_1795 = torch.ops.aten.view.default(expand_109, [384, 256, 1024]);  expand_109 = None
    bmm_154 = torch.ops.aten.bmm.default(view_1794, view_1795);  view_1794 = view_1795 = None
    view_1796 = torch.ops.aten.view.default(bmm_154, [1, 384, 384, 1024]);  bmm_154 = None
    select_46 = torch.ops.aten.select.int(view_1788, 0, 1);  view_1788 = None
    view_1797 = torch.ops.aten.view.default(view_1796, [1, 384, 384, 4, 4, 64]);  view_1796 = None
    permute_1008 = torch.ops.aten.permute.default(view_1797, [4, 0, 3, 1, 2, 5]);  view_1797 = None
    view_1798 = torch.ops.aten.view.default(permute_1008, [4, 4, 384, 384, 64]);  permute_1008 = None
    unbind_int_83 = torch.ops.aten.unbind.int(view_1798);  view_1798 = None
    getitem_1685 = unbind_int_83[0]
    getitem_1686 = unbind_int_83[1]
    getitem_1687 = unbind_int_83[2]
    getitem_1688 = unbind_int_83[3];  unbind_int_83 = None
    expand_110 = torch.ops.aten.expand.default(select_46, [4, 384, 384, 384]);  select_46 = None
    _scaled_dot_product_efficient_attention_default_61 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1685, getitem_1686, getitem_1687, expand_110, False);  getitem_1685 = getitem_1686 = getitem_1687 = expand_110 = None
    getitem_1689 = _scaled_dot_product_efficient_attention_default_61[0];  _scaled_dot_product_efficient_attention_default_61 = None
    sigmoid_136 = torch.ops.aten.sigmoid.default(getitem_1688);  getitem_1688 = None
    mul_222 = torch.ops.aten.mul.Tensor(getitem_1689, sigmoid_136);  getitem_1689 = sigmoid_136 = None
    view_1799 = torch.ops.aten.view.default(mul_222, [1, 4, 384, 384, 64]);  mul_222 = None
    permute_1009 = torch.ops.aten.permute.default(view_1799, [0, 2, 3, 1, 4]);  view_1799 = None
    clone_171 = torch.ops.aten.clone.default(permute_1009, memory_format = torch.contiguous_format);  permute_1009 = None
    _unsafe_view_147 = torch.ops.aten._unsafe_view.default(clone_171, [1, 384, 384, 256]);  clone_171 = None
    cat_28 = torch.ops.aten.cat.default([_unsafe_view_146, _unsafe_view_147], dim = -1);  _unsafe_view_146 = _unsafe_view_147 = None
    slice_179 = torch.ops.aten.slice.Tensor(pairformer_stack_blocks_16_triangle_attention_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  pairformer_stack_blocks_16_triangle_attention_out_scalers = None
    unsqueeze_642 = torch.ops.aten.unsqueeze.default(slice_179, 1);  slice_179 = None
    mul_223 = torch.ops.aten.mul.Tensor(pairformer_stack_blocks_16_triangle_attention_linear_out_weight, unsqueeze_642);  pairformer_stack_blocks_16_triangle_attention_linear_out_weight = unsqueeze_642 = None
    _to_copy_1007 = torch.ops.aten._to_copy.default(mul_223, dtype = torch.bfloat16);  mul_223 = None
    t_366 = torch.ops.aten.t.default(_to_copy_1007);  _to_copy_1007 = None
    view_1800 = torch.ops.aten.view.default(cat_28, [147456, 512]);  cat_28 = None
    mm_339 = torch.ops.aten.mm.default(view_1800, t_366);  view_1800 = t_366 = None
    view_1801 = torch.ops.aten.view.default(mm_339, [1, 384, 384, 256]);  mm_339 = None
    add_186 = torch.ops.aten.add.Tensor(add_185, view_1801);  add_185 = view_1801 = None
    split_tensor_180 = torch.ops.aten.split.Tensor(add_179, 384, dim = -2)
    getitem_1693 = split_tensor_180[0];  split_tensor_180 = None
    _to_copy_1008 = torch.ops.aten._to_copy.default(getitem_1693, dtype = torch.float32);  getitem_1693 = None
    native_layer_norm_default_207 = torch.ops.aten.native_layer_norm.default(_to_copy_1008, [256], pairformer_stack_blocks_16_transition_pair_layer_norm_weight, pairformer_stack_blocks_16_transition_pair_layer_norm_bias, 1e-05);  _to_copy_1008 = pairformer_stack_blocks_16_transition_pair_layer_norm_weight = pairformer_stack_blocks_16_transition_pair_layer_norm_bias = None
    getitem_1694 = native_layer_norm_default_207[0];  native_layer_norm_default_207 = None
    _to_copy_1009 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_16_transition_pair_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_16_transition_pair_linear_no_bias_ab_weight = None
    _to_copy_1010 = torch.ops.aten._to_copy.default(getitem_1694, dtype = torch.bfloat16);  getitem_1694 = None
    t_367 = torch.ops.aten.t.default(_to_copy_1009);  _to_copy_1009 = None
    view_1802 = torch.ops.aten.view.default(_to_copy_1010, [147456, 256]);  _to_copy_1010 = None
    mm_340 = torch.ops.aten.mm.default(view_1802, t_367);  view_1802 = t_367 = None
    view_1803 = torch.ops.aten.view.default(mm_340, [1, 384, 384, 1024]);  mm_340 = None
    split_tensor_181 = torch.ops.aten.split.Tensor(view_1803, 512, dim = -1);  view_1803 = None
    getitem_1697 = split_tensor_181[0]
    getitem_1698 = split_tensor_181[1];  split_tensor_181 = None
    silu_47 = torch.ops.aten.silu.default(getitem_1697);  getitem_1697 = None
    mul_224 = torch.ops.aten.mul.Tensor(silu_47, getitem_1698);  silu_47 = getitem_1698 = None
    _to_copy_1011 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_16_transition_pair_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_16_transition_pair_linear_out_weight = None
    t_368 = torch.ops.aten.t.default(_to_copy_1011);  _to_copy_1011 = None
    view_1805 = torch.ops.aten.view.default(mul_224, [147456, 512]);  mul_224 = None
    mm_341 = torch.ops.aten.mm.default(view_1805, t_368);  view_1805 = t_368 = None
    view_1806 = torch.ops.aten.view.default(mm_341, [1, 384, 384, 256]);  mm_341 = None
    add_187 = torch.ops.aten.add.Tensor(add_186, view_1806);  add_186 = view_1806 = None
    _to_copy_1012 = torch.ops.aten._to_copy.default(add_183, dtype = torch.float32)
    native_layer_norm_default_208 = torch.ops.aten.native_layer_norm.default(_to_copy_1012, [384], pairformer_stack_blocks_16_attention_pair_bias_single_layer_norm_weight, pairformer_stack_blocks_16_attention_pair_bias_single_layer_norm_bias, 1e-05);  _to_copy_1012 = pairformer_stack_blocks_16_attention_pair_bias_single_layer_norm_weight = pairformer_stack_blocks_16_attention_pair_bias_single_layer_norm_bias = None
    getitem_1699 = native_layer_norm_default_208[0];  native_layer_norm_default_208 = None
    _to_copy_1013 = torch.ops.aten._to_copy.default(add_179, dtype = torch.float32);  add_179 = None
    native_layer_norm_default_209 = torch.ops.aten.native_layer_norm.default(_to_copy_1013, [256], pairformer_stack_blocks_16_attention_pair_bias_pair_layer_norm_weight, pairformer_stack_blocks_16_attention_pair_bias_pair_layer_norm_bias, 1e-05);  _to_copy_1013 = pairformer_stack_blocks_16_attention_pair_bias_pair_layer_norm_weight = pairformer_stack_blocks_16_attention_pair_bias_pair_layer_norm_bias = None
    getitem_1702 = native_layer_norm_default_209[0];  native_layer_norm_default_209 = None
    _to_copy_1014 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_16_attention_pair_bias_pair_linear_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_16_attention_pair_bias_pair_linear_weight = None
    _to_copy_1015 = torch.ops.aten._to_copy.default(getitem_1702, dtype = torch.bfloat16);  getitem_1702 = None
    t_369 = torch.ops.aten.t.default(_to_copy_1014);  _to_copy_1014 = None
    view_1807 = torch.ops.aten.view.default(_to_copy_1015, [147456, 256]);  _to_copy_1015 = None
    mm_342 = torch.ops.aten.mm.default(view_1807, t_369);  view_1807 = t_369 = None
    view_1808 = torch.ops.aten.view.default(mm_342, [1, 384, 384, 16]);  mm_342 = None
    permute_1010 = torch.ops.aten.permute.default(view_1808, [0, 3, 1, 2]);  view_1808 = None
    view_1809 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384])
    bitwise_not_113 = torch.ops.aten.bitwise_not.default(view_1809);  view_1809 = None
    masked_fill_113 = torch.ops.aten.masked_fill.Scalar(permute_1010, bitwise_not_113, -10000);  permute_1010 = bitwise_not_113 = None
    _to_copy_1016 = torch.ops.aten._to_copy.default(getitem_1699, dtype = torch.bfloat16);  getitem_1699 = None
    _to_copy_1017 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_16_attention_pair_bias_attention_input2qkvg_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_16_attention_pair_bias_attention_input2qkvg_weight = None
    unsqueeze_643 = torch.ops.aten.unsqueeze.default(_to_copy_1016, 3);  _to_copy_1016 = None
    unsqueeze_644 = torch.ops.aten.unsqueeze.default(unsqueeze_643, 4);  unsqueeze_643 = None
    unsqueeze_645 = torch.ops.aten.unsqueeze.default(unsqueeze_644, 5);  unsqueeze_644 = None
    permute_1011 = torch.ops.aten.permute.default(unsqueeze_645, [3, 0, 4, 1, 5, 2]);  unsqueeze_645 = None
    unsqueeze_646 = torch.ops.aten.unsqueeze.default(_to_copy_1017, 4);  _to_copy_1017 = None
    unsqueeze_647 = torch.ops.aten.unsqueeze.default(unsqueeze_646, 5);  unsqueeze_646 = None
    permute_1012 = torch.ops.aten.permute.default(unsqueeze_647, [1, 4, 2, 5, 3, 0]);  unsqueeze_647 = None
    permute_1013 = torch.ops.aten.permute.default(permute_1011, [3, 5, 0, 1, 2, 4]);  permute_1011 = None
    view_1810 = torch.ops.aten.view.default(permute_1013, [1, 384, 384]);  permute_1013 = None
    permute_1014 = torch.ops.aten.permute.default(permute_1012, [5, 0, 1, 2, 4, 3]);  permute_1012 = None
    view_1811 = torch.ops.aten.view.default(permute_1014, [1, 384, 1536]);  permute_1014 = None
    bmm_155 = torch.ops.aten.bmm.default(view_1810, view_1811);  view_1810 = view_1811 = None
    view_1812 = torch.ops.aten.view.default(bmm_155, [384, 1, 4, 1, 16, 24]);  bmm_155 = None
    permute_1015 = torch.ops.aten.permute.default(view_1812, [2, 3, 4, 0, 5, 1]);  view_1812 = None
    view_1813 = torch.ops.aten.view.default(permute_1015, [4, 1, 16, 384, 24]);  permute_1015 = None
    unbind_int_84 = torch.ops.aten.unbind.int(view_1813);  view_1813 = None
    getitem_1705 = unbind_int_84[0]
    getitem_1706 = unbind_int_84[1]
    getitem_1707 = unbind_int_84[2]
    getitem_1708 = unbind_int_84[3];  unbind_int_84 = None
    view_1814 = torch.ops.aten.view.default(pairformer_stack_blocks_16_attention_pair_bias_attention_query_bias, [1, 16, 1, 24]);  pairformer_stack_blocks_16_attention_pair_bias_attention_query_bias = None
    add_188 = torch.ops.aten.add.Tensor(getitem_1705, view_1814);  getitem_1705 = view_1814 = None
    _to_copy_1018 = torch.ops.aten._to_copy.default(add_188, dtype = torch.bfloat16);  add_188 = None
    expand_111 = torch.ops.aten.expand.default(masked_fill_113, [1, 16, 384, 384]);  masked_fill_113 = None
    _scaled_dot_product_efficient_attention_default_62 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_1018, getitem_1706, getitem_1707, expand_111, False);  _to_copy_1018 = getitem_1706 = getitem_1707 = expand_111 = None
    getitem_1709 = _scaled_dot_product_efficient_attention_default_62[0];  _scaled_dot_product_efficient_attention_default_62 = None
    add_189 = torch.ops.aten.add.Tensor(getitem_1708, 1);  getitem_1708 = None
    sigmoid_137 = torch.ops.aten.sigmoid.default(add_189);  add_189 = None
    mul_225 = torch.ops.aten.mul.Tensor(getitem_1709, sigmoid_137);  getitem_1709 = sigmoid_137 = None
    _to_copy_1019 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_16_attention_pair_bias_attention_output_proj_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_16_attention_pair_bias_attention_output_proj_weight = None
    unsqueeze_648 = torch.ops.aten.unsqueeze.default(mul_225, 4);  mul_225 = None
    permute_1016 = torch.ops.aten.permute.default(unsqueeze_648, [0, 2, 4, 3, 1]);  unsqueeze_648 = None
    unsqueeze_649 = torch.ops.aten.unsqueeze.default(_to_copy_1019, 3);  _to_copy_1019 = None
    unsqueeze_650 = torch.ops.aten.unsqueeze.default(unsqueeze_649, 4);  unsqueeze_649 = None
    permute_1017 = torch.ops.aten.permute.default(unsqueeze_650, [3, 4, 2, 1, 0]);  unsqueeze_650 = None
    permute_1018 = torch.ops.aten.permute.default(permute_1016, [1, 3, 4, 0, 2]);  permute_1016 = None
    clone_172 = torch.ops.aten.clone.default(permute_1018, memory_format = torch.contiguous_format);  permute_1018 = None
    _unsafe_view_148 = torch.ops.aten._unsafe_view.default(clone_172, [1, 384, 384]);  clone_172 = None
    permute_1019 = torch.ops.aten.permute.default(permute_1017, [3, 4, 0, 2, 1]);  permute_1017 = None
    clone_173 = torch.ops.aten.clone.default(permute_1019, memory_format = torch.contiguous_format);  permute_1019 = None
    _unsafe_view_149 = torch.ops.aten._unsafe_view.default(clone_173, [1, 384, 384]);  clone_173 = None
    bmm_156 = torch.ops.aten.bmm.default(_unsafe_view_148, _unsafe_view_149);  _unsafe_view_148 = _unsafe_view_149 = None
    view_1815 = torch.ops.aten.view.default(bmm_156, [384, 1, 1, 1, 384]);  bmm_156 = None
    permute_1020 = torch.ops.aten.permute.default(view_1815, [3, 0, 4, 1, 2]);  view_1815 = None
    view_1816 = torch.ops.aten.view.default(permute_1020, [1, 384, 384]);  permute_1020 = None
    unsqueeze_651 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
    mul_226 = torch.ops.aten.mul.Tensor(view_1816, unsqueeze_651);  view_1816 = unsqueeze_651 = None
    add_190 = torch.ops.aten.add.Tensor(add_183, mul_226);  mul_226 = None
    split_tensor_182 = torch.ops.aten.split.Tensor(add_183, 384, dim = -2);  add_183 = None
    getitem_1713 = split_tensor_182[0];  split_tensor_182 = None
    _to_copy_1020 = torch.ops.aten._to_copy.default(getitem_1713, dtype = torch.float32);  getitem_1713 = None
    native_layer_norm_default_210 = torch.ops.aten.native_layer_norm.default(_to_copy_1020, [384], pairformer_stack_blocks_16_transition_single_layer_norm_weight, pairformer_stack_blocks_16_transition_single_layer_norm_bias, 1e-05);  _to_copy_1020 = pairformer_stack_blocks_16_transition_single_layer_norm_weight = pairformer_stack_blocks_16_transition_single_layer_norm_bias = None
    getitem_1714 = native_layer_norm_default_210[0];  native_layer_norm_default_210 = None
    _to_copy_1021 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_16_transition_single_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_16_transition_single_linear_no_bias_ab_weight = None
    _to_copy_1022 = torch.ops.aten._to_copy.default(getitem_1714, dtype = torch.bfloat16);  getitem_1714 = None
    t_370 = torch.ops.aten.t.default(_to_copy_1021);  _to_copy_1021 = None
    view_1817 = torch.ops.aten.view.default(_to_copy_1022, [384, 384]);  _to_copy_1022 = None
    mm_343 = torch.ops.aten.mm.default(view_1817, t_370);  view_1817 = t_370 = None
    view_1818 = torch.ops.aten.view.default(mm_343, [1, 384, 1536]);  mm_343 = None
    split_tensor_183 = torch.ops.aten.split.Tensor(view_1818, 768, dim = -1);  view_1818 = None
    getitem_1717 = split_tensor_183[0]
    getitem_1718 = split_tensor_183[1];  split_tensor_183 = None
    silu_48 = torch.ops.aten.silu.default(getitem_1717);  getitem_1717 = None
    mul_227 = torch.ops.aten.mul.Tensor(silu_48, getitem_1718);  silu_48 = getitem_1718 = None
    _to_copy_1023 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_16_transition_single_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_16_transition_single_linear_out_weight = None
    t_371 = torch.ops.aten.t.default(_to_copy_1023);  _to_copy_1023 = None
    view_1820 = torch.ops.aten.view.default(mul_227, [384, 768]);  mul_227 = None
    mm_344 = torch.ops.aten.mm.default(view_1820, t_371);  view_1820 = t_371 = None
    view_1821 = torch.ops.aten.view.default(mm_344, [1, 384, 384]);  mm_344 = None
    add_191 = torch.ops.aten.add.Tensor(add_190, view_1821);  add_190 = view_1821 = None
    _to_copy_1024 = torch.ops.aten._to_copy.default(add_187, dtype = torch.float32)
    native_layer_norm_default_211 = torch.ops.aten.native_layer_norm.default(_to_copy_1024, [256], pairformer_stack_blocks_17_triangle_multiplication_layernorm_z_in_weight, pairformer_stack_blocks_17_triangle_multiplication_layernorm_z_in_bias, 1e-05);  _to_copy_1024 = pairformer_stack_blocks_17_triangle_multiplication_layernorm_z_in_weight = pairformer_stack_blocks_17_triangle_multiplication_layernorm_z_in_bias = None
    getitem_1719 = native_layer_norm_default_211[0];  native_layer_norm_default_211 = None
    split_with_sizes_default_46 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_17_triangle_multiplication_merged_linear_p_weight, [512, 512]);  pairformer_stack_blocks_17_triangle_multiplication_merged_linear_p_weight = None
    getitem_1722 = split_with_sizes_default_46[0]
    getitem_1723 = split_with_sizes_default_46[1];  split_with_sizes_default_46 = None
    split_with_sizes_default_47 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_17_triangle_multiplication_merged_linear_g_weight, [512, 512, 256]);  pairformer_stack_blocks_17_triangle_multiplication_merged_linear_g_weight = None
    getitem_1724 = split_with_sizes_default_47[0]
    getitem_1725 = split_with_sizes_default_47[1]
    getitem_1726 = split_with_sizes_default_47[2];  split_with_sizes_default_47 = None
    _to_copy_1025 = torch.ops.aten._to_copy.default(getitem_1722, dtype = torch.bfloat16);  getitem_1722 = None
    _to_copy_1026 = torch.ops.aten._to_copy.default(getitem_1719, dtype = torch.bfloat16)
    t_372 = torch.ops.aten.t.default(_to_copy_1025);  _to_copy_1025 = None
    view_1822 = torch.ops.aten.view.default(_to_copy_1026, [147456, 256]);  _to_copy_1026 = None
    mm_345 = torch.ops.aten.mm.default(view_1822, t_372);  view_1822 = t_372 = None
    view_1823 = torch.ops.aten.view.default(mm_345, [1, 384, 384, 512]);  mm_345 = None
    _to_copy_1027 = torch.ops.aten._to_copy.default(getitem_1724, dtype = torch.bfloat16);  getitem_1724 = None
    _to_copy_1028 = torch.ops.aten._to_copy.default(getitem_1719, dtype = torch.bfloat16)
    t_373 = torch.ops.aten.t.default(_to_copy_1027);  _to_copy_1027 = None
    view_1824 = torch.ops.aten.view.default(_to_copy_1028, [147456, 256]);  _to_copy_1028 = None
    mm_346 = torch.ops.aten.mm.default(view_1824, t_373);  view_1824 = t_373 = None
    view_1825 = torch.ops.aten.view.default(mm_346, [1, 384, 384, 512]);  mm_346 = None
    sigmoid_138 = torch.ops.aten.sigmoid.default(view_1825);  view_1825 = None
    mul_228 = torch.ops.aten.mul.Tensor(view_1823, sigmoid_138);  view_1823 = sigmoid_138 = None
    unsqueeze_652 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
    bitwise_not_114 = torch.ops.aten.bitwise_not.default(unsqueeze_652);  unsqueeze_652 = None
    masked_fill_114 = torch.ops.aten.masked_fill.Scalar(mul_228, bitwise_not_114, 0);  mul_228 = bitwise_not_114 = None
    split_tensor_184 = torch.ops.aten.split.Tensor(masked_fill_114, 256, dim = -1)
    getitem_1729 = split_tensor_184[0];  split_tensor_184 = None
    unsqueeze_655 = torch.ops.aten.unsqueeze.default(getitem_1729, 4);  getitem_1729 = None
    permute_1025 = torch.ops.aten.permute.default(unsqueeze_655, [0, 1, 4, 3, 2]);  unsqueeze_655 = None
    permute_1026 = torch.ops.aten.permute.default(permute_1025, [3, 1, 4, 0, 2]);  permute_1025 = None
    view_1828 = torch.ops.aten.view.default(permute_1026, [256, 384, 384]);  permute_1026 = None
    split_tensor_185 = torch.ops.aten.split.Tensor(masked_fill_114, 256, dim = -1);  masked_fill_114 = None
    getitem_1732 = split_tensor_185[1];  split_tensor_185 = None
    unsqueeze_656 = torch.ops.aten.unsqueeze.default(getitem_1732, 4);  getitem_1732 = None
    permute_1027 = torch.ops.aten.permute.default(unsqueeze_656, [0, 4, 1, 3, 2]);  unsqueeze_656 = None
    permute_1028 = torch.ops.aten.permute.default(permute_1027, [3, 4, 0, 2, 1]);  permute_1027 = None
    view_1829 = torch.ops.aten.view.default(permute_1028, [256, 384, 384]);  permute_1028 = None
    bmm_157 = torch.ops.aten.bmm.default(view_1828, view_1829);  view_1828 = view_1829 = None
    view_1830 = torch.ops.aten.view.default(bmm_157, [256, 384, 1, 1, 384]);  bmm_157 = None
    permute_1029 = torch.ops.aten.permute.default(view_1830, [3, 1, 4, 0, 2]);  view_1830 = None
    view_1831 = torch.ops.aten.view.default(permute_1029, [1, 384, 384, 256]);  permute_1029 = None
    _to_copy_1029 = torch.ops.aten._to_copy.default(getitem_1723, dtype = torch.bfloat16);  getitem_1723 = None
    _to_copy_1030 = torch.ops.aten._to_copy.default(getitem_1719, dtype = torch.bfloat16)
    t_374 = torch.ops.aten.t.default(_to_copy_1029);  _to_copy_1029 = None
    view_1832 = torch.ops.aten.view.default(_to_copy_1030, [147456, 256]);  _to_copy_1030 = None
    mm_347 = torch.ops.aten.mm.default(view_1832, t_374);  view_1832 = t_374 = None
    view_1833 = torch.ops.aten.view.default(mm_347, [1, 384, 384, 512]);  mm_347 = None
    _to_copy_1031 = torch.ops.aten._to_copy.default(getitem_1725, dtype = torch.bfloat16);  getitem_1725 = None
    _to_copy_1032 = torch.ops.aten._to_copy.default(getitem_1719, dtype = torch.bfloat16)
    t_375 = torch.ops.aten.t.default(_to_copy_1031);  _to_copy_1031 = None
    view_1834 = torch.ops.aten.view.default(_to_copy_1032, [147456, 256]);  _to_copy_1032 = None
    mm_348 = torch.ops.aten.mm.default(view_1834, t_375);  view_1834 = t_375 = None
    view_1835 = torch.ops.aten.view.default(mm_348, [1, 384, 384, 512]);  mm_348 = None
    sigmoid_139 = torch.ops.aten.sigmoid.default(view_1835);  view_1835 = None
    mul_229 = torch.ops.aten.mul.Tensor(view_1833, sigmoid_139);  view_1833 = sigmoid_139 = None
    view_1836 = torch.ops.aten.view.default(mul_229, [147456, 512]);  mul_229 = None
    view_1837 = torch.ops.aten.view.default(view_1836, [1, 384, 384, 512]);  view_1836 = None
    transpose_46 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
    unsqueeze_657 = torch.ops.aten.unsqueeze.default(transpose_46, 3);  transpose_46 = None
    clone_174 = torch.ops.aten.clone.default(unsqueeze_657, memory_format = torch.contiguous_format);  unsqueeze_657 = None
    bitwise_not_115 = torch.ops.aten.bitwise_not.default(clone_174);  clone_174 = None
    masked_fill_115 = torch.ops.aten.masked_fill.Scalar(view_1837, bitwise_not_115, 0);  view_1837 = bitwise_not_115 = None
    view_1838 = torch.ops.aten.view.default(masked_fill_115, [147456, 512]);  masked_fill_115 = None
    view_1842 = torch.ops.aten.view.default(view_1838, [1, 384, 384, 512])
    split_tensor_186 = torch.ops.aten.split.Tensor(view_1842, 256, dim = -1);  view_1842 = None
    getitem_1735 = split_tensor_186[0];  split_tensor_186 = None
    unsqueeze_660 = torch.ops.aten.unsqueeze.default(getitem_1735, 4);  getitem_1735 = None
    permute_1034 = torch.ops.aten.permute.default(unsqueeze_660, [0, 2, 4, 3, 1]);  unsqueeze_660 = None
    permute_1035 = torch.ops.aten.permute.default(permute_1034, [3, 1, 4, 0, 2]);  permute_1034 = None
    view_1843 = torch.ops.aten.view.default(permute_1035, [256, 384, 384]);  permute_1035 = None
    view_1844 = torch.ops.aten.view.default(view_1838, [1, 384, 384, 512]);  view_1838 = None
    split_tensor_187 = torch.ops.aten.split.Tensor(view_1844, 256, dim = -1);  view_1844 = None
    getitem_1738 = split_tensor_187[1];  split_tensor_187 = None
    unsqueeze_661 = torch.ops.aten.unsqueeze.default(getitem_1738, 4);  getitem_1738 = None
    permute_1036 = torch.ops.aten.permute.default(unsqueeze_661, [0, 4, 2, 3, 1]);  unsqueeze_661 = None
    permute_1037 = torch.ops.aten.permute.default(permute_1036, [3, 4, 0, 2, 1]);  permute_1036 = None
    view_1845 = torch.ops.aten.view.default(permute_1037, [256, 384, 384]);  permute_1037 = None
    bmm_158 = torch.ops.aten.bmm.default(view_1843, view_1845);  view_1843 = view_1845 = None
    view_1846 = torch.ops.aten.view.default(bmm_158, [256, 384, 1, 1, 384]);  bmm_158 = None
    permute_1038 = torch.ops.aten.permute.default(view_1846, [3, 1, 4, 0, 2]);  view_1846 = None
    view_1847 = torch.ops.aten.view.default(permute_1038, [1, 384, 384, 256]);  permute_1038 = None
    _to_copy_1033 = torch.ops.aten._to_copy.default(view_1831, dtype = torch.float32);  view_1831 = None
    native_layer_norm_default_212 = torch.ops.aten.native_layer_norm.default(_to_copy_1033, [256], None, None, 1e-05);  _to_copy_1033 = None
    getitem_1739 = native_layer_norm_default_212[0];  native_layer_norm_default_212 = None
    _to_copy_1034 = torch.ops.aten._to_copy.default(view_1847, dtype = torch.float32);  view_1847 = None
    native_layer_norm_default_213 = torch.ops.aten.native_layer_norm.default(_to_copy_1034, [256], None, None, 1e-05);  _to_copy_1034 = None
    getitem_1742 = native_layer_norm_default_213[0];  native_layer_norm_default_213 = None
    add_192 = torch.ops.aten.add.Tensor(getitem_1739, getitem_1742);  getitem_1739 = getitem_1742 = None
    _to_copy_1035 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_17_triangle_multiplication_linear_z_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_17_triangle_multiplication_linear_z_out_weight = None
    _to_copy_1036 = torch.ops.aten._to_copy.default(add_192, dtype = torch.bfloat16);  add_192 = None
    t_376 = torch.ops.aten.t.default(_to_copy_1035);  _to_copy_1035 = None
    view_1848 = torch.ops.aten.view.default(_to_copy_1036, [147456, 256]);  _to_copy_1036 = None
    mm_349 = torch.ops.aten.mm.default(view_1848, t_376);  view_1848 = t_376 = None
    view_1849 = torch.ops.aten.view.default(mm_349, [1, 384, 384, 256]);  mm_349 = None
    _to_copy_1037 = torch.ops.aten._to_copy.default(getitem_1726, dtype = torch.bfloat16);  getitem_1726 = None
    _to_copy_1038 = torch.ops.aten._to_copy.default(getitem_1719, dtype = torch.bfloat16);  getitem_1719 = None
    t_377 = torch.ops.aten.t.default(_to_copy_1037);  _to_copy_1037 = None
    view_1850 = torch.ops.aten.view.default(_to_copy_1038, [147456, 256]);  _to_copy_1038 = None
    mm_350 = torch.ops.aten.mm.default(view_1850, t_377);  view_1850 = t_377 = None
    view_1851 = torch.ops.aten.view.default(mm_350, [1, 384, 384, 256]);  mm_350 = None
    sigmoid_140 = torch.ops.aten.sigmoid.default(view_1851);  view_1851 = None
    mul_230 = torch.ops.aten.mul.Tensor(view_1849, sigmoid_140);  view_1849 = sigmoid_140 = None
    add_193 = torch.ops.aten.add.Tensor(add_187, mul_230);  mul_230 = None
    _to_copy_1039 = torch.ops.aten._to_copy.default(add_187, dtype = torch.float32)
    native_layer_norm_default_214 = torch.ops.aten.native_layer_norm.default(_to_copy_1039, [256], None, None, 1e-05);  _to_copy_1039 = None
    getitem_1745 = native_layer_norm_default_214[0];  native_layer_norm_default_214 = None
    _to_copy_1040 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_17_triangle_attention_pair2b_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_17_triangle_attention_pair2b_weight = None
    _to_copy_1041 = torch.ops.aten._to_copy.default(getitem_1745, dtype = torch.bfloat16)
    t_378 = torch.ops.aten.t.default(_to_copy_1040);  _to_copy_1040 = None
    view_1852 = torch.ops.aten.view.default(_to_copy_1041, [147456, 256]);  _to_copy_1041 = None
    mm_351 = torch.ops.aten.mm.default(view_1852, t_378);  view_1852 = t_378 = None
    view_1853 = torch.ops.aten.view.default(mm_351, [1, 384, 384, 8]);  mm_351 = None
    view_1854 = torch.ops.aten.view.default(view_1853, [1, 384, 384, 2, 4]);  view_1853 = None
    permute_1039 = torch.ops.aten.permute.default(view_1854, [0, 3, 4, 1, 2]);  view_1854 = None
    view_1855 = torch.ops.aten.view.default(permute_1039, [1, 2, 4, 1, 384, 384]);  permute_1039 = None
    view_1856 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 384, 384])
    bitwise_not_116 = torch.ops.aten.bitwise_not.default(view_1856);  view_1856 = None
    masked_fill_116 = torch.ops.aten.masked_fill.Scalar(view_1855, bitwise_not_116, -10000);  view_1855 = bitwise_not_116 = None
    view_1857 = torch.ops.aten.view.default(masked_fill_116, [1, 2, 4, 384, 384]);  masked_fill_116 = None
    permute_1040 = torch.ops.aten.permute.default(view_1857, [1, 0, 2, 3, 4]);  view_1857 = None
    view_1858 = torch.ops.aten.view.default(permute_1040, [2, 4, 1, 384, 384]);  permute_1040 = None
    _to_copy_1042 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_17_triangle_attention_pair2qkvg1_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_17_triangle_attention_pair2qkvg1_weight = None
    _to_copy_1043 = torch.ops.aten._to_copy.default(getitem_1745, dtype = torch.bfloat16)
    t_379 = torch.ops.aten.t.default(_to_copy_1042);  _to_copy_1042 = None
    view_1859 = torch.ops.aten.view.default(_to_copy_1043, [147456, 256]);  _to_copy_1043 = None
    mm_352 = torch.ops.aten.mm.default(view_1859, t_379);  view_1859 = t_379 = None
    view_1860 = torch.ops.aten.view.default(mm_352, [1, 384, 384, 1024]);  mm_352 = None
    select_47 = torch.ops.aten.select.int(view_1858, 0, 0)
    view_1861 = torch.ops.aten.view.default(view_1860, [1, 384, 384, 4, 4, 64]);  view_1860 = None
    permute_1041 = torch.ops.aten.permute.default(view_1861, [4, 0, 3, 1, 2, 5]);  view_1861 = None
    view_1862 = torch.ops.aten.view.default(permute_1041, [4, 4, 384, 384, 64]);  permute_1041 = None
    unbind_int_85 = torch.ops.aten.unbind.int(view_1862);  view_1862 = None
    getitem_1748 = unbind_int_85[0]
    getitem_1749 = unbind_int_85[1]
    getitem_1750 = unbind_int_85[2]
    getitem_1751 = unbind_int_85[3];  unbind_int_85 = None
    expand_112 = torch.ops.aten.expand.default(select_47, [4, 384, 384, 384]);  select_47 = None
    _scaled_dot_product_efficient_attention_default_63 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1748, getitem_1749, getitem_1750, expand_112, False);  getitem_1748 = getitem_1749 = getitem_1750 = expand_112 = None
    getitem_1752 = _scaled_dot_product_efficient_attention_default_63[0];  _scaled_dot_product_efficient_attention_default_63 = None
    sigmoid_141 = torch.ops.aten.sigmoid.default(getitem_1751);  getitem_1751 = None
    mul_231 = torch.ops.aten.mul.Tensor(getitem_1752, sigmoid_141);  getitem_1752 = sigmoid_141 = None
    view_1863 = torch.ops.aten.view.default(mul_231, [1, 4, 384, 384, 64]);  mul_231 = None
    permute_1042 = torch.ops.aten.permute.default(view_1863, [0, 2, 3, 1, 4]);  view_1863 = None
    clone_175 = torch.ops.aten.clone.default(permute_1042, memory_format = torch.contiguous_format);  permute_1042 = None
    _unsafe_view_150 = torch.ops.aten._unsafe_view.default(clone_175, [1, 384, 384, 256]);  clone_175 = None
    transpose_47 = torch.ops.aten.transpose.int(getitem_1745, 1, 2);  getitem_1745 = None
    _to_copy_1044 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_17_triangle_attention_pair2qkvg2_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_17_triangle_attention_pair2qkvg2_weight = None
    _to_copy_1045 = torch.ops.aten._to_copy.default(transpose_47, dtype = torch.bfloat16);  transpose_47 = None
    t_380 = torch.ops.aten.t.default(_to_copy_1044);  _to_copy_1044 = None
    expand_113 = torch.ops.aten.expand.default(_to_copy_1045, [1, 384, 384, 256]);  _to_copy_1045 = None
    view_1864 = torch.ops.aten.view.default(expand_113, [384, 384, 256]);  expand_113 = None
    expand_114 = torch.ops.aten.expand.default(t_380, [1, 384, 256, 1024]);  t_380 = None
    view_1865 = torch.ops.aten.view.default(expand_114, [384, 256, 1024]);  expand_114 = None
    bmm_159 = torch.ops.aten.bmm.default(view_1864, view_1865);  view_1864 = view_1865 = None
    view_1866 = torch.ops.aten.view.default(bmm_159, [1, 384, 384, 1024]);  bmm_159 = None
    select_48 = torch.ops.aten.select.int(view_1858, 0, 1);  view_1858 = None
    view_1867 = torch.ops.aten.view.default(view_1866, [1, 384, 384, 4, 4, 64]);  view_1866 = None
    permute_1043 = torch.ops.aten.permute.default(view_1867, [4, 0, 3, 1, 2, 5]);  view_1867 = None
    view_1868 = torch.ops.aten.view.default(permute_1043, [4, 4, 384, 384, 64]);  permute_1043 = None
    unbind_int_86 = torch.ops.aten.unbind.int(view_1868);  view_1868 = None
    getitem_1756 = unbind_int_86[0]
    getitem_1757 = unbind_int_86[1]
    getitem_1758 = unbind_int_86[2]
    getitem_1759 = unbind_int_86[3];  unbind_int_86 = None
    expand_115 = torch.ops.aten.expand.default(select_48, [4, 384, 384, 384]);  select_48 = None
    _scaled_dot_product_efficient_attention_default_64 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1756, getitem_1757, getitem_1758, expand_115, False);  getitem_1756 = getitem_1757 = getitem_1758 = expand_115 = None
    getitem_1760 = _scaled_dot_product_efficient_attention_default_64[0];  _scaled_dot_product_efficient_attention_default_64 = None
    sigmoid_142 = torch.ops.aten.sigmoid.default(getitem_1759);  getitem_1759 = None
    mul_232 = torch.ops.aten.mul.Tensor(getitem_1760, sigmoid_142);  getitem_1760 = sigmoid_142 = None
    view_1869 = torch.ops.aten.view.default(mul_232, [1, 4, 384, 384, 64]);  mul_232 = None
    permute_1044 = torch.ops.aten.permute.default(view_1869, [0, 2, 3, 1, 4]);  view_1869 = None
    clone_176 = torch.ops.aten.clone.default(permute_1044, memory_format = torch.contiguous_format);  permute_1044 = None
    _unsafe_view_151 = torch.ops.aten._unsafe_view.default(clone_176, [1, 384, 384, 256]);  clone_176 = None
    cat_29 = torch.ops.aten.cat.default([_unsafe_view_150, _unsafe_view_151], dim = -1);  _unsafe_view_150 = _unsafe_view_151 = None
    slice_180 = torch.ops.aten.slice.Tensor(pairformer_stack_blocks_17_triangle_attention_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  pairformer_stack_blocks_17_triangle_attention_out_scalers = None
    unsqueeze_662 = torch.ops.aten.unsqueeze.default(slice_180, 1);  slice_180 = None
    mul_233 = torch.ops.aten.mul.Tensor(pairformer_stack_blocks_17_triangle_attention_linear_out_weight, unsqueeze_662);  pairformer_stack_blocks_17_triangle_attention_linear_out_weight = unsqueeze_662 = None
    _to_copy_1046 = torch.ops.aten._to_copy.default(mul_233, dtype = torch.bfloat16);  mul_233 = None
    t_381 = torch.ops.aten.t.default(_to_copy_1046);  _to_copy_1046 = None
    view_1870 = torch.ops.aten.view.default(cat_29, [147456, 512]);  cat_29 = None
    mm_353 = torch.ops.aten.mm.default(view_1870, t_381);  view_1870 = t_381 = None
    view_1871 = torch.ops.aten.view.default(mm_353, [1, 384, 384, 256]);  mm_353 = None
    add_194 = torch.ops.aten.add.Tensor(add_193, view_1871);  add_193 = view_1871 = None
    split_tensor_188 = torch.ops.aten.split.Tensor(add_187, 384, dim = -2)
    getitem_1764 = split_tensor_188[0];  split_tensor_188 = None
    _to_copy_1047 = torch.ops.aten._to_copy.default(getitem_1764, dtype = torch.float32);  getitem_1764 = None
    native_layer_norm_default_215 = torch.ops.aten.native_layer_norm.default(_to_copy_1047, [256], pairformer_stack_blocks_17_transition_pair_layer_norm_weight, pairformer_stack_blocks_17_transition_pair_layer_norm_bias, 1e-05);  _to_copy_1047 = pairformer_stack_blocks_17_transition_pair_layer_norm_weight = pairformer_stack_blocks_17_transition_pair_layer_norm_bias = None
    getitem_1765 = native_layer_norm_default_215[0];  native_layer_norm_default_215 = None
    _to_copy_1048 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_17_transition_pair_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_17_transition_pair_linear_no_bias_ab_weight = None
    _to_copy_1049 = torch.ops.aten._to_copy.default(getitem_1765, dtype = torch.bfloat16);  getitem_1765 = None
    t_382 = torch.ops.aten.t.default(_to_copy_1048);  _to_copy_1048 = None
    view_1872 = torch.ops.aten.view.default(_to_copy_1049, [147456, 256]);  _to_copy_1049 = None
    mm_354 = torch.ops.aten.mm.default(view_1872, t_382);  view_1872 = t_382 = None
    view_1873 = torch.ops.aten.view.default(mm_354, [1, 384, 384, 1024]);  mm_354 = None
    split_tensor_189 = torch.ops.aten.split.Tensor(view_1873, 512, dim = -1);  view_1873 = None
    getitem_1768 = split_tensor_189[0]
    getitem_1769 = split_tensor_189[1];  split_tensor_189 = None
    silu_49 = torch.ops.aten.silu.default(getitem_1768);  getitem_1768 = None
    mul_234 = torch.ops.aten.mul.Tensor(silu_49, getitem_1769);  silu_49 = getitem_1769 = None
    _to_copy_1050 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_17_transition_pair_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_17_transition_pair_linear_out_weight = None
    t_383 = torch.ops.aten.t.default(_to_copy_1050);  _to_copy_1050 = None
    view_1875 = torch.ops.aten.view.default(mul_234, [147456, 512]);  mul_234 = None
    mm_355 = torch.ops.aten.mm.default(view_1875, t_383);  view_1875 = t_383 = None
    view_1876 = torch.ops.aten.view.default(mm_355, [1, 384, 384, 256]);  mm_355 = None
    add_195 = torch.ops.aten.add.Tensor(add_194, view_1876);  add_194 = view_1876 = None
    _to_copy_1051 = torch.ops.aten._to_copy.default(add_191, dtype = torch.float32)
    native_layer_norm_default_216 = torch.ops.aten.native_layer_norm.default(_to_copy_1051, [384], pairformer_stack_blocks_17_attention_pair_bias_single_layer_norm_weight, pairformer_stack_blocks_17_attention_pair_bias_single_layer_norm_bias, 1e-05);  _to_copy_1051 = pairformer_stack_blocks_17_attention_pair_bias_single_layer_norm_weight = pairformer_stack_blocks_17_attention_pair_bias_single_layer_norm_bias = None
    getitem_1770 = native_layer_norm_default_216[0];  native_layer_norm_default_216 = None
    _to_copy_1052 = torch.ops.aten._to_copy.default(add_187, dtype = torch.float32);  add_187 = None
    native_layer_norm_default_217 = torch.ops.aten.native_layer_norm.default(_to_copy_1052, [256], pairformer_stack_blocks_17_attention_pair_bias_pair_layer_norm_weight, pairformer_stack_blocks_17_attention_pair_bias_pair_layer_norm_bias, 1e-05);  _to_copy_1052 = pairformer_stack_blocks_17_attention_pair_bias_pair_layer_norm_weight = pairformer_stack_blocks_17_attention_pair_bias_pair_layer_norm_bias = None
    getitem_1773 = native_layer_norm_default_217[0];  native_layer_norm_default_217 = None
    _to_copy_1053 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_17_attention_pair_bias_pair_linear_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_17_attention_pair_bias_pair_linear_weight = None
    _to_copy_1054 = torch.ops.aten._to_copy.default(getitem_1773, dtype = torch.bfloat16);  getitem_1773 = None
    t_384 = torch.ops.aten.t.default(_to_copy_1053);  _to_copy_1053 = None
    view_1877 = torch.ops.aten.view.default(_to_copy_1054, [147456, 256]);  _to_copy_1054 = None
    mm_356 = torch.ops.aten.mm.default(view_1877, t_384);  view_1877 = t_384 = None
    view_1878 = torch.ops.aten.view.default(mm_356, [1, 384, 384, 16]);  mm_356 = None
    permute_1045 = torch.ops.aten.permute.default(view_1878, [0, 3, 1, 2]);  view_1878 = None
    view_1879 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384])
    bitwise_not_117 = torch.ops.aten.bitwise_not.default(view_1879);  view_1879 = None
    masked_fill_117 = torch.ops.aten.masked_fill.Scalar(permute_1045, bitwise_not_117, -10000);  permute_1045 = bitwise_not_117 = None
    _to_copy_1055 = torch.ops.aten._to_copy.default(getitem_1770, dtype = torch.bfloat16);  getitem_1770 = None
    _to_copy_1056 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_17_attention_pair_bias_attention_input2qkvg_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_17_attention_pair_bias_attention_input2qkvg_weight = None
    unsqueeze_663 = torch.ops.aten.unsqueeze.default(_to_copy_1055, 3);  _to_copy_1055 = None
    unsqueeze_664 = torch.ops.aten.unsqueeze.default(unsqueeze_663, 4);  unsqueeze_663 = None
    unsqueeze_665 = torch.ops.aten.unsqueeze.default(unsqueeze_664, 5);  unsqueeze_664 = None
    permute_1046 = torch.ops.aten.permute.default(unsqueeze_665, [3, 0, 4, 1, 5, 2]);  unsqueeze_665 = None
    unsqueeze_666 = torch.ops.aten.unsqueeze.default(_to_copy_1056, 4);  _to_copy_1056 = None
    unsqueeze_667 = torch.ops.aten.unsqueeze.default(unsqueeze_666, 5);  unsqueeze_666 = None
    permute_1047 = torch.ops.aten.permute.default(unsqueeze_667, [1, 4, 2, 5, 3, 0]);  unsqueeze_667 = None
    permute_1048 = torch.ops.aten.permute.default(permute_1046, [3, 5, 0, 1, 2, 4]);  permute_1046 = None
    view_1880 = torch.ops.aten.view.default(permute_1048, [1, 384, 384]);  permute_1048 = None
    permute_1049 = torch.ops.aten.permute.default(permute_1047, [5, 0, 1, 2, 4, 3]);  permute_1047 = None
    view_1881 = torch.ops.aten.view.default(permute_1049, [1, 384, 1536]);  permute_1049 = None
    bmm_160 = torch.ops.aten.bmm.default(view_1880, view_1881);  view_1880 = view_1881 = None
    view_1882 = torch.ops.aten.view.default(bmm_160, [384, 1, 4, 1, 16, 24]);  bmm_160 = None
    permute_1050 = torch.ops.aten.permute.default(view_1882, [2, 3, 4, 0, 5, 1]);  view_1882 = None
    view_1883 = torch.ops.aten.view.default(permute_1050, [4, 1, 16, 384, 24]);  permute_1050 = None
    unbind_int_87 = torch.ops.aten.unbind.int(view_1883);  view_1883 = None
    getitem_1776 = unbind_int_87[0]
    getitem_1777 = unbind_int_87[1]
    getitem_1778 = unbind_int_87[2]
    getitem_1779 = unbind_int_87[3];  unbind_int_87 = None
    view_1884 = torch.ops.aten.view.default(pairformer_stack_blocks_17_attention_pair_bias_attention_query_bias, [1, 16, 1, 24]);  pairformer_stack_blocks_17_attention_pair_bias_attention_query_bias = None
    add_196 = torch.ops.aten.add.Tensor(getitem_1776, view_1884);  getitem_1776 = view_1884 = None
    _to_copy_1057 = torch.ops.aten._to_copy.default(add_196, dtype = torch.bfloat16);  add_196 = None
    expand_116 = torch.ops.aten.expand.default(masked_fill_117, [1, 16, 384, 384]);  masked_fill_117 = None
    _scaled_dot_product_efficient_attention_default_65 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_1057, getitem_1777, getitem_1778, expand_116, False);  _to_copy_1057 = getitem_1777 = getitem_1778 = expand_116 = None
    getitem_1780 = _scaled_dot_product_efficient_attention_default_65[0];  _scaled_dot_product_efficient_attention_default_65 = None
    add_197 = torch.ops.aten.add.Tensor(getitem_1779, 1);  getitem_1779 = None
    sigmoid_143 = torch.ops.aten.sigmoid.default(add_197);  add_197 = None
    mul_235 = torch.ops.aten.mul.Tensor(getitem_1780, sigmoid_143);  getitem_1780 = sigmoid_143 = None
    _to_copy_1058 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_17_attention_pair_bias_attention_output_proj_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_17_attention_pair_bias_attention_output_proj_weight = None
    unsqueeze_668 = torch.ops.aten.unsqueeze.default(mul_235, 4);  mul_235 = None
    permute_1051 = torch.ops.aten.permute.default(unsqueeze_668, [0, 2, 4, 3, 1]);  unsqueeze_668 = None
    unsqueeze_669 = torch.ops.aten.unsqueeze.default(_to_copy_1058, 3);  _to_copy_1058 = None
    unsqueeze_670 = torch.ops.aten.unsqueeze.default(unsqueeze_669, 4);  unsqueeze_669 = None
    permute_1052 = torch.ops.aten.permute.default(unsqueeze_670, [3, 4, 2, 1, 0]);  unsqueeze_670 = None
    permute_1053 = torch.ops.aten.permute.default(permute_1051, [1, 3, 4, 0, 2]);  permute_1051 = None
    clone_177 = torch.ops.aten.clone.default(permute_1053, memory_format = torch.contiguous_format);  permute_1053 = None
    _unsafe_view_152 = torch.ops.aten._unsafe_view.default(clone_177, [1, 384, 384]);  clone_177 = None
    permute_1054 = torch.ops.aten.permute.default(permute_1052, [3, 4, 0, 2, 1]);  permute_1052 = None
    clone_178 = torch.ops.aten.clone.default(permute_1054, memory_format = torch.contiguous_format);  permute_1054 = None
    _unsafe_view_153 = torch.ops.aten._unsafe_view.default(clone_178, [1, 384, 384]);  clone_178 = None
    bmm_161 = torch.ops.aten.bmm.default(_unsafe_view_152, _unsafe_view_153);  _unsafe_view_152 = _unsafe_view_153 = None
    view_1885 = torch.ops.aten.view.default(bmm_161, [384, 1, 1, 1, 384]);  bmm_161 = None
    permute_1055 = torch.ops.aten.permute.default(view_1885, [3, 0, 4, 1, 2]);  view_1885 = None
    view_1886 = torch.ops.aten.view.default(permute_1055, [1, 384, 384]);  permute_1055 = None
    unsqueeze_671 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
    mul_236 = torch.ops.aten.mul.Tensor(view_1886, unsqueeze_671);  view_1886 = unsqueeze_671 = None
    add_198 = torch.ops.aten.add.Tensor(add_191, mul_236);  mul_236 = None
    split_tensor_190 = torch.ops.aten.split.Tensor(add_191, 384, dim = -2);  add_191 = None
    getitem_1784 = split_tensor_190[0];  split_tensor_190 = None
    _to_copy_1059 = torch.ops.aten._to_copy.default(getitem_1784, dtype = torch.float32);  getitem_1784 = None
    native_layer_norm_default_218 = torch.ops.aten.native_layer_norm.default(_to_copy_1059, [384], pairformer_stack_blocks_17_transition_single_layer_norm_weight, pairformer_stack_blocks_17_transition_single_layer_norm_bias, 1e-05);  _to_copy_1059 = pairformer_stack_blocks_17_transition_single_layer_norm_weight = pairformer_stack_blocks_17_transition_single_layer_norm_bias = None
    getitem_1785 = native_layer_norm_default_218[0];  native_layer_norm_default_218 = None
    _to_copy_1060 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_17_transition_single_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_17_transition_single_linear_no_bias_ab_weight = None
    _to_copy_1061 = torch.ops.aten._to_copy.default(getitem_1785, dtype = torch.bfloat16);  getitem_1785 = None
    t_385 = torch.ops.aten.t.default(_to_copy_1060);  _to_copy_1060 = None
    view_1887 = torch.ops.aten.view.default(_to_copy_1061, [384, 384]);  _to_copy_1061 = None
    mm_357 = torch.ops.aten.mm.default(view_1887, t_385);  view_1887 = t_385 = None
    view_1888 = torch.ops.aten.view.default(mm_357, [1, 384, 1536]);  mm_357 = None
    split_tensor_191 = torch.ops.aten.split.Tensor(view_1888, 768, dim = -1);  view_1888 = None
    getitem_1788 = split_tensor_191[0]
    getitem_1789 = split_tensor_191[1];  split_tensor_191 = None
    silu_50 = torch.ops.aten.silu.default(getitem_1788);  getitem_1788 = None
    mul_237 = torch.ops.aten.mul.Tensor(silu_50, getitem_1789);  silu_50 = getitem_1789 = None
    _to_copy_1062 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_17_transition_single_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_17_transition_single_linear_out_weight = None
    t_386 = torch.ops.aten.t.default(_to_copy_1062);  _to_copy_1062 = None
    view_1890 = torch.ops.aten.view.default(mul_237, [384, 768]);  mul_237 = None
    mm_358 = torch.ops.aten.mm.default(view_1890, t_386);  view_1890 = t_386 = None
    view_1891 = torch.ops.aten.view.default(mm_358, [1, 384, 384]);  mm_358 = None
    add_199 = torch.ops.aten.add.Tensor(add_198, view_1891);  add_198 = view_1891 = None
    _to_copy_1063 = torch.ops.aten._to_copy.default(add_195, dtype = torch.float32)
    native_layer_norm_default_219 = torch.ops.aten.native_layer_norm.default(_to_copy_1063, [256], pairformer_stack_blocks_18_triangle_multiplication_layernorm_z_in_weight, pairformer_stack_blocks_18_triangle_multiplication_layernorm_z_in_bias, 1e-05);  _to_copy_1063 = pairformer_stack_blocks_18_triangle_multiplication_layernorm_z_in_weight = pairformer_stack_blocks_18_triangle_multiplication_layernorm_z_in_bias = None
    getitem_1790 = native_layer_norm_default_219[0];  native_layer_norm_default_219 = None
    split_with_sizes_default_48 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_18_triangle_multiplication_merged_linear_p_weight, [512, 512]);  pairformer_stack_blocks_18_triangle_multiplication_merged_linear_p_weight = None
    getitem_1793 = split_with_sizes_default_48[0]
    getitem_1794 = split_with_sizes_default_48[1];  split_with_sizes_default_48 = None
    split_with_sizes_default_49 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_18_triangle_multiplication_merged_linear_g_weight, [512, 512, 256]);  pairformer_stack_blocks_18_triangle_multiplication_merged_linear_g_weight = None
    getitem_1795 = split_with_sizes_default_49[0]
    getitem_1796 = split_with_sizes_default_49[1]
    getitem_1797 = split_with_sizes_default_49[2];  split_with_sizes_default_49 = None
    _to_copy_1064 = torch.ops.aten._to_copy.default(getitem_1793, dtype = torch.bfloat16);  getitem_1793 = None
    _to_copy_1065 = torch.ops.aten._to_copy.default(getitem_1790, dtype = torch.bfloat16)
    t_387 = torch.ops.aten.t.default(_to_copy_1064);  _to_copy_1064 = None
    view_1892 = torch.ops.aten.view.default(_to_copy_1065, [147456, 256]);  _to_copy_1065 = None
    mm_359 = torch.ops.aten.mm.default(view_1892, t_387);  view_1892 = t_387 = None
    view_1893 = torch.ops.aten.view.default(mm_359, [1, 384, 384, 512]);  mm_359 = None
    _to_copy_1066 = torch.ops.aten._to_copy.default(getitem_1795, dtype = torch.bfloat16);  getitem_1795 = None
    _to_copy_1067 = torch.ops.aten._to_copy.default(getitem_1790, dtype = torch.bfloat16)
    t_388 = torch.ops.aten.t.default(_to_copy_1066);  _to_copy_1066 = None
    view_1894 = torch.ops.aten.view.default(_to_copy_1067, [147456, 256]);  _to_copy_1067 = None
    mm_360 = torch.ops.aten.mm.default(view_1894, t_388);  view_1894 = t_388 = None
    view_1895 = torch.ops.aten.view.default(mm_360, [1, 384, 384, 512]);  mm_360 = None
    sigmoid_144 = torch.ops.aten.sigmoid.default(view_1895);  view_1895 = None
    mul_238 = torch.ops.aten.mul.Tensor(view_1893, sigmoid_144);  view_1893 = sigmoid_144 = None
    unsqueeze_672 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
    bitwise_not_118 = torch.ops.aten.bitwise_not.default(unsqueeze_672);  unsqueeze_672 = None
    masked_fill_118 = torch.ops.aten.masked_fill.Scalar(mul_238, bitwise_not_118, 0);  mul_238 = bitwise_not_118 = None
    split_tensor_192 = torch.ops.aten.split.Tensor(masked_fill_118, 256, dim = -1)
    getitem_1800 = split_tensor_192[0];  split_tensor_192 = None
    unsqueeze_675 = torch.ops.aten.unsqueeze.default(getitem_1800, 4);  getitem_1800 = None
    permute_1060 = torch.ops.aten.permute.default(unsqueeze_675, [0, 1, 4, 3, 2]);  unsqueeze_675 = None
    permute_1061 = torch.ops.aten.permute.default(permute_1060, [3, 1, 4, 0, 2]);  permute_1060 = None
    view_1898 = torch.ops.aten.view.default(permute_1061, [256, 384, 384]);  permute_1061 = None
    split_tensor_193 = torch.ops.aten.split.Tensor(masked_fill_118, 256, dim = -1);  masked_fill_118 = None
    getitem_1803 = split_tensor_193[1];  split_tensor_193 = None
    unsqueeze_676 = torch.ops.aten.unsqueeze.default(getitem_1803, 4);  getitem_1803 = None
    permute_1062 = torch.ops.aten.permute.default(unsqueeze_676, [0, 4, 1, 3, 2]);  unsqueeze_676 = None
    permute_1063 = torch.ops.aten.permute.default(permute_1062, [3, 4, 0, 2, 1]);  permute_1062 = None
    view_1899 = torch.ops.aten.view.default(permute_1063, [256, 384, 384]);  permute_1063 = None
    bmm_162 = torch.ops.aten.bmm.default(view_1898, view_1899);  view_1898 = view_1899 = None
    view_1900 = torch.ops.aten.view.default(bmm_162, [256, 384, 1, 1, 384]);  bmm_162 = None
    permute_1064 = torch.ops.aten.permute.default(view_1900, [3, 1, 4, 0, 2]);  view_1900 = None
    view_1901 = torch.ops.aten.view.default(permute_1064, [1, 384, 384, 256]);  permute_1064 = None
    _to_copy_1068 = torch.ops.aten._to_copy.default(getitem_1794, dtype = torch.bfloat16);  getitem_1794 = None
    _to_copy_1069 = torch.ops.aten._to_copy.default(getitem_1790, dtype = torch.bfloat16)
    t_389 = torch.ops.aten.t.default(_to_copy_1068);  _to_copy_1068 = None
    view_1902 = torch.ops.aten.view.default(_to_copy_1069, [147456, 256]);  _to_copy_1069 = None
    mm_361 = torch.ops.aten.mm.default(view_1902, t_389);  view_1902 = t_389 = None
    view_1903 = torch.ops.aten.view.default(mm_361, [1, 384, 384, 512]);  mm_361 = None
    _to_copy_1070 = torch.ops.aten._to_copy.default(getitem_1796, dtype = torch.bfloat16);  getitem_1796 = None
    _to_copy_1071 = torch.ops.aten._to_copy.default(getitem_1790, dtype = torch.bfloat16)
    t_390 = torch.ops.aten.t.default(_to_copy_1070);  _to_copy_1070 = None
    view_1904 = torch.ops.aten.view.default(_to_copy_1071, [147456, 256]);  _to_copy_1071 = None
    mm_362 = torch.ops.aten.mm.default(view_1904, t_390);  view_1904 = t_390 = None
    view_1905 = torch.ops.aten.view.default(mm_362, [1, 384, 384, 512]);  mm_362 = None
    sigmoid_145 = torch.ops.aten.sigmoid.default(view_1905);  view_1905 = None
    mul_239 = torch.ops.aten.mul.Tensor(view_1903, sigmoid_145);  view_1903 = sigmoid_145 = None
    view_1906 = torch.ops.aten.view.default(mul_239, [147456, 512]);  mul_239 = None
    view_1907 = torch.ops.aten.view.default(view_1906, [1, 384, 384, 512]);  view_1906 = None
    transpose_48 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
    unsqueeze_677 = torch.ops.aten.unsqueeze.default(transpose_48, 3);  transpose_48 = None
    clone_179 = torch.ops.aten.clone.default(unsqueeze_677, memory_format = torch.contiguous_format);  unsqueeze_677 = None
    bitwise_not_119 = torch.ops.aten.bitwise_not.default(clone_179);  clone_179 = None
    masked_fill_119 = torch.ops.aten.masked_fill.Scalar(view_1907, bitwise_not_119, 0);  view_1907 = bitwise_not_119 = None
    view_1908 = torch.ops.aten.view.default(masked_fill_119, [147456, 512]);  masked_fill_119 = None
    view_1912 = torch.ops.aten.view.default(view_1908, [1, 384, 384, 512])
    split_tensor_194 = torch.ops.aten.split.Tensor(view_1912, 256, dim = -1);  view_1912 = None
    getitem_1806 = split_tensor_194[0];  split_tensor_194 = None
    unsqueeze_680 = torch.ops.aten.unsqueeze.default(getitem_1806, 4);  getitem_1806 = None
    permute_1069 = torch.ops.aten.permute.default(unsqueeze_680, [0, 2, 4, 3, 1]);  unsqueeze_680 = None
    permute_1070 = torch.ops.aten.permute.default(permute_1069, [3, 1, 4, 0, 2]);  permute_1069 = None
    view_1913 = torch.ops.aten.view.default(permute_1070, [256, 384, 384]);  permute_1070 = None
    view_1914 = torch.ops.aten.view.default(view_1908, [1, 384, 384, 512]);  view_1908 = None
    split_tensor_195 = torch.ops.aten.split.Tensor(view_1914, 256, dim = -1);  view_1914 = None
    getitem_1809 = split_tensor_195[1];  split_tensor_195 = None
    unsqueeze_681 = torch.ops.aten.unsqueeze.default(getitem_1809, 4);  getitem_1809 = None
    permute_1071 = torch.ops.aten.permute.default(unsqueeze_681, [0, 4, 2, 3, 1]);  unsqueeze_681 = None
    permute_1072 = torch.ops.aten.permute.default(permute_1071, [3, 4, 0, 2, 1]);  permute_1071 = None
    view_1915 = torch.ops.aten.view.default(permute_1072, [256, 384, 384]);  permute_1072 = None
    bmm_163 = torch.ops.aten.bmm.default(view_1913, view_1915);  view_1913 = view_1915 = None
    view_1916 = torch.ops.aten.view.default(bmm_163, [256, 384, 1, 1, 384]);  bmm_163 = None
    permute_1073 = torch.ops.aten.permute.default(view_1916, [3, 1, 4, 0, 2]);  view_1916 = None
    view_1917 = torch.ops.aten.view.default(permute_1073, [1, 384, 384, 256]);  permute_1073 = None
    _to_copy_1072 = torch.ops.aten._to_copy.default(view_1901, dtype = torch.float32);  view_1901 = None
    native_layer_norm_default_220 = torch.ops.aten.native_layer_norm.default(_to_copy_1072, [256], None, None, 1e-05);  _to_copy_1072 = None
    getitem_1810 = native_layer_norm_default_220[0];  native_layer_norm_default_220 = None
    _to_copy_1073 = torch.ops.aten._to_copy.default(view_1917, dtype = torch.float32);  view_1917 = None
    native_layer_norm_default_221 = torch.ops.aten.native_layer_norm.default(_to_copy_1073, [256], None, None, 1e-05);  _to_copy_1073 = None
    getitem_1813 = native_layer_norm_default_221[0];  native_layer_norm_default_221 = None
    add_200 = torch.ops.aten.add.Tensor(getitem_1810, getitem_1813);  getitem_1810 = getitem_1813 = None
    _to_copy_1074 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_18_triangle_multiplication_linear_z_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_18_triangle_multiplication_linear_z_out_weight = None
    _to_copy_1075 = torch.ops.aten._to_copy.default(add_200, dtype = torch.bfloat16);  add_200 = None
    t_391 = torch.ops.aten.t.default(_to_copy_1074);  _to_copy_1074 = None
    view_1918 = torch.ops.aten.view.default(_to_copy_1075, [147456, 256]);  _to_copy_1075 = None
    mm_363 = torch.ops.aten.mm.default(view_1918, t_391);  view_1918 = t_391 = None
    view_1919 = torch.ops.aten.view.default(mm_363, [1, 384, 384, 256]);  mm_363 = None
    _to_copy_1076 = torch.ops.aten._to_copy.default(getitem_1797, dtype = torch.bfloat16);  getitem_1797 = None
    _to_copy_1077 = torch.ops.aten._to_copy.default(getitem_1790, dtype = torch.bfloat16);  getitem_1790 = None
    t_392 = torch.ops.aten.t.default(_to_copy_1076);  _to_copy_1076 = None
    view_1920 = torch.ops.aten.view.default(_to_copy_1077, [147456, 256]);  _to_copy_1077 = None
    mm_364 = torch.ops.aten.mm.default(view_1920, t_392);  view_1920 = t_392 = None
    view_1921 = torch.ops.aten.view.default(mm_364, [1, 384, 384, 256]);  mm_364 = None
    sigmoid_146 = torch.ops.aten.sigmoid.default(view_1921);  view_1921 = None
    mul_240 = torch.ops.aten.mul.Tensor(view_1919, sigmoid_146);  view_1919 = sigmoid_146 = None
    add_201 = torch.ops.aten.add.Tensor(add_195, mul_240);  mul_240 = None
    _to_copy_1078 = torch.ops.aten._to_copy.default(add_195, dtype = torch.float32)
    native_layer_norm_default_222 = torch.ops.aten.native_layer_norm.default(_to_copy_1078, [256], None, None, 1e-05);  _to_copy_1078 = None
    getitem_1816 = native_layer_norm_default_222[0];  native_layer_norm_default_222 = None
    _to_copy_1079 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_18_triangle_attention_pair2b_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_18_triangle_attention_pair2b_weight = None
    _to_copy_1080 = torch.ops.aten._to_copy.default(getitem_1816, dtype = torch.bfloat16)
    t_393 = torch.ops.aten.t.default(_to_copy_1079);  _to_copy_1079 = None
    view_1922 = torch.ops.aten.view.default(_to_copy_1080, [147456, 256]);  _to_copy_1080 = None
    mm_365 = torch.ops.aten.mm.default(view_1922, t_393);  view_1922 = t_393 = None
    view_1923 = torch.ops.aten.view.default(mm_365, [1, 384, 384, 8]);  mm_365 = None
    view_1924 = torch.ops.aten.view.default(view_1923, [1, 384, 384, 2, 4]);  view_1923 = None
    permute_1074 = torch.ops.aten.permute.default(view_1924, [0, 3, 4, 1, 2]);  view_1924 = None
    view_1925 = torch.ops.aten.view.default(permute_1074, [1, 2, 4, 1, 384, 384]);  permute_1074 = None
    view_1926 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 384, 384])
    bitwise_not_120 = torch.ops.aten.bitwise_not.default(view_1926);  view_1926 = None
    masked_fill_120 = torch.ops.aten.masked_fill.Scalar(view_1925, bitwise_not_120, -10000);  view_1925 = bitwise_not_120 = None
    view_1927 = torch.ops.aten.view.default(masked_fill_120, [1, 2, 4, 384, 384]);  masked_fill_120 = None
    permute_1075 = torch.ops.aten.permute.default(view_1927, [1, 0, 2, 3, 4]);  view_1927 = None
    view_1928 = torch.ops.aten.view.default(permute_1075, [2, 4, 1, 384, 384]);  permute_1075 = None
    _to_copy_1081 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_18_triangle_attention_pair2qkvg1_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_18_triangle_attention_pair2qkvg1_weight = None
    _to_copy_1082 = torch.ops.aten._to_copy.default(getitem_1816, dtype = torch.bfloat16)
    t_394 = torch.ops.aten.t.default(_to_copy_1081);  _to_copy_1081 = None
    view_1929 = torch.ops.aten.view.default(_to_copy_1082, [147456, 256]);  _to_copy_1082 = None
    mm_366 = torch.ops.aten.mm.default(view_1929, t_394);  view_1929 = t_394 = None
    view_1930 = torch.ops.aten.view.default(mm_366, [1, 384, 384, 1024]);  mm_366 = None
    select_49 = torch.ops.aten.select.int(view_1928, 0, 0)
    view_1931 = torch.ops.aten.view.default(view_1930, [1, 384, 384, 4, 4, 64]);  view_1930 = None
    permute_1076 = torch.ops.aten.permute.default(view_1931, [4, 0, 3, 1, 2, 5]);  view_1931 = None
    view_1932 = torch.ops.aten.view.default(permute_1076, [4, 4, 384, 384, 64]);  permute_1076 = None
    unbind_int_88 = torch.ops.aten.unbind.int(view_1932);  view_1932 = None
    getitem_1819 = unbind_int_88[0]
    getitem_1820 = unbind_int_88[1]
    getitem_1821 = unbind_int_88[2]
    getitem_1822 = unbind_int_88[3];  unbind_int_88 = None
    expand_117 = torch.ops.aten.expand.default(select_49, [4, 384, 384, 384]);  select_49 = None
    _scaled_dot_product_efficient_attention_default_66 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1819, getitem_1820, getitem_1821, expand_117, False);  getitem_1819 = getitem_1820 = getitem_1821 = expand_117 = None
    getitem_1823 = _scaled_dot_product_efficient_attention_default_66[0];  _scaled_dot_product_efficient_attention_default_66 = None
    sigmoid_147 = torch.ops.aten.sigmoid.default(getitem_1822);  getitem_1822 = None
    mul_241 = torch.ops.aten.mul.Tensor(getitem_1823, sigmoid_147);  getitem_1823 = sigmoid_147 = None
    view_1933 = torch.ops.aten.view.default(mul_241, [1, 4, 384, 384, 64]);  mul_241 = None
    permute_1077 = torch.ops.aten.permute.default(view_1933, [0, 2, 3, 1, 4]);  view_1933 = None
    clone_180 = torch.ops.aten.clone.default(permute_1077, memory_format = torch.contiguous_format);  permute_1077 = None
    _unsafe_view_154 = torch.ops.aten._unsafe_view.default(clone_180, [1, 384, 384, 256]);  clone_180 = None
    transpose_49 = torch.ops.aten.transpose.int(getitem_1816, 1, 2);  getitem_1816 = None
    _to_copy_1083 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_18_triangle_attention_pair2qkvg2_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_18_triangle_attention_pair2qkvg2_weight = None
    _to_copy_1084 = torch.ops.aten._to_copy.default(transpose_49, dtype = torch.bfloat16);  transpose_49 = None
    t_395 = torch.ops.aten.t.default(_to_copy_1083);  _to_copy_1083 = None
    expand_118 = torch.ops.aten.expand.default(_to_copy_1084, [1, 384, 384, 256]);  _to_copy_1084 = None
    view_1934 = torch.ops.aten.view.default(expand_118, [384, 384, 256]);  expand_118 = None
    expand_119 = torch.ops.aten.expand.default(t_395, [1, 384, 256, 1024]);  t_395 = None
    view_1935 = torch.ops.aten.view.default(expand_119, [384, 256, 1024]);  expand_119 = None
    bmm_164 = torch.ops.aten.bmm.default(view_1934, view_1935);  view_1934 = view_1935 = None
    view_1936 = torch.ops.aten.view.default(bmm_164, [1, 384, 384, 1024]);  bmm_164 = None
    select_50 = torch.ops.aten.select.int(view_1928, 0, 1);  view_1928 = None
    view_1937 = torch.ops.aten.view.default(view_1936, [1, 384, 384, 4, 4, 64]);  view_1936 = None
    permute_1078 = torch.ops.aten.permute.default(view_1937, [4, 0, 3, 1, 2, 5]);  view_1937 = None
    view_1938 = torch.ops.aten.view.default(permute_1078, [4, 4, 384, 384, 64]);  permute_1078 = None
    unbind_int_89 = torch.ops.aten.unbind.int(view_1938);  view_1938 = None
    getitem_1827 = unbind_int_89[0]
    getitem_1828 = unbind_int_89[1]
    getitem_1829 = unbind_int_89[2]
    getitem_1830 = unbind_int_89[3];  unbind_int_89 = None
    expand_120 = torch.ops.aten.expand.default(select_50, [4, 384, 384, 384]);  select_50 = None
    _scaled_dot_product_efficient_attention_default_67 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1827, getitem_1828, getitem_1829, expand_120, False);  getitem_1827 = getitem_1828 = getitem_1829 = expand_120 = None
    getitem_1831 = _scaled_dot_product_efficient_attention_default_67[0];  _scaled_dot_product_efficient_attention_default_67 = None
    sigmoid_148 = torch.ops.aten.sigmoid.default(getitem_1830);  getitem_1830 = None
    mul_242 = torch.ops.aten.mul.Tensor(getitem_1831, sigmoid_148);  getitem_1831 = sigmoid_148 = None
    view_1939 = torch.ops.aten.view.default(mul_242, [1, 4, 384, 384, 64]);  mul_242 = None
    permute_1079 = torch.ops.aten.permute.default(view_1939, [0, 2, 3, 1, 4]);  view_1939 = None
    clone_181 = torch.ops.aten.clone.default(permute_1079, memory_format = torch.contiguous_format);  permute_1079 = None
    _unsafe_view_155 = torch.ops.aten._unsafe_view.default(clone_181, [1, 384, 384, 256]);  clone_181 = None
    cat_30 = torch.ops.aten.cat.default([_unsafe_view_154, _unsafe_view_155], dim = -1);  _unsafe_view_154 = _unsafe_view_155 = None
    slice_181 = torch.ops.aten.slice.Tensor(pairformer_stack_blocks_18_triangle_attention_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  pairformer_stack_blocks_18_triangle_attention_out_scalers = None
    unsqueeze_682 = torch.ops.aten.unsqueeze.default(slice_181, 1);  slice_181 = None
    mul_243 = torch.ops.aten.mul.Tensor(pairformer_stack_blocks_18_triangle_attention_linear_out_weight, unsqueeze_682);  pairformer_stack_blocks_18_triangle_attention_linear_out_weight = unsqueeze_682 = None
    _to_copy_1085 = torch.ops.aten._to_copy.default(mul_243, dtype = torch.bfloat16);  mul_243 = None
    t_396 = torch.ops.aten.t.default(_to_copy_1085);  _to_copy_1085 = None
    view_1940 = torch.ops.aten.view.default(cat_30, [147456, 512]);  cat_30 = None
    mm_367 = torch.ops.aten.mm.default(view_1940, t_396);  view_1940 = t_396 = None
    view_1941 = torch.ops.aten.view.default(mm_367, [1, 384, 384, 256]);  mm_367 = None
    add_202 = torch.ops.aten.add.Tensor(add_201, view_1941);  add_201 = view_1941 = None
    split_tensor_196 = torch.ops.aten.split.Tensor(add_195, 384, dim = -2)
    getitem_1835 = split_tensor_196[0];  split_tensor_196 = None
    _to_copy_1086 = torch.ops.aten._to_copy.default(getitem_1835, dtype = torch.float32);  getitem_1835 = None
    native_layer_norm_default_223 = torch.ops.aten.native_layer_norm.default(_to_copy_1086, [256], pairformer_stack_blocks_18_transition_pair_layer_norm_weight, pairformer_stack_blocks_18_transition_pair_layer_norm_bias, 1e-05);  _to_copy_1086 = pairformer_stack_blocks_18_transition_pair_layer_norm_weight = pairformer_stack_blocks_18_transition_pair_layer_norm_bias = None
    getitem_1836 = native_layer_norm_default_223[0];  native_layer_norm_default_223 = None
    _to_copy_1087 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_18_transition_pair_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_18_transition_pair_linear_no_bias_ab_weight = None
    _to_copy_1088 = torch.ops.aten._to_copy.default(getitem_1836, dtype = torch.bfloat16);  getitem_1836 = None
    t_397 = torch.ops.aten.t.default(_to_copy_1087);  _to_copy_1087 = None
    view_1942 = torch.ops.aten.view.default(_to_copy_1088, [147456, 256]);  _to_copy_1088 = None
    mm_368 = torch.ops.aten.mm.default(view_1942, t_397);  view_1942 = t_397 = None
    view_1943 = torch.ops.aten.view.default(mm_368, [1, 384, 384, 1024]);  mm_368 = None
    split_tensor_197 = torch.ops.aten.split.Tensor(view_1943, 512, dim = -1);  view_1943 = None
    getitem_1839 = split_tensor_197[0]
    getitem_1840 = split_tensor_197[1];  split_tensor_197 = None
    silu_51 = torch.ops.aten.silu.default(getitem_1839);  getitem_1839 = None
    mul_244 = torch.ops.aten.mul.Tensor(silu_51, getitem_1840);  silu_51 = getitem_1840 = None
    _to_copy_1089 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_18_transition_pair_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_18_transition_pair_linear_out_weight = None
    t_398 = torch.ops.aten.t.default(_to_copy_1089);  _to_copy_1089 = None
    view_1945 = torch.ops.aten.view.default(mul_244, [147456, 512]);  mul_244 = None
    mm_369 = torch.ops.aten.mm.default(view_1945, t_398);  view_1945 = t_398 = None
    view_1946 = torch.ops.aten.view.default(mm_369, [1, 384, 384, 256]);  mm_369 = None
    add_203 = torch.ops.aten.add.Tensor(add_202, view_1946);  add_202 = view_1946 = None
    _to_copy_1090 = torch.ops.aten._to_copy.default(add_199, dtype = torch.float32)
    native_layer_norm_default_224 = torch.ops.aten.native_layer_norm.default(_to_copy_1090, [384], pairformer_stack_blocks_18_attention_pair_bias_single_layer_norm_weight, pairformer_stack_blocks_18_attention_pair_bias_single_layer_norm_bias, 1e-05);  _to_copy_1090 = pairformer_stack_blocks_18_attention_pair_bias_single_layer_norm_weight = pairformer_stack_blocks_18_attention_pair_bias_single_layer_norm_bias = None
    getitem_1841 = native_layer_norm_default_224[0];  native_layer_norm_default_224 = None
    _to_copy_1091 = torch.ops.aten._to_copy.default(add_195, dtype = torch.float32);  add_195 = None
    native_layer_norm_default_225 = torch.ops.aten.native_layer_norm.default(_to_copy_1091, [256], pairformer_stack_blocks_18_attention_pair_bias_pair_layer_norm_weight, pairformer_stack_blocks_18_attention_pair_bias_pair_layer_norm_bias, 1e-05);  _to_copy_1091 = pairformer_stack_blocks_18_attention_pair_bias_pair_layer_norm_weight = pairformer_stack_blocks_18_attention_pair_bias_pair_layer_norm_bias = None
    getitem_1844 = native_layer_norm_default_225[0];  native_layer_norm_default_225 = None
    _to_copy_1092 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_18_attention_pair_bias_pair_linear_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_18_attention_pair_bias_pair_linear_weight = None
    _to_copy_1093 = torch.ops.aten._to_copy.default(getitem_1844, dtype = torch.bfloat16);  getitem_1844 = None
    t_399 = torch.ops.aten.t.default(_to_copy_1092);  _to_copy_1092 = None
    view_1947 = torch.ops.aten.view.default(_to_copy_1093, [147456, 256]);  _to_copy_1093 = None
    mm_370 = torch.ops.aten.mm.default(view_1947, t_399);  view_1947 = t_399 = None
    view_1948 = torch.ops.aten.view.default(mm_370, [1, 384, 384, 16]);  mm_370 = None
    permute_1080 = torch.ops.aten.permute.default(view_1948, [0, 3, 1, 2]);  view_1948 = None
    view_1949 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384])
    bitwise_not_121 = torch.ops.aten.bitwise_not.default(view_1949);  view_1949 = None
    masked_fill_121 = torch.ops.aten.masked_fill.Scalar(permute_1080, bitwise_not_121, -10000);  permute_1080 = bitwise_not_121 = None
    _to_copy_1094 = torch.ops.aten._to_copy.default(getitem_1841, dtype = torch.bfloat16);  getitem_1841 = None
    _to_copy_1095 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_18_attention_pair_bias_attention_input2qkvg_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_18_attention_pair_bias_attention_input2qkvg_weight = None
    unsqueeze_683 = torch.ops.aten.unsqueeze.default(_to_copy_1094, 3);  _to_copy_1094 = None
    unsqueeze_684 = torch.ops.aten.unsqueeze.default(unsqueeze_683, 4);  unsqueeze_683 = None
    unsqueeze_685 = torch.ops.aten.unsqueeze.default(unsqueeze_684, 5);  unsqueeze_684 = None
    permute_1081 = torch.ops.aten.permute.default(unsqueeze_685, [3, 0, 4, 1, 5, 2]);  unsqueeze_685 = None
    unsqueeze_686 = torch.ops.aten.unsqueeze.default(_to_copy_1095, 4);  _to_copy_1095 = None
    unsqueeze_687 = torch.ops.aten.unsqueeze.default(unsqueeze_686, 5);  unsqueeze_686 = None
    permute_1082 = torch.ops.aten.permute.default(unsqueeze_687, [1, 4, 2, 5, 3, 0]);  unsqueeze_687 = None
    permute_1083 = torch.ops.aten.permute.default(permute_1081, [3, 5, 0, 1, 2, 4]);  permute_1081 = None
    view_1950 = torch.ops.aten.view.default(permute_1083, [1, 384, 384]);  permute_1083 = None
    permute_1084 = torch.ops.aten.permute.default(permute_1082, [5, 0, 1, 2, 4, 3]);  permute_1082 = None
    view_1951 = torch.ops.aten.view.default(permute_1084, [1, 384, 1536]);  permute_1084 = None
    bmm_165 = torch.ops.aten.bmm.default(view_1950, view_1951);  view_1950 = view_1951 = None
    view_1952 = torch.ops.aten.view.default(bmm_165, [384, 1, 4, 1, 16, 24]);  bmm_165 = None
    permute_1085 = torch.ops.aten.permute.default(view_1952, [2, 3, 4, 0, 5, 1]);  view_1952 = None
    view_1953 = torch.ops.aten.view.default(permute_1085, [4, 1, 16, 384, 24]);  permute_1085 = None
    unbind_int_90 = torch.ops.aten.unbind.int(view_1953);  view_1953 = None
    getitem_1847 = unbind_int_90[0]
    getitem_1848 = unbind_int_90[1]
    getitem_1849 = unbind_int_90[2]
    getitem_1850 = unbind_int_90[3];  unbind_int_90 = None
    view_1954 = torch.ops.aten.view.default(pairformer_stack_blocks_18_attention_pair_bias_attention_query_bias, [1, 16, 1, 24]);  pairformer_stack_blocks_18_attention_pair_bias_attention_query_bias = None
    add_204 = torch.ops.aten.add.Tensor(getitem_1847, view_1954);  getitem_1847 = view_1954 = None
    _to_copy_1096 = torch.ops.aten._to_copy.default(add_204, dtype = torch.bfloat16);  add_204 = None
    expand_121 = torch.ops.aten.expand.default(masked_fill_121, [1, 16, 384, 384]);  masked_fill_121 = None
    _scaled_dot_product_efficient_attention_default_68 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_1096, getitem_1848, getitem_1849, expand_121, False);  _to_copy_1096 = getitem_1848 = getitem_1849 = expand_121 = None
    getitem_1851 = _scaled_dot_product_efficient_attention_default_68[0];  _scaled_dot_product_efficient_attention_default_68 = None
    add_205 = torch.ops.aten.add.Tensor(getitem_1850, 1);  getitem_1850 = None
    sigmoid_149 = torch.ops.aten.sigmoid.default(add_205);  add_205 = None
    mul_245 = torch.ops.aten.mul.Tensor(getitem_1851, sigmoid_149);  getitem_1851 = sigmoid_149 = None
    _to_copy_1097 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_18_attention_pair_bias_attention_output_proj_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_18_attention_pair_bias_attention_output_proj_weight = None
    unsqueeze_688 = torch.ops.aten.unsqueeze.default(mul_245, 4);  mul_245 = None
    permute_1086 = torch.ops.aten.permute.default(unsqueeze_688, [0, 2, 4, 3, 1]);  unsqueeze_688 = None
    unsqueeze_689 = torch.ops.aten.unsqueeze.default(_to_copy_1097, 3);  _to_copy_1097 = None
    unsqueeze_690 = torch.ops.aten.unsqueeze.default(unsqueeze_689, 4);  unsqueeze_689 = None
    permute_1087 = torch.ops.aten.permute.default(unsqueeze_690, [3, 4, 2, 1, 0]);  unsqueeze_690 = None
    permute_1088 = torch.ops.aten.permute.default(permute_1086, [1, 3, 4, 0, 2]);  permute_1086 = None
    clone_182 = torch.ops.aten.clone.default(permute_1088, memory_format = torch.contiguous_format);  permute_1088 = None
    _unsafe_view_156 = torch.ops.aten._unsafe_view.default(clone_182, [1, 384, 384]);  clone_182 = None
    permute_1089 = torch.ops.aten.permute.default(permute_1087, [3, 4, 0, 2, 1]);  permute_1087 = None
    clone_183 = torch.ops.aten.clone.default(permute_1089, memory_format = torch.contiguous_format);  permute_1089 = None
    _unsafe_view_157 = torch.ops.aten._unsafe_view.default(clone_183, [1, 384, 384]);  clone_183 = None
    bmm_166 = torch.ops.aten.bmm.default(_unsafe_view_156, _unsafe_view_157);  _unsafe_view_156 = _unsafe_view_157 = None
    view_1955 = torch.ops.aten.view.default(bmm_166, [384, 1, 1, 1, 384]);  bmm_166 = None
    permute_1090 = torch.ops.aten.permute.default(view_1955, [3, 0, 4, 1, 2]);  view_1955 = None
    view_1956 = torch.ops.aten.view.default(permute_1090, [1, 384, 384]);  permute_1090 = None
    unsqueeze_691 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
    mul_246 = torch.ops.aten.mul.Tensor(view_1956, unsqueeze_691);  view_1956 = unsqueeze_691 = None
    add_206 = torch.ops.aten.add.Tensor(add_199, mul_246);  mul_246 = None
    split_tensor_198 = torch.ops.aten.split.Tensor(add_199, 384, dim = -2);  add_199 = None
    getitem_1855 = split_tensor_198[0];  split_tensor_198 = None
    _to_copy_1098 = torch.ops.aten._to_copy.default(getitem_1855, dtype = torch.float32);  getitem_1855 = None
    native_layer_norm_default_226 = torch.ops.aten.native_layer_norm.default(_to_copy_1098, [384], pairformer_stack_blocks_18_transition_single_layer_norm_weight, pairformer_stack_blocks_18_transition_single_layer_norm_bias, 1e-05);  _to_copy_1098 = pairformer_stack_blocks_18_transition_single_layer_norm_weight = pairformer_stack_blocks_18_transition_single_layer_norm_bias = None
    getitem_1856 = native_layer_norm_default_226[0];  native_layer_norm_default_226 = None
    _to_copy_1099 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_18_transition_single_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_18_transition_single_linear_no_bias_ab_weight = None
    _to_copy_1100 = torch.ops.aten._to_copy.default(getitem_1856, dtype = torch.bfloat16);  getitem_1856 = None
    t_400 = torch.ops.aten.t.default(_to_copy_1099);  _to_copy_1099 = None
    view_1957 = torch.ops.aten.view.default(_to_copy_1100, [384, 384]);  _to_copy_1100 = None
    mm_371 = torch.ops.aten.mm.default(view_1957, t_400);  view_1957 = t_400 = None
    view_1958 = torch.ops.aten.view.default(mm_371, [1, 384, 1536]);  mm_371 = None
    split_tensor_199 = torch.ops.aten.split.Tensor(view_1958, 768, dim = -1);  view_1958 = None
    getitem_1859 = split_tensor_199[0]
    getitem_1860 = split_tensor_199[1];  split_tensor_199 = None
    silu_52 = torch.ops.aten.silu.default(getitem_1859);  getitem_1859 = None
    mul_247 = torch.ops.aten.mul.Tensor(silu_52, getitem_1860);  silu_52 = getitem_1860 = None
    _to_copy_1101 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_18_transition_single_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_18_transition_single_linear_out_weight = None
    t_401 = torch.ops.aten.t.default(_to_copy_1101);  _to_copy_1101 = None
    view_1960 = torch.ops.aten.view.default(mul_247, [384, 768]);  mul_247 = None
    mm_372 = torch.ops.aten.mm.default(view_1960, t_401);  view_1960 = t_401 = None
    view_1961 = torch.ops.aten.view.default(mm_372, [1, 384, 384]);  mm_372 = None
    add_207 = torch.ops.aten.add.Tensor(add_206, view_1961);  add_206 = view_1961 = None
    _to_copy_1102 = torch.ops.aten._to_copy.default(add_203, dtype = torch.float32)
    native_layer_norm_default_227 = torch.ops.aten.native_layer_norm.default(_to_copy_1102, [256], pairformer_stack_blocks_19_triangle_multiplication_layernorm_z_in_weight, pairformer_stack_blocks_19_triangle_multiplication_layernorm_z_in_bias, 1e-05);  _to_copy_1102 = pairformer_stack_blocks_19_triangle_multiplication_layernorm_z_in_weight = pairformer_stack_blocks_19_triangle_multiplication_layernorm_z_in_bias = None
    getitem_1861 = native_layer_norm_default_227[0];  native_layer_norm_default_227 = None
    split_with_sizes_default_50 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_19_triangle_multiplication_merged_linear_p_weight, [512, 512]);  pairformer_stack_blocks_19_triangle_multiplication_merged_linear_p_weight = None
    getitem_1864 = split_with_sizes_default_50[0]
    getitem_1865 = split_with_sizes_default_50[1];  split_with_sizes_default_50 = None
    split_with_sizes_default_51 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_19_triangle_multiplication_merged_linear_g_weight, [512, 512, 256]);  pairformer_stack_blocks_19_triangle_multiplication_merged_linear_g_weight = None
    getitem_1866 = split_with_sizes_default_51[0]
    getitem_1867 = split_with_sizes_default_51[1]
    getitem_1868 = split_with_sizes_default_51[2];  split_with_sizes_default_51 = None
    _to_copy_1103 = torch.ops.aten._to_copy.default(getitem_1864, dtype = torch.bfloat16);  getitem_1864 = None
    _to_copy_1104 = torch.ops.aten._to_copy.default(getitem_1861, dtype = torch.bfloat16)
    t_402 = torch.ops.aten.t.default(_to_copy_1103);  _to_copy_1103 = None
    view_1962 = torch.ops.aten.view.default(_to_copy_1104, [147456, 256]);  _to_copy_1104 = None
    mm_373 = torch.ops.aten.mm.default(view_1962, t_402);  view_1962 = t_402 = None
    view_1963 = torch.ops.aten.view.default(mm_373, [1, 384, 384, 512]);  mm_373 = None
    _to_copy_1105 = torch.ops.aten._to_copy.default(getitem_1866, dtype = torch.bfloat16);  getitem_1866 = None
    _to_copy_1106 = torch.ops.aten._to_copy.default(getitem_1861, dtype = torch.bfloat16)
    t_403 = torch.ops.aten.t.default(_to_copy_1105);  _to_copy_1105 = None
    view_1964 = torch.ops.aten.view.default(_to_copy_1106, [147456, 256]);  _to_copy_1106 = None
    mm_374 = torch.ops.aten.mm.default(view_1964, t_403);  view_1964 = t_403 = None
    view_1965 = torch.ops.aten.view.default(mm_374, [1, 384, 384, 512]);  mm_374 = None
    sigmoid_150 = torch.ops.aten.sigmoid.default(view_1965);  view_1965 = None
    mul_248 = torch.ops.aten.mul.Tensor(view_1963, sigmoid_150);  view_1963 = sigmoid_150 = None
    unsqueeze_692 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
    bitwise_not_122 = torch.ops.aten.bitwise_not.default(unsqueeze_692);  unsqueeze_692 = None
    masked_fill_122 = torch.ops.aten.masked_fill.Scalar(mul_248, bitwise_not_122, 0);  mul_248 = bitwise_not_122 = None
    split_tensor_200 = torch.ops.aten.split.Tensor(masked_fill_122, 256, dim = -1)
    getitem_1871 = split_tensor_200[0];  split_tensor_200 = None
    unsqueeze_695 = torch.ops.aten.unsqueeze.default(getitem_1871, 4);  getitem_1871 = None
    permute_1095 = torch.ops.aten.permute.default(unsqueeze_695, [0, 1, 4, 3, 2]);  unsqueeze_695 = None
    permute_1096 = torch.ops.aten.permute.default(permute_1095, [3, 1, 4, 0, 2]);  permute_1095 = None
    view_1968 = torch.ops.aten.view.default(permute_1096, [256, 384, 384]);  permute_1096 = None
    split_tensor_201 = torch.ops.aten.split.Tensor(masked_fill_122, 256, dim = -1);  masked_fill_122 = None
    getitem_1874 = split_tensor_201[1];  split_tensor_201 = None
    unsqueeze_696 = torch.ops.aten.unsqueeze.default(getitem_1874, 4);  getitem_1874 = None
    permute_1097 = torch.ops.aten.permute.default(unsqueeze_696, [0, 4, 1, 3, 2]);  unsqueeze_696 = None
    permute_1098 = torch.ops.aten.permute.default(permute_1097, [3, 4, 0, 2, 1]);  permute_1097 = None
    view_1969 = torch.ops.aten.view.default(permute_1098, [256, 384, 384]);  permute_1098 = None
    bmm_167 = torch.ops.aten.bmm.default(view_1968, view_1969);  view_1968 = view_1969 = None
    view_1970 = torch.ops.aten.view.default(bmm_167, [256, 384, 1, 1, 384]);  bmm_167 = None
    permute_1099 = torch.ops.aten.permute.default(view_1970, [3, 1, 4, 0, 2]);  view_1970 = None
    view_1971 = torch.ops.aten.view.default(permute_1099, [1, 384, 384, 256]);  permute_1099 = None
    _to_copy_1107 = torch.ops.aten._to_copy.default(getitem_1865, dtype = torch.bfloat16);  getitem_1865 = None
    _to_copy_1108 = torch.ops.aten._to_copy.default(getitem_1861, dtype = torch.bfloat16)
    t_404 = torch.ops.aten.t.default(_to_copy_1107);  _to_copy_1107 = None
    view_1972 = torch.ops.aten.view.default(_to_copy_1108, [147456, 256]);  _to_copy_1108 = None
    mm_375 = torch.ops.aten.mm.default(view_1972, t_404);  view_1972 = t_404 = None
    view_1973 = torch.ops.aten.view.default(mm_375, [1, 384, 384, 512]);  mm_375 = None
    _to_copy_1109 = torch.ops.aten._to_copy.default(getitem_1867, dtype = torch.bfloat16);  getitem_1867 = None
    _to_copy_1110 = torch.ops.aten._to_copy.default(getitem_1861, dtype = torch.bfloat16)
    t_405 = torch.ops.aten.t.default(_to_copy_1109);  _to_copy_1109 = None
    view_1974 = torch.ops.aten.view.default(_to_copy_1110, [147456, 256]);  _to_copy_1110 = None
    mm_376 = torch.ops.aten.mm.default(view_1974, t_405);  view_1974 = t_405 = None
    view_1975 = torch.ops.aten.view.default(mm_376, [1, 384, 384, 512]);  mm_376 = None
    sigmoid_151 = torch.ops.aten.sigmoid.default(view_1975);  view_1975 = None
    mul_249 = torch.ops.aten.mul.Tensor(view_1973, sigmoid_151);  view_1973 = sigmoid_151 = None
    view_1976 = torch.ops.aten.view.default(mul_249, [147456, 512]);  mul_249 = None
    view_1977 = torch.ops.aten.view.default(view_1976, [1, 384, 384, 512]);  view_1976 = None
    transpose_50 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
    unsqueeze_697 = torch.ops.aten.unsqueeze.default(transpose_50, 3);  transpose_50 = None
    clone_184 = torch.ops.aten.clone.default(unsqueeze_697, memory_format = torch.contiguous_format);  unsqueeze_697 = None
    bitwise_not_123 = torch.ops.aten.bitwise_not.default(clone_184);  clone_184 = None
    masked_fill_123 = torch.ops.aten.masked_fill.Scalar(view_1977, bitwise_not_123, 0);  view_1977 = bitwise_not_123 = None
    view_1978 = torch.ops.aten.view.default(masked_fill_123, [147456, 512]);  masked_fill_123 = None
    view_1982 = torch.ops.aten.view.default(view_1978, [1, 384, 384, 512])
    split_tensor_202 = torch.ops.aten.split.Tensor(view_1982, 256, dim = -1);  view_1982 = None
    getitem_1877 = split_tensor_202[0];  split_tensor_202 = None
    unsqueeze_700 = torch.ops.aten.unsqueeze.default(getitem_1877, 4);  getitem_1877 = None
    permute_1104 = torch.ops.aten.permute.default(unsqueeze_700, [0, 2, 4, 3, 1]);  unsqueeze_700 = None
    permute_1105 = torch.ops.aten.permute.default(permute_1104, [3, 1, 4, 0, 2]);  permute_1104 = None
    view_1983 = torch.ops.aten.view.default(permute_1105, [256, 384, 384]);  permute_1105 = None
    view_1984 = torch.ops.aten.view.default(view_1978, [1, 384, 384, 512]);  view_1978 = None
    split_tensor_203 = torch.ops.aten.split.Tensor(view_1984, 256, dim = -1);  view_1984 = None
    getitem_1880 = split_tensor_203[1];  split_tensor_203 = None
    unsqueeze_701 = torch.ops.aten.unsqueeze.default(getitem_1880, 4);  getitem_1880 = None
    permute_1106 = torch.ops.aten.permute.default(unsqueeze_701, [0, 4, 2, 3, 1]);  unsqueeze_701 = None
    permute_1107 = torch.ops.aten.permute.default(permute_1106, [3, 4, 0, 2, 1]);  permute_1106 = None
    view_1985 = torch.ops.aten.view.default(permute_1107, [256, 384, 384]);  permute_1107 = None
    bmm_168 = torch.ops.aten.bmm.default(view_1983, view_1985);  view_1983 = view_1985 = None
    view_1986 = torch.ops.aten.view.default(bmm_168, [256, 384, 1, 1, 384]);  bmm_168 = None
    permute_1108 = torch.ops.aten.permute.default(view_1986, [3, 1, 4, 0, 2]);  view_1986 = None
    view_1987 = torch.ops.aten.view.default(permute_1108, [1, 384, 384, 256]);  permute_1108 = None
    _to_copy_1111 = torch.ops.aten._to_copy.default(view_1971, dtype = torch.float32);  view_1971 = None
    native_layer_norm_default_228 = torch.ops.aten.native_layer_norm.default(_to_copy_1111, [256], None, None, 1e-05);  _to_copy_1111 = None
    getitem_1881 = native_layer_norm_default_228[0];  native_layer_norm_default_228 = None
    _to_copy_1112 = torch.ops.aten._to_copy.default(view_1987, dtype = torch.float32);  view_1987 = None
    native_layer_norm_default_229 = torch.ops.aten.native_layer_norm.default(_to_copy_1112, [256], None, None, 1e-05);  _to_copy_1112 = None
    getitem_1884 = native_layer_norm_default_229[0];  native_layer_norm_default_229 = None
    add_208 = torch.ops.aten.add.Tensor(getitem_1881, getitem_1884);  getitem_1881 = getitem_1884 = None
    _to_copy_1113 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_19_triangle_multiplication_linear_z_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_19_triangle_multiplication_linear_z_out_weight = None
    _to_copy_1114 = torch.ops.aten._to_copy.default(add_208, dtype = torch.bfloat16);  add_208 = None
    t_406 = torch.ops.aten.t.default(_to_copy_1113);  _to_copy_1113 = None
    view_1988 = torch.ops.aten.view.default(_to_copy_1114, [147456, 256]);  _to_copy_1114 = None
    mm_377 = torch.ops.aten.mm.default(view_1988, t_406);  view_1988 = t_406 = None
    view_1989 = torch.ops.aten.view.default(mm_377, [1, 384, 384, 256]);  mm_377 = None
    _to_copy_1115 = torch.ops.aten._to_copy.default(getitem_1868, dtype = torch.bfloat16);  getitem_1868 = None
    _to_copy_1116 = torch.ops.aten._to_copy.default(getitem_1861, dtype = torch.bfloat16);  getitem_1861 = None
    t_407 = torch.ops.aten.t.default(_to_copy_1115);  _to_copy_1115 = None
    view_1990 = torch.ops.aten.view.default(_to_copy_1116, [147456, 256]);  _to_copy_1116 = None
    mm_378 = torch.ops.aten.mm.default(view_1990, t_407);  view_1990 = t_407 = None
    view_1991 = torch.ops.aten.view.default(mm_378, [1, 384, 384, 256]);  mm_378 = None
    sigmoid_152 = torch.ops.aten.sigmoid.default(view_1991);  view_1991 = None
    mul_250 = torch.ops.aten.mul.Tensor(view_1989, sigmoid_152);  view_1989 = sigmoid_152 = None
    add_209 = torch.ops.aten.add.Tensor(add_203, mul_250);  mul_250 = None
    _to_copy_1117 = torch.ops.aten._to_copy.default(add_203, dtype = torch.float32)
    native_layer_norm_default_230 = torch.ops.aten.native_layer_norm.default(_to_copy_1117, [256], None, None, 1e-05);  _to_copy_1117 = None
    getitem_1887 = native_layer_norm_default_230[0];  native_layer_norm_default_230 = None
    _to_copy_1118 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_19_triangle_attention_pair2b_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_19_triangle_attention_pair2b_weight = None
    _to_copy_1119 = torch.ops.aten._to_copy.default(getitem_1887, dtype = torch.bfloat16)
    t_408 = torch.ops.aten.t.default(_to_copy_1118);  _to_copy_1118 = None
    view_1992 = torch.ops.aten.view.default(_to_copy_1119, [147456, 256]);  _to_copy_1119 = None
    mm_379 = torch.ops.aten.mm.default(view_1992, t_408);  view_1992 = t_408 = None
    view_1993 = torch.ops.aten.view.default(mm_379, [1, 384, 384, 8]);  mm_379 = None
    view_1994 = torch.ops.aten.view.default(view_1993, [1, 384, 384, 2, 4]);  view_1993 = None
    permute_1109 = torch.ops.aten.permute.default(view_1994, [0, 3, 4, 1, 2]);  view_1994 = None
    view_1995 = torch.ops.aten.view.default(permute_1109, [1, 2, 4, 1, 384, 384]);  permute_1109 = None
    view_1996 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 384, 384])
    bitwise_not_124 = torch.ops.aten.bitwise_not.default(view_1996);  view_1996 = None
    masked_fill_124 = torch.ops.aten.masked_fill.Scalar(view_1995, bitwise_not_124, -10000);  view_1995 = bitwise_not_124 = None
    view_1997 = torch.ops.aten.view.default(masked_fill_124, [1, 2, 4, 384, 384]);  masked_fill_124 = None
    permute_1110 = torch.ops.aten.permute.default(view_1997, [1, 0, 2, 3, 4]);  view_1997 = None
    view_1998 = torch.ops.aten.view.default(permute_1110, [2, 4, 1, 384, 384]);  permute_1110 = None
    _to_copy_1120 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_19_triangle_attention_pair2qkvg1_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_19_triangle_attention_pair2qkvg1_weight = None
    _to_copy_1121 = torch.ops.aten._to_copy.default(getitem_1887, dtype = torch.bfloat16)
    t_409 = torch.ops.aten.t.default(_to_copy_1120);  _to_copy_1120 = None
    view_1999 = torch.ops.aten.view.default(_to_copy_1121, [147456, 256]);  _to_copy_1121 = None
    mm_380 = torch.ops.aten.mm.default(view_1999, t_409);  view_1999 = t_409 = None
    view_2000 = torch.ops.aten.view.default(mm_380, [1, 384, 384, 1024]);  mm_380 = None
    select_51 = torch.ops.aten.select.int(view_1998, 0, 0)
    view_2001 = torch.ops.aten.view.default(view_2000, [1, 384, 384, 4, 4, 64]);  view_2000 = None
    permute_1111 = torch.ops.aten.permute.default(view_2001, [4, 0, 3, 1, 2, 5]);  view_2001 = None
    view_2002 = torch.ops.aten.view.default(permute_1111, [4, 4, 384, 384, 64]);  permute_1111 = None
    unbind_int_91 = torch.ops.aten.unbind.int(view_2002);  view_2002 = None
    getitem_1890 = unbind_int_91[0]
    getitem_1891 = unbind_int_91[1]
    getitem_1892 = unbind_int_91[2]
    getitem_1893 = unbind_int_91[3];  unbind_int_91 = None
    expand_122 = torch.ops.aten.expand.default(select_51, [4, 384, 384, 384]);  select_51 = None
    _scaled_dot_product_efficient_attention_default_69 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1890, getitem_1891, getitem_1892, expand_122, False);  getitem_1890 = getitem_1891 = getitem_1892 = expand_122 = None
    getitem_1894 = _scaled_dot_product_efficient_attention_default_69[0];  _scaled_dot_product_efficient_attention_default_69 = None
    sigmoid_153 = torch.ops.aten.sigmoid.default(getitem_1893);  getitem_1893 = None
    mul_251 = torch.ops.aten.mul.Tensor(getitem_1894, sigmoid_153);  getitem_1894 = sigmoid_153 = None
    view_2003 = torch.ops.aten.view.default(mul_251, [1, 4, 384, 384, 64]);  mul_251 = None
    permute_1112 = torch.ops.aten.permute.default(view_2003, [0, 2, 3, 1, 4]);  view_2003 = None
    clone_185 = torch.ops.aten.clone.default(permute_1112, memory_format = torch.contiguous_format);  permute_1112 = None
    _unsafe_view_158 = torch.ops.aten._unsafe_view.default(clone_185, [1, 384, 384, 256]);  clone_185 = None
    transpose_51 = torch.ops.aten.transpose.int(getitem_1887, 1, 2);  getitem_1887 = None
    _to_copy_1122 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_19_triangle_attention_pair2qkvg2_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_19_triangle_attention_pair2qkvg2_weight = None
    _to_copy_1123 = torch.ops.aten._to_copy.default(transpose_51, dtype = torch.bfloat16);  transpose_51 = None
    t_410 = torch.ops.aten.t.default(_to_copy_1122);  _to_copy_1122 = None
    expand_123 = torch.ops.aten.expand.default(_to_copy_1123, [1, 384, 384, 256]);  _to_copy_1123 = None
    view_2004 = torch.ops.aten.view.default(expand_123, [384, 384, 256]);  expand_123 = None
    expand_124 = torch.ops.aten.expand.default(t_410, [1, 384, 256, 1024]);  t_410 = None
    view_2005 = torch.ops.aten.view.default(expand_124, [384, 256, 1024]);  expand_124 = None
    bmm_169 = torch.ops.aten.bmm.default(view_2004, view_2005);  view_2004 = view_2005 = None
    view_2006 = torch.ops.aten.view.default(bmm_169, [1, 384, 384, 1024]);  bmm_169 = None
    select_52 = torch.ops.aten.select.int(view_1998, 0, 1);  view_1998 = None
    view_2007 = torch.ops.aten.view.default(view_2006, [1, 384, 384, 4, 4, 64]);  view_2006 = None
    permute_1113 = torch.ops.aten.permute.default(view_2007, [4, 0, 3, 1, 2, 5]);  view_2007 = None
    view_2008 = torch.ops.aten.view.default(permute_1113, [4, 4, 384, 384, 64]);  permute_1113 = None
    unbind_int_92 = torch.ops.aten.unbind.int(view_2008);  view_2008 = None
    getitem_1898 = unbind_int_92[0]
    getitem_1899 = unbind_int_92[1]
    getitem_1900 = unbind_int_92[2]
    getitem_1901 = unbind_int_92[3];  unbind_int_92 = None
    expand_125 = torch.ops.aten.expand.default(select_52, [4, 384, 384, 384]);  select_52 = None
    _scaled_dot_product_efficient_attention_default_70 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1898, getitem_1899, getitem_1900, expand_125, False);  getitem_1898 = getitem_1899 = getitem_1900 = expand_125 = None
    getitem_1902 = _scaled_dot_product_efficient_attention_default_70[0];  _scaled_dot_product_efficient_attention_default_70 = None
    sigmoid_154 = torch.ops.aten.sigmoid.default(getitem_1901);  getitem_1901 = None
    mul_252 = torch.ops.aten.mul.Tensor(getitem_1902, sigmoid_154);  getitem_1902 = sigmoid_154 = None
    view_2009 = torch.ops.aten.view.default(mul_252, [1, 4, 384, 384, 64]);  mul_252 = None
    permute_1114 = torch.ops.aten.permute.default(view_2009, [0, 2, 3, 1, 4]);  view_2009 = None
    clone_186 = torch.ops.aten.clone.default(permute_1114, memory_format = torch.contiguous_format);  permute_1114 = None
    _unsafe_view_159 = torch.ops.aten._unsafe_view.default(clone_186, [1, 384, 384, 256]);  clone_186 = None
    cat_31 = torch.ops.aten.cat.default([_unsafe_view_158, _unsafe_view_159], dim = -1);  _unsafe_view_158 = _unsafe_view_159 = None
    slice_182 = torch.ops.aten.slice.Tensor(pairformer_stack_blocks_19_triangle_attention_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  pairformer_stack_blocks_19_triangle_attention_out_scalers = None
    unsqueeze_702 = torch.ops.aten.unsqueeze.default(slice_182, 1);  slice_182 = None
    mul_253 = torch.ops.aten.mul.Tensor(pairformer_stack_blocks_19_triangle_attention_linear_out_weight, unsqueeze_702);  pairformer_stack_blocks_19_triangle_attention_linear_out_weight = unsqueeze_702 = None
    _to_copy_1124 = torch.ops.aten._to_copy.default(mul_253, dtype = torch.bfloat16);  mul_253 = None
    t_411 = torch.ops.aten.t.default(_to_copy_1124);  _to_copy_1124 = None
    view_2010 = torch.ops.aten.view.default(cat_31, [147456, 512]);  cat_31 = None
    mm_381 = torch.ops.aten.mm.default(view_2010, t_411);  view_2010 = t_411 = None
    view_2011 = torch.ops.aten.view.default(mm_381, [1, 384, 384, 256]);  mm_381 = None
    add_210 = torch.ops.aten.add.Tensor(add_209, view_2011);  add_209 = view_2011 = None
    split_tensor_204 = torch.ops.aten.split.Tensor(add_203, 384, dim = -2)
    getitem_1906 = split_tensor_204[0];  split_tensor_204 = None
    _to_copy_1125 = torch.ops.aten._to_copy.default(getitem_1906, dtype = torch.float32);  getitem_1906 = None
    native_layer_norm_default_231 = torch.ops.aten.native_layer_norm.default(_to_copy_1125, [256], pairformer_stack_blocks_19_transition_pair_layer_norm_weight, pairformer_stack_blocks_19_transition_pair_layer_norm_bias, 1e-05);  _to_copy_1125 = pairformer_stack_blocks_19_transition_pair_layer_norm_weight = pairformer_stack_blocks_19_transition_pair_layer_norm_bias = None
    getitem_1907 = native_layer_norm_default_231[0];  native_layer_norm_default_231 = None
    _to_copy_1126 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_19_transition_pair_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_19_transition_pair_linear_no_bias_ab_weight = None
    _to_copy_1127 = torch.ops.aten._to_copy.default(getitem_1907, dtype = torch.bfloat16);  getitem_1907 = None
    t_412 = torch.ops.aten.t.default(_to_copy_1126);  _to_copy_1126 = None
    view_2012 = torch.ops.aten.view.default(_to_copy_1127, [147456, 256]);  _to_copy_1127 = None
    mm_382 = torch.ops.aten.mm.default(view_2012, t_412);  view_2012 = t_412 = None
    view_2013 = torch.ops.aten.view.default(mm_382, [1, 384, 384, 1024]);  mm_382 = None
    split_tensor_205 = torch.ops.aten.split.Tensor(view_2013, 512, dim = -1);  view_2013 = None
    getitem_1910 = split_tensor_205[0]
    getitem_1911 = split_tensor_205[1];  split_tensor_205 = None
    silu_53 = torch.ops.aten.silu.default(getitem_1910);  getitem_1910 = None
    mul_254 = torch.ops.aten.mul.Tensor(silu_53, getitem_1911);  silu_53 = getitem_1911 = None
    _to_copy_1128 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_19_transition_pair_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_19_transition_pair_linear_out_weight = None
    t_413 = torch.ops.aten.t.default(_to_copy_1128);  _to_copy_1128 = None
    view_2015 = torch.ops.aten.view.default(mul_254, [147456, 512]);  mul_254 = None
    mm_383 = torch.ops.aten.mm.default(view_2015, t_413);  view_2015 = t_413 = None
    view_2016 = torch.ops.aten.view.default(mm_383, [1, 384, 384, 256]);  mm_383 = None
    add_211 = torch.ops.aten.add.Tensor(add_210, view_2016);  add_210 = view_2016 = None
    _to_copy_1129 = torch.ops.aten._to_copy.default(add_207, dtype = torch.float32)
    native_layer_norm_default_232 = torch.ops.aten.native_layer_norm.default(_to_copy_1129, [384], pairformer_stack_blocks_19_attention_pair_bias_single_layer_norm_weight, pairformer_stack_blocks_19_attention_pair_bias_single_layer_norm_bias, 1e-05);  _to_copy_1129 = pairformer_stack_blocks_19_attention_pair_bias_single_layer_norm_weight = pairformer_stack_blocks_19_attention_pair_bias_single_layer_norm_bias = None
    getitem_1912 = native_layer_norm_default_232[0];  native_layer_norm_default_232 = None
    _to_copy_1130 = torch.ops.aten._to_copy.default(add_203, dtype = torch.float32);  add_203 = None
    native_layer_norm_default_233 = torch.ops.aten.native_layer_norm.default(_to_copy_1130, [256], pairformer_stack_blocks_19_attention_pair_bias_pair_layer_norm_weight, pairformer_stack_blocks_19_attention_pair_bias_pair_layer_norm_bias, 1e-05);  _to_copy_1130 = pairformer_stack_blocks_19_attention_pair_bias_pair_layer_norm_weight = pairformer_stack_blocks_19_attention_pair_bias_pair_layer_norm_bias = None
    getitem_1915 = native_layer_norm_default_233[0];  native_layer_norm_default_233 = None
    _to_copy_1131 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_19_attention_pair_bias_pair_linear_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_19_attention_pair_bias_pair_linear_weight = None
    _to_copy_1132 = torch.ops.aten._to_copy.default(getitem_1915, dtype = torch.bfloat16);  getitem_1915 = None
    t_414 = torch.ops.aten.t.default(_to_copy_1131);  _to_copy_1131 = None
    view_2017 = torch.ops.aten.view.default(_to_copy_1132, [147456, 256]);  _to_copy_1132 = None
    mm_384 = torch.ops.aten.mm.default(view_2017, t_414);  view_2017 = t_414 = None
    view_2018 = torch.ops.aten.view.default(mm_384, [1, 384, 384, 16]);  mm_384 = None
    permute_1115 = torch.ops.aten.permute.default(view_2018, [0, 3, 1, 2]);  view_2018 = None
    view_2019 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384])
    bitwise_not_125 = torch.ops.aten.bitwise_not.default(view_2019);  view_2019 = None
    masked_fill_125 = torch.ops.aten.masked_fill.Scalar(permute_1115, bitwise_not_125, -10000);  permute_1115 = bitwise_not_125 = None
    _to_copy_1133 = torch.ops.aten._to_copy.default(getitem_1912, dtype = torch.bfloat16);  getitem_1912 = None
    _to_copy_1134 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_19_attention_pair_bias_attention_input2qkvg_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_19_attention_pair_bias_attention_input2qkvg_weight = None
    unsqueeze_703 = torch.ops.aten.unsqueeze.default(_to_copy_1133, 3);  _to_copy_1133 = None
    unsqueeze_704 = torch.ops.aten.unsqueeze.default(unsqueeze_703, 4);  unsqueeze_703 = None
    unsqueeze_705 = torch.ops.aten.unsqueeze.default(unsqueeze_704, 5);  unsqueeze_704 = None
    permute_1116 = torch.ops.aten.permute.default(unsqueeze_705, [3, 0, 4, 1, 5, 2]);  unsqueeze_705 = None
    unsqueeze_706 = torch.ops.aten.unsqueeze.default(_to_copy_1134, 4);  _to_copy_1134 = None
    unsqueeze_707 = torch.ops.aten.unsqueeze.default(unsqueeze_706, 5);  unsqueeze_706 = None
    permute_1117 = torch.ops.aten.permute.default(unsqueeze_707, [1, 4, 2, 5, 3, 0]);  unsqueeze_707 = None
    permute_1118 = torch.ops.aten.permute.default(permute_1116, [3, 5, 0, 1, 2, 4]);  permute_1116 = None
    view_2020 = torch.ops.aten.view.default(permute_1118, [1, 384, 384]);  permute_1118 = None
    permute_1119 = torch.ops.aten.permute.default(permute_1117, [5, 0, 1, 2, 4, 3]);  permute_1117 = None
    view_2021 = torch.ops.aten.view.default(permute_1119, [1, 384, 1536]);  permute_1119 = None
    bmm_170 = torch.ops.aten.bmm.default(view_2020, view_2021);  view_2020 = view_2021 = None
    view_2022 = torch.ops.aten.view.default(bmm_170, [384, 1, 4, 1, 16, 24]);  bmm_170 = None
    permute_1120 = torch.ops.aten.permute.default(view_2022, [2, 3, 4, 0, 5, 1]);  view_2022 = None
    view_2023 = torch.ops.aten.view.default(permute_1120, [4, 1, 16, 384, 24]);  permute_1120 = None
    unbind_int_93 = torch.ops.aten.unbind.int(view_2023);  view_2023 = None
    getitem_1918 = unbind_int_93[0]
    getitem_1919 = unbind_int_93[1]
    getitem_1920 = unbind_int_93[2]
    getitem_1921 = unbind_int_93[3];  unbind_int_93 = None
    view_2024 = torch.ops.aten.view.default(pairformer_stack_blocks_19_attention_pair_bias_attention_query_bias, [1, 16, 1, 24]);  pairformer_stack_blocks_19_attention_pair_bias_attention_query_bias = None
    add_212 = torch.ops.aten.add.Tensor(getitem_1918, view_2024);  getitem_1918 = view_2024 = None
    _to_copy_1135 = torch.ops.aten._to_copy.default(add_212, dtype = torch.bfloat16);  add_212 = None
    expand_126 = torch.ops.aten.expand.default(masked_fill_125, [1, 16, 384, 384]);  masked_fill_125 = None
    _scaled_dot_product_efficient_attention_default_71 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_1135, getitem_1919, getitem_1920, expand_126, False);  _to_copy_1135 = getitem_1919 = getitem_1920 = expand_126 = None
    getitem_1922 = _scaled_dot_product_efficient_attention_default_71[0];  _scaled_dot_product_efficient_attention_default_71 = None
    add_213 = torch.ops.aten.add.Tensor(getitem_1921, 1);  getitem_1921 = None
    sigmoid_155 = torch.ops.aten.sigmoid.default(add_213);  add_213 = None
    mul_255 = torch.ops.aten.mul.Tensor(getitem_1922, sigmoid_155);  getitem_1922 = sigmoid_155 = None
    _to_copy_1136 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_19_attention_pair_bias_attention_output_proj_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_19_attention_pair_bias_attention_output_proj_weight = None
    unsqueeze_708 = torch.ops.aten.unsqueeze.default(mul_255, 4);  mul_255 = None
    permute_1121 = torch.ops.aten.permute.default(unsqueeze_708, [0, 2, 4, 3, 1]);  unsqueeze_708 = None
    unsqueeze_709 = torch.ops.aten.unsqueeze.default(_to_copy_1136, 3);  _to_copy_1136 = None
    unsqueeze_710 = torch.ops.aten.unsqueeze.default(unsqueeze_709, 4);  unsqueeze_709 = None
    permute_1122 = torch.ops.aten.permute.default(unsqueeze_710, [3, 4, 2, 1, 0]);  unsqueeze_710 = None
    permute_1123 = torch.ops.aten.permute.default(permute_1121, [1, 3, 4, 0, 2]);  permute_1121 = None
    clone_187 = torch.ops.aten.clone.default(permute_1123, memory_format = torch.contiguous_format);  permute_1123 = None
    _unsafe_view_160 = torch.ops.aten._unsafe_view.default(clone_187, [1, 384, 384]);  clone_187 = None
    permute_1124 = torch.ops.aten.permute.default(permute_1122, [3, 4, 0, 2, 1]);  permute_1122 = None
    clone_188 = torch.ops.aten.clone.default(permute_1124, memory_format = torch.contiguous_format);  permute_1124 = None
    _unsafe_view_161 = torch.ops.aten._unsafe_view.default(clone_188, [1, 384, 384]);  clone_188 = None
    bmm_171 = torch.ops.aten.bmm.default(_unsafe_view_160, _unsafe_view_161);  _unsafe_view_160 = _unsafe_view_161 = None
    view_2025 = torch.ops.aten.view.default(bmm_171, [384, 1, 1, 1, 384]);  bmm_171 = None
    permute_1125 = torch.ops.aten.permute.default(view_2025, [3, 0, 4, 1, 2]);  view_2025 = None
    view_2026 = torch.ops.aten.view.default(permute_1125, [1, 384, 384]);  permute_1125 = None
    unsqueeze_711 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
    mul_256 = torch.ops.aten.mul.Tensor(view_2026, unsqueeze_711);  view_2026 = unsqueeze_711 = None
    add_214 = torch.ops.aten.add.Tensor(add_207, mul_256);  mul_256 = None
    split_tensor_206 = torch.ops.aten.split.Tensor(add_207, 384, dim = -2);  add_207 = None
    getitem_1926 = split_tensor_206[0];  split_tensor_206 = None
    _to_copy_1137 = torch.ops.aten._to_copy.default(getitem_1926, dtype = torch.float32);  getitem_1926 = None
    native_layer_norm_default_234 = torch.ops.aten.native_layer_norm.default(_to_copy_1137, [384], pairformer_stack_blocks_19_transition_single_layer_norm_weight, pairformer_stack_blocks_19_transition_single_layer_norm_bias, 1e-05);  _to_copy_1137 = pairformer_stack_blocks_19_transition_single_layer_norm_weight = pairformer_stack_blocks_19_transition_single_layer_norm_bias = None
    getitem_1927 = native_layer_norm_default_234[0];  native_layer_norm_default_234 = None
    _to_copy_1138 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_19_transition_single_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_19_transition_single_linear_no_bias_ab_weight = None
    _to_copy_1139 = torch.ops.aten._to_copy.default(getitem_1927, dtype = torch.bfloat16);  getitem_1927 = None
    t_415 = torch.ops.aten.t.default(_to_copy_1138);  _to_copy_1138 = None
    view_2027 = torch.ops.aten.view.default(_to_copy_1139, [384, 384]);  _to_copy_1139 = None
    mm_385 = torch.ops.aten.mm.default(view_2027, t_415);  view_2027 = t_415 = None
    view_2028 = torch.ops.aten.view.default(mm_385, [1, 384, 1536]);  mm_385 = None
    split_tensor_207 = torch.ops.aten.split.Tensor(view_2028, 768, dim = -1);  view_2028 = None
    getitem_1930 = split_tensor_207[0]
    getitem_1931 = split_tensor_207[1];  split_tensor_207 = None
    silu_54 = torch.ops.aten.silu.default(getitem_1930);  getitem_1930 = None
    mul_257 = torch.ops.aten.mul.Tensor(silu_54, getitem_1931);  silu_54 = getitem_1931 = None
    _to_copy_1140 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_19_transition_single_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_19_transition_single_linear_out_weight = None
    t_416 = torch.ops.aten.t.default(_to_copy_1140);  _to_copy_1140 = None
    view_2030 = torch.ops.aten.view.default(mul_257, [384, 768]);  mul_257 = None
    mm_386 = torch.ops.aten.mm.default(view_2030, t_416);  view_2030 = t_416 = None
    view_2031 = torch.ops.aten.view.default(mm_386, [1, 384, 384]);  mm_386 = None
    add_215 = torch.ops.aten.add.Tensor(add_214, view_2031);  add_214 = view_2031 = None
    _to_copy_1141 = torch.ops.aten._to_copy.default(add_211, dtype = torch.float32)
    native_layer_norm_default_235 = torch.ops.aten.native_layer_norm.default(_to_copy_1141, [256], pairformer_stack_blocks_20_triangle_multiplication_layernorm_z_in_weight, pairformer_stack_blocks_20_triangle_multiplication_layernorm_z_in_bias, 1e-05);  _to_copy_1141 = pairformer_stack_blocks_20_triangle_multiplication_layernorm_z_in_weight = pairformer_stack_blocks_20_triangle_multiplication_layernorm_z_in_bias = None
    getitem_1932 = native_layer_norm_default_235[0];  native_layer_norm_default_235 = None
    split_with_sizes_default_52 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_20_triangle_multiplication_merged_linear_p_weight, [512, 512]);  pairformer_stack_blocks_20_triangle_multiplication_merged_linear_p_weight = None
    getitem_1935 = split_with_sizes_default_52[0]
    getitem_1936 = split_with_sizes_default_52[1];  split_with_sizes_default_52 = None
    split_with_sizes_default_53 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_20_triangle_multiplication_merged_linear_g_weight, [512, 512, 256]);  pairformer_stack_blocks_20_triangle_multiplication_merged_linear_g_weight = None
    getitem_1937 = split_with_sizes_default_53[0]
    getitem_1938 = split_with_sizes_default_53[1]
    getitem_1939 = split_with_sizes_default_53[2];  split_with_sizes_default_53 = None
    _to_copy_1142 = torch.ops.aten._to_copy.default(getitem_1935, dtype = torch.bfloat16);  getitem_1935 = None
    _to_copy_1143 = torch.ops.aten._to_copy.default(getitem_1932, dtype = torch.bfloat16)
    t_417 = torch.ops.aten.t.default(_to_copy_1142);  _to_copy_1142 = None
    view_2032 = torch.ops.aten.view.default(_to_copy_1143, [147456, 256]);  _to_copy_1143 = None
    mm_387 = torch.ops.aten.mm.default(view_2032, t_417);  view_2032 = t_417 = None
    view_2033 = torch.ops.aten.view.default(mm_387, [1, 384, 384, 512]);  mm_387 = None
    _to_copy_1144 = torch.ops.aten._to_copy.default(getitem_1937, dtype = torch.bfloat16);  getitem_1937 = None
    _to_copy_1145 = torch.ops.aten._to_copy.default(getitem_1932, dtype = torch.bfloat16)
    t_418 = torch.ops.aten.t.default(_to_copy_1144);  _to_copy_1144 = None
    view_2034 = torch.ops.aten.view.default(_to_copy_1145, [147456, 256]);  _to_copy_1145 = None
    mm_388 = torch.ops.aten.mm.default(view_2034, t_418);  view_2034 = t_418 = None
    view_2035 = torch.ops.aten.view.default(mm_388, [1, 384, 384, 512]);  mm_388 = None
    sigmoid_156 = torch.ops.aten.sigmoid.default(view_2035);  view_2035 = None
    mul_258 = torch.ops.aten.mul.Tensor(view_2033, sigmoid_156);  view_2033 = sigmoid_156 = None
    unsqueeze_712 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
    bitwise_not_126 = torch.ops.aten.bitwise_not.default(unsqueeze_712);  unsqueeze_712 = None
    masked_fill_126 = torch.ops.aten.masked_fill.Scalar(mul_258, bitwise_not_126, 0);  mul_258 = bitwise_not_126 = None
    split_tensor_208 = torch.ops.aten.split.Tensor(masked_fill_126, 256, dim = -1)
    getitem_1942 = split_tensor_208[0];  split_tensor_208 = None
    unsqueeze_715 = torch.ops.aten.unsqueeze.default(getitem_1942, 4);  getitem_1942 = None
    permute_1130 = torch.ops.aten.permute.default(unsqueeze_715, [0, 1, 4, 3, 2]);  unsqueeze_715 = None
    permute_1131 = torch.ops.aten.permute.default(permute_1130, [3, 1, 4, 0, 2]);  permute_1130 = None
    view_2038 = torch.ops.aten.view.default(permute_1131, [256, 384, 384]);  permute_1131 = None
    split_tensor_209 = torch.ops.aten.split.Tensor(masked_fill_126, 256, dim = -1);  masked_fill_126 = None
    getitem_1945 = split_tensor_209[1];  split_tensor_209 = None
    unsqueeze_716 = torch.ops.aten.unsqueeze.default(getitem_1945, 4);  getitem_1945 = None
    permute_1132 = torch.ops.aten.permute.default(unsqueeze_716, [0, 4, 1, 3, 2]);  unsqueeze_716 = None
    permute_1133 = torch.ops.aten.permute.default(permute_1132, [3, 4, 0, 2, 1]);  permute_1132 = None
    view_2039 = torch.ops.aten.view.default(permute_1133, [256, 384, 384]);  permute_1133 = None
    bmm_172 = torch.ops.aten.bmm.default(view_2038, view_2039);  view_2038 = view_2039 = None
    view_2040 = torch.ops.aten.view.default(bmm_172, [256, 384, 1, 1, 384]);  bmm_172 = None
    permute_1134 = torch.ops.aten.permute.default(view_2040, [3, 1, 4, 0, 2]);  view_2040 = None
    view_2041 = torch.ops.aten.view.default(permute_1134, [1, 384, 384, 256]);  permute_1134 = None
    _to_copy_1146 = torch.ops.aten._to_copy.default(getitem_1936, dtype = torch.bfloat16);  getitem_1936 = None
    _to_copy_1147 = torch.ops.aten._to_copy.default(getitem_1932, dtype = torch.bfloat16)
    t_419 = torch.ops.aten.t.default(_to_copy_1146);  _to_copy_1146 = None
    view_2042 = torch.ops.aten.view.default(_to_copy_1147, [147456, 256]);  _to_copy_1147 = None
    mm_389 = torch.ops.aten.mm.default(view_2042, t_419);  view_2042 = t_419 = None
    view_2043 = torch.ops.aten.view.default(mm_389, [1, 384, 384, 512]);  mm_389 = None
    _to_copy_1148 = torch.ops.aten._to_copy.default(getitem_1938, dtype = torch.bfloat16);  getitem_1938 = None
    _to_copy_1149 = torch.ops.aten._to_copy.default(getitem_1932, dtype = torch.bfloat16)
    t_420 = torch.ops.aten.t.default(_to_copy_1148);  _to_copy_1148 = None
    view_2044 = torch.ops.aten.view.default(_to_copy_1149, [147456, 256]);  _to_copy_1149 = None
    mm_390 = torch.ops.aten.mm.default(view_2044, t_420);  view_2044 = t_420 = None
    view_2045 = torch.ops.aten.view.default(mm_390, [1, 384, 384, 512]);  mm_390 = None
    sigmoid_157 = torch.ops.aten.sigmoid.default(view_2045);  view_2045 = None
    mul_259 = torch.ops.aten.mul.Tensor(view_2043, sigmoid_157);  view_2043 = sigmoid_157 = None
    view_2046 = torch.ops.aten.view.default(mul_259, [147456, 512]);  mul_259 = None
    view_2047 = torch.ops.aten.view.default(view_2046, [1, 384, 384, 512]);  view_2046 = None
    transpose_52 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
    unsqueeze_717 = torch.ops.aten.unsqueeze.default(transpose_52, 3);  transpose_52 = None
    clone_189 = torch.ops.aten.clone.default(unsqueeze_717, memory_format = torch.contiguous_format);  unsqueeze_717 = None
    bitwise_not_127 = torch.ops.aten.bitwise_not.default(clone_189);  clone_189 = None
    masked_fill_127 = torch.ops.aten.masked_fill.Scalar(view_2047, bitwise_not_127, 0);  view_2047 = bitwise_not_127 = None
    view_2048 = torch.ops.aten.view.default(masked_fill_127, [147456, 512]);  masked_fill_127 = None
    view_2052 = torch.ops.aten.view.default(view_2048, [1, 384, 384, 512])
    split_tensor_210 = torch.ops.aten.split.Tensor(view_2052, 256, dim = -1);  view_2052 = None
    getitem_1948 = split_tensor_210[0];  split_tensor_210 = None
    unsqueeze_720 = torch.ops.aten.unsqueeze.default(getitem_1948, 4);  getitem_1948 = None
    permute_1139 = torch.ops.aten.permute.default(unsqueeze_720, [0, 2, 4, 3, 1]);  unsqueeze_720 = None
    permute_1140 = torch.ops.aten.permute.default(permute_1139, [3, 1, 4, 0, 2]);  permute_1139 = None
    view_2053 = torch.ops.aten.view.default(permute_1140, [256, 384, 384]);  permute_1140 = None
    view_2054 = torch.ops.aten.view.default(view_2048, [1, 384, 384, 512]);  view_2048 = None
    split_tensor_211 = torch.ops.aten.split.Tensor(view_2054, 256, dim = -1);  view_2054 = None
    getitem_1951 = split_tensor_211[1];  split_tensor_211 = None
    unsqueeze_721 = torch.ops.aten.unsqueeze.default(getitem_1951, 4);  getitem_1951 = None
    permute_1141 = torch.ops.aten.permute.default(unsqueeze_721, [0, 4, 2, 3, 1]);  unsqueeze_721 = None
    permute_1142 = torch.ops.aten.permute.default(permute_1141, [3, 4, 0, 2, 1]);  permute_1141 = None
    view_2055 = torch.ops.aten.view.default(permute_1142, [256, 384, 384]);  permute_1142 = None
    bmm_173 = torch.ops.aten.bmm.default(view_2053, view_2055);  view_2053 = view_2055 = None
    view_2056 = torch.ops.aten.view.default(bmm_173, [256, 384, 1, 1, 384]);  bmm_173 = None
    permute_1143 = torch.ops.aten.permute.default(view_2056, [3, 1, 4, 0, 2]);  view_2056 = None
    view_2057 = torch.ops.aten.view.default(permute_1143, [1, 384, 384, 256]);  permute_1143 = None
    _to_copy_1150 = torch.ops.aten._to_copy.default(view_2041, dtype = torch.float32);  view_2041 = None
    native_layer_norm_default_236 = torch.ops.aten.native_layer_norm.default(_to_copy_1150, [256], None, None, 1e-05);  _to_copy_1150 = None
    getitem_1952 = native_layer_norm_default_236[0];  native_layer_norm_default_236 = None
    _to_copy_1151 = torch.ops.aten._to_copy.default(view_2057, dtype = torch.float32);  view_2057 = None
    native_layer_norm_default_237 = torch.ops.aten.native_layer_norm.default(_to_copy_1151, [256], None, None, 1e-05);  _to_copy_1151 = None
    getitem_1955 = native_layer_norm_default_237[0];  native_layer_norm_default_237 = None
    add_216 = torch.ops.aten.add.Tensor(getitem_1952, getitem_1955);  getitem_1952 = getitem_1955 = None
    _to_copy_1152 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_20_triangle_multiplication_linear_z_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_20_triangle_multiplication_linear_z_out_weight = None
    _to_copy_1153 = torch.ops.aten._to_copy.default(add_216, dtype = torch.bfloat16);  add_216 = None
    t_421 = torch.ops.aten.t.default(_to_copy_1152);  _to_copy_1152 = None
    view_2058 = torch.ops.aten.view.default(_to_copy_1153, [147456, 256]);  _to_copy_1153 = None
    mm_391 = torch.ops.aten.mm.default(view_2058, t_421);  view_2058 = t_421 = None
    view_2059 = torch.ops.aten.view.default(mm_391, [1, 384, 384, 256]);  mm_391 = None
    _to_copy_1154 = torch.ops.aten._to_copy.default(getitem_1939, dtype = torch.bfloat16);  getitem_1939 = None
    _to_copy_1155 = torch.ops.aten._to_copy.default(getitem_1932, dtype = torch.bfloat16);  getitem_1932 = None
    t_422 = torch.ops.aten.t.default(_to_copy_1154);  _to_copy_1154 = None
    view_2060 = torch.ops.aten.view.default(_to_copy_1155, [147456, 256]);  _to_copy_1155 = None
    mm_392 = torch.ops.aten.mm.default(view_2060, t_422);  view_2060 = t_422 = None
    view_2061 = torch.ops.aten.view.default(mm_392, [1, 384, 384, 256]);  mm_392 = None
    sigmoid_158 = torch.ops.aten.sigmoid.default(view_2061);  view_2061 = None
    mul_260 = torch.ops.aten.mul.Tensor(view_2059, sigmoid_158);  view_2059 = sigmoid_158 = None
    add_217 = torch.ops.aten.add.Tensor(add_211, mul_260);  mul_260 = None
    _to_copy_1156 = torch.ops.aten._to_copy.default(add_211, dtype = torch.float32)
    native_layer_norm_default_238 = torch.ops.aten.native_layer_norm.default(_to_copy_1156, [256], None, None, 1e-05);  _to_copy_1156 = None
    getitem_1958 = native_layer_norm_default_238[0];  native_layer_norm_default_238 = None
    _to_copy_1157 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_20_triangle_attention_pair2b_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_20_triangle_attention_pair2b_weight = None
    _to_copy_1158 = torch.ops.aten._to_copy.default(getitem_1958, dtype = torch.bfloat16)
    t_423 = torch.ops.aten.t.default(_to_copy_1157);  _to_copy_1157 = None
    view_2062 = torch.ops.aten.view.default(_to_copy_1158, [147456, 256]);  _to_copy_1158 = None
    mm_393 = torch.ops.aten.mm.default(view_2062, t_423);  view_2062 = t_423 = None
    view_2063 = torch.ops.aten.view.default(mm_393, [1, 384, 384, 8]);  mm_393 = None
    view_2064 = torch.ops.aten.view.default(view_2063, [1, 384, 384, 2, 4]);  view_2063 = None
    permute_1144 = torch.ops.aten.permute.default(view_2064, [0, 3, 4, 1, 2]);  view_2064 = None
    view_2065 = torch.ops.aten.view.default(permute_1144, [1, 2, 4, 1, 384, 384]);  permute_1144 = None
    view_2066 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 384, 384])
    bitwise_not_128 = torch.ops.aten.bitwise_not.default(view_2066);  view_2066 = None
    masked_fill_128 = torch.ops.aten.masked_fill.Scalar(view_2065, bitwise_not_128, -10000);  view_2065 = bitwise_not_128 = None
    view_2067 = torch.ops.aten.view.default(masked_fill_128, [1, 2, 4, 384, 384]);  masked_fill_128 = None
    permute_1145 = torch.ops.aten.permute.default(view_2067, [1, 0, 2, 3, 4]);  view_2067 = None
    view_2068 = torch.ops.aten.view.default(permute_1145, [2, 4, 1, 384, 384]);  permute_1145 = None
    _to_copy_1159 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_20_triangle_attention_pair2qkvg1_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_20_triangle_attention_pair2qkvg1_weight = None
    _to_copy_1160 = torch.ops.aten._to_copy.default(getitem_1958, dtype = torch.bfloat16)
    t_424 = torch.ops.aten.t.default(_to_copy_1159);  _to_copy_1159 = None
    view_2069 = torch.ops.aten.view.default(_to_copy_1160, [147456, 256]);  _to_copy_1160 = None
    mm_394 = torch.ops.aten.mm.default(view_2069, t_424);  view_2069 = t_424 = None
    view_2070 = torch.ops.aten.view.default(mm_394, [1, 384, 384, 1024]);  mm_394 = None
    select_53 = torch.ops.aten.select.int(view_2068, 0, 0)
    view_2071 = torch.ops.aten.view.default(view_2070, [1, 384, 384, 4, 4, 64]);  view_2070 = None
    permute_1146 = torch.ops.aten.permute.default(view_2071, [4, 0, 3, 1, 2, 5]);  view_2071 = None
    view_2072 = torch.ops.aten.view.default(permute_1146, [4, 4, 384, 384, 64]);  permute_1146 = None
    unbind_int_94 = torch.ops.aten.unbind.int(view_2072);  view_2072 = None
    getitem_1961 = unbind_int_94[0]
    getitem_1962 = unbind_int_94[1]
    getitem_1963 = unbind_int_94[2]
    getitem_1964 = unbind_int_94[3];  unbind_int_94 = None
    expand_127 = torch.ops.aten.expand.default(select_53, [4, 384, 384, 384]);  select_53 = None
    _scaled_dot_product_efficient_attention_default_72 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1961, getitem_1962, getitem_1963, expand_127, False);  getitem_1961 = getitem_1962 = getitem_1963 = expand_127 = None
    getitem_1965 = _scaled_dot_product_efficient_attention_default_72[0];  _scaled_dot_product_efficient_attention_default_72 = None
    sigmoid_159 = torch.ops.aten.sigmoid.default(getitem_1964);  getitem_1964 = None
    mul_261 = torch.ops.aten.mul.Tensor(getitem_1965, sigmoid_159);  getitem_1965 = sigmoid_159 = None
    view_2073 = torch.ops.aten.view.default(mul_261, [1, 4, 384, 384, 64]);  mul_261 = None
    permute_1147 = torch.ops.aten.permute.default(view_2073, [0, 2, 3, 1, 4]);  view_2073 = None
    clone_190 = torch.ops.aten.clone.default(permute_1147, memory_format = torch.contiguous_format);  permute_1147 = None
    _unsafe_view_162 = torch.ops.aten._unsafe_view.default(clone_190, [1, 384, 384, 256]);  clone_190 = None
    transpose_53 = torch.ops.aten.transpose.int(getitem_1958, 1, 2);  getitem_1958 = None
    _to_copy_1161 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_20_triangle_attention_pair2qkvg2_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_20_triangle_attention_pair2qkvg2_weight = None
    _to_copy_1162 = torch.ops.aten._to_copy.default(transpose_53, dtype = torch.bfloat16);  transpose_53 = None
    t_425 = torch.ops.aten.t.default(_to_copy_1161);  _to_copy_1161 = None
    expand_128 = torch.ops.aten.expand.default(_to_copy_1162, [1, 384, 384, 256]);  _to_copy_1162 = None
    view_2074 = torch.ops.aten.view.default(expand_128, [384, 384, 256]);  expand_128 = None
    expand_129 = torch.ops.aten.expand.default(t_425, [1, 384, 256, 1024]);  t_425 = None
    view_2075 = torch.ops.aten.view.default(expand_129, [384, 256, 1024]);  expand_129 = None
    bmm_174 = torch.ops.aten.bmm.default(view_2074, view_2075);  view_2074 = view_2075 = None
    view_2076 = torch.ops.aten.view.default(bmm_174, [1, 384, 384, 1024]);  bmm_174 = None
    select_54 = torch.ops.aten.select.int(view_2068, 0, 1);  view_2068 = None
    view_2077 = torch.ops.aten.view.default(view_2076, [1, 384, 384, 4, 4, 64]);  view_2076 = None
    permute_1148 = torch.ops.aten.permute.default(view_2077, [4, 0, 3, 1, 2, 5]);  view_2077 = None
    view_2078 = torch.ops.aten.view.default(permute_1148, [4, 4, 384, 384, 64]);  permute_1148 = None
    unbind_int_95 = torch.ops.aten.unbind.int(view_2078);  view_2078 = None
    getitem_1969 = unbind_int_95[0]
    getitem_1970 = unbind_int_95[1]
    getitem_1971 = unbind_int_95[2]
    getitem_1972 = unbind_int_95[3];  unbind_int_95 = None
    expand_130 = torch.ops.aten.expand.default(select_54, [4, 384, 384, 384]);  select_54 = None
    _scaled_dot_product_efficient_attention_default_73 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1969, getitem_1970, getitem_1971, expand_130, False);  getitem_1969 = getitem_1970 = getitem_1971 = expand_130 = None
    getitem_1973 = _scaled_dot_product_efficient_attention_default_73[0];  _scaled_dot_product_efficient_attention_default_73 = None
    sigmoid_160 = torch.ops.aten.sigmoid.default(getitem_1972);  getitem_1972 = None
    mul_262 = torch.ops.aten.mul.Tensor(getitem_1973, sigmoid_160);  getitem_1973 = sigmoid_160 = None
    view_2079 = torch.ops.aten.view.default(mul_262, [1, 4, 384, 384, 64]);  mul_262 = None
    permute_1149 = torch.ops.aten.permute.default(view_2079, [0, 2, 3, 1, 4]);  view_2079 = None
    clone_191 = torch.ops.aten.clone.default(permute_1149, memory_format = torch.contiguous_format);  permute_1149 = None
    _unsafe_view_163 = torch.ops.aten._unsafe_view.default(clone_191, [1, 384, 384, 256]);  clone_191 = None
    cat_32 = torch.ops.aten.cat.default([_unsafe_view_162, _unsafe_view_163], dim = -1);  _unsafe_view_162 = _unsafe_view_163 = None
    slice_183 = torch.ops.aten.slice.Tensor(pairformer_stack_blocks_20_triangle_attention_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  pairformer_stack_blocks_20_triangle_attention_out_scalers = None
    unsqueeze_722 = torch.ops.aten.unsqueeze.default(slice_183, 1);  slice_183 = None
    mul_263 = torch.ops.aten.mul.Tensor(pairformer_stack_blocks_20_triangle_attention_linear_out_weight, unsqueeze_722);  pairformer_stack_blocks_20_triangle_attention_linear_out_weight = unsqueeze_722 = None
    _to_copy_1163 = torch.ops.aten._to_copy.default(mul_263, dtype = torch.bfloat16);  mul_263 = None
    t_426 = torch.ops.aten.t.default(_to_copy_1163);  _to_copy_1163 = None
    view_2080 = torch.ops.aten.view.default(cat_32, [147456, 512]);  cat_32 = None
    mm_395 = torch.ops.aten.mm.default(view_2080, t_426);  view_2080 = t_426 = None
    view_2081 = torch.ops.aten.view.default(mm_395, [1, 384, 384, 256]);  mm_395 = None
    add_218 = torch.ops.aten.add.Tensor(add_217, view_2081);  add_217 = view_2081 = None
    split_tensor_212 = torch.ops.aten.split.Tensor(add_211, 384, dim = -2)
    getitem_1977 = split_tensor_212[0];  split_tensor_212 = None
    _to_copy_1164 = torch.ops.aten._to_copy.default(getitem_1977, dtype = torch.float32);  getitem_1977 = None
    native_layer_norm_default_239 = torch.ops.aten.native_layer_norm.default(_to_copy_1164, [256], pairformer_stack_blocks_20_transition_pair_layer_norm_weight, pairformer_stack_blocks_20_transition_pair_layer_norm_bias, 1e-05);  _to_copy_1164 = pairformer_stack_blocks_20_transition_pair_layer_norm_weight = pairformer_stack_blocks_20_transition_pair_layer_norm_bias = None
    getitem_1978 = native_layer_norm_default_239[0];  native_layer_norm_default_239 = None
    _to_copy_1165 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_20_transition_pair_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_20_transition_pair_linear_no_bias_ab_weight = None
    _to_copy_1166 = torch.ops.aten._to_copy.default(getitem_1978, dtype = torch.bfloat16);  getitem_1978 = None
    t_427 = torch.ops.aten.t.default(_to_copy_1165);  _to_copy_1165 = None
    view_2082 = torch.ops.aten.view.default(_to_copy_1166, [147456, 256]);  _to_copy_1166 = None
    mm_396 = torch.ops.aten.mm.default(view_2082, t_427);  view_2082 = t_427 = None
    view_2083 = torch.ops.aten.view.default(mm_396, [1, 384, 384, 1024]);  mm_396 = None
    split_tensor_213 = torch.ops.aten.split.Tensor(view_2083, 512, dim = -1);  view_2083 = None
    getitem_1981 = split_tensor_213[0]
    getitem_1982 = split_tensor_213[1];  split_tensor_213 = None
    silu_55 = torch.ops.aten.silu.default(getitem_1981);  getitem_1981 = None
    mul_264 = torch.ops.aten.mul.Tensor(silu_55, getitem_1982);  silu_55 = getitem_1982 = None
    _to_copy_1167 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_20_transition_pair_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_20_transition_pair_linear_out_weight = None
    t_428 = torch.ops.aten.t.default(_to_copy_1167);  _to_copy_1167 = None
    view_2085 = torch.ops.aten.view.default(mul_264, [147456, 512]);  mul_264 = None
    mm_397 = torch.ops.aten.mm.default(view_2085, t_428);  view_2085 = t_428 = None
    view_2086 = torch.ops.aten.view.default(mm_397, [1, 384, 384, 256]);  mm_397 = None
    add_219 = torch.ops.aten.add.Tensor(add_218, view_2086);  add_218 = view_2086 = None
    _to_copy_1168 = torch.ops.aten._to_copy.default(add_215, dtype = torch.float32)
    native_layer_norm_default_240 = torch.ops.aten.native_layer_norm.default(_to_copy_1168, [384], pairformer_stack_blocks_20_attention_pair_bias_single_layer_norm_weight, pairformer_stack_blocks_20_attention_pair_bias_single_layer_norm_bias, 1e-05);  _to_copy_1168 = pairformer_stack_blocks_20_attention_pair_bias_single_layer_norm_weight = pairformer_stack_blocks_20_attention_pair_bias_single_layer_norm_bias = None
    getitem_1983 = native_layer_norm_default_240[0];  native_layer_norm_default_240 = None
    _to_copy_1169 = torch.ops.aten._to_copy.default(add_211, dtype = torch.float32);  add_211 = None
    native_layer_norm_default_241 = torch.ops.aten.native_layer_norm.default(_to_copy_1169, [256], pairformer_stack_blocks_20_attention_pair_bias_pair_layer_norm_weight, pairformer_stack_blocks_20_attention_pair_bias_pair_layer_norm_bias, 1e-05);  _to_copy_1169 = pairformer_stack_blocks_20_attention_pair_bias_pair_layer_norm_weight = pairformer_stack_blocks_20_attention_pair_bias_pair_layer_norm_bias = None
    getitem_1986 = native_layer_norm_default_241[0];  native_layer_norm_default_241 = None
    _to_copy_1170 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_20_attention_pair_bias_pair_linear_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_20_attention_pair_bias_pair_linear_weight = None
    _to_copy_1171 = torch.ops.aten._to_copy.default(getitem_1986, dtype = torch.bfloat16);  getitem_1986 = None
    t_429 = torch.ops.aten.t.default(_to_copy_1170);  _to_copy_1170 = None
    view_2087 = torch.ops.aten.view.default(_to_copy_1171, [147456, 256]);  _to_copy_1171 = None
    mm_398 = torch.ops.aten.mm.default(view_2087, t_429);  view_2087 = t_429 = None
    view_2088 = torch.ops.aten.view.default(mm_398, [1, 384, 384, 16]);  mm_398 = None
    permute_1150 = torch.ops.aten.permute.default(view_2088, [0, 3, 1, 2]);  view_2088 = None
    view_2089 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384])
    bitwise_not_129 = torch.ops.aten.bitwise_not.default(view_2089);  view_2089 = None
    masked_fill_129 = torch.ops.aten.masked_fill.Scalar(permute_1150, bitwise_not_129, -10000);  permute_1150 = bitwise_not_129 = None
    _to_copy_1172 = torch.ops.aten._to_copy.default(getitem_1983, dtype = torch.bfloat16);  getitem_1983 = None
    _to_copy_1173 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_20_attention_pair_bias_attention_input2qkvg_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_20_attention_pair_bias_attention_input2qkvg_weight = None
    unsqueeze_723 = torch.ops.aten.unsqueeze.default(_to_copy_1172, 3);  _to_copy_1172 = None
    unsqueeze_724 = torch.ops.aten.unsqueeze.default(unsqueeze_723, 4);  unsqueeze_723 = None
    unsqueeze_725 = torch.ops.aten.unsqueeze.default(unsqueeze_724, 5);  unsqueeze_724 = None
    permute_1151 = torch.ops.aten.permute.default(unsqueeze_725, [3, 0, 4, 1, 5, 2]);  unsqueeze_725 = None
    unsqueeze_726 = torch.ops.aten.unsqueeze.default(_to_copy_1173, 4);  _to_copy_1173 = None
    unsqueeze_727 = torch.ops.aten.unsqueeze.default(unsqueeze_726, 5);  unsqueeze_726 = None
    permute_1152 = torch.ops.aten.permute.default(unsqueeze_727, [1, 4, 2, 5, 3, 0]);  unsqueeze_727 = None
    permute_1153 = torch.ops.aten.permute.default(permute_1151, [3, 5, 0, 1, 2, 4]);  permute_1151 = None
    view_2090 = torch.ops.aten.view.default(permute_1153, [1, 384, 384]);  permute_1153 = None
    permute_1154 = torch.ops.aten.permute.default(permute_1152, [5, 0, 1, 2, 4, 3]);  permute_1152 = None
    view_2091 = torch.ops.aten.view.default(permute_1154, [1, 384, 1536]);  permute_1154 = None
    bmm_175 = torch.ops.aten.bmm.default(view_2090, view_2091);  view_2090 = view_2091 = None
    view_2092 = torch.ops.aten.view.default(bmm_175, [384, 1, 4, 1, 16, 24]);  bmm_175 = None
    permute_1155 = torch.ops.aten.permute.default(view_2092, [2, 3, 4, 0, 5, 1]);  view_2092 = None
    view_2093 = torch.ops.aten.view.default(permute_1155, [4, 1, 16, 384, 24]);  permute_1155 = None
    unbind_int_96 = torch.ops.aten.unbind.int(view_2093);  view_2093 = None
    getitem_1989 = unbind_int_96[0]
    getitem_1990 = unbind_int_96[1]
    getitem_1991 = unbind_int_96[2]
    getitem_1992 = unbind_int_96[3];  unbind_int_96 = None
    view_2094 = torch.ops.aten.view.default(pairformer_stack_blocks_20_attention_pair_bias_attention_query_bias, [1, 16, 1, 24]);  pairformer_stack_blocks_20_attention_pair_bias_attention_query_bias = None
    add_220 = torch.ops.aten.add.Tensor(getitem_1989, view_2094);  getitem_1989 = view_2094 = None
    _to_copy_1174 = torch.ops.aten._to_copy.default(add_220, dtype = torch.bfloat16);  add_220 = None
    expand_131 = torch.ops.aten.expand.default(masked_fill_129, [1, 16, 384, 384]);  masked_fill_129 = None
    _scaled_dot_product_efficient_attention_default_74 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_1174, getitem_1990, getitem_1991, expand_131, False);  _to_copy_1174 = getitem_1990 = getitem_1991 = expand_131 = None
    getitem_1993 = _scaled_dot_product_efficient_attention_default_74[0];  _scaled_dot_product_efficient_attention_default_74 = None
    add_221 = torch.ops.aten.add.Tensor(getitem_1992, 1);  getitem_1992 = None
    sigmoid_161 = torch.ops.aten.sigmoid.default(add_221);  add_221 = None
    mul_265 = torch.ops.aten.mul.Tensor(getitem_1993, sigmoid_161);  getitem_1993 = sigmoid_161 = None
    _to_copy_1175 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_20_attention_pair_bias_attention_output_proj_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_20_attention_pair_bias_attention_output_proj_weight = None
    unsqueeze_728 = torch.ops.aten.unsqueeze.default(mul_265, 4);  mul_265 = None
    permute_1156 = torch.ops.aten.permute.default(unsqueeze_728, [0, 2, 4, 3, 1]);  unsqueeze_728 = None
    unsqueeze_729 = torch.ops.aten.unsqueeze.default(_to_copy_1175, 3);  _to_copy_1175 = None
    unsqueeze_730 = torch.ops.aten.unsqueeze.default(unsqueeze_729, 4);  unsqueeze_729 = None
    permute_1157 = torch.ops.aten.permute.default(unsqueeze_730, [3, 4, 2, 1, 0]);  unsqueeze_730 = None
    permute_1158 = torch.ops.aten.permute.default(permute_1156, [1, 3, 4, 0, 2]);  permute_1156 = None
    clone_192 = torch.ops.aten.clone.default(permute_1158, memory_format = torch.contiguous_format);  permute_1158 = None
    _unsafe_view_164 = torch.ops.aten._unsafe_view.default(clone_192, [1, 384, 384]);  clone_192 = None
    permute_1159 = torch.ops.aten.permute.default(permute_1157, [3, 4, 0, 2, 1]);  permute_1157 = None
    clone_193 = torch.ops.aten.clone.default(permute_1159, memory_format = torch.contiguous_format);  permute_1159 = None
    _unsafe_view_165 = torch.ops.aten._unsafe_view.default(clone_193, [1, 384, 384]);  clone_193 = None
    bmm_176 = torch.ops.aten.bmm.default(_unsafe_view_164, _unsafe_view_165);  _unsafe_view_164 = _unsafe_view_165 = None
    view_2095 = torch.ops.aten.view.default(bmm_176, [384, 1, 1, 1, 384]);  bmm_176 = None
    permute_1160 = torch.ops.aten.permute.default(view_2095, [3, 0, 4, 1, 2]);  view_2095 = None
    view_2096 = torch.ops.aten.view.default(permute_1160, [1, 384, 384]);  permute_1160 = None
    unsqueeze_731 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
    mul_266 = torch.ops.aten.mul.Tensor(view_2096, unsqueeze_731);  view_2096 = unsqueeze_731 = None
    add_222 = torch.ops.aten.add.Tensor(add_215, mul_266);  mul_266 = None
    split_tensor_214 = torch.ops.aten.split.Tensor(add_215, 384, dim = -2);  add_215 = None
    getitem_1997 = split_tensor_214[0];  split_tensor_214 = None
    _to_copy_1176 = torch.ops.aten._to_copy.default(getitem_1997, dtype = torch.float32);  getitem_1997 = None
    native_layer_norm_default_242 = torch.ops.aten.native_layer_norm.default(_to_copy_1176, [384], pairformer_stack_blocks_20_transition_single_layer_norm_weight, pairformer_stack_blocks_20_transition_single_layer_norm_bias, 1e-05);  _to_copy_1176 = pairformer_stack_blocks_20_transition_single_layer_norm_weight = pairformer_stack_blocks_20_transition_single_layer_norm_bias = None
    getitem_1998 = native_layer_norm_default_242[0];  native_layer_norm_default_242 = None
    _to_copy_1177 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_20_transition_single_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_20_transition_single_linear_no_bias_ab_weight = None
    _to_copy_1178 = torch.ops.aten._to_copy.default(getitem_1998, dtype = torch.bfloat16);  getitem_1998 = None
    t_430 = torch.ops.aten.t.default(_to_copy_1177);  _to_copy_1177 = None
    view_2097 = torch.ops.aten.view.default(_to_copy_1178, [384, 384]);  _to_copy_1178 = None
    mm_399 = torch.ops.aten.mm.default(view_2097, t_430);  view_2097 = t_430 = None
    view_2098 = torch.ops.aten.view.default(mm_399, [1, 384, 1536]);  mm_399 = None
    split_tensor_215 = torch.ops.aten.split.Tensor(view_2098, 768, dim = -1);  view_2098 = None
    getitem_2001 = split_tensor_215[0]
    getitem_2002 = split_tensor_215[1];  split_tensor_215 = None
    silu_56 = torch.ops.aten.silu.default(getitem_2001);  getitem_2001 = None
    mul_267 = torch.ops.aten.mul.Tensor(silu_56, getitem_2002);  silu_56 = getitem_2002 = None
    _to_copy_1179 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_20_transition_single_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_20_transition_single_linear_out_weight = None
    t_431 = torch.ops.aten.t.default(_to_copy_1179);  _to_copy_1179 = None
    view_2100 = torch.ops.aten.view.default(mul_267, [384, 768]);  mul_267 = None
    mm_400 = torch.ops.aten.mm.default(view_2100, t_431);  view_2100 = t_431 = None
    view_2101 = torch.ops.aten.view.default(mm_400, [1, 384, 384]);  mm_400 = None
    add_223 = torch.ops.aten.add.Tensor(add_222, view_2101);  add_222 = view_2101 = None
    _to_copy_1180 = torch.ops.aten._to_copy.default(add_219, dtype = torch.float32)
    native_layer_norm_default_243 = torch.ops.aten.native_layer_norm.default(_to_copy_1180, [256], pairformer_stack_blocks_21_triangle_multiplication_layernorm_z_in_weight, pairformer_stack_blocks_21_triangle_multiplication_layernorm_z_in_bias, 1e-05);  _to_copy_1180 = pairformer_stack_blocks_21_triangle_multiplication_layernorm_z_in_weight = pairformer_stack_blocks_21_triangle_multiplication_layernorm_z_in_bias = None
    getitem_2003 = native_layer_norm_default_243[0];  native_layer_norm_default_243 = None
    split_with_sizes_default_54 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_21_triangle_multiplication_merged_linear_p_weight, [512, 512]);  pairformer_stack_blocks_21_triangle_multiplication_merged_linear_p_weight = None
    getitem_2006 = split_with_sizes_default_54[0]
    getitem_2007 = split_with_sizes_default_54[1];  split_with_sizes_default_54 = None
    split_with_sizes_default_55 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_21_triangle_multiplication_merged_linear_g_weight, [512, 512, 256]);  pairformer_stack_blocks_21_triangle_multiplication_merged_linear_g_weight = None
    getitem_2008 = split_with_sizes_default_55[0]
    getitem_2009 = split_with_sizes_default_55[1]
    getitem_2010 = split_with_sizes_default_55[2];  split_with_sizes_default_55 = None
    _to_copy_1181 = torch.ops.aten._to_copy.default(getitem_2006, dtype = torch.bfloat16);  getitem_2006 = None
    _to_copy_1182 = torch.ops.aten._to_copy.default(getitem_2003, dtype = torch.bfloat16)
    t_432 = torch.ops.aten.t.default(_to_copy_1181);  _to_copy_1181 = None
    view_2102 = torch.ops.aten.view.default(_to_copy_1182, [147456, 256]);  _to_copy_1182 = None
    mm_401 = torch.ops.aten.mm.default(view_2102, t_432);  view_2102 = t_432 = None
    view_2103 = torch.ops.aten.view.default(mm_401, [1, 384, 384, 512]);  mm_401 = None
    _to_copy_1183 = torch.ops.aten._to_copy.default(getitem_2008, dtype = torch.bfloat16);  getitem_2008 = None
    _to_copy_1184 = torch.ops.aten._to_copy.default(getitem_2003, dtype = torch.bfloat16)
    t_433 = torch.ops.aten.t.default(_to_copy_1183);  _to_copy_1183 = None
    view_2104 = torch.ops.aten.view.default(_to_copy_1184, [147456, 256]);  _to_copy_1184 = None
    mm_402 = torch.ops.aten.mm.default(view_2104, t_433);  view_2104 = t_433 = None
    view_2105 = torch.ops.aten.view.default(mm_402, [1, 384, 384, 512]);  mm_402 = None
    sigmoid_162 = torch.ops.aten.sigmoid.default(view_2105);  view_2105 = None
    mul_268 = torch.ops.aten.mul.Tensor(view_2103, sigmoid_162);  view_2103 = sigmoid_162 = None
    unsqueeze_732 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
    bitwise_not_130 = torch.ops.aten.bitwise_not.default(unsqueeze_732);  unsqueeze_732 = None
    masked_fill_130 = torch.ops.aten.masked_fill.Scalar(mul_268, bitwise_not_130, 0);  mul_268 = bitwise_not_130 = None
    split_tensor_216 = torch.ops.aten.split.Tensor(masked_fill_130, 256, dim = -1)
    getitem_2013 = split_tensor_216[0];  split_tensor_216 = None
    unsqueeze_735 = torch.ops.aten.unsqueeze.default(getitem_2013, 4);  getitem_2013 = None
    permute_1165 = torch.ops.aten.permute.default(unsqueeze_735, [0, 1, 4, 3, 2]);  unsqueeze_735 = None
    permute_1166 = torch.ops.aten.permute.default(permute_1165, [3, 1, 4, 0, 2]);  permute_1165 = None
    view_2108 = torch.ops.aten.view.default(permute_1166, [256, 384, 384]);  permute_1166 = None
    split_tensor_217 = torch.ops.aten.split.Tensor(masked_fill_130, 256, dim = -1);  masked_fill_130 = None
    getitem_2016 = split_tensor_217[1];  split_tensor_217 = None
    unsqueeze_736 = torch.ops.aten.unsqueeze.default(getitem_2016, 4);  getitem_2016 = None
    permute_1167 = torch.ops.aten.permute.default(unsqueeze_736, [0, 4, 1, 3, 2]);  unsqueeze_736 = None
    permute_1168 = torch.ops.aten.permute.default(permute_1167, [3, 4, 0, 2, 1]);  permute_1167 = None
    view_2109 = torch.ops.aten.view.default(permute_1168, [256, 384, 384]);  permute_1168 = None
    bmm_177 = torch.ops.aten.bmm.default(view_2108, view_2109);  view_2108 = view_2109 = None
    view_2110 = torch.ops.aten.view.default(bmm_177, [256, 384, 1, 1, 384]);  bmm_177 = None
    permute_1169 = torch.ops.aten.permute.default(view_2110, [3, 1, 4, 0, 2]);  view_2110 = None
    view_2111 = torch.ops.aten.view.default(permute_1169, [1, 384, 384, 256]);  permute_1169 = None
    _to_copy_1185 = torch.ops.aten._to_copy.default(getitem_2007, dtype = torch.bfloat16);  getitem_2007 = None
    _to_copy_1186 = torch.ops.aten._to_copy.default(getitem_2003, dtype = torch.bfloat16)
    t_434 = torch.ops.aten.t.default(_to_copy_1185);  _to_copy_1185 = None
    view_2112 = torch.ops.aten.view.default(_to_copy_1186, [147456, 256]);  _to_copy_1186 = None
    mm_403 = torch.ops.aten.mm.default(view_2112, t_434);  view_2112 = t_434 = None
    view_2113 = torch.ops.aten.view.default(mm_403, [1, 384, 384, 512]);  mm_403 = None
    _to_copy_1187 = torch.ops.aten._to_copy.default(getitem_2009, dtype = torch.bfloat16);  getitem_2009 = None
    _to_copy_1188 = torch.ops.aten._to_copy.default(getitem_2003, dtype = torch.bfloat16)
    t_435 = torch.ops.aten.t.default(_to_copy_1187);  _to_copy_1187 = None
    view_2114 = torch.ops.aten.view.default(_to_copy_1188, [147456, 256]);  _to_copy_1188 = None
    mm_404 = torch.ops.aten.mm.default(view_2114, t_435);  view_2114 = t_435 = None
    view_2115 = torch.ops.aten.view.default(mm_404, [1, 384, 384, 512]);  mm_404 = None
    sigmoid_163 = torch.ops.aten.sigmoid.default(view_2115);  view_2115 = None
    mul_269 = torch.ops.aten.mul.Tensor(view_2113, sigmoid_163);  view_2113 = sigmoid_163 = None
    view_2116 = torch.ops.aten.view.default(mul_269, [147456, 512]);  mul_269 = None
    view_2117 = torch.ops.aten.view.default(view_2116, [1, 384, 384, 512]);  view_2116 = None
    transpose_54 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
    unsqueeze_737 = torch.ops.aten.unsqueeze.default(transpose_54, 3);  transpose_54 = None
    clone_194 = torch.ops.aten.clone.default(unsqueeze_737, memory_format = torch.contiguous_format);  unsqueeze_737 = None
    bitwise_not_131 = torch.ops.aten.bitwise_not.default(clone_194);  clone_194 = None
    masked_fill_131 = torch.ops.aten.masked_fill.Scalar(view_2117, bitwise_not_131, 0);  view_2117 = bitwise_not_131 = None
    view_2118 = torch.ops.aten.view.default(masked_fill_131, [147456, 512]);  masked_fill_131 = None
    view_2122 = torch.ops.aten.view.default(view_2118, [1, 384, 384, 512])
    split_tensor_218 = torch.ops.aten.split.Tensor(view_2122, 256, dim = -1);  view_2122 = None
    getitem_2019 = split_tensor_218[0];  split_tensor_218 = None
    unsqueeze_740 = torch.ops.aten.unsqueeze.default(getitem_2019, 4);  getitem_2019 = None
    permute_1174 = torch.ops.aten.permute.default(unsqueeze_740, [0, 2, 4, 3, 1]);  unsqueeze_740 = None
    permute_1175 = torch.ops.aten.permute.default(permute_1174, [3, 1, 4, 0, 2]);  permute_1174 = None
    view_2123 = torch.ops.aten.view.default(permute_1175, [256, 384, 384]);  permute_1175 = None
    view_2124 = torch.ops.aten.view.default(view_2118, [1, 384, 384, 512]);  view_2118 = None
    split_tensor_219 = torch.ops.aten.split.Tensor(view_2124, 256, dim = -1);  view_2124 = None
    getitem_2022 = split_tensor_219[1];  split_tensor_219 = None
    unsqueeze_741 = torch.ops.aten.unsqueeze.default(getitem_2022, 4);  getitem_2022 = None
    permute_1176 = torch.ops.aten.permute.default(unsqueeze_741, [0, 4, 2, 3, 1]);  unsqueeze_741 = None
    permute_1177 = torch.ops.aten.permute.default(permute_1176, [3, 4, 0, 2, 1]);  permute_1176 = None
    view_2125 = torch.ops.aten.view.default(permute_1177, [256, 384, 384]);  permute_1177 = None
    bmm_178 = torch.ops.aten.bmm.default(view_2123, view_2125);  view_2123 = view_2125 = None
    view_2126 = torch.ops.aten.view.default(bmm_178, [256, 384, 1, 1, 384]);  bmm_178 = None
    permute_1178 = torch.ops.aten.permute.default(view_2126, [3, 1, 4, 0, 2]);  view_2126 = None
    view_2127 = torch.ops.aten.view.default(permute_1178, [1, 384, 384, 256]);  permute_1178 = None
    _to_copy_1189 = torch.ops.aten._to_copy.default(view_2111, dtype = torch.float32);  view_2111 = None
    native_layer_norm_default_244 = torch.ops.aten.native_layer_norm.default(_to_copy_1189, [256], None, None, 1e-05);  _to_copy_1189 = None
    getitem_2023 = native_layer_norm_default_244[0];  native_layer_norm_default_244 = None
    _to_copy_1190 = torch.ops.aten._to_copy.default(view_2127, dtype = torch.float32);  view_2127 = None
    native_layer_norm_default_245 = torch.ops.aten.native_layer_norm.default(_to_copy_1190, [256], None, None, 1e-05);  _to_copy_1190 = None
    getitem_2026 = native_layer_norm_default_245[0];  native_layer_norm_default_245 = None
    add_224 = torch.ops.aten.add.Tensor(getitem_2023, getitem_2026);  getitem_2023 = getitem_2026 = None
    _to_copy_1191 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_21_triangle_multiplication_linear_z_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_21_triangle_multiplication_linear_z_out_weight = None
    _to_copy_1192 = torch.ops.aten._to_copy.default(add_224, dtype = torch.bfloat16);  add_224 = None
    t_436 = torch.ops.aten.t.default(_to_copy_1191);  _to_copy_1191 = None
    view_2128 = torch.ops.aten.view.default(_to_copy_1192, [147456, 256]);  _to_copy_1192 = None
    mm_405 = torch.ops.aten.mm.default(view_2128, t_436);  view_2128 = t_436 = None
    view_2129 = torch.ops.aten.view.default(mm_405, [1, 384, 384, 256]);  mm_405 = None
    _to_copy_1193 = torch.ops.aten._to_copy.default(getitem_2010, dtype = torch.bfloat16);  getitem_2010 = None
    _to_copy_1194 = torch.ops.aten._to_copy.default(getitem_2003, dtype = torch.bfloat16);  getitem_2003 = None
    t_437 = torch.ops.aten.t.default(_to_copy_1193);  _to_copy_1193 = None
    view_2130 = torch.ops.aten.view.default(_to_copy_1194, [147456, 256]);  _to_copy_1194 = None
    mm_406 = torch.ops.aten.mm.default(view_2130, t_437);  view_2130 = t_437 = None
    view_2131 = torch.ops.aten.view.default(mm_406, [1, 384, 384, 256]);  mm_406 = None
    sigmoid_164 = torch.ops.aten.sigmoid.default(view_2131);  view_2131 = None
    mul_270 = torch.ops.aten.mul.Tensor(view_2129, sigmoid_164);  view_2129 = sigmoid_164 = None
    add_225 = torch.ops.aten.add.Tensor(add_219, mul_270);  mul_270 = None
    _to_copy_1195 = torch.ops.aten._to_copy.default(add_219, dtype = torch.float32)
    native_layer_norm_default_246 = torch.ops.aten.native_layer_norm.default(_to_copy_1195, [256], None, None, 1e-05);  _to_copy_1195 = None
    getitem_2029 = native_layer_norm_default_246[0];  native_layer_norm_default_246 = None
    _to_copy_1196 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_21_triangle_attention_pair2b_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_21_triangle_attention_pair2b_weight = None
    _to_copy_1197 = torch.ops.aten._to_copy.default(getitem_2029, dtype = torch.bfloat16)
    t_438 = torch.ops.aten.t.default(_to_copy_1196);  _to_copy_1196 = None
    view_2132 = torch.ops.aten.view.default(_to_copy_1197, [147456, 256]);  _to_copy_1197 = None
    mm_407 = torch.ops.aten.mm.default(view_2132, t_438);  view_2132 = t_438 = None
    view_2133 = torch.ops.aten.view.default(mm_407, [1, 384, 384, 8]);  mm_407 = None
    view_2134 = torch.ops.aten.view.default(view_2133, [1, 384, 384, 2, 4]);  view_2133 = None
    permute_1179 = torch.ops.aten.permute.default(view_2134, [0, 3, 4, 1, 2]);  view_2134 = None
    view_2135 = torch.ops.aten.view.default(permute_1179, [1, 2, 4, 1, 384, 384]);  permute_1179 = None
    view_2136 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 384, 384])
    bitwise_not_132 = torch.ops.aten.bitwise_not.default(view_2136);  view_2136 = None
    masked_fill_132 = torch.ops.aten.masked_fill.Scalar(view_2135, bitwise_not_132, -10000);  view_2135 = bitwise_not_132 = None
    view_2137 = torch.ops.aten.view.default(masked_fill_132, [1, 2, 4, 384, 384]);  masked_fill_132 = None
    permute_1180 = torch.ops.aten.permute.default(view_2137, [1, 0, 2, 3, 4]);  view_2137 = None
    view_2138 = torch.ops.aten.view.default(permute_1180, [2, 4, 1, 384, 384]);  permute_1180 = None
    _to_copy_1198 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_21_triangle_attention_pair2qkvg1_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_21_triangle_attention_pair2qkvg1_weight = None
    _to_copy_1199 = torch.ops.aten._to_copy.default(getitem_2029, dtype = torch.bfloat16)
    t_439 = torch.ops.aten.t.default(_to_copy_1198);  _to_copy_1198 = None
    view_2139 = torch.ops.aten.view.default(_to_copy_1199, [147456, 256]);  _to_copy_1199 = None
    mm_408 = torch.ops.aten.mm.default(view_2139, t_439);  view_2139 = t_439 = None
    view_2140 = torch.ops.aten.view.default(mm_408, [1, 384, 384, 1024]);  mm_408 = None
    select_55 = torch.ops.aten.select.int(view_2138, 0, 0)
    view_2141 = torch.ops.aten.view.default(view_2140, [1, 384, 384, 4, 4, 64]);  view_2140 = None
    permute_1181 = torch.ops.aten.permute.default(view_2141, [4, 0, 3, 1, 2, 5]);  view_2141 = None
    view_2142 = torch.ops.aten.view.default(permute_1181, [4, 4, 384, 384, 64]);  permute_1181 = None
    unbind_int_97 = torch.ops.aten.unbind.int(view_2142);  view_2142 = None
    getitem_2032 = unbind_int_97[0]
    getitem_2033 = unbind_int_97[1]
    getitem_2034 = unbind_int_97[2]
    getitem_2035 = unbind_int_97[3];  unbind_int_97 = None
    expand_132 = torch.ops.aten.expand.default(select_55, [4, 384, 384, 384]);  select_55 = None
    _scaled_dot_product_efficient_attention_default_75 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2032, getitem_2033, getitem_2034, expand_132, False);  getitem_2032 = getitem_2033 = getitem_2034 = expand_132 = None
    getitem_2036 = _scaled_dot_product_efficient_attention_default_75[0];  _scaled_dot_product_efficient_attention_default_75 = None
    sigmoid_165 = torch.ops.aten.sigmoid.default(getitem_2035);  getitem_2035 = None
    mul_271 = torch.ops.aten.mul.Tensor(getitem_2036, sigmoid_165);  getitem_2036 = sigmoid_165 = None
    view_2143 = torch.ops.aten.view.default(mul_271, [1, 4, 384, 384, 64]);  mul_271 = None
    permute_1182 = torch.ops.aten.permute.default(view_2143, [0, 2, 3, 1, 4]);  view_2143 = None
    clone_195 = torch.ops.aten.clone.default(permute_1182, memory_format = torch.contiguous_format);  permute_1182 = None
    _unsafe_view_166 = torch.ops.aten._unsafe_view.default(clone_195, [1, 384, 384, 256]);  clone_195 = None
    transpose_55 = torch.ops.aten.transpose.int(getitem_2029, 1, 2);  getitem_2029 = None
    _to_copy_1200 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_21_triangle_attention_pair2qkvg2_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_21_triangle_attention_pair2qkvg2_weight = None
    _to_copy_1201 = torch.ops.aten._to_copy.default(transpose_55, dtype = torch.bfloat16);  transpose_55 = None
    t_440 = torch.ops.aten.t.default(_to_copy_1200);  _to_copy_1200 = None
    expand_133 = torch.ops.aten.expand.default(_to_copy_1201, [1, 384, 384, 256]);  _to_copy_1201 = None
    view_2144 = torch.ops.aten.view.default(expand_133, [384, 384, 256]);  expand_133 = None
    expand_134 = torch.ops.aten.expand.default(t_440, [1, 384, 256, 1024]);  t_440 = None
    view_2145 = torch.ops.aten.view.default(expand_134, [384, 256, 1024]);  expand_134 = None
    bmm_179 = torch.ops.aten.bmm.default(view_2144, view_2145);  view_2144 = view_2145 = None
    view_2146 = torch.ops.aten.view.default(bmm_179, [1, 384, 384, 1024]);  bmm_179 = None
    select_56 = torch.ops.aten.select.int(view_2138, 0, 1);  view_2138 = None
    view_2147 = torch.ops.aten.view.default(view_2146, [1, 384, 384, 4, 4, 64]);  view_2146 = None
    permute_1183 = torch.ops.aten.permute.default(view_2147, [4, 0, 3, 1, 2, 5]);  view_2147 = None
    view_2148 = torch.ops.aten.view.default(permute_1183, [4, 4, 384, 384, 64]);  permute_1183 = None
    unbind_int_98 = torch.ops.aten.unbind.int(view_2148);  view_2148 = None
    getitem_2040 = unbind_int_98[0]
    getitem_2041 = unbind_int_98[1]
    getitem_2042 = unbind_int_98[2]
    getitem_2043 = unbind_int_98[3];  unbind_int_98 = None
    expand_135 = torch.ops.aten.expand.default(select_56, [4, 384, 384, 384]);  select_56 = None
    _scaled_dot_product_efficient_attention_default_76 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2040, getitem_2041, getitem_2042, expand_135, False);  getitem_2040 = getitem_2041 = getitem_2042 = expand_135 = None
    getitem_2044 = _scaled_dot_product_efficient_attention_default_76[0];  _scaled_dot_product_efficient_attention_default_76 = None
    sigmoid_166 = torch.ops.aten.sigmoid.default(getitem_2043);  getitem_2043 = None
    mul_272 = torch.ops.aten.mul.Tensor(getitem_2044, sigmoid_166);  getitem_2044 = sigmoid_166 = None
    view_2149 = torch.ops.aten.view.default(mul_272, [1, 4, 384, 384, 64]);  mul_272 = None
    permute_1184 = torch.ops.aten.permute.default(view_2149, [0, 2, 3, 1, 4]);  view_2149 = None
    clone_196 = torch.ops.aten.clone.default(permute_1184, memory_format = torch.contiguous_format);  permute_1184 = None
    _unsafe_view_167 = torch.ops.aten._unsafe_view.default(clone_196, [1, 384, 384, 256]);  clone_196 = None
    cat_33 = torch.ops.aten.cat.default([_unsafe_view_166, _unsafe_view_167], dim = -1);  _unsafe_view_166 = _unsafe_view_167 = None
    slice_184 = torch.ops.aten.slice.Tensor(pairformer_stack_blocks_21_triangle_attention_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  pairformer_stack_blocks_21_triangle_attention_out_scalers = None
    unsqueeze_742 = torch.ops.aten.unsqueeze.default(slice_184, 1);  slice_184 = None
    mul_273 = torch.ops.aten.mul.Tensor(pairformer_stack_blocks_21_triangle_attention_linear_out_weight, unsqueeze_742);  pairformer_stack_blocks_21_triangle_attention_linear_out_weight = unsqueeze_742 = None
    _to_copy_1202 = torch.ops.aten._to_copy.default(mul_273, dtype = torch.bfloat16);  mul_273 = None
    t_441 = torch.ops.aten.t.default(_to_copy_1202);  _to_copy_1202 = None
    view_2150 = torch.ops.aten.view.default(cat_33, [147456, 512]);  cat_33 = None
    mm_409 = torch.ops.aten.mm.default(view_2150, t_441);  view_2150 = t_441 = None
    view_2151 = torch.ops.aten.view.default(mm_409, [1, 384, 384, 256]);  mm_409 = None
    add_226 = torch.ops.aten.add.Tensor(add_225, view_2151);  add_225 = view_2151 = None
    split_tensor_220 = torch.ops.aten.split.Tensor(add_219, 384, dim = -2)
    getitem_2048 = split_tensor_220[0];  split_tensor_220 = None
    _to_copy_1203 = torch.ops.aten._to_copy.default(getitem_2048, dtype = torch.float32);  getitem_2048 = None
    native_layer_norm_default_247 = torch.ops.aten.native_layer_norm.default(_to_copy_1203, [256], pairformer_stack_blocks_21_transition_pair_layer_norm_weight, pairformer_stack_blocks_21_transition_pair_layer_norm_bias, 1e-05);  _to_copy_1203 = pairformer_stack_blocks_21_transition_pair_layer_norm_weight = pairformer_stack_blocks_21_transition_pair_layer_norm_bias = None
    getitem_2049 = native_layer_norm_default_247[0];  native_layer_norm_default_247 = None
    _to_copy_1204 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_21_transition_pair_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_21_transition_pair_linear_no_bias_ab_weight = None
    _to_copy_1205 = torch.ops.aten._to_copy.default(getitem_2049, dtype = torch.bfloat16);  getitem_2049 = None
    t_442 = torch.ops.aten.t.default(_to_copy_1204);  _to_copy_1204 = None
    view_2152 = torch.ops.aten.view.default(_to_copy_1205, [147456, 256]);  _to_copy_1205 = None
    mm_410 = torch.ops.aten.mm.default(view_2152, t_442);  view_2152 = t_442 = None
    view_2153 = torch.ops.aten.view.default(mm_410, [1, 384, 384, 1024]);  mm_410 = None
    split_tensor_221 = torch.ops.aten.split.Tensor(view_2153, 512, dim = -1);  view_2153 = None
    getitem_2052 = split_tensor_221[0]
    getitem_2053 = split_tensor_221[1];  split_tensor_221 = None
    silu_57 = torch.ops.aten.silu.default(getitem_2052);  getitem_2052 = None
    mul_274 = torch.ops.aten.mul.Tensor(silu_57, getitem_2053);  silu_57 = getitem_2053 = None
    _to_copy_1206 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_21_transition_pair_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_21_transition_pair_linear_out_weight = None
    t_443 = torch.ops.aten.t.default(_to_copy_1206);  _to_copy_1206 = None
    view_2155 = torch.ops.aten.view.default(mul_274, [147456, 512]);  mul_274 = None
    mm_411 = torch.ops.aten.mm.default(view_2155, t_443);  view_2155 = t_443 = None
    view_2156 = torch.ops.aten.view.default(mm_411, [1, 384, 384, 256]);  mm_411 = None
    add_227 = torch.ops.aten.add.Tensor(add_226, view_2156);  add_226 = view_2156 = None
    _to_copy_1207 = torch.ops.aten._to_copy.default(add_223, dtype = torch.float32)
    native_layer_norm_default_248 = torch.ops.aten.native_layer_norm.default(_to_copy_1207, [384], pairformer_stack_blocks_21_attention_pair_bias_single_layer_norm_weight, pairformer_stack_blocks_21_attention_pair_bias_single_layer_norm_bias, 1e-05);  _to_copy_1207 = pairformer_stack_blocks_21_attention_pair_bias_single_layer_norm_weight = pairformer_stack_blocks_21_attention_pair_bias_single_layer_norm_bias = None
    getitem_2054 = native_layer_norm_default_248[0];  native_layer_norm_default_248 = None
    _to_copy_1208 = torch.ops.aten._to_copy.default(add_219, dtype = torch.float32);  add_219 = None
    native_layer_norm_default_249 = torch.ops.aten.native_layer_norm.default(_to_copy_1208, [256], pairformer_stack_blocks_21_attention_pair_bias_pair_layer_norm_weight, pairformer_stack_blocks_21_attention_pair_bias_pair_layer_norm_bias, 1e-05);  _to_copy_1208 = pairformer_stack_blocks_21_attention_pair_bias_pair_layer_norm_weight = pairformer_stack_blocks_21_attention_pair_bias_pair_layer_norm_bias = None
    getitem_2057 = native_layer_norm_default_249[0];  native_layer_norm_default_249 = None
    _to_copy_1209 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_21_attention_pair_bias_pair_linear_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_21_attention_pair_bias_pair_linear_weight = None
    _to_copy_1210 = torch.ops.aten._to_copy.default(getitem_2057, dtype = torch.bfloat16);  getitem_2057 = None
    t_444 = torch.ops.aten.t.default(_to_copy_1209);  _to_copy_1209 = None
    view_2157 = torch.ops.aten.view.default(_to_copy_1210, [147456, 256]);  _to_copy_1210 = None
    mm_412 = torch.ops.aten.mm.default(view_2157, t_444);  view_2157 = t_444 = None
    view_2158 = torch.ops.aten.view.default(mm_412, [1, 384, 384, 16]);  mm_412 = None
    permute_1185 = torch.ops.aten.permute.default(view_2158, [0, 3, 1, 2]);  view_2158 = None
    view_2159 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384])
    bitwise_not_133 = torch.ops.aten.bitwise_not.default(view_2159);  view_2159 = None
    masked_fill_133 = torch.ops.aten.masked_fill.Scalar(permute_1185, bitwise_not_133, -10000);  permute_1185 = bitwise_not_133 = None
    _to_copy_1211 = torch.ops.aten._to_copy.default(getitem_2054, dtype = torch.bfloat16);  getitem_2054 = None
    _to_copy_1212 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_21_attention_pair_bias_attention_input2qkvg_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_21_attention_pair_bias_attention_input2qkvg_weight = None
    unsqueeze_743 = torch.ops.aten.unsqueeze.default(_to_copy_1211, 3);  _to_copy_1211 = None
    unsqueeze_744 = torch.ops.aten.unsqueeze.default(unsqueeze_743, 4);  unsqueeze_743 = None
    unsqueeze_745 = torch.ops.aten.unsqueeze.default(unsqueeze_744, 5);  unsqueeze_744 = None
    permute_1186 = torch.ops.aten.permute.default(unsqueeze_745, [3, 0, 4, 1, 5, 2]);  unsqueeze_745 = None
    unsqueeze_746 = torch.ops.aten.unsqueeze.default(_to_copy_1212, 4);  _to_copy_1212 = None
    unsqueeze_747 = torch.ops.aten.unsqueeze.default(unsqueeze_746, 5);  unsqueeze_746 = None
    permute_1187 = torch.ops.aten.permute.default(unsqueeze_747, [1, 4, 2, 5, 3, 0]);  unsqueeze_747 = None
    permute_1188 = torch.ops.aten.permute.default(permute_1186, [3, 5, 0, 1, 2, 4]);  permute_1186 = None
    view_2160 = torch.ops.aten.view.default(permute_1188, [1, 384, 384]);  permute_1188 = None
    permute_1189 = torch.ops.aten.permute.default(permute_1187, [5, 0, 1, 2, 4, 3]);  permute_1187 = None
    view_2161 = torch.ops.aten.view.default(permute_1189, [1, 384, 1536]);  permute_1189 = None
    bmm_180 = torch.ops.aten.bmm.default(view_2160, view_2161);  view_2160 = view_2161 = None
    view_2162 = torch.ops.aten.view.default(bmm_180, [384, 1, 4, 1, 16, 24]);  bmm_180 = None
    permute_1190 = torch.ops.aten.permute.default(view_2162, [2, 3, 4, 0, 5, 1]);  view_2162 = None
    view_2163 = torch.ops.aten.view.default(permute_1190, [4, 1, 16, 384, 24]);  permute_1190 = None
    unbind_int_99 = torch.ops.aten.unbind.int(view_2163);  view_2163 = None
    getitem_2060 = unbind_int_99[0]
    getitem_2061 = unbind_int_99[1]
    getitem_2062 = unbind_int_99[2]
    getitem_2063 = unbind_int_99[3];  unbind_int_99 = None
    view_2164 = torch.ops.aten.view.default(pairformer_stack_blocks_21_attention_pair_bias_attention_query_bias, [1, 16, 1, 24]);  pairformer_stack_blocks_21_attention_pair_bias_attention_query_bias = None
    add_228 = torch.ops.aten.add.Tensor(getitem_2060, view_2164);  getitem_2060 = view_2164 = None
    _to_copy_1213 = torch.ops.aten._to_copy.default(add_228, dtype = torch.bfloat16);  add_228 = None
    expand_136 = torch.ops.aten.expand.default(masked_fill_133, [1, 16, 384, 384]);  masked_fill_133 = None
    _scaled_dot_product_efficient_attention_default_77 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_1213, getitem_2061, getitem_2062, expand_136, False);  _to_copy_1213 = getitem_2061 = getitem_2062 = expand_136 = None
    getitem_2064 = _scaled_dot_product_efficient_attention_default_77[0];  _scaled_dot_product_efficient_attention_default_77 = None
    add_229 = torch.ops.aten.add.Tensor(getitem_2063, 1);  getitem_2063 = None
    sigmoid_167 = torch.ops.aten.sigmoid.default(add_229);  add_229 = None
    mul_275 = torch.ops.aten.mul.Tensor(getitem_2064, sigmoid_167);  getitem_2064 = sigmoid_167 = None
    _to_copy_1214 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_21_attention_pair_bias_attention_output_proj_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_21_attention_pair_bias_attention_output_proj_weight = None
    unsqueeze_748 = torch.ops.aten.unsqueeze.default(mul_275, 4);  mul_275 = None
    permute_1191 = torch.ops.aten.permute.default(unsqueeze_748, [0, 2, 4, 3, 1]);  unsqueeze_748 = None
    unsqueeze_749 = torch.ops.aten.unsqueeze.default(_to_copy_1214, 3);  _to_copy_1214 = None
    unsqueeze_750 = torch.ops.aten.unsqueeze.default(unsqueeze_749, 4);  unsqueeze_749 = None
    permute_1192 = torch.ops.aten.permute.default(unsqueeze_750, [3, 4, 2, 1, 0]);  unsqueeze_750 = None
    permute_1193 = torch.ops.aten.permute.default(permute_1191, [1, 3, 4, 0, 2]);  permute_1191 = None
    clone_197 = torch.ops.aten.clone.default(permute_1193, memory_format = torch.contiguous_format);  permute_1193 = None
    _unsafe_view_168 = torch.ops.aten._unsafe_view.default(clone_197, [1, 384, 384]);  clone_197 = None
    permute_1194 = torch.ops.aten.permute.default(permute_1192, [3, 4, 0, 2, 1]);  permute_1192 = None
    clone_198 = torch.ops.aten.clone.default(permute_1194, memory_format = torch.contiguous_format);  permute_1194 = None
    _unsafe_view_169 = torch.ops.aten._unsafe_view.default(clone_198, [1, 384, 384]);  clone_198 = None
    bmm_181 = torch.ops.aten.bmm.default(_unsafe_view_168, _unsafe_view_169);  _unsafe_view_168 = _unsafe_view_169 = None
    view_2165 = torch.ops.aten.view.default(bmm_181, [384, 1, 1, 1, 384]);  bmm_181 = None
    permute_1195 = torch.ops.aten.permute.default(view_2165, [3, 0, 4, 1, 2]);  view_2165 = None
    view_2166 = torch.ops.aten.view.default(permute_1195, [1, 384, 384]);  permute_1195 = None
    unsqueeze_751 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
    mul_276 = torch.ops.aten.mul.Tensor(view_2166, unsqueeze_751);  view_2166 = unsqueeze_751 = None
    add_230 = torch.ops.aten.add.Tensor(add_223, mul_276);  mul_276 = None
    split_tensor_222 = torch.ops.aten.split.Tensor(add_223, 384, dim = -2);  add_223 = None
    getitem_2068 = split_tensor_222[0];  split_tensor_222 = None
    _to_copy_1215 = torch.ops.aten._to_copy.default(getitem_2068, dtype = torch.float32);  getitem_2068 = None
    native_layer_norm_default_250 = torch.ops.aten.native_layer_norm.default(_to_copy_1215, [384], pairformer_stack_blocks_21_transition_single_layer_norm_weight, pairformer_stack_blocks_21_transition_single_layer_norm_bias, 1e-05);  _to_copy_1215 = pairformer_stack_blocks_21_transition_single_layer_norm_weight = pairformer_stack_blocks_21_transition_single_layer_norm_bias = None
    getitem_2069 = native_layer_norm_default_250[0];  native_layer_norm_default_250 = None
    _to_copy_1216 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_21_transition_single_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_21_transition_single_linear_no_bias_ab_weight = None
    _to_copy_1217 = torch.ops.aten._to_copy.default(getitem_2069, dtype = torch.bfloat16);  getitem_2069 = None
    t_445 = torch.ops.aten.t.default(_to_copy_1216);  _to_copy_1216 = None
    view_2167 = torch.ops.aten.view.default(_to_copy_1217, [384, 384]);  _to_copy_1217 = None
    mm_413 = torch.ops.aten.mm.default(view_2167, t_445);  view_2167 = t_445 = None
    view_2168 = torch.ops.aten.view.default(mm_413, [1, 384, 1536]);  mm_413 = None
    split_tensor_223 = torch.ops.aten.split.Tensor(view_2168, 768, dim = -1);  view_2168 = None
    getitem_2072 = split_tensor_223[0]
    getitem_2073 = split_tensor_223[1];  split_tensor_223 = None
    silu_58 = torch.ops.aten.silu.default(getitem_2072);  getitem_2072 = None
    mul_277 = torch.ops.aten.mul.Tensor(silu_58, getitem_2073);  silu_58 = getitem_2073 = None
    _to_copy_1218 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_21_transition_single_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_21_transition_single_linear_out_weight = None
    t_446 = torch.ops.aten.t.default(_to_copy_1218);  _to_copy_1218 = None
    view_2170 = torch.ops.aten.view.default(mul_277, [384, 768]);  mul_277 = None
    mm_414 = torch.ops.aten.mm.default(view_2170, t_446);  view_2170 = t_446 = None
    view_2171 = torch.ops.aten.view.default(mm_414, [1, 384, 384]);  mm_414 = None
    add_231 = torch.ops.aten.add.Tensor(add_230, view_2171);  add_230 = view_2171 = None
    _to_copy_1219 = torch.ops.aten._to_copy.default(add_227, dtype = torch.float32)
    native_layer_norm_default_251 = torch.ops.aten.native_layer_norm.default(_to_copy_1219, [256], pairformer_stack_blocks_22_triangle_multiplication_layernorm_z_in_weight, pairformer_stack_blocks_22_triangle_multiplication_layernorm_z_in_bias, 1e-05);  _to_copy_1219 = pairformer_stack_blocks_22_triangle_multiplication_layernorm_z_in_weight = pairformer_stack_blocks_22_triangle_multiplication_layernorm_z_in_bias = None
    getitem_2074 = native_layer_norm_default_251[0];  native_layer_norm_default_251 = None
    split_with_sizes_default_56 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_22_triangle_multiplication_merged_linear_p_weight, [512, 512]);  pairformer_stack_blocks_22_triangle_multiplication_merged_linear_p_weight = None
    getitem_2077 = split_with_sizes_default_56[0]
    getitem_2078 = split_with_sizes_default_56[1];  split_with_sizes_default_56 = None
    split_with_sizes_default_57 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_22_triangle_multiplication_merged_linear_g_weight, [512, 512, 256]);  pairformer_stack_blocks_22_triangle_multiplication_merged_linear_g_weight = None
    getitem_2079 = split_with_sizes_default_57[0]
    getitem_2080 = split_with_sizes_default_57[1]
    getitem_2081 = split_with_sizes_default_57[2];  split_with_sizes_default_57 = None
    _to_copy_1220 = torch.ops.aten._to_copy.default(getitem_2077, dtype = torch.bfloat16);  getitem_2077 = None
    _to_copy_1221 = torch.ops.aten._to_copy.default(getitem_2074, dtype = torch.bfloat16)
    t_447 = torch.ops.aten.t.default(_to_copy_1220);  _to_copy_1220 = None
    view_2172 = torch.ops.aten.view.default(_to_copy_1221, [147456, 256]);  _to_copy_1221 = None
    mm_415 = torch.ops.aten.mm.default(view_2172, t_447);  view_2172 = t_447 = None
    view_2173 = torch.ops.aten.view.default(mm_415, [1, 384, 384, 512]);  mm_415 = None
    _to_copy_1222 = torch.ops.aten._to_copy.default(getitem_2079, dtype = torch.bfloat16);  getitem_2079 = None
    _to_copy_1223 = torch.ops.aten._to_copy.default(getitem_2074, dtype = torch.bfloat16)
    t_448 = torch.ops.aten.t.default(_to_copy_1222);  _to_copy_1222 = None
    view_2174 = torch.ops.aten.view.default(_to_copy_1223, [147456, 256]);  _to_copy_1223 = None
    mm_416 = torch.ops.aten.mm.default(view_2174, t_448);  view_2174 = t_448 = None
    view_2175 = torch.ops.aten.view.default(mm_416, [1, 384, 384, 512]);  mm_416 = None
    sigmoid_168 = torch.ops.aten.sigmoid.default(view_2175);  view_2175 = None
    mul_278 = torch.ops.aten.mul.Tensor(view_2173, sigmoid_168);  view_2173 = sigmoid_168 = None
    unsqueeze_752 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
    bitwise_not_134 = torch.ops.aten.bitwise_not.default(unsqueeze_752);  unsqueeze_752 = None
    masked_fill_134 = torch.ops.aten.masked_fill.Scalar(mul_278, bitwise_not_134, 0);  mul_278 = bitwise_not_134 = None
    split_tensor_224 = torch.ops.aten.split.Tensor(masked_fill_134, 256, dim = -1)
    getitem_2084 = split_tensor_224[0];  split_tensor_224 = None
    unsqueeze_755 = torch.ops.aten.unsqueeze.default(getitem_2084, 4);  getitem_2084 = None
    permute_1200 = torch.ops.aten.permute.default(unsqueeze_755, [0, 1, 4, 3, 2]);  unsqueeze_755 = None
    permute_1201 = torch.ops.aten.permute.default(permute_1200, [3, 1, 4, 0, 2]);  permute_1200 = None
    view_2178 = torch.ops.aten.view.default(permute_1201, [256, 384, 384]);  permute_1201 = None
    split_tensor_225 = torch.ops.aten.split.Tensor(masked_fill_134, 256, dim = -1);  masked_fill_134 = None
    getitem_2087 = split_tensor_225[1];  split_tensor_225 = None
    unsqueeze_756 = torch.ops.aten.unsqueeze.default(getitem_2087, 4);  getitem_2087 = None
    permute_1202 = torch.ops.aten.permute.default(unsqueeze_756, [0, 4, 1, 3, 2]);  unsqueeze_756 = None
    permute_1203 = torch.ops.aten.permute.default(permute_1202, [3, 4, 0, 2, 1]);  permute_1202 = None
    view_2179 = torch.ops.aten.view.default(permute_1203, [256, 384, 384]);  permute_1203 = None
    bmm_182 = torch.ops.aten.bmm.default(view_2178, view_2179);  view_2178 = view_2179 = None
    view_2180 = torch.ops.aten.view.default(bmm_182, [256, 384, 1, 1, 384]);  bmm_182 = None
    permute_1204 = torch.ops.aten.permute.default(view_2180, [3, 1, 4, 0, 2]);  view_2180 = None
    view_2181 = torch.ops.aten.view.default(permute_1204, [1, 384, 384, 256]);  permute_1204 = None
    _to_copy_1224 = torch.ops.aten._to_copy.default(getitem_2078, dtype = torch.bfloat16);  getitem_2078 = None
    _to_copy_1225 = torch.ops.aten._to_copy.default(getitem_2074, dtype = torch.bfloat16)
    t_449 = torch.ops.aten.t.default(_to_copy_1224);  _to_copy_1224 = None
    view_2182 = torch.ops.aten.view.default(_to_copy_1225, [147456, 256]);  _to_copy_1225 = None
    mm_417 = torch.ops.aten.mm.default(view_2182, t_449);  view_2182 = t_449 = None
    view_2183 = torch.ops.aten.view.default(mm_417, [1, 384, 384, 512]);  mm_417 = None
    _to_copy_1226 = torch.ops.aten._to_copy.default(getitem_2080, dtype = torch.bfloat16);  getitem_2080 = None
    _to_copy_1227 = torch.ops.aten._to_copy.default(getitem_2074, dtype = torch.bfloat16)
    t_450 = torch.ops.aten.t.default(_to_copy_1226);  _to_copy_1226 = None
    view_2184 = torch.ops.aten.view.default(_to_copy_1227, [147456, 256]);  _to_copy_1227 = None
    mm_418 = torch.ops.aten.mm.default(view_2184, t_450);  view_2184 = t_450 = None
    view_2185 = torch.ops.aten.view.default(mm_418, [1, 384, 384, 512]);  mm_418 = None
    sigmoid_169 = torch.ops.aten.sigmoid.default(view_2185);  view_2185 = None
    mul_279 = torch.ops.aten.mul.Tensor(view_2183, sigmoid_169);  view_2183 = sigmoid_169 = None
    view_2186 = torch.ops.aten.view.default(mul_279, [147456, 512]);  mul_279 = None
    view_2187 = torch.ops.aten.view.default(view_2186, [1, 384, 384, 512]);  view_2186 = None
    transpose_56 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
    unsqueeze_757 = torch.ops.aten.unsqueeze.default(transpose_56, 3);  transpose_56 = None
    clone_199 = torch.ops.aten.clone.default(unsqueeze_757, memory_format = torch.contiguous_format);  unsqueeze_757 = None
    bitwise_not_135 = torch.ops.aten.bitwise_not.default(clone_199);  clone_199 = None
    masked_fill_135 = torch.ops.aten.masked_fill.Scalar(view_2187, bitwise_not_135, 0);  view_2187 = bitwise_not_135 = None
    view_2188 = torch.ops.aten.view.default(masked_fill_135, [147456, 512]);  masked_fill_135 = None
    view_2192 = torch.ops.aten.view.default(view_2188, [1, 384, 384, 512])
    split_tensor_226 = torch.ops.aten.split.Tensor(view_2192, 256, dim = -1);  view_2192 = None
    getitem_2090 = split_tensor_226[0];  split_tensor_226 = None
    unsqueeze_760 = torch.ops.aten.unsqueeze.default(getitem_2090, 4);  getitem_2090 = None
    permute_1209 = torch.ops.aten.permute.default(unsqueeze_760, [0, 2, 4, 3, 1]);  unsqueeze_760 = None
    permute_1210 = torch.ops.aten.permute.default(permute_1209, [3, 1, 4, 0, 2]);  permute_1209 = None
    view_2193 = torch.ops.aten.view.default(permute_1210, [256, 384, 384]);  permute_1210 = None
    view_2194 = torch.ops.aten.view.default(view_2188, [1, 384, 384, 512]);  view_2188 = None
    split_tensor_227 = torch.ops.aten.split.Tensor(view_2194, 256, dim = -1);  view_2194 = None
    getitem_2093 = split_tensor_227[1];  split_tensor_227 = None
    unsqueeze_761 = torch.ops.aten.unsqueeze.default(getitem_2093, 4);  getitem_2093 = None
    permute_1211 = torch.ops.aten.permute.default(unsqueeze_761, [0, 4, 2, 3, 1]);  unsqueeze_761 = None
    permute_1212 = torch.ops.aten.permute.default(permute_1211, [3, 4, 0, 2, 1]);  permute_1211 = None
    view_2195 = torch.ops.aten.view.default(permute_1212, [256, 384, 384]);  permute_1212 = None
    bmm_183 = torch.ops.aten.bmm.default(view_2193, view_2195);  view_2193 = view_2195 = None
    view_2196 = torch.ops.aten.view.default(bmm_183, [256, 384, 1, 1, 384]);  bmm_183 = None
    permute_1213 = torch.ops.aten.permute.default(view_2196, [3, 1, 4, 0, 2]);  view_2196 = None
    view_2197 = torch.ops.aten.view.default(permute_1213, [1, 384, 384, 256]);  permute_1213 = None
    _to_copy_1228 = torch.ops.aten._to_copy.default(view_2181, dtype = torch.float32);  view_2181 = None
    native_layer_norm_default_252 = torch.ops.aten.native_layer_norm.default(_to_copy_1228, [256], None, None, 1e-05);  _to_copy_1228 = None
    getitem_2094 = native_layer_norm_default_252[0];  native_layer_norm_default_252 = None
    _to_copy_1229 = torch.ops.aten._to_copy.default(view_2197, dtype = torch.float32);  view_2197 = None
    native_layer_norm_default_253 = torch.ops.aten.native_layer_norm.default(_to_copy_1229, [256], None, None, 1e-05);  _to_copy_1229 = None
    getitem_2097 = native_layer_norm_default_253[0];  native_layer_norm_default_253 = None
    add_232 = torch.ops.aten.add.Tensor(getitem_2094, getitem_2097);  getitem_2094 = getitem_2097 = None
    _to_copy_1230 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_22_triangle_multiplication_linear_z_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_22_triangle_multiplication_linear_z_out_weight = None
    _to_copy_1231 = torch.ops.aten._to_copy.default(add_232, dtype = torch.bfloat16);  add_232 = None
    t_451 = torch.ops.aten.t.default(_to_copy_1230);  _to_copy_1230 = None
    view_2198 = torch.ops.aten.view.default(_to_copy_1231, [147456, 256]);  _to_copy_1231 = None
    mm_419 = torch.ops.aten.mm.default(view_2198, t_451);  view_2198 = t_451 = None
    view_2199 = torch.ops.aten.view.default(mm_419, [1, 384, 384, 256]);  mm_419 = None
    _to_copy_1232 = torch.ops.aten._to_copy.default(getitem_2081, dtype = torch.bfloat16);  getitem_2081 = None
    _to_copy_1233 = torch.ops.aten._to_copy.default(getitem_2074, dtype = torch.bfloat16);  getitem_2074 = None
    t_452 = torch.ops.aten.t.default(_to_copy_1232);  _to_copy_1232 = None
    view_2200 = torch.ops.aten.view.default(_to_copy_1233, [147456, 256]);  _to_copy_1233 = None
    mm_420 = torch.ops.aten.mm.default(view_2200, t_452);  view_2200 = t_452 = None
    view_2201 = torch.ops.aten.view.default(mm_420, [1, 384, 384, 256]);  mm_420 = None
    sigmoid_170 = torch.ops.aten.sigmoid.default(view_2201);  view_2201 = None
    mul_280 = torch.ops.aten.mul.Tensor(view_2199, sigmoid_170);  view_2199 = sigmoid_170 = None
    add_233 = torch.ops.aten.add.Tensor(add_227, mul_280);  mul_280 = None
    _to_copy_1234 = torch.ops.aten._to_copy.default(add_227, dtype = torch.float32)
    native_layer_norm_default_254 = torch.ops.aten.native_layer_norm.default(_to_copy_1234, [256], None, None, 1e-05);  _to_copy_1234 = None
    getitem_2100 = native_layer_norm_default_254[0];  native_layer_norm_default_254 = None
    _to_copy_1235 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_22_triangle_attention_pair2b_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_22_triangle_attention_pair2b_weight = None
    _to_copy_1236 = torch.ops.aten._to_copy.default(getitem_2100, dtype = torch.bfloat16)
    t_453 = torch.ops.aten.t.default(_to_copy_1235);  _to_copy_1235 = None
    view_2202 = torch.ops.aten.view.default(_to_copy_1236, [147456, 256]);  _to_copy_1236 = None
    mm_421 = torch.ops.aten.mm.default(view_2202, t_453);  view_2202 = t_453 = None
    view_2203 = torch.ops.aten.view.default(mm_421, [1, 384, 384, 8]);  mm_421 = None
    view_2204 = torch.ops.aten.view.default(view_2203, [1, 384, 384, 2, 4]);  view_2203 = None
    permute_1214 = torch.ops.aten.permute.default(view_2204, [0, 3, 4, 1, 2]);  view_2204 = None
    view_2205 = torch.ops.aten.view.default(permute_1214, [1, 2, 4, 1, 384, 384]);  permute_1214 = None
    view_2206 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 384, 384])
    bitwise_not_136 = torch.ops.aten.bitwise_not.default(view_2206);  view_2206 = None
    masked_fill_136 = torch.ops.aten.masked_fill.Scalar(view_2205, bitwise_not_136, -10000);  view_2205 = bitwise_not_136 = None
    view_2207 = torch.ops.aten.view.default(masked_fill_136, [1, 2, 4, 384, 384]);  masked_fill_136 = None
    permute_1215 = torch.ops.aten.permute.default(view_2207, [1, 0, 2, 3, 4]);  view_2207 = None
    view_2208 = torch.ops.aten.view.default(permute_1215, [2, 4, 1, 384, 384]);  permute_1215 = None
    _to_copy_1237 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_22_triangle_attention_pair2qkvg1_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_22_triangle_attention_pair2qkvg1_weight = None
    _to_copy_1238 = torch.ops.aten._to_copy.default(getitem_2100, dtype = torch.bfloat16)
    t_454 = torch.ops.aten.t.default(_to_copy_1237);  _to_copy_1237 = None
    view_2209 = torch.ops.aten.view.default(_to_copy_1238, [147456, 256]);  _to_copy_1238 = None
    mm_422 = torch.ops.aten.mm.default(view_2209, t_454);  view_2209 = t_454 = None
    view_2210 = torch.ops.aten.view.default(mm_422, [1, 384, 384, 1024]);  mm_422 = None
    select_57 = torch.ops.aten.select.int(view_2208, 0, 0)
    view_2211 = torch.ops.aten.view.default(view_2210, [1, 384, 384, 4, 4, 64]);  view_2210 = None
    permute_1216 = torch.ops.aten.permute.default(view_2211, [4, 0, 3, 1, 2, 5]);  view_2211 = None
    view_2212 = torch.ops.aten.view.default(permute_1216, [4, 4, 384, 384, 64]);  permute_1216 = None
    unbind_int_100 = torch.ops.aten.unbind.int(view_2212);  view_2212 = None
    getitem_2103 = unbind_int_100[0]
    getitem_2104 = unbind_int_100[1]
    getitem_2105 = unbind_int_100[2]
    getitem_2106 = unbind_int_100[3];  unbind_int_100 = None
    expand_137 = torch.ops.aten.expand.default(select_57, [4, 384, 384, 384]);  select_57 = None
    _scaled_dot_product_efficient_attention_default_78 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2103, getitem_2104, getitem_2105, expand_137, False);  getitem_2103 = getitem_2104 = getitem_2105 = expand_137 = None
    getitem_2107 = _scaled_dot_product_efficient_attention_default_78[0];  _scaled_dot_product_efficient_attention_default_78 = None
    sigmoid_171 = torch.ops.aten.sigmoid.default(getitem_2106);  getitem_2106 = None
    mul_281 = torch.ops.aten.mul.Tensor(getitem_2107, sigmoid_171);  getitem_2107 = sigmoid_171 = None
    view_2213 = torch.ops.aten.view.default(mul_281, [1, 4, 384, 384, 64]);  mul_281 = None
    permute_1217 = torch.ops.aten.permute.default(view_2213, [0, 2, 3, 1, 4]);  view_2213 = None
    clone_200 = torch.ops.aten.clone.default(permute_1217, memory_format = torch.contiguous_format);  permute_1217 = None
    _unsafe_view_170 = torch.ops.aten._unsafe_view.default(clone_200, [1, 384, 384, 256]);  clone_200 = None
    transpose_57 = torch.ops.aten.transpose.int(getitem_2100, 1, 2);  getitem_2100 = None
    _to_copy_1239 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_22_triangle_attention_pair2qkvg2_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_22_triangle_attention_pair2qkvg2_weight = None
    _to_copy_1240 = torch.ops.aten._to_copy.default(transpose_57, dtype = torch.bfloat16);  transpose_57 = None
    t_455 = torch.ops.aten.t.default(_to_copy_1239);  _to_copy_1239 = None
    expand_138 = torch.ops.aten.expand.default(_to_copy_1240, [1, 384, 384, 256]);  _to_copy_1240 = None
    view_2214 = torch.ops.aten.view.default(expand_138, [384, 384, 256]);  expand_138 = None
    expand_139 = torch.ops.aten.expand.default(t_455, [1, 384, 256, 1024]);  t_455 = None
    view_2215 = torch.ops.aten.view.default(expand_139, [384, 256, 1024]);  expand_139 = None
    bmm_184 = torch.ops.aten.bmm.default(view_2214, view_2215);  view_2214 = view_2215 = None
    view_2216 = torch.ops.aten.view.default(bmm_184, [1, 384, 384, 1024]);  bmm_184 = None
    select_58 = torch.ops.aten.select.int(view_2208, 0, 1);  view_2208 = None
    view_2217 = torch.ops.aten.view.default(view_2216, [1, 384, 384, 4, 4, 64]);  view_2216 = None
    permute_1218 = torch.ops.aten.permute.default(view_2217, [4, 0, 3, 1, 2, 5]);  view_2217 = None
    view_2218 = torch.ops.aten.view.default(permute_1218, [4, 4, 384, 384, 64]);  permute_1218 = None
    unbind_int_101 = torch.ops.aten.unbind.int(view_2218);  view_2218 = None
    getitem_2111 = unbind_int_101[0]
    getitem_2112 = unbind_int_101[1]
    getitem_2113 = unbind_int_101[2]
    getitem_2114 = unbind_int_101[3];  unbind_int_101 = None
    expand_140 = torch.ops.aten.expand.default(select_58, [4, 384, 384, 384]);  select_58 = None
    _scaled_dot_product_efficient_attention_default_79 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2111, getitem_2112, getitem_2113, expand_140, False);  getitem_2111 = getitem_2112 = getitem_2113 = expand_140 = None
    getitem_2115 = _scaled_dot_product_efficient_attention_default_79[0];  _scaled_dot_product_efficient_attention_default_79 = None
    sigmoid_172 = torch.ops.aten.sigmoid.default(getitem_2114);  getitem_2114 = None
    mul_282 = torch.ops.aten.mul.Tensor(getitem_2115, sigmoid_172);  getitem_2115 = sigmoid_172 = None
    view_2219 = torch.ops.aten.view.default(mul_282, [1, 4, 384, 384, 64]);  mul_282 = None
    permute_1219 = torch.ops.aten.permute.default(view_2219, [0, 2, 3, 1, 4]);  view_2219 = None
    clone_201 = torch.ops.aten.clone.default(permute_1219, memory_format = torch.contiguous_format);  permute_1219 = None
    _unsafe_view_171 = torch.ops.aten._unsafe_view.default(clone_201, [1, 384, 384, 256]);  clone_201 = None
    cat_34 = torch.ops.aten.cat.default([_unsafe_view_170, _unsafe_view_171], dim = -1);  _unsafe_view_170 = _unsafe_view_171 = None
    slice_185 = torch.ops.aten.slice.Tensor(pairformer_stack_blocks_22_triangle_attention_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  pairformer_stack_blocks_22_triangle_attention_out_scalers = None
    unsqueeze_762 = torch.ops.aten.unsqueeze.default(slice_185, 1);  slice_185 = None
    mul_283 = torch.ops.aten.mul.Tensor(pairformer_stack_blocks_22_triangle_attention_linear_out_weight, unsqueeze_762);  pairformer_stack_blocks_22_triangle_attention_linear_out_weight = unsqueeze_762 = None
    _to_copy_1241 = torch.ops.aten._to_copy.default(mul_283, dtype = torch.bfloat16);  mul_283 = None
    t_456 = torch.ops.aten.t.default(_to_copy_1241);  _to_copy_1241 = None
    view_2220 = torch.ops.aten.view.default(cat_34, [147456, 512]);  cat_34 = None
    mm_423 = torch.ops.aten.mm.default(view_2220, t_456);  view_2220 = t_456 = None
    view_2221 = torch.ops.aten.view.default(mm_423, [1, 384, 384, 256]);  mm_423 = None
    add_234 = torch.ops.aten.add.Tensor(add_233, view_2221);  add_233 = view_2221 = None
    split_tensor_228 = torch.ops.aten.split.Tensor(add_227, 384, dim = -2)
    getitem_2119 = split_tensor_228[0];  split_tensor_228 = None
    _to_copy_1242 = torch.ops.aten._to_copy.default(getitem_2119, dtype = torch.float32);  getitem_2119 = None
    native_layer_norm_default_255 = torch.ops.aten.native_layer_norm.default(_to_copy_1242, [256], pairformer_stack_blocks_22_transition_pair_layer_norm_weight, pairformer_stack_blocks_22_transition_pair_layer_norm_bias, 1e-05);  _to_copy_1242 = pairformer_stack_blocks_22_transition_pair_layer_norm_weight = pairformer_stack_blocks_22_transition_pair_layer_norm_bias = None
    getitem_2120 = native_layer_norm_default_255[0];  native_layer_norm_default_255 = None
    _to_copy_1243 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_22_transition_pair_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_22_transition_pair_linear_no_bias_ab_weight = None
    _to_copy_1244 = torch.ops.aten._to_copy.default(getitem_2120, dtype = torch.bfloat16);  getitem_2120 = None
    t_457 = torch.ops.aten.t.default(_to_copy_1243);  _to_copy_1243 = None
    view_2222 = torch.ops.aten.view.default(_to_copy_1244, [147456, 256]);  _to_copy_1244 = None
    mm_424 = torch.ops.aten.mm.default(view_2222, t_457);  view_2222 = t_457 = None
    view_2223 = torch.ops.aten.view.default(mm_424, [1, 384, 384, 1024]);  mm_424 = None
    split_tensor_229 = torch.ops.aten.split.Tensor(view_2223, 512, dim = -1);  view_2223 = None
    getitem_2123 = split_tensor_229[0]
    getitem_2124 = split_tensor_229[1];  split_tensor_229 = None
    silu_59 = torch.ops.aten.silu.default(getitem_2123);  getitem_2123 = None
    mul_284 = torch.ops.aten.mul.Tensor(silu_59, getitem_2124);  silu_59 = getitem_2124 = None
    _to_copy_1245 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_22_transition_pair_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_22_transition_pair_linear_out_weight = None
    t_458 = torch.ops.aten.t.default(_to_copy_1245);  _to_copy_1245 = None
    view_2225 = torch.ops.aten.view.default(mul_284, [147456, 512]);  mul_284 = None
    mm_425 = torch.ops.aten.mm.default(view_2225, t_458);  view_2225 = t_458 = None
    view_2226 = torch.ops.aten.view.default(mm_425, [1, 384, 384, 256]);  mm_425 = None
    add_235 = torch.ops.aten.add.Tensor(add_234, view_2226);  add_234 = view_2226 = None
    _to_copy_1246 = torch.ops.aten._to_copy.default(add_231, dtype = torch.float32)
    native_layer_norm_default_256 = torch.ops.aten.native_layer_norm.default(_to_copy_1246, [384], pairformer_stack_blocks_22_attention_pair_bias_single_layer_norm_weight, pairformer_stack_blocks_22_attention_pair_bias_single_layer_norm_bias, 1e-05);  _to_copy_1246 = pairformer_stack_blocks_22_attention_pair_bias_single_layer_norm_weight = pairformer_stack_blocks_22_attention_pair_bias_single_layer_norm_bias = None
    getitem_2125 = native_layer_norm_default_256[0];  native_layer_norm_default_256 = None
    _to_copy_1247 = torch.ops.aten._to_copy.default(add_227, dtype = torch.float32);  add_227 = None
    native_layer_norm_default_257 = torch.ops.aten.native_layer_norm.default(_to_copy_1247, [256], pairformer_stack_blocks_22_attention_pair_bias_pair_layer_norm_weight, pairformer_stack_blocks_22_attention_pair_bias_pair_layer_norm_bias, 1e-05);  _to_copy_1247 = pairformer_stack_blocks_22_attention_pair_bias_pair_layer_norm_weight = pairformer_stack_blocks_22_attention_pair_bias_pair_layer_norm_bias = None
    getitem_2128 = native_layer_norm_default_257[0];  native_layer_norm_default_257 = None
    _to_copy_1248 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_22_attention_pair_bias_pair_linear_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_22_attention_pair_bias_pair_linear_weight = None
    _to_copy_1249 = torch.ops.aten._to_copy.default(getitem_2128, dtype = torch.bfloat16);  getitem_2128 = None
    t_459 = torch.ops.aten.t.default(_to_copy_1248);  _to_copy_1248 = None
    view_2227 = torch.ops.aten.view.default(_to_copy_1249, [147456, 256]);  _to_copy_1249 = None
    mm_426 = torch.ops.aten.mm.default(view_2227, t_459);  view_2227 = t_459 = None
    view_2228 = torch.ops.aten.view.default(mm_426, [1, 384, 384, 16]);  mm_426 = None
    permute_1220 = torch.ops.aten.permute.default(view_2228, [0, 3, 1, 2]);  view_2228 = None
    view_2229 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384])
    bitwise_not_137 = torch.ops.aten.bitwise_not.default(view_2229);  view_2229 = None
    masked_fill_137 = torch.ops.aten.masked_fill.Scalar(permute_1220, bitwise_not_137, -10000);  permute_1220 = bitwise_not_137 = None
    _to_copy_1250 = torch.ops.aten._to_copy.default(getitem_2125, dtype = torch.bfloat16);  getitem_2125 = None
    _to_copy_1251 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_22_attention_pair_bias_attention_input2qkvg_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_22_attention_pair_bias_attention_input2qkvg_weight = None
    unsqueeze_763 = torch.ops.aten.unsqueeze.default(_to_copy_1250, 3);  _to_copy_1250 = None
    unsqueeze_764 = torch.ops.aten.unsqueeze.default(unsqueeze_763, 4);  unsqueeze_763 = None
    unsqueeze_765 = torch.ops.aten.unsqueeze.default(unsqueeze_764, 5);  unsqueeze_764 = None
    permute_1221 = torch.ops.aten.permute.default(unsqueeze_765, [3, 0, 4, 1, 5, 2]);  unsqueeze_765 = None
    unsqueeze_766 = torch.ops.aten.unsqueeze.default(_to_copy_1251, 4);  _to_copy_1251 = None
    unsqueeze_767 = torch.ops.aten.unsqueeze.default(unsqueeze_766, 5);  unsqueeze_766 = None
    permute_1222 = torch.ops.aten.permute.default(unsqueeze_767, [1, 4, 2, 5, 3, 0]);  unsqueeze_767 = None
    permute_1223 = torch.ops.aten.permute.default(permute_1221, [3, 5, 0, 1, 2, 4]);  permute_1221 = None
    view_2230 = torch.ops.aten.view.default(permute_1223, [1, 384, 384]);  permute_1223 = None
    permute_1224 = torch.ops.aten.permute.default(permute_1222, [5, 0, 1, 2, 4, 3]);  permute_1222 = None
    view_2231 = torch.ops.aten.view.default(permute_1224, [1, 384, 1536]);  permute_1224 = None
    bmm_185 = torch.ops.aten.bmm.default(view_2230, view_2231);  view_2230 = view_2231 = None
    view_2232 = torch.ops.aten.view.default(bmm_185, [384, 1, 4, 1, 16, 24]);  bmm_185 = None
    permute_1225 = torch.ops.aten.permute.default(view_2232, [2, 3, 4, 0, 5, 1]);  view_2232 = None
    view_2233 = torch.ops.aten.view.default(permute_1225, [4, 1, 16, 384, 24]);  permute_1225 = None
    unbind_int_102 = torch.ops.aten.unbind.int(view_2233);  view_2233 = None
    getitem_2131 = unbind_int_102[0]
    getitem_2132 = unbind_int_102[1]
    getitem_2133 = unbind_int_102[2]
    getitem_2134 = unbind_int_102[3];  unbind_int_102 = None
    view_2234 = torch.ops.aten.view.default(pairformer_stack_blocks_22_attention_pair_bias_attention_query_bias, [1, 16, 1, 24]);  pairformer_stack_blocks_22_attention_pair_bias_attention_query_bias = None
    add_236 = torch.ops.aten.add.Tensor(getitem_2131, view_2234);  getitem_2131 = view_2234 = None
    _to_copy_1252 = torch.ops.aten._to_copy.default(add_236, dtype = torch.bfloat16);  add_236 = None
    expand_141 = torch.ops.aten.expand.default(masked_fill_137, [1, 16, 384, 384]);  masked_fill_137 = None
    _scaled_dot_product_efficient_attention_default_80 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_1252, getitem_2132, getitem_2133, expand_141, False);  _to_copy_1252 = getitem_2132 = getitem_2133 = expand_141 = None
    getitem_2135 = _scaled_dot_product_efficient_attention_default_80[0];  _scaled_dot_product_efficient_attention_default_80 = None
    add_237 = torch.ops.aten.add.Tensor(getitem_2134, 1);  getitem_2134 = None
    sigmoid_173 = torch.ops.aten.sigmoid.default(add_237);  add_237 = None
    mul_285 = torch.ops.aten.mul.Tensor(getitem_2135, sigmoid_173);  getitem_2135 = sigmoid_173 = None
    _to_copy_1253 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_22_attention_pair_bias_attention_output_proj_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_22_attention_pair_bias_attention_output_proj_weight = None
    unsqueeze_768 = torch.ops.aten.unsqueeze.default(mul_285, 4);  mul_285 = None
    permute_1226 = torch.ops.aten.permute.default(unsqueeze_768, [0, 2, 4, 3, 1]);  unsqueeze_768 = None
    unsqueeze_769 = torch.ops.aten.unsqueeze.default(_to_copy_1253, 3);  _to_copy_1253 = None
    unsqueeze_770 = torch.ops.aten.unsqueeze.default(unsqueeze_769, 4);  unsqueeze_769 = None
    permute_1227 = torch.ops.aten.permute.default(unsqueeze_770, [3, 4, 2, 1, 0]);  unsqueeze_770 = None
    permute_1228 = torch.ops.aten.permute.default(permute_1226, [1, 3, 4, 0, 2]);  permute_1226 = None
    clone_202 = torch.ops.aten.clone.default(permute_1228, memory_format = torch.contiguous_format);  permute_1228 = None
    _unsafe_view_172 = torch.ops.aten._unsafe_view.default(clone_202, [1, 384, 384]);  clone_202 = None
    permute_1229 = torch.ops.aten.permute.default(permute_1227, [3, 4, 0, 2, 1]);  permute_1227 = None
    clone_203 = torch.ops.aten.clone.default(permute_1229, memory_format = torch.contiguous_format);  permute_1229 = None
    _unsafe_view_173 = torch.ops.aten._unsafe_view.default(clone_203, [1, 384, 384]);  clone_203 = None
    bmm_186 = torch.ops.aten.bmm.default(_unsafe_view_172, _unsafe_view_173);  _unsafe_view_172 = _unsafe_view_173 = None
    view_2235 = torch.ops.aten.view.default(bmm_186, [384, 1, 1, 1, 384]);  bmm_186 = None
    permute_1230 = torch.ops.aten.permute.default(view_2235, [3, 0, 4, 1, 2]);  view_2235 = None
    view_2236 = torch.ops.aten.view.default(permute_1230, [1, 384, 384]);  permute_1230 = None
    unsqueeze_771 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
    mul_286 = torch.ops.aten.mul.Tensor(view_2236, unsqueeze_771);  view_2236 = unsqueeze_771 = None
    add_238 = torch.ops.aten.add.Tensor(add_231, mul_286);  mul_286 = None
    split_tensor_230 = torch.ops.aten.split.Tensor(add_231, 384, dim = -2);  add_231 = None
    getitem_2139 = split_tensor_230[0];  split_tensor_230 = None
    _to_copy_1254 = torch.ops.aten._to_copy.default(getitem_2139, dtype = torch.float32);  getitem_2139 = None
    native_layer_norm_default_258 = torch.ops.aten.native_layer_norm.default(_to_copy_1254, [384], pairformer_stack_blocks_22_transition_single_layer_norm_weight, pairformer_stack_blocks_22_transition_single_layer_norm_bias, 1e-05);  _to_copy_1254 = pairformer_stack_blocks_22_transition_single_layer_norm_weight = pairformer_stack_blocks_22_transition_single_layer_norm_bias = None
    getitem_2140 = native_layer_norm_default_258[0];  native_layer_norm_default_258 = None
    _to_copy_1255 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_22_transition_single_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_22_transition_single_linear_no_bias_ab_weight = None
    _to_copy_1256 = torch.ops.aten._to_copy.default(getitem_2140, dtype = torch.bfloat16);  getitem_2140 = None
    t_460 = torch.ops.aten.t.default(_to_copy_1255);  _to_copy_1255 = None
    view_2237 = torch.ops.aten.view.default(_to_copy_1256, [384, 384]);  _to_copy_1256 = None
    mm_427 = torch.ops.aten.mm.default(view_2237, t_460);  view_2237 = t_460 = None
    view_2238 = torch.ops.aten.view.default(mm_427, [1, 384, 1536]);  mm_427 = None
    split_tensor_231 = torch.ops.aten.split.Tensor(view_2238, 768, dim = -1);  view_2238 = None
    getitem_2143 = split_tensor_231[0]
    getitem_2144 = split_tensor_231[1];  split_tensor_231 = None
    silu_60 = torch.ops.aten.silu.default(getitem_2143);  getitem_2143 = None
    mul_287 = torch.ops.aten.mul.Tensor(silu_60, getitem_2144);  silu_60 = getitem_2144 = None
    _to_copy_1257 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_22_transition_single_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_22_transition_single_linear_out_weight = None
    t_461 = torch.ops.aten.t.default(_to_copy_1257);  _to_copy_1257 = None
    view_2240 = torch.ops.aten.view.default(mul_287, [384, 768]);  mul_287 = None
    mm_428 = torch.ops.aten.mm.default(view_2240, t_461);  view_2240 = t_461 = None
    view_2241 = torch.ops.aten.view.default(mm_428, [1, 384, 384]);  mm_428 = None
    add_239 = torch.ops.aten.add.Tensor(add_238, view_2241);  add_238 = view_2241 = None
    _to_copy_1258 = torch.ops.aten._to_copy.default(add_235, dtype = torch.float32)
    native_layer_norm_default_259 = torch.ops.aten.native_layer_norm.default(_to_copy_1258, [256], pairformer_stack_blocks_23_triangle_multiplication_layernorm_z_in_weight, pairformer_stack_blocks_23_triangle_multiplication_layernorm_z_in_bias, 1e-05);  _to_copy_1258 = pairformer_stack_blocks_23_triangle_multiplication_layernorm_z_in_weight = pairformer_stack_blocks_23_triangle_multiplication_layernorm_z_in_bias = None
    getitem_2145 = native_layer_norm_default_259[0];  native_layer_norm_default_259 = None
    split_with_sizes_default_58 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_23_triangle_multiplication_merged_linear_p_weight, [512, 512]);  pairformer_stack_blocks_23_triangle_multiplication_merged_linear_p_weight = None
    getitem_2148 = split_with_sizes_default_58[0]
    getitem_2149 = split_with_sizes_default_58[1];  split_with_sizes_default_58 = None
    split_with_sizes_default_59 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_23_triangle_multiplication_merged_linear_g_weight, [512, 512, 256]);  pairformer_stack_blocks_23_triangle_multiplication_merged_linear_g_weight = None
    getitem_2150 = split_with_sizes_default_59[0]
    getitem_2151 = split_with_sizes_default_59[1]
    getitem_2152 = split_with_sizes_default_59[2];  split_with_sizes_default_59 = None
    _to_copy_1259 = torch.ops.aten._to_copy.default(getitem_2148, dtype = torch.bfloat16);  getitem_2148 = None
    _to_copy_1260 = torch.ops.aten._to_copy.default(getitem_2145, dtype = torch.bfloat16)
    t_462 = torch.ops.aten.t.default(_to_copy_1259);  _to_copy_1259 = None
    view_2242 = torch.ops.aten.view.default(_to_copy_1260, [147456, 256]);  _to_copy_1260 = None
    mm_429 = torch.ops.aten.mm.default(view_2242, t_462);  view_2242 = t_462 = None
    view_2243 = torch.ops.aten.view.default(mm_429, [1, 384, 384, 512]);  mm_429 = None
    _to_copy_1261 = torch.ops.aten._to_copy.default(getitem_2150, dtype = torch.bfloat16);  getitem_2150 = None
    _to_copy_1262 = torch.ops.aten._to_copy.default(getitem_2145, dtype = torch.bfloat16)
    t_463 = torch.ops.aten.t.default(_to_copy_1261);  _to_copy_1261 = None
    view_2244 = torch.ops.aten.view.default(_to_copy_1262, [147456, 256]);  _to_copy_1262 = None
    mm_430 = torch.ops.aten.mm.default(view_2244, t_463);  view_2244 = t_463 = None
    view_2245 = torch.ops.aten.view.default(mm_430, [1, 384, 384, 512]);  mm_430 = None
    sigmoid_174 = torch.ops.aten.sigmoid.default(view_2245);  view_2245 = None
    mul_288 = torch.ops.aten.mul.Tensor(view_2243, sigmoid_174);  view_2243 = sigmoid_174 = None
    unsqueeze_772 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
    bitwise_not_138 = torch.ops.aten.bitwise_not.default(unsqueeze_772);  unsqueeze_772 = None
    masked_fill_138 = torch.ops.aten.masked_fill.Scalar(mul_288, bitwise_not_138, 0);  mul_288 = bitwise_not_138 = None
    split_tensor_232 = torch.ops.aten.split.Tensor(masked_fill_138, 256, dim = -1)
    getitem_2155 = split_tensor_232[0];  split_tensor_232 = None
    unsqueeze_775 = torch.ops.aten.unsqueeze.default(getitem_2155, 4);  getitem_2155 = None
    permute_1235 = torch.ops.aten.permute.default(unsqueeze_775, [0, 1, 4, 3, 2]);  unsqueeze_775 = None
    permute_1236 = torch.ops.aten.permute.default(permute_1235, [3, 1, 4, 0, 2]);  permute_1235 = None
    view_2248 = torch.ops.aten.view.default(permute_1236, [256, 384, 384]);  permute_1236 = None
    split_tensor_233 = torch.ops.aten.split.Tensor(masked_fill_138, 256, dim = -1);  masked_fill_138 = None
    getitem_2158 = split_tensor_233[1];  split_tensor_233 = None
    unsqueeze_776 = torch.ops.aten.unsqueeze.default(getitem_2158, 4);  getitem_2158 = None
    permute_1237 = torch.ops.aten.permute.default(unsqueeze_776, [0, 4, 1, 3, 2]);  unsqueeze_776 = None
    permute_1238 = torch.ops.aten.permute.default(permute_1237, [3, 4, 0, 2, 1]);  permute_1237 = None
    view_2249 = torch.ops.aten.view.default(permute_1238, [256, 384, 384]);  permute_1238 = None
    bmm_187 = torch.ops.aten.bmm.default(view_2248, view_2249);  view_2248 = view_2249 = None
    view_2250 = torch.ops.aten.view.default(bmm_187, [256, 384, 1, 1, 384]);  bmm_187 = None
    permute_1239 = torch.ops.aten.permute.default(view_2250, [3, 1, 4, 0, 2]);  view_2250 = None
    view_2251 = torch.ops.aten.view.default(permute_1239, [1, 384, 384, 256]);  permute_1239 = None
    _to_copy_1263 = torch.ops.aten._to_copy.default(getitem_2149, dtype = torch.bfloat16);  getitem_2149 = None
    _to_copy_1264 = torch.ops.aten._to_copy.default(getitem_2145, dtype = torch.bfloat16)
    t_464 = torch.ops.aten.t.default(_to_copy_1263);  _to_copy_1263 = None
    view_2252 = torch.ops.aten.view.default(_to_copy_1264, [147456, 256]);  _to_copy_1264 = None
    mm_431 = torch.ops.aten.mm.default(view_2252, t_464);  view_2252 = t_464 = None
    view_2253 = torch.ops.aten.view.default(mm_431, [1, 384, 384, 512]);  mm_431 = None
    _to_copy_1265 = torch.ops.aten._to_copy.default(getitem_2151, dtype = torch.bfloat16);  getitem_2151 = None
    _to_copy_1266 = torch.ops.aten._to_copy.default(getitem_2145, dtype = torch.bfloat16)
    t_465 = torch.ops.aten.t.default(_to_copy_1265);  _to_copy_1265 = None
    view_2254 = torch.ops.aten.view.default(_to_copy_1266, [147456, 256]);  _to_copy_1266 = None
    mm_432 = torch.ops.aten.mm.default(view_2254, t_465);  view_2254 = t_465 = None
    view_2255 = torch.ops.aten.view.default(mm_432, [1, 384, 384, 512]);  mm_432 = None
    sigmoid_175 = torch.ops.aten.sigmoid.default(view_2255);  view_2255 = None
    mul_289 = torch.ops.aten.mul.Tensor(view_2253, sigmoid_175);  view_2253 = sigmoid_175 = None
    view_2256 = torch.ops.aten.view.default(mul_289, [147456, 512]);  mul_289 = None
    view_2257 = torch.ops.aten.view.default(view_2256, [1, 384, 384, 512]);  view_2256 = None
    transpose_58 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
    unsqueeze_777 = torch.ops.aten.unsqueeze.default(transpose_58, 3);  transpose_58 = None
    clone_204 = torch.ops.aten.clone.default(unsqueeze_777, memory_format = torch.contiguous_format);  unsqueeze_777 = None
    bitwise_not_139 = torch.ops.aten.bitwise_not.default(clone_204);  clone_204 = None
    masked_fill_139 = torch.ops.aten.masked_fill.Scalar(view_2257, bitwise_not_139, 0);  view_2257 = bitwise_not_139 = None
    view_2258 = torch.ops.aten.view.default(masked_fill_139, [147456, 512]);  masked_fill_139 = None
    view_2262 = torch.ops.aten.view.default(view_2258, [1, 384, 384, 512])
    split_tensor_234 = torch.ops.aten.split.Tensor(view_2262, 256, dim = -1);  view_2262 = None
    getitem_2161 = split_tensor_234[0];  split_tensor_234 = None
    unsqueeze_780 = torch.ops.aten.unsqueeze.default(getitem_2161, 4);  getitem_2161 = None
    permute_1244 = torch.ops.aten.permute.default(unsqueeze_780, [0, 2, 4, 3, 1]);  unsqueeze_780 = None
    permute_1245 = torch.ops.aten.permute.default(permute_1244, [3, 1, 4, 0, 2]);  permute_1244 = None
    view_2263 = torch.ops.aten.view.default(permute_1245, [256, 384, 384]);  permute_1245 = None
    view_2264 = torch.ops.aten.view.default(view_2258, [1, 384, 384, 512]);  view_2258 = None
    split_tensor_235 = torch.ops.aten.split.Tensor(view_2264, 256, dim = -1);  view_2264 = None
    getitem_2164 = split_tensor_235[1];  split_tensor_235 = None
    unsqueeze_781 = torch.ops.aten.unsqueeze.default(getitem_2164, 4);  getitem_2164 = None
    permute_1246 = torch.ops.aten.permute.default(unsqueeze_781, [0, 4, 2, 3, 1]);  unsqueeze_781 = None
    permute_1247 = torch.ops.aten.permute.default(permute_1246, [3, 4, 0, 2, 1]);  permute_1246 = None
    view_2265 = torch.ops.aten.view.default(permute_1247, [256, 384, 384]);  permute_1247 = None
    bmm_188 = torch.ops.aten.bmm.default(view_2263, view_2265);  view_2263 = view_2265 = None
    view_2266 = torch.ops.aten.view.default(bmm_188, [256, 384, 1, 1, 384]);  bmm_188 = None
    permute_1248 = torch.ops.aten.permute.default(view_2266, [3, 1, 4, 0, 2]);  view_2266 = None
    view_2267 = torch.ops.aten.view.default(permute_1248, [1, 384, 384, 256]);  permute_1248 = None
    _to_copy_1267 = torch.ops.aten._to_copy.default(view_2251, dtype = torch.float32);  view_2251 = None
    native_layer_norm_default_260 = torch.ops.aten.native_layer_norm.default(_to_copy_1267, [256], None, None, 1e-05);  _to_copy_1267 = None
    getitem_2165 = native_layer_norm_default_260[0];  native_layer_norm_default_260 = None
    _to_copy_1268 = torch.ops.aten._to_copy.default(view_2267, dtype = torch.float32);  view_2267 = None
    native_layer_norm_default_261 = torch.ops.aten.native_layer_norm.default(_to_copy_1268, [256], None, None, 1e-05);  _to_copy_1268 = None
    getitem_2168 = native_layer_norm_default_261[0];  native_layer_norm_default_261 = None
    add_240 = torch.ops.aten.add.Tensor(getitem_2165, getitem_2168);  getitem_2165 = getitem_2168 = None
    _to_copy_1269 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_23_triangle_multiplication_linear_z_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_23_triangle_multiplication_linear_z_out_weight = None
    _to_copy_1270 = torch.ops.aten._to_copy.default(add_240, dtype = torch.bfloat16);  add_240 = None
    t_466 = torch.ops.aten.t.default(_to_copy_1269);  _to_copy_1269 = None
    view_2268 = torch.ops.aten.view.default(_to_copy_1270, [147456, 256]);  _to_copy_1270 = None
    mm_433 = torch.ops.aten.mm.default(view_2268, t_466);  view_2268 = t_466 = None
    view_2269 = torch.ops.aten.view.default(mm_433, [1, 384, 384, 256]);  mm_433 = None
    _to_copy_1271 = torch.ops.aten._to_copy.default(getitem_2152, dtype = torch.bfloat16);  getitem_2152 = None
    _to_copy_1272 = torch.ops.aten._to_copy.default(getitem_2145, dtype = torch.bfloat16);  getitem_2145 = None
    t_467 = torch.ops.aten.t.default(_to_copy_1271);  _to_copy_1271 = None
    view_2270 = torch.ops.aten.view.default(_to_copy_1272, [147456, 256]);  _to_copy_1272 = None
    mm_434 = torch.ops.aten.mm.default(view_2270, t_467);  view_2270 = t_467 = None
    view_2271 = torch.ops.aten.view.default(mm_434, [1, 384, 384, 256]);  mm_434 = None
    sigmoid_176 = torch.ops.aten.sigmoid.default(view_2271);  view_2271 = None
    mul_290 = torch.ops.aten.mul.Tensor(view_2269, sigmoid_176);  view_2269 = sigmoid_176 = None
    add_241 = torch.ops.aten.add.Tensor(add_235, mul_290);  mul_290 = None
    _to_copy_1273 = torch.ops.aten._to_copy.default(add_235, dtype = torch.float32)
    native_layer_norm_default_262 = torch.ops.aten.native_layer_norm.default(_to_copy_1273, [256], None, None, 1e-05);  _to_copy_1273 = None
    getitem_2171 = native_layer_norm_default_262[0];  native_layer_norm_default_262 = None
    _to_copy_1274 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_23_triangle_attention_pair2b_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_23_triangle_attention_pair2b_weight = None
    _to_copy_1275 = torch.ops.aten._to_copy.default(getitem_2171, dtype = torch.bfloat16)
    t_468 = torch.ops.aten.t.default(_to_copy_1274);  _to_copy_1274 = None
    view_2272 = torch.ops.aten.view.default(_to_copy_1275, [147456, 256]);  _to_copy_1275 = None
    mm_435 = torch.ops.aten.mm.default(view_2272, t_468);  view_2272 = t_468 = None
    view_2273 = torch.ops.aten.view.default(mm_435, [1, 384, 384, 8]);  mm_435 = None
    view_2274 = torch.ops.aten.view.default(view_2273, [1, 384, 384, 2, 4]);  view_2273 = None
    permute_1249 = torch.ops.aten.permute.default(view_2274, [0, 3, 4, 1, 2]);  view_2274 = None
    view_2275 = torch.ops.aten.view.default(permute_1249, [1, 2, 4, 1, 384, 384]);  permute_1249 = None
    view_2276 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 384, 384])
    bitwise_not_140 = torch.ops.aten.bitwise_not.default(view_2276);  view_2276 = None
    masked_fill_140 = torch.ops.aten.masked_fill.Scalar(view_2275, bitwise_not_140, -10000);  view_2275 = bitwise_not_140 = None
    view_2277 = torch.ops.aten.view.default(masked_fill_140, [1, 2, 4, 384, 384]);  masked_fill_140 = None
    permute_1250 = torch.ops.aten.permute.default(view_2277, [1, 0, 2, 3, 4]);  view_2277 = None
    view_2278 = torch.ops.aten.view.default(permute_1250, [2, 4, 1, 384, 384]);  permute_1250 = None
    _to_copy_1276 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_23_triangle_attention_pair2qkvg1_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_23_triangle_attention_pair2qkvg1_weight = None
    _to_copy_1277 = torch.ops.aten._to_copy.default(getitem_2171, dtype = torch.bfloat16)
    t_469 = torch.ops.aten.t.default(_to_copy_1276);  _to_copy_1276 = None
    view_2279 = torch.ops.aten.view.default(_to_copy_1277, [147456, 256]);  _to_copy_1277 = None
    mm_436 = torch.ops.aten.mm.default(view_2279, t_469);  view_2279 = t_469 = None
    view_2280 = torch.ops.aten.view.default(mm_436, [1, 384, 384, 1024]);  mm_436 = None
    select_59 = torch.ops.aten.select.int(view_2278, 0, 0)
    view_2281 = torch.ops.aten.view.default(view_2280, [1, 384, 384, 4, 4, 64]);  view_2280 = None
    permute_1251 = torch.ops.aten.permute.default(view_2281, [4, 0, 3, 1, 2, 5]);  view_2281 = None
    view_2282 = torch.ops.aten.view.default(permute_1251, [4, 4, 384, 384, 64]);  permute_1251 = None
    unbind_int_103 = torch.ops.aten.unbind.int(view_2282);  view_2282 = None
    getitem_2174 = unbind_int_103[0]
    getitem_2175 = unbind_int_103[1]
    getitem_2176 = unbind_int_103[2]
    getitem_2177 = unbind_int_103[3];  unbind_int_103 = None
    expand_142 = torch.ops.aten.expand.default(select_59, [4, 384, 384, 384]);  select_59 = None
    _scaled_dot_product_efficient_attention_default_81 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2174, getitem_2175, getitem_2176, expand_142, False);  getitem_2174 = getitem_2175 = getitem_2176 = expand_142 = None
    getitem_2178 = _scaled_dot_product_efficient_attention_default_81[0];  _scaled_dot_product_efficient_attention_default_81 = None
    sigmoid_177 = torch.ops.aten.sigmoid.default(getitem_2177);  getitem_2177 = None
    mul_291 = torch.ops.aten.mul.Tensor(getitem_2178, sigmoid_177);  getitem_2178 = sigmoid_177 = None
    view_2283 = torch.ops.aten.view.default(mul_291, [1, 4, 384, 384, 64]);  mul_291 = None
    permute_1252 = torch.ops.aten.permute.default(view_2283, [0, 2, 3, 1, 4]);  view_2283 = None
    clone_205 = torch.ops.aten.clone.default(permute_1252, memory_format = torch.contiguous_format);  permute_1252 = None
    _unsafe_view_174 = torch.ops.aten._unsafe_view.default(clone_205, [1, 384, 384, 256]);  clone_205 = None
    transpose_59 = torch.ops.aten.transpose.int(getitem_2171, 1, 2);  getitem_2171 = None
    _to_copy_1278 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_23_triangle_attention_pair2qkvg2_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_23_triangle_attention_pair2qkvg2_weight = None
    _to_copy_1279 = torch.ops.aten._to_copy.default(transpose_59, dtype = torch.bfloat16);  transpose_59 = None
    t_470 = torch.ops.aten.t.default(_to_copy_1278);  _to_copy_1278 = None
    expand_143 = torch.ops.aten.expand.default(_to_copy_1279, [1, 384, 384, 256]);  _to_copy_1279 = None
    view_2284 = torch.ops.aten.view.default(expand_143, [384, 384, 256]);  expand_143 = None
    expand_144 = torch.ops.aten.expand.default(t_470, [1, 384, 256, 1024]);  t_470 = None
    view_2285 = torch.ops.aten.view.default(expand_144, [384, 256, 1024]);  expand_144 = None
    bmm_189 = torch.ops.aten.bmm.default(view_2284, view_2285);  view_2284 = view_2285 = None
    view_2286 = torch.ops.aten.view.default(bmm_189, [1, 384, 384, 1024]);  bmm_189 = None
    select_60 = torch.ops.aten.select.int(view_2278, 0, 1);  view_2278 = None
    view_2287 = torch.ops.aten.view.default(view_2286, [1, 384, 384, 4, 4, 64]);  view_2286 = None
    permute_1253 = torch.ops.aten.permute.default(view_2287, [4, 0, 3, 1, 2, 5]);  view_2287 = None
    view_2288 = torch.ops.aten.view.default(permute_1253, [4, 4, 384, 384, 64]);  permute_1253 = None
    unbind_int_104 = torch.ops.aten.unbind.int(view_2288);  view_2288 = None
    getitem_2182 = unbind_int_104[0]
    getitem_2183 = unbind_int_104[1]
    getitem_2184 = unbind_int_104[2]
    getitem_2185 = unbind_int_104[3];  unbind_int_104 = None
    expand_145 = torch.ops.aten.expand.default(select_60, [4, 384, 384, 384]);  select_60 = None
    _scaled_dot_product_efficient_attention_default_82 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2182, getitem_2183, getitem_2184, expand_145, False);  getitem_2182 = getitem_2183 = getitem_2184 = expand_145 = None
    getitem_2186 = _scaled_dot_product_efficient_attention_default_82[0];  _scaled_dot_product_efficient_attention_default_82 = None
    sigmoid_178 = torch.ops.aten.sigmoid.default(getitem_2185);  getitem_2185 = None
    mul_292 = torch.ops.aten.mul.Tensor(getitem_2186, sigmoid_178);  getitem_2186 = sigmoid_178 = None
    view_2289 = torch.ops.aten.view.default(mul_292, [1, 4, 384, 384, 64]);  mul_292 = None
    permute_1254 = torch.ops.aten.permute.default(view_2289, [0, 2, 3, 1, 4]);  view_2289 = None
    clone_206 = torch.ops.aten.clone.default(permute_1254, memory_format = torch.contiguous_format);  permute_1254 = None
    _unsafe_view_175 = torch.ops.aten._unsafe_view.default(clone_206, [1, 384, 384, 256]);  clone_206 = None
    cat_35 = torch.ops.aten.cat.default([_unsafe_view_174, _unsafe_view_175], dim = -1);  _unsafe_view_174 = _unsafe_view_175 = None
    slice_186 = torch.ops.aten.slice.Tensor(pairformer_stack_blocks_23_triangle_attention_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  pairformer_stack_blocks_23_triangle_attention_out_scalers = None
    unsqueeze_782 = torch.ops.aten.unsqueeze.default(slice_186, 1);  slice_186 = None
    mul_293 = torch.ops.aten.mul.Tensor(pairformer_stack_blocks_23_triangle_attention_linear_out_weight, unsqueeze_782);  pairformer_stack_blocks_23_triangle_attention_linear_out_weight = unsqueeze_782 = None
    _to_copy_1280 = torch.ops.aten._to_copy.default(mul_293, dtype = torch.bfloat16);  mul_293 = None
    t_471 = torch.ops.aten.t.default(_to_copy_1280);  _to_copy_1280 = None
    view_2290 = torch.ops.aten.view.default(cat_35, [147456, 512]);  cat_35 = None
    mm_437 = torch.ops.aten.mm.default(view_2290, t_471);  view_2290 = t_471 = None
    view_2291 = torch.ops.aten.view.default(mm_437, [1, 384, 384, 256]);  mm_437 = None
    add_242 = torch.ops.aten.add.Tensor(add_241, view_2291);  add_241 = view_2291 = None
    split_tensor_236 = torch.ops.aten.split.Tensor(add_235, 384, dim = -2)
    getitem_2190 = split_tensor_236[0];  split_tensor_236 = None
    _to_copy_1281 = torch.ops.aten._to_copy.default(getitem_2190, dtype = torch.float32);  getitem_2190 = None
    native_layer_norm_default_263 = torch.ops.aten.native_layer_norm.default(_to_copy_1281, [256], pairformer_stack_blocks_23_transition_pair_layer_norm_weight, pairformer_stack_blocks_23_transition_pair_layer_norm_bias, 1e-05);  _to_copy_1281 = pairformer_stack_blocks_23_transition_pair_layer_norm_weight = pairformer_stack_blocks_23_transition_pair_layer_norm_bias = None
    getitem_2191 = native_layer_norm_default_263[0];  native_layer_norm_default_263 = None
    _to_copy_1282 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_23_transition_pair_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_23_transition_pair_linear_no_bias_ab_weight = None
    _to_copy_1283 = torch.ops.aten._to_copy.default(getitem_2191, dtype = torch.bfloat16);  getitem_2191 = None
    t_472 = torch.ops.aten.t.default(_to_copy_1282);  _to_copy_1282 = None
    view_2292 = torch.ops.aten.view.default(_to_copy_1283, [147456, 256]);  _to_copy_1283 = None
    mm_438 = torch.ops.aten.mm.default(view_2292, t_472);  view_2292 = t_472 = None
    view_2293 = torch.ops.aten.view.default(mm_438, [1, 384, 384, 1024]);  mm_438 = None
    split_tensor_237 = torch.ops.aten.split.Tensor(view_2293, 512, dim = -1);  view_2293 = None
    getitem_2194 = split_tensor_237[0]
    getitem_2195 = split_tensor_237[1];  split_tensor_237 = None
    silu_61 = torch.ops.aten.silu.default(getitem_2194);  getitem_2194 = None
    mul_294 = torch.ops.aten.mul.Tensor(silu_61, getitem_2195);  silu_61 = getitem_2195 = None
    _to_copy_1284 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_23_transition_pair_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_23_transition_pair_linear_out_weight = None
    t_473 = torch.ops.aten.t.default(_to_copy_1284);  _to_copy_1284 = None
    view_2295 = torch.ops.aten.view.default(mul_294, [147456, 512]);  mul_294 = None
    mm_439 = torch.ops.aten.mm.default(view_2295, t_473);  view_2295 = t_473 = None
    view_2296 = torch.ops.aten.view.default(mm_439, [1, 384, 384, 256]);  mm_439 = None
    add_243 = torch.ops.aten.add.Tensor(add_242, view_2296);  add_242 = view_2296 = None
    _to_copy_1285 = torch.ops.aten._to_copy.default(add_239, dtype = torch.float32)
    native_layer_norm_default_264 = torch.ops.aten.native_layer_norm.default(_to_copy_1285, [384], pairformer_stack_blocks_23_attention_pair_bias_single_layer_norm_weight, pairformer_stack_blocks_23_attention_pair_bias_single_layer_norm_bias, 1e-05);  _to_copy_1285 = pairformer_stack_blocks_23_attention_pair_bias_single_layer_norm_weight = pairformer_stack_blocks_23_attention_pair_bias_single_layer_norm_bias = None
    getitem_2196 = native_layer_norm_default_264[0];  native_layer_norm_default_264 = None
    _to_copy_1286 = torch.ops.aten._to_copy.default(add_235, dtype = torch.float32);  add_235 = None
    native_layer_norm_default_265 = torch.ops.aten.native_layer_norm.default(_to_copy_1286, [256], pairformer_stack_blocks_23_attention_pair_bias_pair_layer_norm_weight, pairformer_stack_blocks_23_attention_pair_bias_pair_layer_norm_bias, 1e-05);  _to_copy_1286 = pairformer_stack_blocks_23_attention_pair_bias_pair_layer_norm_weight = pairformer_stack_blocks_23_attention_pair_bias_pair_layer_norm_bias = None
    getitem_2199 = native_layer_norm_default_265[0];  native_layer_norm_default_265 = None
    _to_copy_1287 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_23_attention_pair_bias_pair_linear_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_23_attention_pair_bias_pair_linear_weight = None
    _to_copy_1288 = torch.ops.aten._to_copy.default(getitem_2199, dtype = torch.bfloat16);  getitem_2199 = None
    t_474 = torch.ops.aten.t.default(_to_copy_1287);  _to_copy_1287 = None
    view_2297 = torch.ops.aten.view.default(_to_copy_1288, [147456, 256]);  _to_copy_1288 = None
    mm_440 = torch.ops.aten.mm.default(view_2297, t_474);  view_2297 = t_474 = None
    view_2298 = torch.ops.aten.view.default(mm_440, [1, 384, 384, 16]);  mm_440 = None
    permute_1255 = torch.ops.aten.permute.default(view_2298, [0, 3, 1, 2]);  view_2298 = None
    view_2299 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384])
    bitwise_not_141 = torch.ops.aten.bitwise_not.default(view_2299);  view_2299 = None
    masked_fill_141 = torch.ops.aten.masked_fill.Scalar(permute_1255, bitwise_not_141, -10000);  permute_1255 = bitwise_not_141 = None
    _to_copy_1289 = torch.ops.aten._to_copy.default(getitem_2196, dtype = torch.bfloat16);  getitem_2196 = None
    _to_copy_1290 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_23_attention_pair_bias_attention_input2qkvg_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_23_attention_pair_bias_attention_input2qkvg_weight = None
    unsqueeze_783 = torch.ops.aten.unsqueeze.default(_to_copy_1289, 3);  _to_copy_1289 = None
    unsqueeze_784 = torch.ops.aten.unsqueeze.default(unsqueeze_783, 4);  unsqueeze_783 = None
    unsqueeze_785 = torch.ops.aten.unsqueeze.default(unsqueeze_784, 5);  unsqueeze_784 = None
    permute_1256 = torch.ops.aten.permute.default(unsqueeze_785, [3, 0, 4, 1, 5, 2]);  unsqueeze_785 = None
    unsqueeze_786 = torch.ops.aten.unsqueeze.default(_to_copy_1290, 4);  _to_copy_1290 = None
    unsqueeze_787 = torch.ops.aten.unsqueeze.default(unsqueeze_786, 5);  unsqueeze_786 = None
    permute_1257 = torch.ops.aten.permute.default(unsqueeze_787, [1, 4, 2, 5, 3, 0]);  unsqueeze_787 = None
    permute_1258 = torch.ops.aten.permute.default(permute_1256, [3, 5, 0, 1, 2, 4]);  permute_1256 = None
    view_2300 = torch.ops.aten.view.default(permute_1258, [1, 384, 384]);  permute_1258 = None
    permute_1259 = torch.ops.aten.permute.default(permute_1257, [5, 0, 1, 2, 4, 3]);  permute_1257 = None
    view_2301 = torch.ops.aten.view.default(permute_1259, [1, 384, 1536]);  permute_1259 = None
    bmm_190 = torch.ops.aten.bmm.default(view_2300, view_2301);  view_2300 = view_2301 = None
    view_2302 = torch.ops.aten.view.default(bmm_190, [384, 1, 4, 1, 16, 24]);  bmm_190 = None
    permute_1260 = torch.ops.aten.permute.default(view_2302, [2, 3, 4, 0, 5, 1]);  view_2302 = None
    view_2303 = torch.ops.aten.view.default(permute_1260, [4, 1, 16, 384, 24]);  permute_1260 = None
    unbind_int_105 = torch.ops.aten.unbind.int(view_2303);  view_2303 = None
    getitem_2202 = unbind_int_105[0]
    getitem_2203 = unbind_int_105[1]
    getitem_2204 = unbind_int_105[2]
    getitem_2205 = unbind_int_105[3];  unbind_int_105 = None
    view_2304 = torch.ops.aten.view.default(pairformer_stack_blocks_23_attention_pair_bias_attention_query_bias, [1, 16, 1, 24]);  pairformer_stack_blocks_23_attention_pair_bias_attention_query_bias = None
    add_244 = torch.ops.aten.add.Tensor(getitem_2202, view_2304);  getitem_2202 = view_2304 = None
    _to_copy_1291 = torch.ops.aten._to_copy.default(add_244, dtype = torch.bfloat16);  add_244 = None
    expand_146 = torch.ops.aten.expand.default(masked_fill_141, [1, 16, 384, 384]);  masked_fill_141 = None
    _scaled_dot_product_efficient_attention_default_83 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_1291, getitem_2203, getitem_2204, expand_146, False);  _to_copy_1291 = getitem_2203 = getitem_2204 = expand_146 = None
    getitem_2206 = _scaled_dot_product_efficient_attention_default_83[0];  _scaled_dot_product_efficient_attention_default_83 = None
    add_245 = torch.ops.aten.add.Tensor(getitem_2205, 1);  getitem_2205 = None
    sigmoid_179 = torch.ops.aten.sigmoid.default(add_245);  add_245 = None
    mul_295 = torch.ops.aten.mul.Tensor(getitem_2206, sigmoid_179);  getitem_2206 = sigmoid_179 = None
    _to_copy_1292 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_23_attention_pair_bias_attention_output_proj_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_23_attention_pair_bias_attention_output_proj_weight = None
    unsqueeze_788 = torch.ops.aten.unsqueeze.default(mul_295, 4);  mul_295 = None
    permute_1261 = torch.ops.aten.permute.default(unsqueeze_788, [0, 2, 4, 3, 1]);  unsqueeze_788 = None
    unsqueeze_789 = torch.ops.aten.unsqueeze.default(_to_copy_1292, 3);  _to_copy_1292 = None
    unsqueeze_790 = torch.ops.aten.unsqueeze.default(unsqueeze_789, 4);  unsqueeze_789 = None
    permute_1262 = torch.ops.aten.permute.default(unsqueeze_790, [3, 4, 2, 1, 0]);  unsqueeze_790 = None
    permute_1263 = torch.ops.aten.permute.default(permute_1261, [1, 3, 4, 0, 2]);  permute_1261 = None
    clone_207 = torch.ops.aten.clone.default(permute_1263, memory_format = torch.contiguous_format);  permute_1263 = None
    _unsafe_view_176 = torch.ops.aten._unsafe_view.default(clone_207, [1, 384, 384]);  clone_207 = None
    permute_1264 = torch.ops.aten.permute.default(permute_1262, [3, 4, 0, 2, 1]);  permute_1262 = None
    clone_208 = torch.ops.aten.clone.default(permute_1264, memory_format = torch.contiguous_format);  permute_1264 = None
    _unsafe_view_177 = torch.ops.aten._unsafe_view.default(clone_208, [1, 384, 384]);  clone_208 = None
    bmm_191 = torch.ops.aten.bmm.default(_unsafe_view_176, _unsafe_view_177);  _unsafe_view_176 = _unsafe_view_177 = None
    view_2305 = torch.ops.aten.view.default(bmm_191, [384, 1, 1, 1, 384]);  bmm_191 = None
    permute_1265 = torch.ops.aten.permute.default(view_2305, [3, 0, 4, 1, 2]);  view_2305 = None
    view_2306 = torch.ops.aten.view.default(permute_1265, [1, 384, 384]);  permute_1265 = None
    unsqueeze_791 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
    mul_296 = torch.ops.aten.mul.Tensor(view_2306, unsqueeze_791);  view_2306 = unsqueeze_791 = None
    add_246 = torch.ops.aten.add.Tensor(add_239, mul_296);  mul_296 = None
    split_tensor_238 = torch.ops.aten.split.Tensor(add_239, 384, dim = -2);  add_239 = None
    getitem_2210 = split_tensor_238[0];  split_tensor_238 = None
    _to_copy_1293 = torch.ops.aten._to_copy.default(getitem_2210, dtype = torch.float32);  getitem_2210 = None
    native_layer_norm_default_266 = torch.ops.aten.native_layer_norm.default(_to_copy_1293, [384], pairformer_stack_blocks_23_transition_single_layer_norm_weight, pairformer_stack_blocks_23_transition_single_layer_norm_bias, 1e-05);  _to_copy_1293 = pairformer_stack_blocks_23_transition_single_layer_norm_weight = pairformer_stack_blocks_23_transition_single_layer_norm_bias = None
    getitem_2211 = native_layer_norm_default_266[0];  native_layer_norm_default_266 = None
    _to_copy_1294 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_23_transition_single_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_23_transition_single_linear_no_bias_ab_weight = None
    _to_copy_1295 = torch.ops.aten._to_copy.default(getitem_2211, dtype = torch.bfloat16);  getitem_2211 = None
    t_475 = torch.ops.aten.t.default(_to_copy_1294);  _to_copy_1294 = None
    view_2307 = torch.ops.aten.view.default(_to_copy_1295, [384, 384]);  _to_copy_1295 = None
    mm_441 = torch.ops.aten.mm.default(view_2307, t_475);  view_2307 = t_475 = None
    view_2308 = torch.ops.aten.view.default(mm_441, [1, 384, 1536]);  mm_441 = None
    split_tensor_239 = torch.ops.aten.split.Tensor(view_2308, 768, dim = -1);  view_2308 = None
    getitem_2214 = split_tensor_239[0]
    getitem_2215 = split_tensor_239[1];  split_tensor_239 = None
    silu_62 = torch.ops.aten.silu.default(getitem_2214);  getitem_2214 = None
    mul_297 = torch.ops.aten.mul.Tensor(silu_62, getitem_2215);  silu_62 = getitem_2215 = None
    _to_copy_1296 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_23_transition_single_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_23_transition_single_linear_out_weight = None
    t_476 = torch.ops.aten.t.default(_to_copy_1296);  _to_copy_1296 = None
    view_2310 = torch.ops.aten.view.default(mul_297, [384, 768]);  mul_297 = None
    mm_442 = torch.ops.aten.mm.default(view_2310, t_476);  view_2310 = t_476 = None
    view_2311 = torch.ops.aten.view.default(mm_442, [1, 384, 384]);  mm_442 = None
    add_247 = torch.ops.aten.add.Tensor(add_246, view_2311);  add_246 = view_2311 = None
    _to_copy_1297 = torch.ops.aten._to_copy.default(add_243, dtype = torch.float32)
    native_layer_norm_default_267 = torch.ops.aten.native_layer_norm.default(_to_copy_1297, [256], pairformer_stack_blocks_24_triangle_multiplication_layernorm_z_in_weight, pairformer_stack_blocks_24_triangle_multiplication_layernorm_z_in_bias, 1e-05);  _to_copy_1297 = pairformer_stack_blocks_24_triangle_multiplication_layernorm_z_in_weight = pairformer_stack_blocks_24_triangle_multiplication_layernorm_z_in_bias = None
    getitem_2216 = native_layer_norm_default_267[0];  native_layer_norm_default_267 = None
    split_with_sizes_default_60 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_24_triangle_multiplication_merged_linear_p_weight, [512, 512]);  pairformer_stack_blocks_24_triangle_multiplication_merged_linear_p_weight = None
    getitem_2219 = split_with_sizes_default_60[0]
    getitem_2220 = split_with_sizes_default_60[1];  split_with_sizes_default_60 = None
    split_with_sizes_default_61 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_24_triangle_multiplication_merged_linear_g_weight, [512, 512, 256]);  pairformer_stack_blocks_24_triangle_multiplication_merged_linear_g_weight = None
    getitem_2221 = split_with_sizes_default_61[0]
    getitem_2222 = split_with_sizes_default_61[1]
    getitem_2223 = split_with_sizes_default_61[2];  split_with_sizes_default_61 = None
    _to_copy_1298 = torch.ops.aten._to_copy.default(getitem_2219, dtype = torch.bfloat16);  getitem_2219 = None
    _to_copy_1299 = torch.ops.aten._to_copy.default(getitem_2216, dtype = torch.bfloat16)
    t_477 = torch.ops.aten.t.default(_to_copy_1298);  _to_copy_1298 = None
    view_2312 = torch.ops.aten.view.default(_to_copy_1299, [147456, 256]);  _to_copy_1299 = None
    mm_443 = torch.ops.aten.mm.default(view_2312, t_477);  view_2312 = t_477 = None
    view_2313 = torch.ops.aten.view.default(mm_443, [1, 384, 384, 512]);  mm_443 = None
    _to_copy_1300 = torch.ops.aten._to_copy.default(getitem_2221, dtype = torch.bfloat16);  getitem_2221 = None
    _to_copy_1301 = torch.ops.aten._to_copy.default(getitem_2216, dtype = torch.bfloat16)
    t_478 = torch.ops.aten.t.default(_to_copy_1300);  _to_copy_1300 = None
    view_2314 = torch.ops.aten.view.default(_to_copy_1301, [147456, 256]);  _to_copy_1301 = None
    mm_444 = torch.ops.aten.mm.default(view_2314, t_478);  view_2314 = t_478 = None
    view_2315 = torch.ops.aten.view.default(mm_444, [1, 384, 384, 512]);  mm_444 = None
    sigmoid_180 = torch.ops.aten.sigmoid.default(view_2315);  view_2315 = None
    mul_298 = torch.ops.aten.mul.Tensor(view_2313, sigmoid_180);  view_2313 = sigmoid_180 = None
    unsqueeze_792 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
    bitwise_not_142 = torch.ops.aten.bitwise_not.default(unsqueeze_792);  unsqueeze_792 = None
    masked_fill_142 = torch.ops.aten.masked_fill.Scalar(mul_298, bitwise_not_142, 0);  mul_298 = bitwise_not_142 = None
    split_tensor_240 = torch.ops.aten.split.Tensor(masked_fill_142, 256, dim = -1)
    getitem_2226 = split_tensor_240[0];  split_tensor_240 = None
    unsqueeze_795 = torch.ops.aten.unsqueeze.default(getitem_2226, 4);  getitem_2226 = None
    permute_1270 = torch.ops.aten.permute.default(unsqueeze_795, [0, 1, 4, 3, 2]);  unsqueeze_795 = None
    permute_1271 = torch.ops.aten.permute.default(permute_1270, [3, 1, 4, 0, 2]);  permute_1270 = None
    view_2318 = torch.ops.aten.view.default(permute_1271, [256, 384, 384]);  permute_1271 = None
    split_tensor_241 = torch.ops.aten.split.Tensor(masked_fill_142, 256, dim = -1);  masked_fill_142 = None
    getitem_2229 = split_tensor_241[1];  split_tensor_241 = None
    unsqueeze_796 = torch.ops.aten.unsqueeze.default(getitem_2229, 4);  getitem_2229 = None
    permute_1272 = torch.ops.aten.permute.default(unsqueeze_796, [0, 4, 1, 3, 2]);  unsqueeze_796 = None
    permute_1273 = torch.ops.aten.permute.default(permute_1272, [3, 4, 0, 2, 1]);  permute_1272 = None
    view_2319 = torch.ops.aten.view.default(permute_1273, [256, 384, 384]);  permute_1273 = None
    bmm_192 = torch.ops.aten.bmm.default(view_2318, view_2319);  view_2318 = view_2319 = None
    view_2320 = torch.ops.aten.view.default(bmm_192, [256, 384, 1, 1, 384]);  bmm_192 = None
    permute_1274 = torch.ops.aten.permute.default(view_2320, [3, 1, 4, 0, 2]);  view_2320 = None
    view_2321 = torch.ops.aten.view.default(permute_1274, [1, 384, 384, 256]);  permute_1274 = None
    _to_copy_1302 = torch.ops.aten._to_copy.default(getitem_2220, dtype = torch.bfloat16);  getitem_2220 = None
    _to_copy_1303 = torch.ops.aten._to_copy.default(getitem_2216, dtype = torch.bfloat16)
    t_479 = torch.ops.aten.t.default(_to_copy_1302);  _to_copy_1302 = None
    view_2322 = torch.ops.aten.view.default(_to_copy_1303, [147456, 256]);  _to_copy_1303 = None
    mm_445 = torch.ops.aten.mm.default(view_2322, t_479);  view_2322 = t_479 = None
    view_2323 = torch.ops.aten.view.default(mm_445, [1, 384, 384, 512]);  mm_445 = None
    _to_copy_1304 = torch.ops.aten._to_copy.default(getitem_2222, dtype = torch.bfloat16);  getitem_2222 = None
    _to_copy_1305 = torch.ops.aten._to_copy.default(getitem_2216, dtype = torch.bfloat16)
    t_480 = torch.ops.aten.t.default(_to_copy_1304);  _to_copy_1304 = None
    view_2324 = torch.ops.aten.view.default(_to_copy_1305, [147456, 256]);  _to_copy_1305 = None
    mm_446 = torch.ops.aten.mm.default(view_2324, t_480);  view_2324 = t_480 = None
    view_2325 = torch.ops.aten.view.default(mm_446, [1, 384, 384, 512]);  mm_446 = None
    sigmoid_181 = torch.ops.aten.sigmoid.default(view_2325);  view_2325 = None
    mul_299 = torch.ops.aten.mul.Tensor(view_2323, sigmoid_181);  view_2323 = sigmoid_181 = None
    view_2326 = torch.ops.aten.view.default(mul_299, [147456, 512]);  mul_299 = None
    view_2327 = torch.ops.aten.view.default(view_2326, [1, 384, 384, 512]);  view_2326 = None
    transpose_60 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
    unsqueeze_797 = torch.ops.aten.unsqueeze.default(transpose_60, 3);  transpose_60 = None
    clone_209 = torch.ops.aten.clone.default(unsqueeze_797, memory_format = torch.contiguous_format);  unsqueeze_797 = None
    bitwise_not_143 = torch.ops.aten.bitwise_not.default(clone_209);  clone_209 = None
    masked_fill_143 = torch.ops.aten.masked_fill.Scalar(view_2327, bitwise_not_143, 0);  view_2327 = bitwise_not_143 = None
    view_2328 = torch.ops.aten.view.default(masked_fill_143, [147456, 512]);  masked_fill_143 = None
    view_2332 = torch.ops.aten.view.default(view_2328, [1, 384, 384, 512])
    split_tensor_242 = torch.ops.aten.split.Tensor(view_2332, 256, dim = -1);  view_2332 = None
    getitem_2232 = split_tensor_242[0];  split_tensor_242 = None
    unsqueeze_800 = torch.ops.aten.unsqueeze.default(getitem_2232, 4);  getitem_2232 = None
    permute_1279 = torch.ops.aten.permute.default(unsqueeze_800, [0, 2, 4, 3, 1]);  unsqueeze_800 = None
    permute_1280 = torch.ops.aten.permute.default(permute_1279, [3, 1, 4, 0, 2]);  permute_1279 = None
    view_2333 = torch.ops.aten.view.default(permute_1280, [256, 384, 384]);  permute_1280 = None
    view_2334 = torch.ops.aten.view.default(view_2328, [1, 384, 384, 512]);  view_2328 = None
    split_tensor_243 = torch.ops.aten.split.Tensor(view_2334, 256, dim = -1);  view_2334 = None
    getitem_2235 = split_tensor_243[1];  split_tensor_243 = None
    unsqueeze_801 = torch.ops.aten.unsqueeze.default(getitem_2235, 4);  getitem_2235 = None
    permute_1281 = torch.ops.aten.permute.default(unsqueeze_801, [0, 4, 2, 3, 1]);  unsqueeze_801 = None
    permute_1282 = torch.ops.aten.permute.default(permute_1281, [3, 4, 0, 2, 1]);  permute_1281 = None
    view_2335 = torch.ops.aten.view.default(permute_1282, [256, 384, 384]);  permute_1282 = None
    bmm_193 = torch.ops.aten.bmm.default(view_2333, view_2335);  view_2333 = view_2335 = None
    view_2336 = torch.ops.aten.view.default(bmm_193, [256, 384, 1, 1, 384]);  bmm_193 = None
    permute_1283 = torch.ops.aten.permute.default(view_2336, [3, 1, 4, 0, 2]);  view_2336 = None
    view_2337 = torch.ops.aten.view.default(permute_1283, [1, 384, 384, 256]);  permute_1283 = None
    _to_copy_1306 = torch.ops.aten._to_copy.default(view_2321, dtype = torch.float32);  view_2321 = None
    native_layer_norm_default_268 = torch.ops.aten.native_layer_norm.default(_to_copy_1306, [256], None, None, 1e-05);  _to_copy_1306 = None
    getitem_2236 = native_layer_norm_default_268[0];  native_layer_norm_default_268 = None
    _to_copy_1307 = torch.ops.aten._to_copy.default(view_2337, dtype = torch.float32);  view_2337 = None
    native_layer_norm_default_269 = torch.ops.aten.native_layer_norm.default(_to_copy_1307, [256], None, None, 1e-05);  _to_copy_1307 = None
    getitem_2239 = native_layer_norm_default_269[0];  native_layer_norm_default_269 = None
    add_248 = torch.ops.aten.add.Tensor(getitem_2236, getitem_2239);  getitem_2236 = getitem_2239 = None
    _to_copy_1308 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_24_triangle_multiplication_linear_z_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_24_triangle_multiplication_linear_z_out_weight = None
    _to_copy_1309 = torch.ops.aten._to_copy.default(add_248, dtype = torch.bfloat16);  add_248 = None
    t_481 = torch.ops.aten.t.default(_to_copy_1308);  _to_copy_1308 = None
    view_2338 = torch.ops.aten.view.default(_to_copy_1309, [147456, 256]);  _to_copy_1309 = None
    mm_447 = torch.ops.aten.mm.default(view_2338, t_481);  view_2338 = t_481 = None
    view_2339 = torch.ops.aten.view.default(mm_447, [1, 384, 384, 256]);  mm_447 = None
    _to_copy_1310 = torch.ops.aten._to_copy.default(getitem_2223, dtype = torch.bfloat16);  getitem_2223 = None
    _to_copy_1311 = torch.ops.aten._to_copy.default(getitem_2216, dtype = torch.bfloat16);  getitem_2216 = None
    t_482 = torch.ops.aten.t.default(_to_copy_1310);  _to_copy_1310 = None
    view_2340 = torch.ops.aten.view.default(_to_copy_1311, [147456, 256]);  _to_copy_1311 = None
    mm_448 = torch.ops.aten.mm.default(view_2340, t_482);  view_2340 = t_482 = None
    view_2341 = torch.ops.aten.view.default(mm_448, [1, 384, 384, 256]);  mm_448 = None
    sigmoid_182 = torch.ops.aten.sigmoid.default(view_2341);  view_2341 = None
    mul_300 = torch.ops.aten.mul.Tensor(view_2339, sigmoid_182);  view_2339 = sigmoid_182 = None
    add_249 = torch.ops.aten.add.Tensor(add_243, mul_300);  mul_300 = None
    _to_copy_1312 = torch.ops.aten._to_copy.default(add_243, dtype = torch.float32)
    native_layer_norm_default_270 = torch.ops.aten.native_layer_norm.default(_to_copy_1312, [256], None, None, 1e-05);  _to_copy_1312 = None
    getitem_2242 = native_layer_norm_default_270[0];  native_layer_norm_default_270 = None
    _to_copy_1313 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_24_triangle_attention_pair2b_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_24_triangle_attention_pair2b_weight = None
    _to_copy_1314 = torch.ops.aten._to_copy.default(getitem_2242, dtype = torch.bfloat16)
    t_483 = torch.ops.aten.t.default(_to_copy_1313);  _to_copy_1313 = None
    view_2342 = torch.ops.aten.view.default(_to_copy_1314, [147456, 256]);  _to_copy_1314 = None
    mm_449 = torch.ops.aten.mm.default(view_2342, t_483);  view_2342 = t_483 = None
    view_2343 = torch.ops.aten.view.default(mm_449, [1, 384, 384, 8]);  mm_449 = None
    view_2344 = torch.ops.aten.view.default(view_2343, [1, 384, 384, 2, 4]);  view_2343 = None
    permute_1284 = torch.ops.aten.permute.default(view_2344, [0, 3, 4, 1, 2]);  view_2344 = None
    view_2345 = torch.ops.aten.view.default(permute_1284, [1, 2, 4, 1, 384, 384]);  permute_1284 = None
    view_2346 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 384, 384])
    bitwise_not_144 = torch.ops.aten.bitwise_not.default(view_2346);  view_2346 = None
    masked_fill_144 = torch.ops.aten.masked_fill.Scalar(view_2345, bitwise_not_144, -10000);  view_2345 = bitwise_not_144 = None
    view_2347 = torch.ops.aten.view.default(masked_fill_144, [1, 2, 4, 384, 384]);  masked_fill_144 = None
    permute_1285 = torch.ops.aten.permute.default(view_2347, [1, 0, 2, 3, 4]);  view_2347 = None
    view_2348 = torch.ops.aten.view.default(permute_1285, [2, 4, 1, 384, 384]);  permute_1285 = None
    _to_copy_1315 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_24_triangle_attention_pair2qkvg1_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_24_triangle_attention_pair2qkvg1_weight = None
    _to_copy_1316 = torch.ops.aten._to_copy.default(getitem_2242, dtype = torch.bfloat16)
    t_484 = torch.ops.aten.t.default(_to_copy_1315);  _to_copy_1315 = None
    view_2349 = torch.ops.aten.view.default(_to_copy_1316, [147456, 256]);  _to_copy_1316 = None
    mm_450 = torch.ops.aten.mm.default(view_2349, t_484);  view_2349 = t_484 = None
    view_2350 = torch.ops.aten.view.default(mm_450, [1, 384, 384, 1024]);  mm_450 = None
    select_61 = torch.ops.aten.select.int(view_2348, 0, 0)
    view_2351 = torch.ops.aten.view.default(view_2350, [1, 384, 384, 4, 4, 64]);  view_2350 = None
    permute_1286 = torch.ops.aten.permute.default(view_2351, [4, 0, 3, 1, 2, 5]);  view_2351 = None
    view_2352 = torch.ops.aten.view.default(permute_1286, [4, 4, 384, 384, 64]);  permute_1286 = None
    unbind_int_106 = torch.ops.aten.unbind.int(view_2352);  view_2352 = None
    getitem_2245 = unbind_int_106[0]
    getitem_2246 = unbind_int_106[1]
    getitem_2247 = unbind_int_106[2]
    getitem_2248 = unbind_int_106[3];  unbind_int_106 = None
    expand_147 = torch.ops.aten.expand.default(select_61, [4, 384, 384, 384]);  select_61 = None
    _scaled_dot_product_efficient_attention_default_84 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2245, getitem_2246, getitem_2247, expand_147, False);  getitem_2245 = getitem_2246 = getitem_2247 = expand_147 = None
    getitem_2249 = _scaled_dot_product_efficient_attention_default_84[0];  _scaled_dot_product_efficient_attention_default_84 = None
    sigmoid_183 = torch.ops.aten.sigmoid.default(getitem_2248);  getitem_2248 = None
    mul_301 = torch.ops.aten.mul.Tensor(getitem_2249, sigmoid_183);  getitem_2249 = sigmoid_183 = None
    view_2353 = torch.ops.aten.view.default(mul_301, [1, 4, 384, 384, 64]);  mul_301 = None
    permute_1287 = torch.ops.aten.permute.default(view_2353, [0, 2, 3, 1, 4]);  view_2353 = None
    clone_210 = torch.ops.aten.clone.default(permute_1287, memory_format = torch.contiguous_format);  permute_1287 = None
    _unsafe_view_178 = torch.ops.aten._unsafe_view.default(clone_210, [1, 384, 384, 256]);  clone_210 = None
    transpose_61 = torch.ops.aten.transpose.int(getitem_2242, 1, 2);  getitem_2242 = None
    _to_copy_1317 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_24_triangle_attention_pair2qkvg2_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_24_triangle_attention_pair2qkvg2_weight = None
    _to_copy_1318 = torch.ops.aten._to_copy.default(transpose_61, dtype = torch.bfloat16);  transpose_61 = None
    t_485 = torch.ops.aten.t.default(_to_copy_1317);  _to_copy_1317 = None
    expand_148 = torch.ops.aten.expand.default(_to_copy_1318, [1, 384, 384, 256]);  _to_copy_1318 = None
    view_2354 = torch.ops.aten.view.default(expand_148, [384, 384, 256]);  expand_148 = None
    expand_149 = torch.ops.aten.expand.default(t_485, [1, 384, 256, 1024]);  t_485 = None
    view_2355 = torch.ops.aten.view.default(expand_149, [384, 256, 1024]);  expand_149 = None
    bmm_194 = torch.ops.aten.bmm.default(view_2354, view_2355);  view_2354 = view_2355 = None
    view_2356 = torch.ops.aten.view.default(bmm_194, [1, 384, 384, 1024]);  bmm_194 = None
    select_62 = torch.ops.aten.select.int(view_2348, 0, 1);  view_2348 = None
    view_2357 = torch.ops.aten.view.default(view_2356, [1, 384, 384, 4, 4, 64]);  view_2356 = None
    permute_1288 = torch.ops.aten.permute.default(view_2357, [4, 0, 3, 1, 2, 5]);  view_2357 = None
    view_2358 = torch.ops.aten.view.default(permute_1288, [4, 4, 384, 384, 64]);  permute_1288 = None
    unbind_int_107 = torch.ops.aten.unbind.int(view_2358);  view_2358 = None
    getitem_2253 = unbind_int_107[0]
    getitem_2254 = unbind_int_107[1]
    getitem_2255 = unbind_int_107[2]
    getitem_2256 = unbind_int_107[3];  unbind_int_107 = None
    expand_150 = torch.ops.aten.expand.default(select_62, [4, 384, 384, 384]);  select_62 = None
    _scaled_dot_product_efficient_attention_default_85 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2253, getitem_2254, getitem_2255, expand_150, False);  getitem_2253 = getitem_2254 = getitem_2255 = expand_150 = None
    getitem_2257 = _scaled_dot_product_efficient_attention_default_85[0];  _scaled_dot_product_efficient_attention_default_85 = None
    sigmoid_184 = torch.ops.aten.sigmoid.default(getitem_2256);  getitem_2256 = None
    mul_302 = torch.ops.aten.mul.Tensor(getitem_2257, sigmoid_184);  getitem_2257 = sigmoid_184 = None
    view_2359 = torch.ops.aten.view.default(mul_302, [1, 4, 384, 384, 64]);  mul_302 = None
    permute_1289 = torch.ops.aten.permute.default(view_2359, [0, 2, 3, 1, 4]);  view_2359 = None
    clone_211 = torch.ops.aten.clone.default(permute_1289, memory_format = torch.contiguous_format);  permute_1289 = None
    _unsafe_view_179 = torch.ops.aten._unsafe_view.default(clone_211, [1, 384, 384, 256]);  clone_211 = None
    cat_36 = torch.ops.aten.cat.default([_unsafe_view_178, _unsafe_view_179], dim = -1);  _unsafe_view_178 = _unsafe_view_179 = None
    slice_187 = torch.ops.aten.slice.Tensor(pairformer_stack_blocks_24_triangle_attention_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  pairformer_stack_blocks_24_triangle_attention_out_scalers = None
    unsqueeze_802 = torch.ops.aten.unsqueeze.default(slice_187, 1);  slice_187 = None
    mul_303 = torch.ops.aten.mul.Tensor(pairformer_stack_blocks_24_triangle_attention_linear_out_weight, unsqueeze_802);  pairformer_stack_blocks_24_triangle_attention_linear_out_weight = unsqueeze_802 = None
    _to_copy_1319 = torch.ops.aten._to_copy.default(mul_303, dtype = torch.bfloat16);  mul_303 = None
    t_486 = torch.ops.aten.t.default(_to_copy_1319);  _to_copy_1319 = None
    view_2360 = torch.ops.aten.view.default(cat_36, [147456, 512]);  cat_36 = None
    mm_451 = torch.ops.aten.mm.default(view_2360, t_486);  view_2360 = t_486 = None
    view_2361 = torch.ops.aten.view.default(mm_451, [1, 384, 384, 256]);  mm_451 = None
    add_250 = torch.ops.aten.add.Tensor(add_249, view_2361);  add_249 = view_2361 = None
    split_tensor_244 = torch.ops.aten.split.Tensor(add_243, 384, dim = -2)
    getitem_2261 = split_tensor_244[0];  split_tensor_244 = None
    _to_copy_1320 = torch.ops.aten._to_copy.default(getitem_2261, dtype = torch.float32);  getitem_2261 = None
    native_layer_norm_default_271 = torch.ops.aten.native_layer_norm.default(_to_copy_1320, [256], pairformer_stack_blocks_24_transition_pair_layer_norm_weight, pairformer_stack_blocks_24_transition_pair_layer_norm_bias, 1e-05);  _to_copy_1320 = pairformer_stack_blocks_24_transition_pair_layer_norm_weight = pairformer_stack_blocks_24_transition_pair_layer_norm_bias = None
    getitem_2262 = native_layer_norm_default_271[0];  native_layer_norm_default_271 = None
    _to_copy_1321 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_24_transition_pair_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_24_transition_pair_linear_no_bias_ab_weight = None
    _to_copy_1322 = torch.ops.aten._to_copy.default(getitem_2262, dtype = torch.bfloat16);  getitem_2262 = None
    t_487 = torch.ops.aten.t.default(_to_copy_1321);  _to_copy_1321 = None
    view_2362 = torch.ops.aten.view.default(_to_copy_1322, [147456, 256]);  _to_copy_1322 = None
    mm_452 = torch.ops.aten.mm.default(view_2362, t_487);  view_2362 = t_487 = None
    view_2363 = torch.ops.aten.view.default(mm_452, [1, 384, 384, 1024]);  mm_452 = None
    split_tensor_245 = torch.ops.aten.split.Tensor(view_2363, 512, dim = -1);  view_2363 = None
    getitem_2265 = split_tensor_245[0]
    getitem_2266 = split_tensor_245[1];  split_tensor_245 = None
    silu_63 = torch.ops.aten.silu.default(getitem_2265);  getitem_2265 = None
    mul_304 = torch.ops.aten.mul.Tensor(silu_63, getitem_2266);  silu_63 = getitem_2266 = None
    _to_copy_1323 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_24_transition_pair_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_24_transition_pair_linear_out_weight = None
    t_488 = torch.ops.aten.t.default(_to_copy_1323);  _to_copy_1323 = None
    view_2365 = torch.ops.aten.view.default(mul_304, [147456, 512]);  mul_304 = None
    mm_453 = torch.ops.aten.mm.default(view_2365, t_488);  view_2365 = t_488 = None
    view_2366 = torch.ops.aten.view.default(mm_453, [1, 384, 384, 256]);  mm_453 = None
    add_251 = torch.ops.aten.add.Tensor(add_250, view_2366);  add_250 = view_2366 = None
    _to_copy_1324 = torch.ops.aten._to_copy.default(add_247, dtype = torch.float32)
    native_layer_norm_default_272 = torch.ops.aten.native_layer_norm.default(_to_copy_1324, [384], pairformer_stack_blocks_24_attention_pair_bias_single_layer_norm_weight, pairformer_stack_blocks_24_attention_pair_bias_single_layer_norm_bias, 1e-05);  _to_copy_1324 = pairformer_stack_blocks_24_attention_pair_bias_single_layer_norm_weight = pairformer_stack_blocks_24_attention_pair_bias_single_layer_norm_bias = None
    getitem_2267 = native_layer_norm_default_272[0];  native_layer_norm_default_272 = None
    _to_copy_1325 = torch.ops.aten._to_copy.default(add_243, dtype = torch.float32);  add_243 = None
    native_layer_norm_default_273 = torch.ops.aten.native_layer_norm.default(_to_copy_1325, [256], pairformer_stack_blocks_24_attention_pair_bias_pair_layer_norm_weight, pairformer_stack_blocks_24_attention_pair_bias_pair_layer_norm_bias, 1e-05);  _to_copy_1325 = pairformer_stack_blocks_24_attention_pair_bias_pair_layer_norm_weight = pairformer_stack_blocks_24_attention_pair_bias_pair_layer_norm_bias = None
    getitem_2270 = native_layer_norm_default_273[0];  native_layer_norm_default_273 = None
    _to_copy_1326 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_24_attention_pair_bias_pair_linear_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_24_attention_pair_bias_pair_linear_weight = None
    _to_copy_1327 = torch.ops.aten._to_copy.default(getitem_2270, dtype = torch.bfloat16);  getitem_2270 = None
    t_489 = torch.ops.aten.t.default(_to_copy_1326);  _to_copy_1326 = None
    view_2367 = torch.ops.aten.view.default(_to_copy_1327, [147456, 256]);  _to_copy_1327 = None
    mm_454 = torch.ops.aten.mm.default(view_2367, t_489);  view_2367 = t_489 = None
    view_2368 = torch.ops.aten.view.default(mm_454, [1, 384, 384, 16]);  mm_454 = None
    permute_1290 = torch.ops.aten.permute.default(view_2368, [0, 3, 1, 2]);  view_2368 = None
    view_2369 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384])
    bitwise_not_145 = torch.ops.aten.bitwise_not.default(view_2369);  view_2369 = None
    masked_fill_145 = torch.ops.aten.masked_fill.Scalar(permute_1290, bitwise_not_145, -10000);  permute_1290 = bitwise_not_145 = None
    _to_copy_1328 = torch.ops.aten._to_copy.default(getitem_2267, dtype = torch.bfloat16);  getitem_2267 = None
    _to_copy_1329 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_24_attention_pair_bias_attention_input2qkvg_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_24_attention_pair_bias_attention_input2qkvg_weight = None
    unsqueeze_803 = torch.ops.aten.unsqueeze.default(_to_copy_1328, 3);  _to_copy_1328 = None
    unsqueeze_804 = torch.ops.aten.unsqueeze.default(unsqueeze_803, 4);  unsqueeze_803 = None
    unsqueeze_805 = torch.ops.aten.unsqueeze.default(unsqueeze_804, 5);  unsqueeze_804 = None
    permute_1291 = torch.ops.aten.permute.default(unsqueeze_805, [3, 0, 4, 1, 5, 2]);  unsqueeze_805 = None
    unsqueeze_806 = torch.ops.aten.unsqueeze.default(_to_copy_1329, 4);  _to_copy_1329 = None
    unsqueeze_807 = torch.ops.aten.unsqueeze.default(unsqueeze_806, 5);  unsqueeze_806 = None
    permute_1292 = torch.ops.aten.permute.default(unsqueeze_807, [1, 4, 2, 5, 3, 0]);  unsqueeze_807 = None
    permute_1293 = torch.ops.aten.permute.default(permute_1291, [3, 5, 0, 1, 2, 4]);  permute_1291 = None
    view_2370 = torch.ops.aten.view.default(permute_1293, [1, 384, 384]);  permute_1293 = None
    permute_1294 = torch.ops.aten.permute.default(permute_1292, [5, 0, 1, 2, 4, 3]);  permute_1292 = None
    view_2371 = torch.ops.aten.view.default(permute_1294, [1, 384, 1536]);  permute_1294 = None
    bmm_195 = torch.ops.aten.bmm.default(view_2370, view_2371);  view_2370 = view_2371 = None
    view_2372 = torch.ops.aten.view.default(bmm_195, [384, 1, 4, 1, 16, 24]);  bmm_195 = None
    permute_1295 = torch.ops.aten.permute.default(view_2372, [2, 3, 4, 0, 5, 1]);  view_2372 = None
    view_2373 = torch.ops.aten.view.default(permute_1295, [4, 1, 16, 384, 24]);  permute_1295 = None
    unbind_int_108 = torch.ops.aten.unbind.int(view_2373);  view_2373 = None
    getitem_2273 = unbind_int_108[0]
    getitem_2274 = unbind_int_108[1]
    getitem_2275 = unbind_int_108[2]
    getitem_2276 = unbind_int_108[3];  unbind_int_108 = None
    view_2374 = torch.ops.aten.view.default(pairformer_stack_blocks_24_attention_pair_bias_attention_query_bias, [1, 16, 1, 24]);  pairformer_stack_blocks_24_attention_pair_bias_attention_query_bias = None
    add_252 = torch.ops.aten.add.Tensor(getitem_2273, view_2374);  getitem_2273 = view_2374 = None
    _to_copy_1330 = torch.ops.aten._to_copy.default(add_252, dtype = torch.bfloat16);  add_252 = None
    expand_151 = torch.ops.aten.expand.default(masked_fill_145, [1, 16, 384, 384]);  masked_fill_145 = None
    _scaled_dot_product_efficient_attention_default_86 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_1330, getitem_2274, getitem_2275, expand_151, False);  _to_copy_1330 = getitem_2274 = getitem_2275 = expand_151 = None
    getitem_2277 = _scaled_dot_product_efficient_attention_default_86[0];  _scaled_dot_product_efficient_attention_default_86 = None
    add_253 = torch.ops.aten.add.Tensor(getitem_2276, 1);  getitem_2276 = None
    sigmoid_185 = torch.ops.aten.sigmoid.default(add_253);  add_253 = None
    mul_305 = torch.ops.aten.mul.Tensor(getitem_2277, sigmoid_185);  getitem_2277 = sigmoid_185 = None
    _to_copy_1331 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_24_attention_pair_bias_attention_output_proj_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_24_attention_pair_bias_attention_output_proj_weight = None
    unsqueeze_808 = torch.ops.aten.unsqueeze.default(mul_305, 4);  mul_305 = None
    permute_1296 = torch.ops.aten.permute.default(unsqueeze_808, [0, 2, 4, 3, 1]);  unsqueeze_808 = None
    unsqueeze_809 = torch.ops.aten.unsqueeze.default(_to_copy_1331, 3);  _to_copy_1331 = None
    unsqueeze_810 = torch.ops.aten.unsqueeze.default(unsqueeze_809, 4);  unsqueeze_809 = None
    permute_1297 = torch.ops.aten.permute.default(unsqueeze_810, [3, 4, 2, 1, 0]);  unsqueeze_810 = None
    permute_1298 = torch.ops.aten.permute.default(permute_1296, [1, 3, 4, 0, 2]);  permute_1296 = None
    clone_212 = torch.ops.aten.clone.default(permute_1298, memory_format = torch.contiguous_format);  permute_1298 = None
    _unsafe_view_180 = torch.ops.aten._unsafe_view.default(clone_212, [1, 384, 384]);  clone_212 = None
    permute_1299 = torch.ops.aten.permute.default(permute_1297, [3, 4, 0, 2, 1]);  permute_1297 = None
    clone_213 = torch.ops.aten.clone.default(permute_1299, memory_format = torch.contiguous_format);  permute_1299 = None
    _unsafe_view_181 = torch.ops.aten._unsafe_view.default(clone_213, [1, 384, 384]);  clone_213 = None
    bmm_196 = torch.ops.aten.bmm.default(_unsafe_view_180, _unsafe_view_181);  _unsafe_view_180 = _unsafe_view_181 = None
    view_2375 = torch.ops.aten.view.default(bmm_196, [384, 1, 1, 1, 384]);  bmm_196 = None
    permute_1300 = torch.ops.aten.permute.default(view_2375, [3, 0, 4, 1, 2]);  view_2375 = None
    view_2376 = torch.ops.aten.view.default(permute_1300, [1, 384, 384]);  permute_1300 = None
    unsqueeze_811 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
    mul_306 = torch.ops.aten.mul.Tensor(view_2376, unsqueeze_811);  view_2376 = unsqueeze_811 = None
    add_254 = torch.ops.aten.add.Tensor(add_247, mul_306);  mul_306 = None
    split_tensor_246 = torch.ops.aten.split.Tensor(add_247, 384, dim = -2);  add_247 = None
    getitem_2281 = split_tensor_246[0];  split_tensor_246 = None
    _to_copy_1332 = torch.ops.aten._to_copy.default(getitem_2281, dtype = torch.float32);  getitem_2281 = None
    native_layer_norm_default_274 = torch.ops.aten.native_layer_norm.default(_to_copy_1332, [384], pairformer_stack_blocks_24_transition_single_layer_norm_weight, pairformer_stack_blocks_24_transition_single_layer_norm_bias, 1e-05);  _to_copy_1332 = pairformer_stack_blocks_24_transition_single_layer_norm_weight = pairformer_stack_blocks_24_transition_single_layer_norm_bias = None
    getitem_2282 = native_layer_norm_default_274[0];  native_layer_norm_default_274 = None
    _to_copy_1333 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_24_transition_single_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_24_transition_single_linear_no_bias_ab_weight = None
    _to_copy_1334 = torch.ops.aten._to_copy.default(getitem_2282, dtype = torch.bfloat16);  getitem_2282 = None
    t_490 = torch.ops.aten.t.default(_to_copy_1333);  _to_copy_1333 = None
    view_2377 = torch.ops.aten.view.default(_to_copy_1334, [384, 384]);  _to_copy_1334 = None
    mm_455 = torch.ops.aten.mm.default(view_2377, t_490);  view_2377 = t_490 = None
    view_2378 = torch.ops.aten.view.default(mm_455, [1, 384, 1536]);  mm_455 = None
    split_tensor_247 = torch.ops.aten.split.Tensor(view_2378, 768, dim = -1);  view_2378 = None
    getitem_2285 = split_tensor_247[0]
    getitem_2286 = split_tensor_247[1];  split_tensor_247 = None
    silu_64 = torch.ops.aten.silu.default(getitem_2285);  getitem_2285 = None
    mul_307 = torch.ops.aten.mul.Tensor(silu_64, getitem_2286);  silu_64 = getitem_2286 = None
    _to_copy_1335 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_24_transition_single_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_24_transition_single_linear_out_weight = None
    t_491 = torch.ops.aten.t.default(_to_copy_1335);  _to_copy_1335 = None
    view_2380 = torch.ops.aten.view.default(mul_307, [384, 768]);  mul_307 = None
    mm_456 = torch.ops.aten.mm.default(view_2380, t_491);  view_2380 = t_491 = None
    view_2381 = torch.ops.aten.view.default(mm_456, [1, 384, 384]);  mm_456 = None
    add_255 = torch.ops.aten.add.Tensor(add_254, view_2381);  add_254 = view_2381 = None
    _to_copy_1336 = torch.ops.aten._to_copy.default(add_251, dtype = torch.float32)
    native_layer_norm_default_275 = torch.ops.aten.native_layer_norm.default(_to_copy_1336, [256], pairformer_stack_blocks_25_triangle_multiplication_layernorm_z_in_weight, pairformer_stack_blocks_25_triangle_multiplication_layernorm_z_in_bias, 1e-05);  _to_copy_1336 = pairformer_stack_blocks_25_triangle_multiplication_layernorm_z_in_weight = pairformer_stack_blocks_25_triangle_multiplication_layernorm_z_in_bias = None
    getitem_2287 = native_layer_norm_default_275[0];  native_layer_norm_default_275 = None
    split_with_sizes_default_62 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_25_triangle_multiplication_merged_linear_p_weight, [512, 512]);  pairformer_stack_blocks_25_triangle_multiplication_merged_linear_p_weight = None
    getitem_2290 = split_with_sizes_default_62[0]
    getitem_2291 = split_with_sizes_default_62[1];  split_with_sizes_default_62 = None
    split_with_sizes_default_63 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_25_triangle_multiplication_merged_linear_g_weight, [512, 512, 256]);  pairformer_stack_blocks_25_triangle_multiplication_merged_linear_g_weight = None
    getitem_2292 = split_with_sizes_default_63[0]
    getitem_2293 = split_with_sizes_default_63[1]
    getitem_2294 = split_with_sizes_default_63[2];  split_with_sizes_default_63 = None
    _to_copy_1337 = torch.ops.aten._to_copy.default(getitem_2290, dtype = torch.bfloat16);  getitem_2290 = None
    _to_copy_1338 = torch.ops.aten._to_copy.default(getitem_2287, dtype = torch.bfloat16)
    t_492 = torch.ops.aten.t.default(_to_copy_1337);  _to_copy_1337 = None
    view_2382 = torch.ops.aten.view.default(_to_copy_1338, [147456, 256]);  _to_copy_1338 = None
    mm_457 = torch.ops.aten.mm.default(view_2382, t_492);  view_2382 = t_492 = None
    view_2383 = torch.ops.aten.view.default(mm_457, [1, 384, 384, 512]);  mm_457 = None
    _to_copy_1339 = torch.ops.aten._to_copy.default(getitem_2292, dtype = torch.bfloat16);  getitem_2292 = None
    _to_copy_1340 = torch.ops.aten._to_copy.default(getitem_2287, dtype = torch.bfloat16)
    t_493 = torch.ops.aten.t.default(_to_copy_1339);  _to_copy_1339 = None
    view_2384 = torch.ops.aten.view.default(_to_copy_1340, [147456, 256]);  _to_copy_1340 = None
    mm_458 = torch.ops.aten.mm.default(view_2384, t_493);  view_2384 = t_493 = None
    view_2385 = torch.ops.aten.view.default(mm_458, [1, 384, 384, 512]);  mm_458 = None
    sigmoid_186 = torch.ops.aten.sigmoid.default(view_2385);  view_2385 = None
    mul_308 = torch.ops.aten.mul.Tensor(view_2383, sigmoid_186);  view_2383 = sigmoid_186 = None
    unsqueeze_812 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
    bitwise_not_146 = torch.ops.aten.bitwise_not.default(unsqueeze_812);  unsqueeze_812 = None
    masked_fill_146 = torch.ops.aten.masked_fill.Scalar(mul_308, bitwise_not_146, 0);  mul_308 = bitwise_not_146 = None
    split_tensor_248 = torch.ops.aten.split.Tensor(masked_fill_146, 256, dim = -1)
    getitem_2297 = split_tensor_248[0];  split_tensor_248 = None
    unsqueeze_815 = torch.ops.aten.unsqueeze.default(getitem_2297, 4);  getitem_2297 = None
    permute_1305 = torch.ops.aten.permute.default(unsqueeze_815, [0, 1, 4, 3, 2]);  unsqueeze_815 = None
    permute_1306 = torch.ops.aten.permute.default(permute_1305, [3, 1, 4, 0, 2]);  permute_1305 = None
    view_2388 = torch.ops.aten.view.default(permute_1306, [256, 384, 384]);  permute_1306 = None
    split_tensor_249 = torch.ops.aten.split.Tensor(masked_fill_146, 256, dim = -1);  masked_fill_146 = None
    getitem_2300 = split_tensor_249[1];  split_tensor_249 = None
    unsqueeze_816 = torch.ops.aten.unsqueeze.default(getitem_2300, 4);  getitem_2300 = None
    permute_1307 = torch.ops.aten.permute.default(unsqueeze_816, [0, 4, 1, 3, 2]);  unsqueeze_816 = None
    permute_1308 = torch.ops.aten.permute.default(permute_1307, [3, 4, 0, 2, 1]);  permute_1307 = None
    view_2389 = torch.ops.aten.view.default(permute_1308, [256, 384, 384]);  permute_1308 = None
    bmm_197 = torch.ops.aten.bmm.default(view_2388, view_2389);  view_2388 = view_2389 = None
    view_2390 = torch.ops.aten.view.default(bmm_197, [256, 384, 1, 1, 384]);  bmm_197 = None
    permute_1309 = torch.ops.aten.permute.default(view_2390, [3, 1, 4, 0, 2]);  view_2390 = None
    view_2391 = torch.ops.aten.view.default(permute_1309, [1, 384, 384, 256]);  permute_1309 = None
    _to_copy_1341 = torch.ops.aten._to_copy.default(getitem_2291, dtype = torch.bfloat16);  getitem_2291 = None
    _to_copy_1342 = torch.ops.aten._to_copy.default(getitem_2287, dtype = torch.bfloat16)
    t_494 = torch.ops.aten.t.default(_to_copy_1341);  _to_copy_1341 = None
    view_2392 = torch.ops.aten.view.default(_to_copy_1342, [147456, 256]);  _to_copy_1342 = None
    mm_459 = torch.ops.aten.mm.default(view_2392, t_494);  view_2392 = t_494 = None
    view_2393 = torch.ops.aten.view.default(mm_459, [1, 384, 384, 512]);  mm_459 = None
    _to_copy_1343 = torch.ops.aten._to_copy.default(getitem_2293, dtype = torch.bfloat16);  getitem_2293 = None
    _to_copy_1344 = torch.ops.aten._to_copy.default(getitem_2287, dtype = torch.bfloat16)
    t_495 = torch.ops.aten.t.default(_to_copy_1343);  _to_copy_1343 = None
    view_2394 = torch.ops.aten.view.default(_to_copy_1344, [147456, 256]);  _to_copy_1344 = None
    mm_460 = torch.ops.aten.mm.default(view_2394, t_495);  view_2394 = t_495 = None
    view_2395 = torch.ops.aten.view.default(mm_460, [1, 384, 384, 512]);  mm_460 = None
    sigmoid_187 = torch.ops.aten.sigmoid.default(view_2395);  view_2395 = None
    mul_309 = torch.ops.aten.mul.Tensor(view_2393, sigmoid_187);  view_2393 = sigmoid_187 = None
    view_2396 = torch.ops.aten.view.default(mul_309, [147456, 512]);  mul_309 = None
    view_2397 = torch.ops.aten.view.default(view_2396, [1, 384, 384, 512]);  view_2396 = None
    transpose_62 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
    unsqueeze_817 = torch.ops.aten.unsqueeze.default(transpose_62, 3);  transpose_62 = None
    clone_214 = torch.ops.aten.clone.default(unsqueeze_817, memory_format = torch.contiguous_format);  unsqueeze_817 = None
    bitwise_not_147 = torch.ops.aten.bitwise_not.default(clone_214);  clone_214 = None
    masked_fill_147 = torch.ops.aten.masked_fill.Scalar(view_2397, bitwise_not_147, 0);  view_2397 = bitwise_not_147 = None
    view_2398 = torch.ops.aten.view.default(masked_fill_147, [147456, 512]);  masked_fill_147 = None
    view_2402 = torch.ops.aten.view.default(view_2398, [1, 384, 384, 512])
    split_tensor_250 = torch.ops.aten.split.Tensor(view_2402, 256, dim = -1);  view_2402 = None
    getitem_2303 = split_tensor_250[0];  split_tensor_250 = None
    unsqueeze_820 = torch.ops.aten.unsqueeze.default(getitem_2303, 4);  getitem_2303 = None
    permute_1314 = torch.ops.aten.permute.default(unsqueeze_820, [0, 2, 4, 3, 1]);  unsqueeze_820 = None
    permute_1315 = torch.ops.aten.permute.default(permute_1314, [3, 1, 4, 0, 2]);  permute_1314 = None
    view_2403 = torch.ops.aten.view.default(permute_1315, [256, 384, 384]);  permute_1315 = None
    view_2404 = torch.ops.aten.view.default(view_2398, [1, 384, 384, 512]);  view_2398 = None
    split_tensor_251 = torch.ops.aten.split.Tensor(view_2404, 256, dim = -1);  view_2404 = None
    getitem_2306 = split_tensor_251[1];  split_tensor_251 = None
    unsqueeze_821 = torch.ops.aten.unsqueeze.default(getitem_2306, 4);  getitem_2306 = None
    permute_1316 = torch.ops.aten.permute.default(unsqueeze_821, [0, 4, 2, 3, 1]);  unsqueeze_821 = None
    permute_1317 = torch.ops.aten.permute.default(permute_1316, [3, 4, 0, 2, 1]);  permute_1316 = None
    view_2405 = torch.ops.aten.view.default(permute_1317, [256, 384, 384]);  permute_1317 = None
    bmm_198 = torch.ops.aten.bmm.default(view_2403, view_2405);  view_2403 = view_2405 = None
    view_2406 = torch.ops.aten.view.default(bmm_198, [256, 384, 1, 1, 384]);  bmm_198 = None
    permute_1318 = torch.ops.aten.permute.default(view_2406, [3, 1, 4, 0, 2]);  view_2406 = None
    view_2407 = torch.ops.aten.view.default(permute_1318, [1, 384, 384, 256]);  permute_1318 = None
    _to_copy_1345 = torch.ops.aten._to_copy.default(view_2391, dtype = torch.float32);  view_2391 = None
    native_layer_norm_default_276 = torch.ops.aten.native_layer_norm.default(_to_copy_1345, [256], None, None, 1e-05);  _to_copy_1345 = None
    getitem_2307 = native_layer_norm_default_276[0];  native_layer_norm_default_276 = None
    _to_copy_1346 = torch.ops.aten._to_copy.default(view_2407, dtype = torch.float32);  view_2407 = None
    native_layer_norm_default_277 = torch.ops.aten.native_layer_norm.default(_to_copy_1346, [256], None, None, 1e-05);  _to_copy_1346 = None
    getitem_2310 = native_layer_norm_default_277[0];  native_layer_norm_default_277 = None
    add_256 = torch.ops.aten.add.Tensor(getitem_2307, getitem_2310);  getitem_2307 = getitem_2310 = None
    _to_copy_1347 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_25_triangle_multiplication_linear_z_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_25_triangle_multiplication_linear_z_out_weight = None
    _to_copy_1348 = torch.ops.aten._to_copy.default(add_256, dtype = torch.bfloat16);  add_256 = None
    t_496 = torch.ops.aten.t.default(_to_copy_1347);  _to_copy_1347 = None
    view_2408 = torch.ops.aten.view.default(_to_copy_1348, [147456, 256]);  _to_copy_1348 = None
    mm_461 = torch.ops.aten.mm.default(view_2408, t_496);  view_2408 = t_496 = None
    view_2409 = torch.ops.aten.view.default(mm_461, [1, 384, 384, 256]);  mm_461 = None
    _to_copy_1349 = torch.ops.aten._to_copy.default(getitem_2294, dtype = torch.bfloat16);  getitem_2294 = None
    _to_copy_1350 = torch.ops.aten._to_copy.default(getitem_2287, dtype = torch.bfloat16);  getitem_2287 = None
    t_497 = torch.ops.aten.t.default(_to_copy_1349);  _to_copy_1349 = None
    view_2410 = torch.ops.aten.view.default(_to_copy_1350, [147456, 256]);  _to_copy_1350 = None
    mm_462 = torch.ops.aten.mm.default(view_2410, t_497);  view_2410 = t_497 = None
    view_2411 = torch.ops.aten.view.default(mm_462, [1, 384, 384, 256]);  mm_462 = None
    sigmoid_188 = torch.ops.aten.sigmoid.default(view_2411);  view_2411 = None
    mul_310 = torch.ops.aten.mul.Tensor(view_2409, sigmoid_188);  view_2409 = sigmoid_188 = None
    add_257 = torch.ops.aten.add.Tensor(add_251, mul_310);  mul_310 = None
    _to_copy_1351 = torch.ops.aten._to_copy.default(add_251, dtype = torch.float32)
    native_layer_norm_default_278 = torch.ops.aten.native_layer_norm.default(_to_copy_1351, [256], None, None, 1e-05);  _to_copy_1351 = None
    getitem_2313 = native_layer_norm_default_278[0];  native_layer_norm_default_278 = None
    _to_copy_1352 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_25_triangle_attention_pair2b_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_25_triangle_attention_pair2b_weight = None
    _to_copy_1353 = torch.ops.aten._to_copy.default(getitem_2313, dtype = torch.bfloat16)
    t_498 = torch.ops.aten.t.default(_to_copy_1352);  _to_copy_1352 = None
    view_2412 = torch.ops.aten.view.default(_to_copy_1353, [147456, 256]);  _to_copy_1353 = None
    mm_463 = torch.ops.aten.mm.default(view_2412, t_498);  view_2412 = t_498 = None
    view_2413 = torch.ops.aten.view.default(mm_463, [1, 384, 384, 8]);  mm_463 = None
    view_2414 = torch.ops.aten.view.default(view_2413, [1, 384, 384, 2, 4]);  view_2413 = None
    permute_1319 = torch.ops.aten.permute.default(view_2414, [0, 3, 4, 1, 2]);  view_2414 = None
    view_2415 = torch.ops.aten.view.default(permute_1319, [1, 2, 4, 1, 384, 384]);  permute_1319 = None
    view_2416 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 384, 384])
    bitwise_not_148 = torch.ops.aten.bitwise_not.default(view_2416);  view_2416 = None
    masked_fill_148 = torch.ops.aten.masked_fill.Scalar(view_2415, bitwise_not_148, -10000);  view_2415 = bitwise_not_148 = None
    view_2417 = torch.ops.aten.view.default(masked_fill_148, [1, 2, 4, 384, 384]);  masked_fill_148 = None
    permute_1320 = torch.ops.aten.permute.default(view_2417, [1, 0, 2, 3, 4]);  view_2417 = None
    view_2418 = torch.ops.aten.view.default(permute_1320, [2, 4, 1, 384, 384]);  permute_1320 = None
    _to_copy_1354 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_25_triangle_attention_pair2qkvg1_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_25_triangle_attention_pair2qkvg1_weight = None
    _to_copy_1355 = torch.ops.aten._to_copy.default(getitem_2313, dtype = torch.bfloat16)
    t_499 = torch.ops.aten.t.default(_to_copy_1354);  _to_copy_1354 = None
    view_2419 = torch.ops.aten.view.default(_to_copy_1355, [147456, 256]);  _to_copy_1355 = None
    mm_464 = torch.ops.aten.mm.default(view_2419, t_499);  view_2419 = t_499 = None
    view_2420 = torch.ops.aten.view.default(mm_464, [1, 384, 384, 1024]);  mm_464 = None
    select_63 = torch.ops.aten.select.int(view_2418, 0, 0)
    view_2421 = torch.ops.aten.view.default(view_2420, [1, 384, 384, 4, 4, 64]);  view_2420 = None
    permute_1321 = torch.ops.aten.permute.default(view_2421, [4, 0, 3, 1, 2, 5]);  view_2421 = None
    view_2422 = torch.ops.aten.view.default(permute_1321, [4, 4, 384, 384, 64]);  permute_1321 = None
    unbind_int_109 = torch.ops.aten.unbind.int(view_2422);  view_2422 = None
    getitem_2316 = unbind_int_109[0]
    getitem_2317 = unbind_int_109[1]
    getitem_2318 = unbind_int_109[2]
    getitem_2319 = unbind_int_109[3];  unbind_int_109 = None
    expand_152 = torch.ops.aten.expand.default(select_63, [4, 384, 384, 384]);  select_63 = None
    _scaled_dot_product_efficient_attention_default_87 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2316, getitem_2317, getitem_2318, expand_152, False);  getitem_2316 = getitem_2317 = getitem_2318 = expand_152 = None
    getitem_2320 = _scaled_dot_product_efficient_attention_default_87[0];  _scaled_dot_product_efficient_attention_default_87 = None
    sigmoid_189 = torch.ops.aten.sigmoid.default(getitem_2319);  getitem_2319 = None
    mul_311 = torch.ops.aten.mul.Tensor(getitem_2320, sigmoid_189);  getitem_2320 = sigmoid_189 = None
    view_2423 = torch.ops.aten.view.default(mul_311, [1, 4, 384, 384, 64]);  mul_311 = None
    permute_1322 = torch.ops.aten.permute.default(view_2423, [0, 2, 3, 1, 4]);  view_2423 = None
    clone_215 = torch.ops.aten.clone.default(permute_1322, memory_format = torch.contiguous_format);  permute_1322 = None
    _unsafe_view_182 = torch.ops.aten._unsafe_view.default(clone_215, [1, 384, 384, 256]);  clone_215 = None
    transpose_63 = torch.ops.aten.transpose.int(getitem_2313, 1, 2);  getitem_2313 = None
    _to_copy_1356 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_25_triangle_attention_pair2qkvg2_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_25_triangle_attention_pair2qkvg2_weight = None
    _to_copy_1357 = torch.ops.aten._to_copy.default(transpose_63, dtype = torch.bfloat16);  transpose_63 = None
    t_500 = torch.ops.aten.t.default(_to_copy_1356);  _to_copy_1356 = None
    expand_153 = torch.ops.aten.expand.default(_to_copy_1357, [1, 384, 384, 256]);  _to_copy_1357 = None
    view_2424 = torch.ops.aten.view.default(expand_153, [384, 384, 256]);  expand_153 = None
    expand_154 = torch.ops.aten.expand.default(t_500, [1, 384, 256, 1024]);  t_500 = None
    view_2425 = torch.ops.aten.view.default(expand_154, [384, 256, 1024]);  expand_154 = None
    bmm_199 = torch.ops.aten.bmm.default(view_2424, view_2425);  view_2424 = view_2425 = None
    view_2426 = torch.ops.aten.view.default(bmm_199, [1, 384, 384, 1024]);  bmm_199 = None
    select_64 = torch.ops.aten.select.int(view_2418, 0, 1);  view_2418 = None
    view_2427 = torch.ops.aten.view.default(view_2426, [1, 384, 384, 4, 4, 64]);  view_2426 = None
    permute_1323 = torch.ops.aten.permute.default(view_2427, [4, 0, 3, 1, 2, 5]);  view_2427 = None
    view_2428 = torch.ops.aten.view.default(permute_1323, [4, 4, 384, 384, 64]);  permute_1323 = None
    unbind_int_110 = torch.ops.aten.unbind.int(view_2428);  view_2428 = None
    getitem_2324 = unbind_int_110[0]
    getitem_2325 = unbind_int_110[1]
    getitem_2326 = unbind_int_110[2]
    getitem_2327 = unbind_int_110[3];  unbind_int_110 = None
    expand_155 = torch.ops.aten.expand.default(select_64, [4, 384, 384, 384]);  select_64 = None
    _scaled_dot_product_efficient_attention_default_88 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2324, getitem_2325, getitem_2326, expand_155, False);  getitem_2324 = getitem_2325 = getitem_2326 = expand_155 = None
    getitem_2328 = _scaled_dot_product_efficient_attention_default_88[0];  _scaled_dot_product_efficient_attention_default_88 = None
    sigmoid_190 = torch.ops.aten.sigmoid.default(getitem_2327);  getitem_2327 = None
    mul_312 = torch.ops.aten.mul.Tensor(getitem_2328, sigmoid_190);  getitem_2328 = sigmoid_190 = None
    view_2429 = torch.ops.aten.view.default(mul_312, [1, 4, 384, 384, 64]);  mul_312 = None
    permute_1324 = torch.ops.aten.permute.default(view_2429, [0, 2, 3, 1, 4]);  view_2429 = None
    clone_216 = torch.ops.aten.clone.default(permute_1324, memory_format = torch.contiguous_format);  permute_1324 = None
    _unsafe_view_183 = torch.ops.aten._unsafe_view.default(clone_216, [1, 384, 384, 256]);  clone_216 = None
    cat_37 = torch.ops.aten.cat.default([_unsafe_view_182, _unsafe_view_183], dim = -1);  _unsafe_view_182 = _unsafe_view_183 = None
    slice_188 = torch.ops.aten.slice.Tensor(pairformer_stack_blocks_25_triangle_attention_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  pairformer_stack_blocks_25_triangle_attention_out_scalers = None
    unsqueeze_822 = torch.ops.aten.unsqueeze.default(slice_188, 1);  slice_188 = None
    mul_313 = torch.ops.aten.mul.Tensor(pairformer_stack_blocks_25_triangle_attention_linear_out_weight, unsqueeze_822);  pairformer_stack_blocks_25_triangle_attention_linear_out_weight = unsqueeze_822 = None
    _to_copy_1358 = torch.ops.aten._to_copy.default(mul_313, dtype = torch.bfloat16);  mul_313 = None
    t_501 = torch.ops.aten.t.default(_to_copy_1358);  _to_copy_1358 = None
    view_2430 = torch.ops.aten.view.default(cat_37, [147456, 512]);  cat_37 = None
    mm_465 = torch.ops.aten.mm.default(view_2430, t_501);  view_2430 = t_501 = None
    view_2431 = torch.ops.aten.view.default(mm_465, [1, 384, 384, 256]);  mm_465 = None
    add_258 = torch.ops.aten.add.Tensor(add_257, view_2431);  add_257 = view_2431 = None
    split_tensor_252 = torch.ops.aten.split.Tensor(add_251, 384, dim = -2)
    getitem_2332 = split_tensor_252[0];  split_tensor_252 = None
    _to_copy_1359 = torch.ops.aten._to_copy.default(getitem_2332, dtype = torch.float32);  getitem_2332 = None
    native_layer_norm_default_279 = torch.ops.aten.native_layer_norm.default(_to_copy_1359, [256], pairformer_stack_blocks_25_transition_pair_layer_norm_weight, pairformer_stack_blocks_25_transition_pair_layer_norm_bias, 1e-05);  _to_copy_1359 = pairformer_stack_blocks_25_transition_pair_layer_norm_weight = pairformer_stack_blocks_25_transition_pair_layer_norm_bias = None
    getitem_2333 = native_layer_norm_default_279[0];  native_layer_norm_default_279 = None
    _to_copy_1360 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_25_transition_pair_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_25_transition_pair_linear_no_bias_ab_weight = None
    _to_copy_1361 = torch.ops.aten._to_copy.default(getitem_2333, dtype = torch.bfloat16);  getitem_2333 = None
    t_502 = torch.ops.aten.t.default(_to_copy_1360);  _to_copy_1360 = None
    view_2432 = torch.ops.aten.view.default(_to_copy_1361, [147456, 256]);  _to_copy_1361 = None
    mm_466 = torch.ops.aten.mm.default(view_2432, t_502);  view_2432 = t_502 = None
    view_2433 = torch.ops.aten.view.default(mm_466, [1, 384, 384, 1024]);  mm_466 = None
    split_tensor_253 = torch.ops.aten.split.Tensor(view_2433, 512, dim = -1);  view_2433 = None
    getitem_2336 = split_tensor_253[0]
    getitem_2337 = split_tensor_253[1];  split_tensor_253 = None
    silu_65 = torch.ops.aten.silu.default(getitem_2336);  getitem_2336 = None
    mul_314 = torch.ops.aten.mul.Tensor(silu_65, getitem_2337);  silu_65 = getitem_2337 = None
    _to_copy_1362 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_25_transition_pair_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_25_transition_pair_linear_out_weight = None
    t_503 = torch.ops.aten.t.default(_to_copy_1362);  _to_copy_1362 = None
    view_2435 = torch.ops.aten.view.default(mul_314, [147456, 512]);  mul_314 = None
    mm_467 = torch.ops.aten.mm.default(view_2435, t_503);  view_2435 = t_503 = None
    view_2436 = torch.ops.aten.view.default(mm_467, [1, 384, 384, 256]);  mm_467 = None
    add_259 = torch.ops.aten.add.Tensor(add_258, view_2436);  add_258 = view_2436 = None
    _to_copy_1363 = torch.ops.aten._to_copy.default(add_255, dtype = torch.float32)
    native_layer_norm_default_280 = torch.ops.aten.native_layer_norm.default(_to_copy_1363, [384], pairformer_stack_blocks_25_attention_pair_bias_single_layer_norm_weight, pairformer_stack_blocks_25_attention_pair_bias_single_layer_norm_bias, 1e-05);  _to_copy_1363 = pairformer_stack_blocks_25_attention_pair_bias_single_layer_norm_weight = pairformer_stack_blocks_25_attention_pair_bias_single_layer_norm_bias = None
    getitem_2338 = native_layer_norm_default_280[0];  native_layer_norm_default_280 = None
    _to_copy_1364 = torch.ops.aten._to_copy.default(add_251, dtype = torch.float32);  add_251 = None
    native_layer_norm_default_281 = torch.ops.aten.native_layer_norm.default(_to_copy_1364, [256], pairformer_stack_blocks_25_attention_pair_bias_pair_layer_norm_weight, pairformer_stack_blocks_25_attention_pair_bias_pair_layer_norm_bias, 1e-05);  _to_copy_1364 = pairformer_stack_blocks_25_attention_pair_bias_pair_layer_norm_weight = pairformer_stack_blocks_25_attention_pair_bias_pair_layer_norm_bias = None
    getitem_2341 = native_layer_norm_default_281[0];  native_layer_norm_default_281 = None
    _to_copy_1365 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_25_attention_pair_bias_pair_linear_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_25_attention_pair_bias_pair_linear_weight = None
    _to_copy_1366 = torch.ops.aten._to_copy.default(getitem_2341, dtype = torch.bfloat16);  getitem_2341 = None
    t_504 = torch.ops.aten.t.default(_to_copy_1365);  _to_copy_1365 = None
    view_2437 = torch.ops.aten.view.default(_to_copy_1366, [147456, 256]);  _to_copy_1366 = None
    mm_468 = torch.ops.aten.mm.default(view_2437, t_504);  view_2437 = t_504 = None
    view_2438 = torch.ops.aten.view.default(mm_468, [1, 384, 384, 16]);  mm_468 = None
    permute_1325 = torch.ops.aten.permute.default(view_2438, [0, 3, 1, 2]);  view_2438 = None
    view_2439 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384])
    bitwise_not_149 = torch.ops.aten.bitwise_not.default(view_2439);  view_2439 = None
    masked_fill_149 = torch.ops.aten.masked_fill.Scalar(permute_1325, bitwise_not_149, -10000);  permute_1325 = bitwise_not_149 = None
    _to_copy_1367 = torch.ops.aten._to_copy.default(getitem_2338, dtype = torch.bfloat16);  getitem_2338 = None
    _to_copy_1368 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_25_attention_pair_bias_attention_input2qkvg_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_25_attention_pair_bias_attention_input2qkvg_weight = None
    unsqueeze_823 = torch.ops.aten.unsqueeze.default(_to_copy_1367, 3);  _to_copy_1367 = None
    unsqueeze_824 = torch.ops.aten.unsqueeze.default(unsqueeze_823, 4);  unsqueeze_823 = None
    unsqueeze_825 = torch.ops.aten.unsqueeze.default(unsqueeze_824, 5);  unsqueeze_824 = None
    permute_1326 = torch.ops.aten.permute.default(unsqueeze_825, [3, 0, 4, 1, 5, 2]);  unsqueeze_825 = None
    unsqueeze_826 = torch.ops.aten.unsqueeze.default(_to_copy_1368, 4);  _to_copy_1368 = None
    unsqueeze_827 = torch.ops.aten.unsqueeze.default(unsqueeze_826, 5);  unsqueeze_826 = None
    permute_1327 = torch.ops.aten.permute.default(unsqueeze_827, [1, 4, 2, 5, 3, 0]);  unsqueeze_827 = None
    permute_1328 = torch.ops.aten.permute.default(permute_1326, [3, 5, 0, 1, 2, 4]);  permute_1326 = None
    view_2440 = torch.ops.aten.view.default(permute_1328, [1, 384, 384]);  permute_1328 = None
    permute_1329 = torch.ops.aten.permute.default(permute_1327, [5, 0, 1, 2, 4, 3]);  permute_1327 = None
    view_2441 = torch.ops.aten.view.default(permute_1329, [1, 384, 1536]);  permute_1329 = None
    bmm_200 = torch.ops.aten.bmm.default(view_2440, view_2441);  view_2440 = view_2441 = None
    view_2442 = torch.ops.aten.view.default(bmm_200, [384, 1, 4, 1, 16, 24]);  bmm_200 = None
    permute_1330 = torch.ops.aten.permute.default(view_2442, [2, 3, 4, 0, 5, 1]);  view_2442 = None
    view_2443 = torch.ops.aten.view.default(permute_1330, [4, 1, 16, 384, 24]);  permute_1330 = None
    unbind_int_111 = torch.ops.aten.unbind.int(view_2443);  view_2443 = None
    getitem_2344 = unbind_int_111[0]
    getitem_2345 = unbind_int_111[1]
    getitem_2346 = unbind_int_111[2]
    getitem_2347 = unbind_int_111[3];  unbind_int_111 = None
    view_2444 = torch.ops.aten.view.default(pairformer_stack_blocks_25_attention_pair_bias_attention_query_bias, [1, 16, 1, 24]);  pairformer_stack_blocks_25_attention_pair_bias_attention_query_bias = None
    add_260 = torch.ops.aten.add.Tensor(getitem_2344, view_2444);  getitem_2344 = view_2444 = None
    _to_copy_1369 = torch.ops.aten._to_copy.default(add_260, dtype = torch.bfloat16);  add_260 = None
    expand_156 = torch.ops.aten.expand.default(masked_fill_149, [1, 16, 384, 384]);  masked_fill_149 = None
    _scaled_dot_product_efficient_attention_default_89 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_1369, getitem_2345, getitem_2346, expand_156, False);  _to_copy_1369 = getitem_2345 = getitem_2346 = expand_156 = None
    getitem_2348 = _scaled_dot_product_efficient_attention_default_89[0];  _scaled_dot_product_efficient_attention_default_89 = None
    add_261 = torch.ops.aten.add.Tensor(getitem_2347, 1);  getitem_2347 = None
    sigmoid_191 = torch.ops.aten.sigmoid.default(add_261);  add_261 = None
    mul_315 = torch.ops.aten.mul.Tensor(getitem_2348, sigmoid_191);  getitem_2348 = sigmoid_191 = None
    _to_copy_1370 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_25_attention_pair_bias_attention_output_proj_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_25_attention_pair_bias_attention_output_proj_weight = None
    unsqueeze_828 = torch.ops.aten.unsqueeze.default(mul_315, 4);  mul_315 = None
    permute_1331 = torch.ops.aten.permute.default(unsqueeze_828, [0, 2, 4, 3, 1]);  unsqueeze_828 = None
    unsqueeze_829 = torch.ops.aten.unsqueeze.default(_to_copy_1370, 3);  _to_copy_1370 = None
    unsqueeze_830 = torch.ops.aten.unsqueeze.default(unsqueeze_829, 4);  unsqueeze_829 = None
    permute_1332 = torch.ops.aten.permute.default(unsqueeze_830, [3, 4, 2, 1, 0]);  unsqueeze_830 = None
    permute_1333 = torch.ops.aten.permute.default(permute_1331, [1, 3, 4, 0, 2]);  permute_1331 = None
    clone_217 = torch.ops.aten.clone.default(permute_1333, memory_format = torch.contiguous_format);  permute_1333 = None
    _unsafe_view_184 = torch.ops.aten._unsafe_view.default(clone_217, [1, 384, 384]);  clone_217 = None
    permute_1334 = torch.ops.aten.permute.default(permute_1332, [3, 4, 0, 2, 1]);  permute_1332 = None
    clone_218 = torch.ops.aten.clone.default(permute_1334, memory_format = torch.contiguous_format);  permute_1334 = None
    _unsafe_view_185 = torch.ops.aten._unsafe_view.default(clone_218, [1, 384, 384]);  clone_218 = None
    bmm_201 = torch.ops.aten.bmm.default(_unsafe_view_184, _unsafe_view_185);  _unsafe_view_184 = _unsafe_view_185 = None
    view_2445 = torch.ops.aten.view.default(bmm_201, [384, 1, 1, 1, 384]);  bmm_201 = None
    permute_1335 = torch.ops.aten.permute.default(view_2445, [3, 0, 4, 1, 2]);  view_2445 = None
    view_2446 = torch.ops.aten.view.default(permute_1335, [1, 384, 384]);  permute_1335 = None
    unsqueeze_831 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
    mul_316 = torch.ops.aten.mul.Tensor(view_2446, unsqueeze_831);  view_2446 = unsqueeze_831 = None
    add_262 = torch.ops.aten.add.Tensor(add_255, mul_316);  mul_316 = None
    split_tensor_254 = torch.ops.aten.split.Tensor(add_255, 384, dim = -2);  add_255 = None
    getitem_2352 = split_tensor_254[0];  split_tensor_254 = None
    _to_copy_1371 = torch.ops.aten._to_copy.default(getitem_2352, dtype = torch.float32);  getitem_2352 = None
    native_layer_norm_default_282 = torch.ops.aten.native_layer_norm.default(_to_copy_1371, [384], pairformer_stack_blocks_25_transition_single_layer_norm_weight, pairformer_stack_blocks_25_transition_single_layer_norm_bias, 1e-05);  _to_copy_1371 = pairformer_stack_blocks_25_transition_single_layer_norm_weight = pairformer_stack_blocks_25_transition_single_layer_norm_bias = None
    getitem_2353 = native_layer_norm_default_282[0];  native_layer_norm_default_282 = None
    _to_copy_1372 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_25_transition_single_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_25_transition_single_linear_no_bias_ab_weight = None
    _to_copy_1373 = torch.ops.aten._to_copy.default(getitem_2353, dtype = torch.bfloat16);  getitem_2353 = None
    t_505 = torch.ops.aten.t.default(_to_copy_1372);  _to_copy_1372 = None
    view_2447 = torch.ops.aten.view.default(_to_copy_1373, [384, 384]);  _to_copy_1373 = None
    mm_469 = torch.ops.aten.mm.default(view_2447, t_505);  view_2447 = t_505 = None
    view_2448 = torch.ops.aten.view.default(mm_469, [1, 384, 1536]);  mm_469 = None
    split_tensor_255 = torch.ops.aten.split.Tensor(view_2448, 768, dim = -1);  view_2448 = None
    getitem_2356 = split_tensor_255[0]
    getitem_2357 = split_tensor_255[1];  split_tensor_255 = None
    silu_66 = torch.ops.aten.silu.default(getitem_2356);  getitem_2356 = None
    mul_317 = torch.ops.aten.mul.Tensor(silu_66, getitem_2357);  silu_66 = getitem_2357 = None
    _to_copy_1374 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_25_transition_single_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_25_transition_single_linear_out_weight = None
    t_506 = torch.ops.aten.t.default(_to_copy_1374);  _to_copy_1374 = None
    view_2450 = torch.ops.aten.view.default(mul_317, [384, 768]);  mul_317 = None
    mm_470 = torch.ops.aten.mm.default(view_2450, t_506);  view_2450 = t_506 = None
    view_2451 = torch.ops.aten.view.default(mm_470, [1, 384, 384]);  mm_470 = None
    add_263 = torch.ops.aten.add.Tensor(add_262, view_2451);  add_262 = view_2451 = None
    _to_copy_1375 = torch.ops.aten._to_copy.default(add_259, dtype = torch.float32)
    native_layer_norm_default_283 = torch.ops.aten.native_layer_norm.default(_to_copy_1375, [256], pairformer_stack_blocks_26_triangle_multiplication_layernorm_z_in_weight, pairformer_stack_blocks_26_triangle_multiplication_layernorm_z_in_bias, 1e-05);  _to_copy_1375 = pairformer_stack_blocks_26_triangle_multiplication_layernorm_z_in_weight = pairformer_stack_blocks_26_triangle_multiplication_layernorm_z_in_bias = None
    getitem_2358 = native_layer_norm_default_283[0];  native_layer_norm_default_283 = None
    split_with_sizes_default_64 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_26_triangle_multiplication_merged_linear_p_weight, [512, 512]);  pairformer_stack_blocks_26_triangle_multiplication_merged_linear_p_weight = None
    getitem_2361 = split_with_sizes_default_64[0]
    getitem_2362 = split_with_sizes_default_64[1];  split_with_sizes_default_64 = None
    split_with_sizes_default_65 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_26_triangle_multiplication_merged_linear_g_weight, [512, 512, 256]);  pairformer_stack_blocks_26_triangle_multiplication_merged_linear_g_weight = None
    getitem_2363 = split_with_sizes_default_65[0]
    getitem_2364 = split_with_sizes_default_65[1]
    getitem_2365 = split_with_sizes_default_65[2];  split_with_sizes_default_65 = None
    _to_copy_1376 = torch.ops.aten._to_copy.default(getitem_2361, dtype = torch.bfloat16);  getitem_2361 = None
    _to_copy_1377 = torch.ops.aten._to_copy.default(getitem_2358, dtype = torch.bfloat16)
    t_507 = torch.ops.aten.t.default(_to_copy_1376);  _to_copy_1376 = None
    view_2452 = torch.ops.aten.view.default(_to_copy_1377, [147456, 256]);  _to_copy_1377 = None
    mm_471 = torch.ops.aten.mm.default(view_2452, t_507);  view_2452 = t_507 = None
    view_2453 = torch.ops.aten.view.default(mm_471, [1, 384, 384, 512]);  mm_471 = None
    _to_copy_1378 = torch.ops.aten._to_copy.default(getitem_2363, dtype = torch.bfloat16);  getitem_2363 = None
    _to_copy_1379 = torch.ops.aten._to_copy.default(getitem_2358, dtype = torch.bfloat16)
    t_508 = torch.ops.aten.t.default(_to_copy_1378);  _to_copy_1378 = None
    view_2454 = torch.ops.aten.view.default(_to_copy_1379, [147456, 256]);  _to_copy_1379 = None
    mm_472 = torch.ops.aten.mm.default(view_2454, t_508);  view_2454 = t_508 = None
    view_2455 = torch.ops.aten.view.default(mm_472, [1, 384, 384, 512]);  mm_472 = None
    sigmoid_192 = torch.ops.aten.sigmoid.default(view_2455);  view_2455 = None
    mul_318 = torch.ops.aten.mul.Tensor(view_2453, sigmoid_192);  view_2453 = sigmoid_192 = None
    unsqueeze_832 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
    bitwise_not_150 = torch.ops.aten.bitwise_not.default(unsqueeze_832);  unsqueeze_832 = None
    masked_fill_150 = torch.ops.aten.masked_fill.Scalar(mul_318, bitwise_not_150, 0);  mul_318 = bitwise_not_150 = None
    split_tensor_256 = torch.ops.aten.split.Tensor(masked_fill_150, 256, dim = -1)
    getitem_2368 = split_tensor_256[0];  split_tensor_256 = None
    unsqueeze_835 = torch.ops.aten.unsqueeze.default(getitem_2368, 4);  getitem_2368 = None
    permute_1340 = torch.ops.aten.permute.default(unsqueeze_835, [0, 1, 4, 3, 2]);  unsqueeze_835 = None
    permute_1341 = torch.ops.aten.permute.default(permute_1340, [3, 1, 4, 0, 2]);  permute_1340 = None
    view_2458 = torch.ops.aten.view.default(permute_1341, [256, 384, 384]);  permute_1341 = None
    split_tensor_257 = torch.ops.aten.split.Tensor(masked_fill_150, 256, dim = -1);  masked_fill_150 = None
    getitem_2371 = split_tensor_257[1];  split_tensor_257 = None
    unsqueeze_836 = torch.ops.aten.unsqueeze.default(getitem_2371, 4);  getitem_2371 = None
    permute_1342 = torch.ops.aten.permute.default(unsqueeze_836, [0, 4, 1, 3, 2]);  unsqueeze_836 = None
    permute_1343 = torch.ops.aten.permute.default(permute_1342, [3, 4, 0, 2, 1]);  permute_1342 = None
    view_2459 = torch.ops.aten.view.default(permute_1343, [256, 384, 384]);  permute_1343 = None
    bmm_202 = torch.ops.aten.bmm.default(view_2458, view_2459);  view_2458 = view_2459 = None
    view_2460 = torch.ops.aten.view.default(bmm_202, [256, 384, 1, 1, 384]);  bmm_202 = None
    permute_1344 = torch.ops.aten.permute.default(view_2460, [3, 1, 4, 0, 2]);  view_2460 = None
    view_2461 = torch.ops.aten.view.default(permute_1344, [1, 384, 384, 256]);  permute_1344 = None
    _to_copy_1380 = torch.ops.aten._to_copy.default(getitem_2362, dtype = torch.bfloat16);  getitem_2362 = None
    _to_copy_1381 = torch.ops.aten._to_copy.default(getitem_2358, dtype = torch.bfloat16)
    t_509 = torch.ops.aten.t.default(_to_copy_1380);  _to_copy_1380 = None
    view_2462 = torch.ops.aten.view.default(_to_copy_1381, [147456, 256]);  _to_copy_1381 = None
    mm_473 = torch.ops.aten.mm.default(view_2462, t_509);  view_2462 = t_509 = None
    view_2463 = torch.ops.aten.view.default(mm_473, [1, 384, 384, 512]);  mm_473 = None
    _to_copy_1382 = torch.ops.aten._to_copy.default(getitem_2364, dtype = torch.bfloat16);  getitem_2364 = None
    _to_copy_1383 = torch.ops.aten._to_copy.default(getitem_2358, dtype = torch.bfloat16)
    t_510 = torch.ops.aten.t.default(_to_copy_1382);  _to_copy_1382 = None
    view_2464 = torch.ops.aten.view.default(_to_copy_1383, [147456, 256]);  _to_copy_1383 = None
    mm_474 = torch.ops.aten.mm.default(view_2464, t_510);  view_2464 = t_510 = None
    view_2465 = torch.ops.aten.view.default(mm_474, [1, 384, 384, 512]);  mm_474 = None
    sigmoid_193 = torch.ops.aten.sigmoid.default(view_2465);  view_2465 = None
    mul_319 = torch.ops.aten.mul.Tensor(view_2463, sigmoid_193);  view_2463 = sigmoid_193 = None
    view_2466 = torch.ops.aten.view.default(mul_319, [147456, 512]);  mul_319 = None
    view_2467 = torch.ops.aten.view.default(view_2466, [1, 384, 384, 512]);  view_2466 = None
    transpose_64 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
    unsqueeze_837 = torch.ops.aten.unsqueeze.default(transpose_64, 3);  transpose_64 = None
    clone_219 = torch.ops.aten.clone.default(unsqueeze_837, memory_format = torch.contiguous_format);  unsqueeze_837 = None
    bitwise_not_151 = torch.ops.aten.bitwise_not.default(clone_219);  clone_219 = None
    masked_fill_151 = torch.ops.aten.masked_fill.Scalar(view_2467, bitwise_not_151, 0);  view_2467 = bitwise_not_151 = None
    view_2468 = torch.ops.aten.view.default(masked_fill_151, [147456, 512]);  masked_fill_151 = None
    view_2472 = torch.ops.aten.view.default(view_2468, [1, 384, 384, 512])
    split_tensor_258 = torch.ops.aten.split.Tensor(view_2472, 256, dim = -1);  view_2472 = None
    getitem_2374 = split_tensor_258[0];  split_tensor_258 = None
    unsqueeze_840 = torch.ops.aten.unsqueeze.default(getitem_2374, 4);  getitem_2374 = None
    permute_1349 = torch.ops.aten.permute.default(unsqueeze_840, [0, 2, 4, 3, 1]);  unsqueeze_840 = None
    permute_1350 = torch.ops.aten.permute.default(permute_1349, [3, 1, 4, 0, 2]);  permute_1349 = None
    view_2473 = torch.ops.aten.view.default(permute_1350, [256, 384, 384]);  permute_1350 = None
    view_2474 = torch.ops.aten.view.default(view_2468, [1, 384, 384, 512]);  view_2468 = None
    split_tensor_259 = torch.ops.aten.split.Tensor(view_2474, 256, dim = -1);  view_2474 = None
    getitem_2377 = split_tensor_259[1];  split_tensor_259 = None
    unsqueeze_841 = torch.ops.aten.unsqueeze.default(getitem_2377, 4);  getitem_2377 = None
    permute_1351 = torch.ops.aten.permute.default(unsqueeze_841, [0, 4, 2, 3, 1]);  unsqueeze_841 = None
    permute_1352 = torch.ops.aten.permute.default(permute_1351, [3, 4, 0, 2, 1]);  permute_1351 = None
    view_2475 = torch.ops.aten.view.default(permute_1352, [256, 384, 384]);  permute_1352 = None
    bmm_203 = torch.ops.aten.bmm.default(view_2473, view_2475);  view_2473 = view_2475 = None
    view_2476 = torch.ops.aten.view.default(bmm_203, [256, 384, 1, 1, 384]);  bmm_203 = None
    permute_1353 = torch.ops.aten.permute.default(view_2476, [3, 1, 4, 0, 2]);  view_2476 = None
    view_2477 = torch.ops.aten.view.default(permute_1353, [1, 384, 384, 256]);  permute_1353 = None
    _to_copy_1384 = torch.ops.aten._to_copy.default(view_2461, dtype = torch.float32);  view_2461 = None
    native_layer_norm_default_284 = torch.ops.aten.native_layer_norm.default(_to_copy_1384, [256], None, None, 1e-05);  _to_copy_1384 = None
    getitem_2378 = native_layer_norm_default_284[0];  native_layer_norm_default_284 = None
    _to_copy_1385 = torch.ops.aten._to_copy.default(view_2477, dtype = torch.float32);  view_2477 = None
    native_layer_norm_default_285 = torch.ops.aten.native_layer_norm.default(_to_copy_1385, [256], None, None, 1e-05);  _to_copy_1385 = None
    getitem_2381 = native_layer_norm_default_285[0];  native_layer_norm_default_285 = None
    add_264 = torch.ops.aten.add.Tensor(getitem_2378, getitem_2381);  getitem_2378 = getitem_2381 = None
    _to_copy_1386 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_26_triangle_multiplication_linear_z_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_26_triangle_multiplication_linear_z_out_weight = None
    _to_copy_1387 = torch.ops.aten._to_copy.default(add_264, dtype = torch.bfloat16);  add_264 = None
    t_511 = torch.ops.aten.t.default(_to_copy_1386);  _to_copy_1386 = None
    view_2478 = torch.ops.aten.view.default(_to_copy_1387, [147456, 256]);  _to_copy_1387 = None
    mm_475 = torch.ops.aten.mm.default(view_2478, t_511);  view_2478 = t_511 = None
    view_2479 = torch.ops.aten.view.default(mm_475, [1, 384, 384, 256]);  mm_475 = None
    _to_copy_1388 = torch.ops.aten._to_copy.default(getitem_2365, dtype = torch.bfloat16);  getitem_2365 = None
    _to_copy_1389 = torch.ops.aten._to_copy.default(getitem_2358, dtype = torch.bfloat16);  getitem_2358 = None
    t_512 = torch.ops.aten.t.default(_to_copy_1388);  _to_copy_1388 = None
    view_2480 = torch.ops.aten.view.default(_to_copy_1389, [147456, 256]);  _to_copy_1389 = None
    mm_476 = torch.ops.aten.mm.default(view_2480, t_512);  view_2480 = t_512 = None
    view_2481 = torch.ops.aten.view.default(mm_476, [1, 384, 384, 256]);  mm_476 = None
    sigmoid_194 = torch.ops.aten.sigmoid.default(view_2481);  view_2481 = None
    mul_320 = torch.ops.aten.mul.Tensor(view_2479, sigmoid_194);  view_2479 = sigmoid_194 = None
    add_265 = torch.ops.aten.add.Tensor(add_259, mul_320);  mul_320 = None
    _to_copy_1390 = torch.ops.aten._to_copy.default(add_259, dtype = torch.float32)
    native_layer_norm_default_286 = torch.ops.aten.native_layer_norm.default(_to_copy_1390, [256], None, None, 1e-05);  _to_copy_1390 = None
    getitem_2384 = native_layer_norm_default_286[0];  native_layer_norm_default_286 = None
    _to_copy_1391 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_26_triangle_attention_pair2b_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_26_triangle_attention_pair2b_weight = None
    _to_copy_1392 = torch.ops.aten._to_copy.default(getitem_2384, dtype = torch.bfloat16)
    t_513 = torch.ops.aten.t.default(_to_copy_1391);  _to_copy_1391 = None
    view_2482 = torch.ops.aten.view.default(_to_copy_1392, [147456, 256]);  _to_copy_1392 = None
    mm_477 = torch.ops.aten.mm.default(view_2482, t_513);  view_2482 = t_513 = None
    view_2483 = torch.ops.aten.view.default(mm_477, [1, 384, 384, 8]);  mm_477 = None
    view_2484 = torch.ops.aten.view.default(view_2483, [1, 384, 384, 2, 4]);  view_2483 = None
    permute_1354 = torch.ops.aten.permute.default(view_2484, [0, 3, 4, 1, 2]);  view_2484 = None
    view_2485 = torch.ops.aten.view.default(permute_1354, [1, 2, 4, 1, 384, 384]);  permute_1354 = None
    view_2486 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 384, 384])
    bitwise_not_152 = torch.ops.aten.bitwise_not.default(view_2486);  view_2486 = None
    masked_fill_152 = torch.ops.aten.masked_fill.Scalar(view_2485, bitwise_not_152, -10000);  view_2485 = bitwise_not_152 = None
    view_2487 = torch.ops.aten.view.default(masked_fill_152, [1, 2, 4, 384, 384]);  masked_fill_152 = None
    permute_1355 = torch.ops.aten.permute.default(view_2487, [1, 0, 2, 3, 4]);  view_2487 = None
    view_2488 = torch.ops.aten.view.default(permute_1355, [2, 4, 1, 384, 384]);  permute_1355 = None
    _to_copy_1393 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_26_triangle_attention_pair2qkvg1_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_26_triangle_attention_pair2qkvg1_weight = None
    _to_copy_1394 = torch.ops.aten._to_copy.default(getitem_2384, dtype = torch.bfloat16)
    t_514 = torch.ops.aten.t.default(_to_copy_1393);  _to_copy_1393 = None
    view_2489 = torch.ops.aten.view.default(_to_copy_1394, [147456, 256]);  _to_copy_1394 = None
    mm_478 = torch.ops.aten.mm.default(view_2489, t_514);  view_2489 = t_514 = None
    view_2490 = torch.ops.aten.view.default(mm_478, [1, 384, 384, 1024]);  mm_478 = None
    select_65 = torch.ops.aten.select.int(view_2488, 0, 0)
    view_2491 = torch.ops.aten.view.default(view_2490, [1, 384, 384, 4, 4, 64]);  view_2490 = None
    permute_1356 = torch.ops.aten.permute.default(view_2491, [4, 0, 3, 1, 2, 5]);  view_2491 = None
    view_2492 = torch.ops.aten.view.default(permute_1356, [4, 4, 384, 384, 64]);  permute_1356 = None
    unbind_int_112 = torch.ops.aten.unbind.int(view_2492);  view_2492 = None
    getitem_2387 = unbind_int_112[0]
    getitem_2388 = unbind_int_112[1]
    getitem_2389 = unbind_int_112[2]
    getitem_2390 = unbind_int_112[3];  unbind_int_112 = None
    expand_157 = torch.ops.aten.expand.default(select_65, [4, 384, 384, 384]);  select_65 = None
    _scaled_dot_product_efficient_attention_default_90 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2387, getitem_2388, getitem_2389, expand_157, False);  getitem_2387 = getitem_2388 = getitem_2389 = expand_157 = None
    getitem_2391 = _scaled_dot_product_efficient_attention_default_90[0];  _scaled_dot_product_efficient_attention_default_90 = None
    sigmoid_195 = torch.ops.aten.sigmoid.default(getitem_2390);  getitem_2390 = None
    mul_321 = torch.ops.aten.mul.Tensor(getitem_2391, sigmoid_195);  getitem_2391 = sigmoid_195 = None
    view_2493 = torch.ops.aten.view.default(mul_321, [1, 4, 384, 384, 64]);  mul_321 = None
    permute_1357 = torch.ops.aten.permute.default(view_2493, [0, 2, 3, 1, 4]);  view_2493 = None
    clone_220 = torch.ops.aten.clone.default(permute_1357, memory_format = torch.contiguous_format);  permute_1357 = None
    _unsafe_view_186 = torch.ops.aten._unsafe_view.default(clone_220, [1, 384, 384, 256]);  clone_220 = None
    transpose_65 = torch.ops.aten.transpose.int(getitem_2384, 1, 2);  getitem_2384 = None
    _to_copy_1395 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_26_triangle_attention_pair2qkvg2_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_26_triangle_attention_pair2qkvg2_weight = None
    _to_copy_1396 = torch.ops.aten._to_copy.default(transpose_65, dtype = torch.bfloat16);  transpose_65 = None
    t_515 = torch.ops.aten.t.default(_to_copy_1395);  _to_copy_1395 = None
    expand_158 = torch.ops.aten.expand.default(_to_copy_1396, [1, 384, 384, 256]);  _to_copy_1396 = None
    view_2494 = torch.ops.aten.view.default(expand_158, [384, 384, 256]);  expand_158 = None
    expand_159 = torch.ops.aten.expand.default(t_515, [1, 384, 256, 1024]);  t_515 = None
    view_2495 = torch.ops.aten.view.default(expand_159, [384, 256, 1024]);  expand_159 = None
    bmm_204 = torch.ops.aten.bmm.default(view_2494, view_2495);  view_2494 = view_2495 = None
    view_2496 = torch.ops.aten.view.default(bmm_204, [1, 384, 384, 1024]);  bmm_204 = None
    select_66 = torch.ops.aten.select.int(view_2488, 0, 1);  view_2488 = None
    view_2497 = torch.ops.aten.view.default(view_2496, [1, 384, 384, 4, 4, 64]);  view_2496 = None
    permute_1358 = torch.ops.aten.permute.default(view_2497, [4, 0, 3, 1, 2, 5]);  view_2497 = None
    view_2498 = torch.ops.aten.view.default(permute_1358, [4, 4, 384, 384, 64]);  permute_1358 = None
    unbind_int_113 = torch.ops.aten.unbind.int(view_2498);  view_2498 = None
    getitem_2395 = unbind_int_113[0]
    getitem_2396 = unbind_int_113[1]
    getitem_2397 = unbind_int_113[2]
    getitem_2398 = unbind_int_113[3];  unbind_int_113 = None
    expand_160 = torch.ops.aten.expand.default(select_66, [4, 384, 384, 384]);  select_66 = None
    _scaled_dot_product_efficient_attention_default_91 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2395, getitem_2396, getitem_2397, expand_160, False);  getitem_2395 = getitem_2396 = getitem_2397 = expand_160 = None
    getitem_2399 = _scaled_dot_product_efficient_attention_default_91[0];  _scaled_dot_product_efficient_attention_default_91 = None
    sigmoid_196 = torch.ops.aten.sigmoid.default(getitem_2398);  getitem_2398 = None
    mul_322 = torch.ops.aten.mul.Tensor(getitem_2399, sigmoid_196);  getitem_2399 = sigmoid_196 = None
    view_2499 = torch.ops.aten.view.default(mul_322, [1, 4, 384, 384, 64]);  mul_322 = None
    permute_1359 = torch.ops.aten.permute.default(view_2499, [0, 2, 3, 1, 4]);  view_2499 = None
    clone_221 = torch.ops.aten.clone.default(permute_1359, memory_format = torch.contiguous_format);  permute_1359 = None
    _unsafe_view_187 = torch.ops.aten._unsafe_view.default(clone_221, [1, 384, 384, 256]);  clone_221 = None
    cat_38 = torch.ops.aten.cat.default([_unsafe_view_186, _unsafe_view_187], dim = -1);  _unsafe_view_186 = _unsafe_view_187 = None
    slice_189 = torch.ops.aten.slice.Tensor(pairformer_stack_blocks_26_triangle_attention_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  pairformer_stack_blocks_26_triangle_attention_out_scalers = None
    unsqueeze_842 = torch.ops.aten.unsqueeze.default(slice_189, 1);  slice_189 = None
    mul_323 = torch.ops.aten.mul.Tensor(pairformer_stack_blocks_26_triangle_attention_linear_out_weight, unsqueeze_842);  pairformer_stack_blocks_26_triangle_attention_linear_out_weight = unsqueeze_842 = None
    _to_copy_1397 = torch.ops.aten._to_copy.default(mul_323, dtype = torch.bfloat16);  mul_323 = None
    t_516 = torch.ops.aten.t.default(_to_copy_1397);  _to_copy_1397 = None
    view_2500 = torch.ops.aten.view.default(cat_38, [147456, 512]);  cat_38 = None
    mm_479 = torch.ops.aten.mm.default(view_2500, t_516);  view_2500 = t_516 = None
    view_2501 = torch.ops.aten.view.default(mm_479, [1, 384, 384, 256]);  mm_479 = None
    add_266 = torch.ops.aten.add.Tensor(add_265, view_2501);  add_265 = view_2501 = None
    split_tensor_260 = torch.ops.aten.split.Tensor(add_259, 384, dim = -2)
    getitem_2403 = split_tensor_260[0];  split_tensor_260 = None
    _to_copy_1398 = torch.ops.aten._to_copy.default(getitem_2403, dtype = torch.float32);  getitem_2403 = None
    native_layer_norm_default_287 = torch.ops.aten.native_layer_norm.default(_to_copy_1398, [256], pairformer_stack_blocks_26_transition_pair_layer_norm_weight, pairformer_stack_blocks_26_transition_pair_layer_norm_bias, 1e-05);  _to_copy_1398 = pairformer_stack_blocks_26_transition_pair_layer_norm_weight = pairformer_stack_blocks_26_transition_pair_layer_norm_bias = None
    getitem_2404 = native_layer_norm_default_287[0];  native_layer_norm_default_287 = None
    _to_copy_1399 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_26_transition_pair_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_26_transition_pair_linear_no_bias_ab_weight = None
    _to_copy_1400 = torch.ops.aten._to_copy.default(getitem_2404, dtype = torch.bfloat16);  getitem_2404 = None
    t_517 = torch.ops.aten.t.default(_to_copy_1399);  _to_copy_1399 = None
    view_2502 = torch.ops.aten.view.default(_to_copy_1400, [147456, 256]);  _to_copy_1400 = None
    mm_480 = torch.ops.aten.mm.default(view_2502, t_517);  view_2502 = t_517 = None
    view_2503 = torch.ops.aten.view.default(mm_480, [1, 384, 384, 1024]);  mm_480 = None
    split_tensor_261 = torch.ops.aten.split.Tensor(view_2503, 512, dim = -1);  view_2503 = None
    getitem_2407 = split_tensor_261[0]
    getitem_2408 = split_tensor_261[1];  split_tensor_261 = None
    silu_67 = torch.ops.aten.silu.default(getitem_2407);  getitem_2407 = None
    mul_324 = torch.ops.aten.mul.Tensor(silu_67, getitem_2408);  silu_67 = getitem_2408 = None
    _to_copy_1401 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_26_transition_pair_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_26_transition_pair_linear_out_weight = None
    t_518 = torch.ops.aten.t.default(_to_copy_1401);  _to_copy_1401 = None
    view_2505 = torch.ops.aten.view.default(mul_324, [147456, 512]);  mul_324 = None
    mm_481 = torch.ops.aten.mm.default(view_2505, t_518);  view_2505 = t_518 = None
    view_2506 = torch.ops.aten.view.default(mm_481, [1, 384, 384, 256]);  mm_481 = None
    add_267 = torch.ops.aten.add.Tensor(add_266, view_2506);  add_266 = view_2506 = None
    _to_copy_1402 = torch.ops.aten._to_copy.default(add_263, dtype = torch.float32)
    native_layer_norm_default_288 = torch.ops.aten.native_layer_norm.default(_to_copy_1402, [384], pairformer_stack_blocks_26_attention_pair_bias_single_layer_norm_weight, pairformer_stack_blocks_26_attention_pair_bias_single_layer_norm_bias, 1e-05);  _to_copy_1402 = pairformer_stack_blocks_26_attention_pair_bias_single_layer_norm_weight = pairformer_stack_blocks_26_attention_pair_bias_single_layer_norm_bias = None
    getitem_2409 = native_layer_norm_default_288[0];  native_layer_norm_default_288 = None
    _to_copy_1403 = torch.ops.aten._to_copy.default(add_259, dtype = torch.float32);  add_259 = None
    native_layer_norm_default_289 = torch.ops.aten.native_layer_norm.default(_to_copy_1403, [256], pairformer_stack_blocks_26_attention_pair_bias_pair_layer_norm_weight, pairformer_stack_blocks_26_attention_pair_bias_pair_layer_norm_bias, 1e-05);  _to_copy_1403 = pairformer_stack_blocks_26_attention_pair_bias_pair_layer_norm_weight = pairformer_stack_blocks_26_attention_pair_bias_pair_layer_norm_bias = None
    getitem_2412 = native_layer_norm_default_289[0];  native_layer_norm_default_289 = None
    _to_copy_1404 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_26_attention_pair_bias_pair_linear_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_26_attention_pair_bias_pair_linear_weight = None
    _to_copy_1405 = torch.ops.aten._to_copy.default(getitem_2412, dtype = torch.bfloat16);  getitem_2412 = None
    t_519 = torch.ops.aten.t.default(_to_copy_1404);  _to_copy_1404 = None
    view_2507 = torch.ops.aten.view.default(_to_copy_1405, [147456, 256]);  _to_copy_1405 = None
    mm_482 = torch.ops.aten.mm.default(view_2507, t_519);  view_2507 = t_519 = None
    view_2508 = torch.ops.aten.view.default(mm_482, [1, 384, 384, 16]);  mm_482 = None
    permute_1360 = torch.ops.aten.permute.default(view_2508, [0, 3, 1, 2]);  view_2508 = None
    view_2509 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384])
    bitwise_not_153 = torch.ops.aten.bitwise_not.default(view_2509);  view_2509 = None
    masked_fill_153 = torch.ops.aten.masked_fill.Scalar(permute_1360, bitwise_not_153, -10000);  permute_1360 = bitwise_not_153 = None
    _to_copy_1406 = torch.ops.aten._to_copy.default(getitem_2409, dtype = torch.bfloat16);  getitem_2409 = None
    _to_copy_1407 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_26_attention_pair_bias_attention_input2qkvg_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_26_attention_pair_bias_attention_input2qkvg_weight = None
    unsqueeze_843 = torch.ops.aten.unsqueeze.default(_to_copy_1406, 3);  _to_copy_1406 = None
    unsqueeze_844 = torch.ops.aten.unsqueeze.default(unsqueeze_843, 4);  unsqueeze_843 = None
    unsqueeze_845 = torch.ops.aten.unsqueeze.default(unsqueeze_844, 5);  unsqueeze_844 = None
    permute_1361 = torch.ops.aten.permute.default(unsqueeze_845, [3, 0, 4, 1, 5, 2]);  unsqueeze_845 = None
    unsqueeze_846 = torch.ops.aten.unsqueeze.default(_to_copy_1407, 4);  _to_copy_1407 = None
    unsqueeze_847 = torch.ops.aten.unsqueeze.default(unsqueeze_846, 5);  unsqueeze_846 = None
    permute_1362 = torch.ops.aten.permute.default(unsqueeze_847, [1, 4, 2, 5, 3, 0]);  unsqueeze_847 = None
    permute_1363 = torch.ops.aten.permute.default(permute_1361, [3, 5, 0, 1, 2, 4]);  permute_1361 = None
    view_2510 = torch.ops.aten.view.default(permute_1363, [1, 384, 384]);  permute_1363 = None
    permute_1364 = torch.ops.aten.permute.default(permute_1362, [5, 0, 1, 2, 4, 3]);  permute_1362 = None
    view_2511 = torch.ops.aten.view.default(permute_1364, [1, 384, 1536]);  permute_1364 = None
    bmm_205 = torch.ops.aten.bmm.default(view_2510, view_2511);  view_2510 = view_2511 = None
    view_2512 = torch.ops.aten.view.default(bmm_205, [384, 1, 4, 1, 16, 24]);  bmm_205 = None
    permute_1365 = torch.ops.aten.permute.default(view_2512, [2, 3, 4, 0, 5, 1]);  view_2512 = None
    view_2513 = torch.ops.aten.view.default(permute_1365, [4, 1, 16, 384, 24]);  permute_1365 = None
    unbind_int_114 = torch.ops.aten.unbind.int(view_2513);  view_2513 = None
    getitem_2415 = unbind_int_114[0]
    getitem_2416 = unbind_int_114[1]
    getitem_2417 = unbind_int_114[2]
    getitem_2418 = unbind_int_114[3];  unbind_int_114 = None
    view_2514 = torch.ops.aten.view.default(pairformer_stack_blocks_26_attention_pair_bias_attention_query_bias, [1, 16, 1, 24]);  pairformer_stack_blocks_26_attention_pair_bias_attention_query_bias = None
    add_268 = torch.ops.aten.add.Tensor(getitem_2415, view_2514);  getitem_2415 = view_2514 = None
    _to_copy_1408 = torch.ops.aten._to_copy.default(add_268, dtype = torch.bfloat16);  add_268 = None
    expand_161 = torch.ops.aten.expand.default(masked_fill_153, [1, 16, 384, 384]);  masked_fill_153 = None
    _scaled_dot_product_efficient_attention_default_92 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_1408, getitem_2416, getitem_2417, expand_161, False);  _to_copy_1408 = getitem_2416 = getitem_2417 = expand_161 = None
    getitem_2419 = _scaled_dot_product_efficient_attention_default_92[0];  _scaled_dot_product_efficient_attention_default_92 = None
    add_269 = torch.ops.aten.add.Tensor(getitem_2418, 1);  getitem_2418 = None
    sigmoid_197 = torch.ops.aten.sigmoid.default(add_269);  add_269 = None
    mul_325 = torch.ops.aten.mul.Tensor(getitem_2419, sigmoid_197);  getitem_2419 = sigmoid_197 = None
    _to_copy_1409 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_26_attention_pair_bias_attention_output_proj_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_26_attention_pair_bias_attention_output_proj_weight = None
    unsqueeze_848 = torch.ops.aten.unsqueeze.default(mul_325, 4);  mul_325 = None
    permute_1366 = torch.ops.aten.permute.default(unsqueeze_848, [0, 2, 4, 3, 1]);  unsqueeze_848 = None
    unsqueeze_849 = torch.ops.aten.unsqueeze.default(_to_copy_1409, 3);  _to_copy_1409 = None
    unsqueeze_850 = torch.ops.aten.unsqueeze.default(unsqueeze_849, 4);  unsqueeze_849 = None
    permute_1367 = torch.ops.aten.permute.default(unsqueeze_850, [3, 4, 2, 1, 0]);  unsqueeze_850 = None
    permute_1368 = torch.ops.aten.permute.default(permute_1366, [1, 3, 4, 0, 2]);  permute_1366 = None
    clone_222 = torch.ops.aten.clone.default(permute_1368, memory_format = torch.contiguous_format);  permute_1368 = None
    _unsafe_view_188 = torch.ops.aten._unsafe_view.default(clone_222, [1, 384, 384]);  clone_222 = None
    permute_1369 = torch.ops.aten.permute.default(permute_1367, [3, 4, 0, 2, 1]);  permute_1367 = None
    clone_223 = torch.ops.aten.clone.default(permute_1369, memory_format = torch.contiguous_format);  permute_1369 = None
    _unsafe_view_189 = torch.ops.aten._unsafe_view.default(clone_223, [1, 384, 384]);  clone_223 = None
    bmm_206 = torch.ops.aten.bmm.default(_unsafe_view_188, _unsafe_view_189);  _unsafe_view_188 = _unsafe_view_189 = None
    view_2515 = torch.ops.aten.view.default(bmm_206, [384, 1, 1, 1, 384]);  bmm_206 = None
    permute_1370 = torch.ops.aten.permute.default(view_2515, [3, 0, 4, 1, 2]);  view_2515 = None
    view_2516 = torch.ops.aten.view.default(permute_1370, [1, 384, 384]);  permute_1370 = None
    unsqueeze_851 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
    mul_326 = torch.ops.aten.mul.Tensor(view_2516, unsqueeze_851);  view_2516 = unsqueeze_851 = None
    add_270 = torch.ops.aten.add.Tensor(add_263, mul_326);  mul_326 = None
    split_tensor_262 = torch.ops.aten.split.Tensor(add_263, 384, dim = -2);  add_263 = None
    getitem_2423 = split_tensor_262[0];  split_tensor_262 = None
    _to_copy_1410 = torch.ops.aten._to_copy.default(getitem_2423, dtype = torch.float32);  getitem_2423 = None
    native_layer_norm_default_290 = torch.ops.aten.native_layer_norm.default(_to_copy_1410, [384], pairformer_stack_blocks_26_transition_single_layer_norm_weight, pairformer_stack_blocks_26_transition_single_layer_norm_bias, 1e-05);  _to_copy_1410 = pairformer_stack_blocks_26_transition_single_layer_norm_weight = pairformer_stack_blocks_26_transition_single_layer_norm_bias = None
    getitem_2424 = native_layer_norm_default_290[0];  native_layer_norm_default_290 = None
    _to_copy_1411 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_26_transition_single_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_26_transition_single_linear_no_bias_ab_weight = None
    _to_copy_1412 = torch.ops.aten._to_copy.default(getitem_2424, dtype = torch.bfloat16);  getitem_2424 = None
    t_520 = torch.ops.aten.t.default(_to_copy_1411);  _to_copy_1411 = None
    view_2517 = torch.ops.aten.view.default(_to_copy_1412, [384, 384]);  _to_copy_1412 = None
    mm_483 = torch.ops.aten.mm.default(view_2517, t_520);  view_2517 = t_520 = None
    view_2518 = torch.ops.aten.view.default(mm_483, [1, 384, 1536]);  mm_483 = None
    split_tensor_263 = torch.ops.aten.split.Tensor(view_2518, 768, dim = -1);  view_2518 = None
    getitem_2427 = split_tensor_263[0]
    getitem_2428 = split_tensor_263[1];  split_tensor_263 = None
    silu_68 = torch.ops.aten.silu.default(getitem_2427);  getitem_2427 = None
    mul_327 = torch.ops.aten.mul.Tensor(silu_68, getitem_2428);  silu_68 = getitem_2428 = None
    _to_copy_1413 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_26_transition_single_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_26_transition_single_linear_out_weight = None
    t_521 = torch.ops.aten.t.default(_to_copy_1413);  _to_copy_1413 = None
    view_2520 = torch.ops.aten.view.default(mul_327, [384, 768]);  mul_327 = None
    mm_484 = torch.ops.aten.mm.default(view_2520, t_521);  view_2520 = t_521 = None
    view_2521 = torch.ops.aten.view.default(mm_484, [1, 384, 384]);  mm_484 = None
    add_271 = torch.ops.aten.add.Tensor(add_270, view_2521);  add_270 = view_2521 = None
    _to_copy_1414 = torch.ops.aten._to_copy.default(add_267, dtype = torch.float32)
    native_layer_norm_default_291 = torch.ops.aten.native_layer_norm.default(_to_copy_1414, [256], pairformer_stack_blocks_27_triangle_multiplication_layernorm_z_in_weight, pairformer_stack_blocks_27_triangle_multiplication_layernorm_z_in_bias, 1e-05);  _to_copy_1414 = pairformer_stack_blocks_27_triangle_multiplication_layernorm_z_in_weight = pairformer_stack_blocks_27_triangle_multiplication_layernorm_z_in_bias = None
    getitem_2429 = native_layer_norm_default_291[0];  native_layer_norm_default_291 = None
    split_with_sizes_default_66 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_27_triangle_multiplication_merged_linear_p_weight, [512, 512]);  pairformer_stack_blocks_27_triangle_multiplication_merged_linear_p_weight = None
    getitem_2432 = split_with_sizes_default_66[0]
    getitem_2433 = split_with_sizes_default_66[1];  split_with_sizes_default_66 = None
    split_with_sizes_default_67 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_27_triangle_multiplication_merged_linear_g_weight, [512, 512, 256]);  pairformer_stack_blocks_27_triangle_multiplication_merged_linear_g_weight = None
    getitem_2434 = split_with_sizes_default_67[0]
    getitem_2435 = split_with_sizes_default_67[1]
    getitem_2436 = split_with_sizes_default_67[2];  split_with_sizes_default_67 = None
    _to_copy_1415 = torch.ops.aten._to_copy.default(getitem_2432, dtype = torch.bfloat16);  getitem_2432 = None
    _to_copy_1416 = torch.ops.aten._to_copy.default(getitem_2429, dtype = torch.bfloat16)
    t_522 = torch.ops.aten.t.default(_to_copy_1415);  _to_copy_1415 = None
    view_2522 = torch.ops.aten.view.default(_to_copy_1416, [147456, 256]);  _to_copy_1416 = None
    mm_485 = torch.ops.aten.mm.default(view_2522, t_522);  view_2522 = t_522 = None
    view_2523 = torch.ops.aten.view.default(mm_485, [1, 384, 384, 512]);  mm_485 = None
    _to_copy_1417 = torch.ops.aten._to_copy.default(getitem_2434, dtype = torch.bfloat16);  getitem_2434 = None
    _to_copy_1418 = torch.ops.aten._to_copy.default(getitem_2429, dtype = torch.bfloat16)
    t_523 = torch.ops.aten.t.default(_to_copy_1417);  _to_copy_1417 = None
    view_2524 = torch.ops.aten.view.default(_to_copy_1418, [147456, 256]);  _to_copy_1418 = None
    mm_486 = torch.ops.aten.mm.default(view_2524, t_523);  view_2524 = t_523 = None
    view_2525 = torch.ops.aten.view.default(mm_486, [1, 384, 384, 512]);  mm_486 = None
    sigmoid_198 = torch.ops.aten.sigmoid.default(view_2525);  view_2525 = None
    mul_328 = torch.ops.aten.mul.Tensor(view_2523, sigmoid_198);  view_2523 = sigmoid_198 = None
    unsqueeze_852 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
    bitwise_not_154 = torch.ops.aten.bitwise_not.default(unsqueeze_852);  unsqueeze_852 = None
    masked_fill_154 = torch.ops.aten.masked_fill.Scalar(mul_328, bitwise_not_154, 0);  mul_328 = bitwise_not_154 = None
    split_tensor_264 = torch.ops.aten.split.Tensor(masked_fill_154, 256, dim = -1)
    getitem_2439 = split_tensor_264[0];  split_tensor_264 = None
    unsqueeze_855 = torch.ops.aten.unsqueeze.default(getitem_2439, 4);  getitem_2439 = None
    permute_1375 = torch.ops.aten.permute.default(unsqueeze_855, [0, 1, 4, 3, 2]);  unsqueeze_855 = None
    permute_1376 = torch.ops.aten.permute.default(permute_1375, [3, 1, 4, 0, 2]);  permute_1375 = None
    view_2528 = torch.ops.aten.view.default(permute_1376, [256, 384, 384]);  permute_1376 = None
    split_tensor_265 = torch.ops.aten.split.Tensor(masked_fill_154, 256, dim = -1);  masked_fill_154 = None
    getitem_2442 = split_tensor_265[1];  split_tensor_265 = None
    unsqueeze_856 = torch.ops.aten.unsqueeze.default(getitem_2442, 4);  getitem_2442 = None
    permute_1377 = torch.ops.aten.permute.default(unsqueeze_856, [0, 4, 1, 3, 2]);  unsqueeze_856 = None
    permute_1378 = torch.ops.aten.permute.default(permute_1377, [3, 4, 0, 2, 1]);  permute_1377 = None
    view_2529 = torch.ops.aten.view.default(permute_1378, [256, 384, 384]);  permute_1378 = None
    bmm_207 = torch.ops.aten.bmm.default(view_2528, view_2529);  view_2528 = view_2529 = None
    view_2530 = torch.ops.aten.view.default(bmm_207, [256, 384, 1, 1, 384]);  bmm_207 = None
    permute_1379 = torch.ops.aten.permute.default(view_2530, [3, 1, 4, 0, 2]);  view_2530 = None
    view_2531 = torch.ops.aten.view.default(permute_1379, [1, 384, 384, 256]);  permute_1379 = None
    _to_copy_1419 = torch.ops.aten._to_copy.default(getitem_2433, dtype = torch.bfloat16);  getitem_2433 = None
    _to_copy_1420 = torch.ops.aten._to_copy.default(getitem_2429, dtype = torch.bfloat16)
    t_524 = torch.ops.aten.t.default(_to_copy_1419);  _to_copy_1419 = None
    view_2532 = torch.ops.aten.view.default(_to_copy_1420, [147456, 256]);  _to_copy_1420 = None
    mm_487 = torch.ops.aten.mm.default(view_2532, t_524);  view_2532 = t_524 = None
    view_2533 = torch.ops.aten.view.default(mm_487, [1, 384, 384, 512]);  mm_487 = None
    _to_copy_1421 = torch.ops.aten._to_copy.default(getitem_2435, dtype = torch.bfloat16);  getitem_2435 = None
    _to_copy_1422 = torch.ops.aten._to_copy.default(getitem_2429, dtype = torch.bfloat16)
    t_525 = torch.ops.aten.t.default(_to_copy_1421);  _to_copy_1421 = None
    view_2534 = torch.ops.aten.view.default(_to_copy_1422, [147456, 256]);  _to_copy_1422 = None
    mm_488 = torch.ops.aten.mm.default(view_2534, t_525);  view_2534 = t_525 = None
    view_2535 = torch.ops.aten.view.default(mm_488, [1, 384, 384, 512]);  mm_488 = None
    sigmoid_199 = torch.ops.aten.sigmoid.default(view_2535);  view_2535 = None
    mul_329 = torch.ops.aten.mul.Tensor(view_2533, sigmoid_199);  view_2533 = sigmoid_199 = None
    view_2536 = torch.ops.aten.view.default(mul_329, [147456, 512]);  mul_329 = None
    view_2537 = torch.ops.aten.view.default(view_2536, [1, 384, 384, 512]);  view_2536 = None
    transpose_66 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
    unsqueeze_857 = torch.ops.aten.unsqueeze.default(transpose_66, 3);  transpose_66 = None
    clone_224 = torch.ops.aten.clone.default(unsqueeze_857, memory_format = torch.contiguous_format);  unsqueeze_857 = None
    bitwise_not_155 = torch.ops.aten.bitwise_not.default(clone_224);  clone_224 = None
    masked_fill_155 = torch.ops.aten.masked_fill.Scalar(view_2537, bitwise_not_155, 0);  view_2537 = bitwise_not_155 = None
    view_2538 = torch.ops.aten.view.default(masked_fill_155, [147456, 512]);  masked_fill_155 = None
    view_2542 = torch.ops.aten.view.default(view_2538, [1, 384, 384, 512])
    split_tensor_266 = torch.ops.aten.split.Tensor(view_2542, 256, dim = -1);  view_2542 = None
    getitem_2445 = split_tensor_266[0];  split_tensor_266 = None
    unsqueeze_860 = torch.ops.aten.unsqueeze.default(getitem_2445, 4);  getitem_2445 = None
    permute_1384 = torch.ops.aten.permute.default(unsqueeze_860, [0, 2, 4, 3, 1]);  unsqueeze_860 = None
    permute_1385 = torch.ops.aten.permute.default(permute_1384, [3, 1, 4, 0, 2]);  permute_1384 = None
    view_2543 = torch.ops.aten.view.default(permute_1385, [256, 384, 384]);  permute_1385 = None
    view_2544 = torch.ops.aten.view.default(view_2538, [1, 384, 384, 512]);  view_2538 = None
    split_tensor_267 = torch.ops.aten.split.Tensor(view_2544, 256, dim = -1);  view_2544 = None
    getitem_2448 = split_tensor_267[1];  split_tensor_267 = None
    unsqueeze_861 = torch.ops.aten.unsqueeze.default(getitem_2448, 4);  getitem_2448 = None
    permute_1386 = torch.ops.aten.permute.default(unsqueeze_861, [0, 4, 2, 3, 1]);  unsqueeze_861 = None
    permute_1387 = torch.ops.aten.permute.default(permute_1386, [3, 4, 0, 2, 1]);  permute_1386 = None
    view_2545 = torch.ops.aten.view.default(permute_1387, [256, 384, 384]);  permute_1387 = None
    bmm_208 = torch.ops.aten.bmm.default(view_2543, view_2545);  view_2543 = view_2545 = None
    view_2546 = torch.ops.aten.view.default(bmm_208, [256, 384, 1, 1, 384]);  bmm_208 = None
    permute_1388 = torch.ops.aten.permute.default(view_2546, [3, 1, 4, 0, 2]);  view_2546 = None
    view_2547 = torch.ops.aten.view.default(permute_1388, [1, 384, 384, 256]);  permute_1388 = None
    _to_copy_1423 = torch.ops.aten._to_copy.default(view_2531, dtype = torch.float32);  view_2531 = None
    native_layer_norm_default_292 = torch.ops.aten.native_layer_norm.default(_to_copy_1423, [256], None, None, 1e-05);  _to_copy_1423 = None
    getitem_2449 = native_layer_norm_default_292[0];  native_layer_norm_default_292 = None
    _to_copy_1424 = torch.ops.aten._to_copy.default(view_2547, dtype = torch.float32);  view_2547 = None
    native_layer_norm_default_293 = torch.ops.aten.native_layer_norm.default(_to_copy_1424, [256], None, None, 1e-05);  _to_copy_1424 = None
    getitem_2452 = native_layer_norm_default_293[0];  native_layer_norm_default_293 = None
    add_272 = torch.ops.aten.add.Tensor(getitem_2449, getitem_2452);  getitem_2449 = getitem_2452 = None
    _to_copy_1425 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_27_triangle_multiplication_linear_z_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_27_triangle_multiplication_linear_z_out_weight = None
    _to_copy_1426 = torch.ops.aten._to_copy.default(add_272, dtype = torch.bfloat16);  add_272 = None
    t_526 = torch.ops.aten.t.default(_to_copy_1425);  _to_copy_1425 = None
    view_2548 = torch.ops.aten.view.default(_to_copy_1426, [147456, 256]);  _to_copy_1426 = None
    mm_489 = torch.ops.aten.mm.default(view_2548, t_526);  view_2548 = t_526 = None
    view_2549 = torch.ops.aten.view.default(mm_489, [1, 384, 384, 256]);  mm_489 = None
    _to_copy_1427 = torch.ops.aten._to_copy.default(getitem_2436, dtype = torch.bfloat16);  getitem_2436 = None
    _to_copy_1428 = torch.ops.aten._to_copy.default(getitem_2429, dtype = torch.bfloat16);  getitem_2429 = None
    t_527 = torch.ops.aten.t.default(_to_copy_1427);  _to_copy_1427 = None
    view_2550 = torch.ops.aten.view.default(_to_copy_1428, [147456, 256]);  _to_copy_1428 = None
    mm_490 = torch.ops.aten.mm.default(view_2550, t_527);  view_2550 = t_527 = None
    view_2551 = torch.ops.aten.view.default(mm_490, [1, 384, 384, 256]);  mm_490 = None
    sigmoid_200 = torch.ops.aten.sigmoid.default(view_2551);  view_2551 = None
    mul_330 = torch.ops.aten.mul.Tensor(view_2549, sigmoid_200);  view_2549 = sigmoid_200 = None
    add_273 = torch.ops.aten.add.Tensor(add_267, mul_330);  mul_330 = None
    _to_copy_1429 = torch.ops.aten._to_copy.default(add_267, dtype = torch.float32)
    native_layer_norm_default_294 = torch.ops.aten.native_layer_norm.default(_to_copy_1429, [256], None, None, 1e-05);  _to_copy_1429 = None
    getitem_2455 = native_layer_norm_default_294[0];  native_layer_norm_default_294 = None
    _to_copy_1430 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_27_triangle_attention_pair2b_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_27_triangle_attention_pair2b_weight = None
    _to_copy_1431 = torch.ops.aten._to_copy.default(getitem_2455, dtype = torch.bfloat16)
    t_528 = torch.ops.aten.t.default(_to_copy_1430);  _to_copy_1430 = None
    view_2552 = torch.ops.aten.view.default(_to_copy_1431, [147456, 256]);  _to_copy_1431 = None
    mm_491 = torch.ops.aten.mm.default(view_2552, t_528);  view_2552 = t_528 = None
    view_2553 = torch.ops.aten.view.default(mm_491, [1, 384, 384, 8]);  mm_491 = None
    view_2554 = torch.ops.aten.view.default(view_2553, [1, 384, 384, 2, 4]);  view_2553 = None
    permute_1389 = torch.ops.aten.permute.default(view_2554, [0, 3, 4, 1, 2]);  view_2554 = None
    view_2555 = torch.ops.aten.view.default(permute_1389, [1, 2, 4, 1, 384, 384]);  permute_1389 = None
    view_2556 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 384, 384])
    bitwise_not_156 = torch.ops.aten.bitwise_not.default(view_2556);  view_2556 = None
    masked_fill_156 = torch.ops.aten.masked_fill.Scalar(view_2555, bitwise_not_156, -10000);  view_2555 = bitwise_not_156 = None
    view_2557 = torch.ops.aten.view.default(masked_fill_156, [1, 2, 4, 384, 384]);  masked_fill_156 = None
    permute_1390 = torch.ops.aten.permute.default(view_2557, [1, 0, 2, 3, 4]);  view_2557 = None
    view_2558 = torch.ops.aten.view.default(permute_1390, [2, 4, 1, 384, 384]);  permute_1390 = None
    _to_copy_1432 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_27_triangle_attention_pair2qkvg1_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_27_triangle_attention_pair2qkvg1_weight = None
    _to_copy_1433 = torch.ops.aten._to_copy.default(getitem_2455, dtype = torch.bfloat16)
    t_529 = torch.ops.aten.t.default(_to_copy_1432);  _to_copy_1432 = None
    view_2559 = torch.ops.aten.view.default(_to_copy_1433, [147456, 256]);  _to_copy_1433 = None
    mm_492 = torch.ops.aten.mm.default(view_2559, t_529);  view_2559 = t_529 = None
    view_2560 = torch.ops.aten.view.default(mm_492, [1, 384, 384, 1024]);  mm_492 = None
    select_67 = torch.ops.aten.select.int(view_2558, 0, 0)
    view_2561 = torch.ops.aten.view.default(view_2560, [1, 384, 384, 4, 4, 64]);  view_2560 = None
    permute_1391 = torch.ops.aten.permute.default(view_2561, [4, 0, 3, 1, 2, 5]);  view_2561 = None
    view_2562 = torch.ops.aten.view.default(permute_1391, [4, 4, 384, 384, 64]);  permute_1391 = None
    unbind_int_115 = torch.ops.aten.unbind.int(view_2562);  view_2562 = None
    getitem_2458 = unbind_int_115[0]
    getitem_2459 = unbind_int_115[1]
    getitem_2460 = unbind_int_115[2]
    getitem_2461 = unbind_int_115[3];  unbind_int_115 = None
    expand_162 = torch.ops.aten.expand.default(select_67, [4, 384, 384, 384]);  select_67 = None
    _scaled_dot_product_efficient_attention_default_93 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2458, getitem_2459, getitem_2460, expand_162, False);  getitem_2458 = getitem_2459 = getitem_2460 = expand_162 = None
    getitem_2462 = _scaled_dot_product_efficient_attention_default_93[0];  _scaled_dot_product_efficient_attention_default_93 = None
    sigmoid_201 = torch.ops.aten.sigmoid.default(getitem_2461);  getitem_2461 = None
    mul_331 = torch.ops.aten.mul.Tensor(getitem_2462, sigmoid_201);  getitem_2462 = sigmoid_201 = None
    view_2563 = torch.ops.aten.view.default(mul_331, [1, 4, 384, 384, 64]);  mul_331 = None
    permute_1392 = torch.ops.aten.permute.default(view_2563, [0, 2, 3, 1, 4]);  view_2563 = None
    clone_225 = torch.ops.aten.clone.default(permute_1392, memory_format = torch.contiguous_format);  permute_1392 = None
    _unsafe_view_190 = torch.ops.aten._unsafe_view.default(clone_225, [1, 384, 384, 256]);  clone_225 = None
    transpose_67 = torch.ops.aten.transpose.int(getitem_2455, 1, 2);  getitem_2455 = None
    _to_copy_1434 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_27_triangle_attention_pair2qkvg2_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_27_triangle_attention_pair2qkvg2_weight = None
    _to_copy_1435 = torch.ops.aten._to_copy.default(transpose_67, dtype = torch.bfloat16);  transpose_67 = None
    t_530 = torch.ops.aten.t.default(_to_copy_1434);  _to_copy_1434 = None
    expand_163 = torch.ops.aten.expand.default(_to_copy_1435, [1, 384, 384, 256]);  _to_copy_1435 = None
    view_2564 = torch.ops.aten.view.default(expand_163, [384, 384, 256]);  expand_163 = None
    expand_164 = torch.ops.aten.expand.default(t_530, [1, 384, 256, 1024]);  t_530 = None
    view_2565 = torch.ops.aten.view.default(expand_164, [384, 256, 1024]);  expand_164 = None
    bmm_209 = torch.ops.aten.bmm.default(view_2564, view_2565);  view_2564 = view_2565 = None
    view_2566 = torch.ops.aten.view.default(bmm_209, [1, 384, 384, 1024]);  bmm_209 = None
    select_68 = torch.ops.aten.select.int(view_2558, 0, 1);  view_2558 = None
    view_2567 = torch.ops.aten.view.default(view_2566, [1, 384, 384, 4, 4, 64]);  view_2566 = None
    permute_1393 = torch.ops.aten.permute.default(view_2567, [4, 0, 3, 1, 2, 5]);  view_2567 = None
    view_2568 = torch.ops.aten.view.default(permute_1393, [4, 4, 384, 384, 64]);  permute_1393 = None
    unbind_int_116 = torch.ops.aten.unbind.int(view_2568);  view_2568 = None
    getitem_2466 = unbind_int_116[0]
    getitem_2467 = unbind_int_116[1]
    getitem_2468 = unbind_int_116[2]
    getitem_2469 = unbind_int_116[3];  unbind_int_116 = None
    expand_165 = torch.ops.aten.expand.default(select_68, [4, 384, 384, 384]);  select_68 = None
    _scaled_dot_product_efficient_attention_default_94 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2466, getitem_2467, getitem_2468, expand_165, False);  getitem_2466 = getitem_2467 = getitem_2468 = expand_165 = None
    getitem_2470 = _scaled_dot_product_efficient_attention_default_94[0];  _scaled_dot_product_efficient_attention_default_94 = None
    sigmoid_202 = torch.ops.aten.sigmoid.default(getitem_2469);  getitem_2469 = None
    mul_332 = torch.ops.aten.mul.Tensor(getitem_2470, sigmoid_202);  getitem_2470 = sigmoid_202 = None
    view_2569 = torch.ops.aten.view.default(mul_332, [1, 4, 384, 384, 64]);  mul_332 = None
    permute_1394 = torch.ops.aten.permute.default(view_2569, [0, 2, 3, 1, 4]);  view_2569 = None
    clone_226 = torch.ops.aten.clone.default(permute_1394, memory_format = torch.contiguous_format);  permute_1394 = None
    _unsafe_view_191 = torch.ops.aten._unsafe_view.default(clone_226, [1, 384, 384, 256]);  clone_226 = None
    cat_39 = torch.ops.aten.cat.default([_unsafe_view_190, _unsafe_view_191], dim = -1);  _unsafe_view_190 = _unsafe_view_191 = None
    slice_190 = torch.ops.aten.slice.Tensor(pairformer_stack_blocks_27_triangle_attention_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  pairformer_stack_blocks_27_triangle_attention_out_scalers = None
    unsqueeze_862 = torch.ops.aten.unsqueeze.default(slice_190, 1);  slice_190 = None
    mul_333 = torch.ops.aten.mul.Tensor(pairformer_stack_blocks_27_triangle_attention_linear_out_weight, unsqueeze_862);  pairformer_stack_blocks_27_triangle_attention_linear_out_weight = unsqueeze_862 = None
    _to_copy_1436 = torch.ops.aten._to_copy.default(mul_333, dtype = torch.bfloat16);  mul_333 = None
    t_531 = torch.ops.aten.t.default(_to_copy_1436);  _to_copy_1436 = None
    view_2570 = torch.ops.aten.view.default(cat_39, [147456, 512]);  cat_39 = None
    mm_493 = torch.ops.aten.mm.default(view_2570, t_531);  view_2570 = t_531 = None
    view_2571 = torch.ops.aten.view.default(mm_493, [1, 384, 384, 256]);  mm_493 = None
    add_274 = torch.ops.aten.add.Tensor(add_273, view_2571);  add_273 = view_2571 = None
    split_tensor_268 = torch.ops.aten.split.Tensor(add_267, 384, dim = -2)
    getitem_2474 = split_tensor_268[0];  split_tensor_268 = None
    _to_copy_1437 = torch.ops.aten._to_copy.default(getitem_2474, dtype = torch.float32);  getitem_2474 = None
    native_layer_norm_default_295 = torch.ops.aten.native_layer_norm.default(_to_copy_1437, [256], pairformer_stack_blocks_27_transition_pair_layer_norm_weight, pairformer_stack_blocks_27_transition_pair_layer_norm_bias, 1e-05);  _to_copy_1437 = pairformer_stack_blocks_27_transition_pair_layer_norm_weight = pairformer_stack_blocks_27_transition_pair_layer_norm_bias = None
    getitem_2475 = native_layer_norm_default_295[0];  native_layer_norm_default_295 = None
    _to_copy_1438 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_27_transition_pair_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_27_transition_pair_linear_no_bias_ab_weight = None
    _to_copy_1439 = torch.ops.aten._to_copy.default(getitem_2475, dtype = torch.bfloat16);  getitem_2475 = None
    t_532 = torch.ops.aten.t.default(_to_copy_1438);  _to_copy_1438 = None
    view_2572 = torch.ops.aten.view.default(_to_copy_1439, [147456, 256]);  _to_copy_1439 = None
    mm_494 = torch.ops.aten.mm.default(view_2572, t_532);  view_2572 = t_532 = None
    view_2573 = torch.ops.aten.view.default(mm_494, [1, 384, 384, 1024]);  mm_494 = None
    split_tensor_269 = torch.ops.aten.split.Tensor(view_2573, 512, dim = -1);  view_2573 = None
    getitem_2478 = split_tensor_269[0]
    getitem_2479 = split_tensor_269[1];  split_tensor_269 = None
    silu_69 = torch.ops.aten.silu.default(getitem_2478);  getitem_2478 = None
    mul_334 = torch.ops.aten.mul.Tensor(silu_69, getitem_2479);  silu_69 = getitem_2479 = None
    _to_copy_1440 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_27_transition_pair_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_27_transition_pair_linear_out_weight = None
    t_533 = torch.ops.aten.t.default(_to_copy_1440);  _to_copy_1440 = None
    view_2575 = torch.ops.aten.view.default(mul_334, [147456, 512]);  mul_334 = None
    mm_495 = torch.ops.aten.mm.default(view_2575, t_533);  view_2575 = t_533 = None
    view_2576 = torch.ops.aten.view.default(mm_495, [1, 384, 384, 256]);  mm_495 = None
    add_275 = torch.ops.aten.add.Tensor(add_274, view_2576);  add_274 = view_2576 = None
    _to_copy_1441 = torch.ops.aten._to_copy.default(add_271, dtype = torch.float32)
    native_layer_norm_default_296 = torch.ops.aten.native_layer_norm.default(_to_copy_1441, [384], pairformer_stack_blocks_27_attention_pair_bias_single_layer_norm_weight, pairformer_stack_blocks_27_attention_pair_bias_single_layer_norm_bias, 1e-05);  _to_copy_1441 = pairformer_stack_blocks_27_attention_pair_bias_single_layer_norm_weight = pairformer_stack_blocks_27_attention_pair_bias_single_layer_norm_bias = None
    getitem_2480 = native_layer_norm_default_296[0];  native_layer_norm_default_296 = None
    _to_copy_1442 = torch.ops.aten._to_copy.default(add_267, dtype = torch.float32);  add_267 = None
    native_layer_norm_default_297 = torch.ops.aten.native_layer_norm.default(_to_copy_1442, [256], pairformer_stack_blocks_27_attention_pair_bias_pair_layer_norm_weight, pairformer_stack_blocks_27_attention_pair_bias_pair_layer_norm_bias, 1e-05);  _to_copy_1442 = pairformer_stack_blocks_27_attention_pair_bias_pair_layer_norm_weight = pairformer_stack_blocks_27_attention_pair_bias_pair_layer_norm_bias = None
    getitem_2483 = native_layer_norm_default_297[0];  native_layer_norm_default_297 = None
    _to_copy_1443 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_27_attention_pair_bias_pair_linear_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_27_attention_pair_bias_pair_linear_weight = None
    _to_copy_1444 = torch.ops.aten._to_copy.default(getitem_2483, dtype = torch.bfloat16);  getitem_2483 = None
    t_534 = torch.ops.aten.t.default(_to_copy_1443);  _to_copy_1443 = None
    view_2577 = torch.ops.aten.view.default(_to_copy_1444, [147456, 256]);  _to_copy_1444 = None
    mm_496 = torch.ops.aten.mm.default(view_2577, t_534);  view_2577 = t_534 = None
    view_2578 = torch.ops.aten.view.default(mm_496, [1, 384, 384, 16]);  mm_496 = None
    permute_1395 = torch.ops.aten.permute.default(view_2578, [0, 3, 1, 2]);  view_2578 = None
    view_2579 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384])
    bitwise_not_157 = torch.ops.aten.bitwise_not.default(view_2579);  view_2579 = None
    masked_fill_157 = torch.ops.aten.masked_fill.Scalar(permute_1395, bitwise_not_157, -10000);  permute_1395 = bitwise_not_157 = None
    _to_copy_1445 = torch.ops.aten._to_copy.default(getitem_2480, dtype = torch.bfloat16);  getitem_2480 = None
    _to_copy_1446 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_27_attention_pair_bias_attention_input2qkvg_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_27_attention_pair_bias_attention_input2qkvg_weight = None
    unsqueeze_863 = torch.ops.aten.unsqueeze.default(_to_copy_1445, 3);  _to_copy_1445 = None
    unsqueeze_864 = torch.ops.aten.unsqueeze.default(unsqueeze_863, 4);  unsqueeze_863 = None
    unsqueeze_865 = torch.ops.aten.unsqueeze.default(unsqueeze_864, 5);  unsqueeze_864 = None
    permute_1396 = torch.ops.aten.permute.default(unsqueeze_865, [3, 0, 4, 1, 5, 2]);  unsqueeze_865 = None
    unsqueeze_866 = torch.ops.aten.unsqueeze.default(_to_copy_1446, 4);  _to_copy_1446 = None
    unsqueeze_867 = torch.ops.aten.unsqueeze.default(unsqueeze_866, 5);  unsqueeze_866 = None
    permute_1397 = torch.ops.aten.permute.default(unsqueeze_867, [1, 4, 2, 5, 3, 0]);  unsqueeze_867 = None
    permute_1398 = torch.ops.aten.permute.default(permute_1396, [3, 5, 0, 1, 2, 4]);  permute_1396 = None
    view_2580 = torch.ops.aten.view.default(permute_1398, [1, 384, 384]);  permute_1398 = None
    permute_1399 = torch.ops.aten.permute.default(permute_1397, [5, 0, 1, 2, 4, 3]);  permute_1397 = None
    view_2581 = torch.ops.aten.view.default(permute_1399, [1, 384, 1536]);  permute_1399 = None
    bmm_210 = torch.ops.aten.bmm.default(view_2580, view_2581);  view_2580 = view_2581 = None
    view_2582 = torch.ops.aten.view.default(bmm_210, [384, 1, 4, 1, 16, 24]);  bmm_210 = None
    permute_1400 = torch.ops.aten.permute.default(view_2582, [2, 3, 4, 0, 5, 1]);  view_2582 = None
    view_2583 = torch.ops.aten.view.default(permute_1400, [4, 1, 16, 384, 24]);  permute_1400 = None
    unbind_int_117 = torch.ops.aten.unbind.int(view_2583);  view_2583 = None
    getitem_2486 = unbind_int_117[0]
    getitem_2487 = unbind_int_117[1]
    getitem_2488 = unbind_int_117[2]
    getitem_2489 = unbind_int_117[3];  unbind_int_117 = None
    view_2584 = torch.ops.aten.view.default(pairformer_stack_blocks_27_attention_pair_bias_attention_query_bias, [1, 16, 1, 24]);  pairformer_stack_blocks_27_attention_pair_bias_attention_query_bias = None
    add_276 = torch.ops.aten.add.Tensor(getitem_2486, view_2584);  getitem_2486 = view_2584 = None
    _to_copy_1447 = torch.ops.aten._to_copy.default(add_276, dtype = torch.bfloat16);  add_276 = None
    expand_166 = torch.ops.aten.expand.default(masked_fill_157, [1, 16, 384, 384]);  masked_fill_157 = None
    _scaled_dot_product_efficient_attention_default_95 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_1447, getitem_2487, getitem_2488, expand_166, False);  _to_copy_1447 = getitem_2487 = getitem_2488 = expand_166 = None
    getitem_2490 = _scaled_dot_product_efficient_attention_default_95[0];  _scaled_dot_product_efficient_attention_default_95 = None
    add_277 = torch.ops.aten.add.Tensor(getitem_2489, 1);  getitem_2489 = None
    sigmoid_203 = torch.ops.aten.sigmoid.default(add_277);  add_277 = None
    mul_335 = torch.ops.aten.mul.Tensor(getitem_2490, sigmoid_203);  getitem_2490 = sigmoid_203 = None
    _to_copy_1448 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_27_attention_pair_bias_attention_output_proj_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_27_attention_pair_bias_attention_output_proj_weight = None
    unsqueeze_868 = torch.ops.aten.unsqueeze.default(mul_335, 4);  mul_335 = None
    permute_1401 = torch.ops.aten.permute.default(unsqueeze_868, [0, 2, 4, 3, 1]);  unsqueeze_868 = None
    unsqueeze_869 = torch.ops.aten.unsqueeze.default(_to_copy_1448, 3);  _to_copy_1448 = None
    unsqueeze_870 = torch.ops.aten.unsqueeze.default(unsqueeze_869, 4);  unsqueeze_869 = None
    permute_1402 = torch.ops.aten.permute.default(unsqueeze_870, [3, 4, 2, 1, 0]);  unsqueeze_870 = None
    permute_1403 = torch.ops.aten.permute.default(permute_1401, [1, 3, 4, 0, 2]);  permute_1401 = None
    clone_227 = torch.ops.aten.clone.default(permute_1403, memory_format = torch.contiguous_format);  permute_1403 = None
    _unsafe_view_192 = torch.ops.aten._unsafe_view.default(clone_227, [1, 384, 384]);  clone_227 = None
    permute_1404 = torch.ops.aten.permute.default(permute_1402, [3, 4, 0, 2, 1]);  permute_1402 = None
    clone_228 = torch.ops.aten.clone.default(permute_1404, memory_format = torch.contiguous_format);  permute_1404 = None
    _unsafe_view_193 = torch.ops.aten._unsafe_view.default(clone_228, [1, 384, 384]);  clone_228 = None
    bmm_211 = torch.ops.aten.bmm.default(_unsafe_view_192, _unsafe_view_193);  _unsafe_view_192 = _unsafe_view_193 = None
    view_2585 = torch.ops.aten.view.default(bmm_211, [384, 1, 1, 1, 384]);  bmm_211 = None
    permute_1405 = torch.ops.aten.permute.default(view_2585, [3, 0, 4, 1, 2]);  view_2585 = None
    view_2586 = torch.ops.aten.view.default(permute_1405, [1, 384, 384]);  permute_1405 = None
    unsqueeze_871 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
    mul_336 = torch.ops.aten.mul.Tensor(view_2586, unsqueeze_871);  view_2586 = unsqueeze_871 = None
    add_278 = torch.ops.aten.add.Tensor(add_271, mul_336);  mul_336 = None
    split_tensor_270 = torch.ops.aten.split.Tensor(add_271, 384, dim = -2);  add_271 = None
    getitem_2494 = split_tensor_270[0];  split_tensor_270 = None
    _to_copy_1449 = torch.ops.aten._to_copy.default(getitem_2494, dtype = torch.float32);  getitem_2494 = None
    native_layer_norm_default_298 = torch.ops.aten.native_layer_norm.default(_to_copy_1449, [384], pairformer_stack_blocks_27_transition_single_layer_norm_weight, pairformer_stack_blocks_27_transition_single_layer_norm_bias, 1e-05);  _to_copy_1449 = pairformer_stack_blocks_27_transition_single_layer_norm_weight = pairformer_stack_blocks_27_transition_single_layer_norm_bias = None
    getitem_2495 = native_layer_norm_default_298[0];  native_layer_norm_default_298 = None
    _to_copy_1450 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_27_transition_single_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_27_transition_single_linear_no_bias_ab_weight = None
    _to_copy_1451 = torch.ops.aten._to_copy.default(getitem_2495, dtype = torch.bfloat16);  getitem_2495 = None
    t_535 = torch.ops.aten.t.default(_to_copy_1450);  _to_copy_1450 = None
    view_2587 = torch.ops.aten.view.default(_to_copy_1451, [384, 384]);  _to_copy_1451 = None
    mm_497 = torch.ops.aten.mm.default(view_2587, t_535);  view_2587 = t_535 = None
    view_2588 = torch.ops.aten.view.default(mm_497, [1, 384, 1536]);  mm_497 = None
    split_tensor_271 = torch.ops.aten.split.Tensor(view_2588, 768, dim = -1);  view_2588 = None
    getitem_2498 = split_tensor_271[0]
    getitem_2499 = split_tensor_271[1];  split_tensor_271 = None
    silu_70 = torch.ops.aten.silu.default(getitem_2498);  getitem_2498 = None
    mul_337 = torch.ops.aten.mul.Tensor(silu_70, getitem_2499);  silu_70 = getitem_2499 = None
    _to_copy_1452 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_27_transition_single_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_27_transition_single_linear_out_weight = None
    t_536 = torch.ops.aten.t.default(_to_copy_1452);  _to_copy_1452 = None
    view_2590 = torch.ops.aten.view.default(mul_337, [384, 768]);  mul_337 = None
    mm_498 = torch.ops.aten.mm.default(view_2590, t_536);  view_2590 = t_536 = None
    view_2591 = torch.ops.aten.view.default(mm_498, [1, 384, 384]);  mm_498 = None
    add_279 = torch.ops.aten.add.Tensor(add_278, view_2591);  add_278 = view_2591 = None
    _to_copy_1453 = torch.ops.aten._to_copy.default(add_275, dtype = torch.float32)
    native_layer_norm_default_299 = torch.ops.aten.native_layer_norm.default(_to_copy_1453, [256], pairformer_stack_blocks_28_triangle_multiplication_layernorm_z_in_weight, pairformer_stack_blocks_28_triangle_multiplication_layernorm_z_in_bias, 1e-05);  _to_copy_1453 = pairformer_stack_blocks_28_triangle_multiplication_layernorm_z_in_weight = pairformer_stack_blocks_28_triangle_multiplication_layernorm_z_in_bias = None
    getitem_2500 = native_layer_norm_default_299[0];  native_layer_norm_default_299 = None
    split_with_sizes_default_68 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_28_triangle_multiplication_merged_linear_p_weight, [512, 512]);  pairformer_stack_blocks_28_triangle_multiplication_merged_linear_p_weight = None
    getitem_2503 = split_with_sizes_default_68[0]
    getitem_2504 = split_with_sizes_default_68[1];  split_with_sizes_default_68 = None
    split_with_sizes_default_69 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_28_triangle_multiplication_merged_linear_g_weight, [512, 512, 256]);  pairformer_stack_blocks_28_triangle_multiplication_merged_linear_g_weight = None
    getitem_2505 = split_with_sizes_default_69[0]
    getitem_2506 = split_with_sizes_default_69[1]
    getitem_2507 = split_with_sizes_default_69[2];  split_with_sizes_default_69 = None
    _to_copy_1454 = torch.ops.aten._to_copy.default(getitem_2503, dtype = torch.bfloat16);  getitem_2503 = None
    _to_copy_1455 = torch.ops.aten._to_copy.default(getitem_2500, dtype = torch.bfloat16)
    t_537 = torch.ops.aten.t.default(_to_copy_1454);  _to_copy_1454 = None
    view_2592 = torch.ops.aten.view.default(_to_copy_1455, [147456, 256]);  _to_copy_1455 = None
    mm_499 = torch.ops.aten.mm.default(view_2592, t_537);  view_2592 = t_537 = None
    view_2593 = torch.ops.aten.view.default(mm_499, [1, 384, 384, 512]);  mm_499 = None
    _to_copy_1456 = torch.ops.aten._to_copy.default(getitem_2505, dtype = torch.bfloat16);  getitem_2505 = None
    _to_copy_1457 = torch.ops.aten._to_copy.default(getitem_2500, dtype = torch.bfloat16)
    t_538 = torch.ops.aten.t.default(_to_copy_1456);  _to_copy_1456 = None
    view_2594 = torch.ops.aten.view.default(_to_copy_1457, [147456, 256]);  _to_copy_1457 = None
    mm_500 = torch.ops.aten.mm.default(view_2594, t_538);  view_2594 = t_538 = None
    view_2595 = torch.ops.aten.view.default(mm_500, [1, 384, 384, 512]);  mm_500 = None
    sigmoid_204 = torch.ops.aten.sigmoid.default(view_2595);  view_2595 = None
    mul_338 = torch.ops.aten.mul.Tensor(view_2593, sigmoid_204);  view_2593 = sigmoid_204 = None
    unsqueeze_872 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
    bitwise_not_158 = torch.ops.aten.bitwise_not.default(unsqueeze_872);  unsqueeze_872 = None
    masked_fill_158 = torch.ops.aten.masked_fill.Scalar(mul_338, bitwise_not_158, 0);  mul_338 = bitwise_not_158 = None
    split_tensor_272 = torch.ops.aten.split.Tensor(masked_fill_158, 256, dim = -1)
    getitem_2510 = split_tensor_272[0];  split_tensor_272 = None
    unsqueeze_875 = torch.ops.aten.unsqueeze.default(getitem_2510, 4);  getitem_2510 = None
    permute_1410 = torch.ops.aten.permute.default(unsqueeze_875, [0, 1, 4, 3, 2]);  unsqueeze_875 = None
    permute_1411 = torch.ops.aten.permute.default(permute_1410, [3, 1, 4, 0, 2]);  permute_1410 = None
    view_2598 = torch.ops.aten.view.default(permute_1411, [256, 384, 384]);  permute_1411 = None
    split_tensor_273 = torch.ops.aten.split.Tensor(masked_fill_158, 256, dim = -1);  masked_fill_158 = None
    getitem_2513 = split_tensor_273[1];  split_tensor_273 = None
    unsqueeze_876 = torch.ops.aten.unsqueeze.default(getitem_2513, 4);  getitem_2513 = None
    permute_1412 = torch.ops.aten.permute.default(unsqueeze_876, [0, 4, 1, 3, 2]);  unsqueeze_876 = None
    permute_1413 = torch.ops.aten.permute.default(permute_1412, [3, 4, 0, 2, 1]);  permute_1412 = None
    view_2599 = torch.ops.aten.view.default(permute_1413, [256, 384, 384]);  permute_1413 = None
    bmm_212 = torch.ops.aten.bmm.default(view_2598, view_2599);  view_2598 = view_2599 = None
    view_2600 = torch.ops.aten.view.default(bmm_212, [256, 384, 1, 1, 384]);  bmm_212 = None
    permute_1414 = torch.ops.aten.permute.default(view_2600, [3, 1, 4, 0, 2]);  view_2600 = None
    view_2601 = torch.ops.aten.view.default(permute_1414, [1, 384, 384, 256]);  permute_1414 = None
    _to_copy_1458 = torch.ops.aten._to_copy.default(getitem_2504, dtype = torch.bfloat16);  getitem_2504 = None
    _to_copy_1459 = torch.ops.aten._to_copy.default(getitem_2500, dtype = torch.bfloat16)
    t_539 = torch.ops.aten.t.default(_to_copy_1458);  _to_copy_1458 = None
    view_2602 = torch.ops.aten.view.default(_to_copy_1459, [147456, 256]);  _to_copy_1459 = None
    mm_501 = torch.ops.aten.mm.default(view_2602, t_539);  view_2602 = t_539 = None
    view_2603 = torch.ops.aten.view.default(mm_501, [1, 384, 384, 512]);  mm_501 = None
    _to_copy_1460 = torch.ops.aten._to_copy.default(getitem_2506, dtype = torch.bfloat16);  getitem_2506 = None
    _to_copy_1461 = torch.ops.aten._to_copy.default(getitem_2500, dtype = torch.bfloat16)
    t_540 = torch.ops.aten.t.default(_to_copy_1460);  _to_copy_1460 = None
    view_2604 = torch.ops.aten.view.default(_to_copy_1461, [147456, 256]);  _to_copy_1461 = None
    mm_502 = torch.ops.aten.mm.default(view_2604, t_540);  view_2604 = t_540 = None
    view_2605 = torch.ops.aten.view.default(mm_502, [1, 384, 384, 512]);  mm_502 = None
    sigmoid_205 = torch.ops.aten.sigmoid.default(view_2605);  view_2605 = None
    mul_339 = torch.ops.aten.mul.Tensor(view_2603, sigmoid_205);  view_2603 = sigmoid_205 = None
    view_2606 = torch.ops.aten.view.default(mul_339, [147456, 512]);  mul_339 = None
    view_2607 = torch.ops.aten.view.default(view_2606, [1, 384, 384, 512]);  view_2606 = None
    transpose_68 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
    unsqueeze_877 = torch.ops.aten.unsqueeze.default(transpose_68, 3);  transpose_68 = None
    clone_229 = torch.ops.aten.clone.default(unsqueeze_877, memory_format = torch.contiguous_format);  unsqueeze_877 = None
    bitwise_not_159 = torch.ops.aten.bitwise_not.default(clone_229);  clone_229 = None
    masked_fill_159 = torch.ops.aten.masked_fill.Scalar(view_2607, bitwise_not_159, 0);  view_2607 = bitwise_not_159 = None
    view_2608 = torch.ops.aten.view.default(masked_fill_159, [147456, 512]);  masked_fill_159 = None
    view_2612 = torch.ops.aten.view.default(view_2608, [1, 384, 384, 512])
    split_tensor_274 = torch.ops.aten.split.Tensor(view_2612, 256, dim = -1);  view_2612 = None
    getitem_2516 = split_tensor_274[0];  split_tensor_274 = None
    unsqueeze_880 = torch.ops.aten.unsqueeze.default(getitem_2516, 4);  getitem_2516 = None
    permute_1419 = torch.ops.aten.permute.default(unsqueeze_880, [0, 2, 4, 3, 1]);  unsqueeze_880 = None
    permute_1420 = torch.ops.aten.permute.default(permute_1419, [3, 1, 4, 0, 2]);  permute_1419 = None
    view_2613 = torch.ops.aten.view.default(permute_1420, [256, 384, 384]);  permute_1420 = None
    view_2614 = torch.ops.aten.view.default(view_2608, [1, 384, 384, 512]);  view_2608 = None
    split_tensor_275 = torch.ops.aten.split.Tensor(view_2614, 256, dim = -1);  view_2614 = None
    getitem_2519 = split_tensor_275[1];  split_tensor_275 = None
    unsqueeze_881 = torch.ops.aten.unsqueeze.default(getitem_2519, 4);  getitem_2519 = None
    permute_1421 = torch.ops.aten.permute.default(unsqueeze_881, [0, 4, 2, 3, 1]);  unsqueeze_881 = None
    permute_1422 = torch.ops.aten.permute.default(permute_1421, [3, 4, 0, 2, 1]);  permute_1421 = None
    view_2615 = torch.ops.aten.view.default(permute_1422, [256, 384, 384]);  permute_1422 = None
    bmm_213 = torch.ops.aten.bmm.default(view_2613, view_2615);  view_2613 = view_2615 = None
    view_2616 = torch.ops.aten.view.default(bmm_213, [256, 384, 1, 1, 384]);  bmm_213 = None
    permute_1423 = torch.ops.aten.permute.default(view_2616, [3, 1, 4, 0, 2]);  view_2616 = None
    view_2617 = torch.ops.aten.view.default(permute_1423, [1, 384, 384, 256]);  permute_1423 = None
    _to_copy_1462 = torch.ops.aten._to_copy.default(view_2601, dtype = torch.float32);  view_2601 = None
    native_layer_norm_default_300 = torch.ops.aten.native_layer_norm.default(_to_copy_1462, [256], None, None, 1e-05);  _to_copy_1462 = None
    getitem_2520 = native_layer_norm_default_300[0];  native_layer_norm_default_300 = None
    _to_copy_1463 = torch.ops.aten._to_copy.default(view_2617, dtype = torch.float32);  view_2617 = None
    native_layer_norm_default_301 = torch.ops.aten.native_layer_norm.default(_to_copy_1463, [256], None, None, 1e-05);  _to_copy_1463 = None
    getitem_2523 = native_layer_norm_default_301[0];  native_layer_norm_default_301 = None
    add_280 = torch.ops.aten.add.Tensor(getitem_2520, getitem_2523);  getitem_2520 = getitem_2523 = None
    _to_copy_1464 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_28_triangle_multiplication_linear_z_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_28_triangle_multiplication_linear_z_out_weight = None
    _to_copy_1465 = torch.ops.aten._to_copy.default(add_280, dtype = torch.bfloat16);  add_280 = None
    t_541 = torch.ops.aten.t.default(_to_copy_1464);  _to_copy_1464 = None
    view_2618 = torch.ops.aten.view.default(_to_copy_1465, [147456, 256]);  _to_copy_1465 = None
    mm_503 = torch.ops.aten.mm.default(view_2618, t_541);  view_2618 = t_541 = None
    view_2619 = torch.ops.aten.view.default(mm_503, [1, 384, 384, 256]);  mm_503 = None
    _to_copy_1466 = torch.ops.aten._to_copy.default(getitem_2507, dtype = torch.bfloat16);  getitem_2507 = None
    _to_copy_1467 = torch.ops.aten._to_copy.default(getitem_2500, dtype = torch.bfloat16);  getitem_2500 = None
    t_542 = torch.ops.aten.t.default(_to_copy_1466);  _to_copy_1466 = None
    view_2620 = torch.ops.aten.view.default(_to_copy_1467, [147456, 256]);  _to_copy_1467 = None
    mm_504 = torch.ops.aten.mm.default(view_2620, t_542);  view_2620 = t_542 = None
    view_2621 = torch.ops.aten.view.default(mm_504, [1, 384, 384, 256]);  mm_504 = None
    sigmoid_206 = torch.ops.aten.sigmoid.default(view_2621);  view_2621 = None
    mul_340 = torch.ops.aten.mul.Tensor(view_2619, sigmoid_206);  view_2619 = sigmoid_206 = None
    add_281 = torch.ops.aten.add.Tensor(add_275, mul_340);  mul_340 = None
    _to_copy_1468 = torch.ops.aten._to_copy.default(add_275, dtype = torch.float32)
    native_layer_norm_default_302 = torch.ops.aten.native_layer_norm.default(_to_copy_1468, [256], None, None, 1e-05);  _to_copy_1468 = None
    getitem_2526 = native_layer_norm_default_302[0];  native_layer_norm_default_302 = None
    _to_copy_1469 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_28_triangle_attention_pair2b_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_28_triangle_attention_pair2b_weight = None
    _to_copy_1470 = torch.ops.aten._to_copy.default(getitem_2526, dtype = torch.bfloat16)
    t_543 = torch.ops.aten.t.default(_to_copy_1469);  _to_copy_1469 = None
    view_2622 = torch.ops.aten.view.default(_to_copy_1470, [147456, 256]);  _to_copy_1470 = None
    mm_505 = torch.ops.aten.mm.default(view_2622, t_543);  view_2622 = t_543 = None
    view_2623 = torch.ops.aten.view.default(mm_505, [1, 384, 384, 8]);  mm_505 = None
    view_2624 = torch.ops.aten.view.default(view_2623, [1, 384, 384, 2, 4]);  view_2623 = None
    permute_1424 = torch.ops.aten.permute.default(view_2624, [0, 3, 4, 1, 2]);  view_2624 = None
    view_2625 = torch.ops.aten.view.default(permute_1424, [1, 2, 4, 1, 384, 384]);  permute_1424 = None
    view_2626 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 384, 384])
    bitwise_not_160 = torch.ops.aten.bitwise_not.default(view_2626);  view_2626 = None
    masked_fill_160 = torch.ops.aten.masked_fill.Scalar(view_2625, bitwise_not_160, -10000);  view_2625 = bitwise_not_160 = None
    view_2627 = torch.ops.aten.view.default(masked_fill_160, [1, 2, 4, 384, 384]);  masked_fill_160 = None
    permute_1425 = torch.ops.aten.permute.default(view_2627, [1, 0, 2, 3, 4]);  view_2627 = None
    view_2628 = torch.ops.aten.view.default(permute_1425, [2, 4, 1, 384, 384]);  permute_1425 = None
    _to_copy_1471 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_28_triangle_attention_pair2qkvg1_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_28_triangle_attention_pair2qkvg1_weight = None
    _to_copy_1472 = torch.ops.aten._to_copy.default(getitem_2526, dtype = torch.bfloat16)
    t_544 = torch.ops.aten.t.default(_to_copy_1471);  _to_copy_1471 = None
    view_2629 = torch.ops.aten.view.default(_to_copy_1472, [147456, 256]);  _to_copy_1472 = None
    mm_506 = torch.ops.aten.mm.default(view_2629, t_544);  view_2629 = t_544 = None
    view_2630 = torch.ops.aten.view.default(mm_506, [1, 384, 384, 1024]);  mm_506 = None
    select_69 = torch.ops.aten.select.int(view_2628, 0, 0)
    view_2631 = torch.ops.aten.view.default(view_2630, [1, 384, 384, 4, 4, 64]);  view_2630 = None
    permute_1426 = torch.ops.aten.permute.default(view_2631, [4, 0, 3, 1, 2, 5]);  view_2631 = None
    view_2632 = torch.ops.aten.view.default(permute_1426, [4, 4, 384, 384, 64]);  permute_1426 = None
    unbind_int_118 = torch.ops.aten.unbind.int(view_2632);  view_2632 = None
    getitem_2529 = unbind_int_118[0]
    getitem_2530 = unbind_int_118[1]
    getitem_2531 = unbind_int_118[2]
    getitem_2532 = unbind_int_118[3];  unbind_int_118 = None
    expand_167 = torch.ops.aten.expand.default(select_69, [4, 384, 384, 384]);  select_69 = None
    _scaled_dot_product_efficient_attention_default_96 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2529, getitem_2530, getitem_2531, expand_167, False);  getitem_2529 = getitem_2530 = getitem_2531 = expand_167 = None
    getitem_2533 = _scaled_dot_product_efficient_attention_default_96[0];  _scaled_dot_product_efficient_attention_default_96 = None
    sigmoid_207 = torch.ops.aten.sigmoid.default(getitem_2532);  getitem_2532 = None
    mul_341 = torch.ops.aten.mul.Tensor(getitem_2533, sigmoid_207);  getitem_2533 = sigmoid_207 = None
    view_2633 = torch.ops.aten.view.default(mul_341, [1, 4, 384, 384, 64]);  mul_341 = None
    permute_1427 = torch.ops.aten.permute.default(view_2633, [0, 2, 3, 1, 4]);  view_2633 = None
    clone_230 = torch.ops.aten.clone.default(permute_1427, memory_format = torch.contiguous_format);  permute_1427 = None
    _unsafe_view_194 = torch.ops.aten._unsafe_view.default(clone_230, [1, 384, 384, 256]);  clone_230 = None
    transpose_69 = torch.ops.aten.transpose.int(getitem_2526, 1, 2);  getitem_2526 = None
    _to_copy_1473 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_28_triangle_attention_pair2qkvg2_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_28_triangle_attention_pair2qkvg2_weight = None
    _to_copy_1474 = torch.ops.aten._to_copy.default(transpose_69, dtype = torch.bfloat16);  transpose_69 = None
    t_545 = torch.ops.aten.t.default(_to_copy_1473);  _to_copy_1473 = None
    expand_168 = torch.ops.aten.expand.default(_to_copy_1474, [1, 384, 384, 256]);  _to_copy_1474 = None
    view_2634 = torch.ops.aten.view.default(expand_168, [384, 384, 256]);  expand_168 = None
    expand_169 = torch.ops.aten.expand.default(t_545, [1, 384, 256, 1024]);  t_545 = None
    view_2635 = torch.ops.aten.view.default(expand_169, [384, 256, 1024]);  expand_169 = None
    bmm_214 = torch.ops.aten.bmm.default(view_2634, view_2635);  view_2634 = view_2635 = None
    view_2636 = torch.ops.aten.view.default(bmm_214, [1, 384, 384, 1024]);  bmm_214 = None
    select_70 = torch.ops.aten.select.int(view_2628, 0, 1);  view_2628 = None
    view_2637 = torch.ops.aten.view.default(view_2636, [1, 384, 384, 4, 4, 64]);  view_2636 = None
    permute_1428 = torch.ops.aten.permute.default(view_2637, [4, 0, 3, 1, 2, 5]);  view_2637 = None
    view_2638 = torch.ops.aten.view.default(permute_1428, [4, 4, 384, 384, 64]);  permute_1428 = None
    unbind_int_119 = torch.ops.aten.unbind.int(view_2638);  view_2638 = None
    getitem_2537 = unbind_int_119[0]
    getitem_2538 = unbind_int_119[1]
    getitem_2539 = unbind_int_119[2]
    getitem_2540 = unbind_int_119[3];  unbind_int_119 = None
    expand_170 = torch.ops.aten.expand.default(select_70, [4, 384, 384, 384]);  select_70 = None
    _scaled_dot_product_efficient_attention_default_97 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2537, getitem_2538, getitem_2539, expand_170, False);  getitem_2537 = getitem_2538 = getitem_2539 = expand_170 = None
    getitem_2541 = _scaled_dot_product_efficient_attention_default_97[0];  _scaled_dot_product_efficient_attention_default_97 = None
    sigmoid_208 = torch.ops.aten.sigmoid.default(getitem_2540);  getitem_2540 = None
    mul_342 = torch.ops.aten.mul.Tensor(getitem_2541, sigmoid_208);  getitem_2541 = sigmoid_208 = None
    view_2639 = torch.ops.aten.view.default(mul_342, [1, 4, 384, 384, 64]);  mul_342 = None
    permute_1429 = torch.ops.aten.permute.default(view_2639, [0, 2, 3, 1, 4]);  view_2639 = None
    clone_231 = torch.ops.aten.clone.default(permute_1429, memory_format = torch.contiguous_format);  permute_1429 = None
    _unsafe_view_195 = torch.ops.aten._unsafe_view.default(clone_231, [1, 384, 384, 256]);  clone_231 = None
    cat_40 = torch.ops.aten.cat.default([_unsafe_view_194, _unsafe_view_195], dim = -1);  _unsafe_view_194 = _unsafe_view_195 = None
    slice_191 = torch.ops.aten.slice.Tensor(pairformer_stack_blocks_28_triangle_attention_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  pairformer_stack_blocks_28_triangle_attention_out_scalers = None
    unsqueeze_882 = torch.ops.aten.unsqueeze.default(slice_191, 1);  slice_191 = None
    mul_343 = torch.ops.aten.mul.Tensor(pairformer_stack_blocks_28_triangle_attention_linear_out_weight, unsqueeze_882);  pairformer_stack_blocks_28_triangle_attention_linear_out_weight = unsqueeze_882 = None
    _to_copy_1475 = torch.ops.aten._to_copy.default(mul_343, dtype = torch.bfloat16);  mul_343 = None
    t_546 = torch.ops.aten.t.default(_to_copy_1475);  _to_copy_1475 = None
    view_2640 = torch.ops.aten.view.default(cat_40, [147456, 512]);  cat_40 = None
    mm_507 = torch.ops.aten.mm.default(view_2640, t_546);  view_2640 = t_546 = None
    view_2641 = torch.ops.aten.view.default(mm_507, [1, 384, 384, 256]);  mm_507 = None
    add_282 = torch.ops.aten.add.Tensor(add_281, view_2641);  add_281 = view_2641 = None
    split_tensor_276 = torch.ops.aten.split.Tensor(add_275, 384, dim = -2)
    getitem_2545 = split_tensor_276[0];  split_tensor_276 = None
    _to_copy_1476 = torch.ops.aten._to_copy.default(getitem_2545, dtype = torch.float32);  getitem_2545 = None
    native_layer_norm_default_303 = torch.ops.aten.native_layer_norm.default(_to_copy_1476, [256], pairformer_stack_blocks_28_transition_pair_layer_norm_weight, pairformer_stack_blocks_28_transition_pair_layer_norm_bias, 1e-05);  _to_copy_1476 = pairformer_stack_blocks_28_transition_pair_layer_norm_weight = pairformer_stack_blocks_28_transition_pair_layer_norm_bias = None
    getitem_2546 = native_layer_norm_default_303[0];  native_layer_norm_default_303 = None
    _to_copy_1477 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_28_transition_pair_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_28_transition_pair_linear_no_bias_ab_weight = None
    _to_copy_1478 = torch.ops.aten._to_copy.default(getitem_2546, dtype = torch.bfloat16);  getitem_2546 = None
    t_547 = torch.ops.aten.t.default(_to_copy_1477);  _to_copy_1477 = None
    view_2642 = torch.ops.aten.view.default(_to_copy_1478, [147456, 256]);  _to_copy_1478 = None
    mm_508 = torch.ops.aten.mm.default(view_2642, t_547);  view_2642 = t_547 = None
    view_2643 = torch.ops.aten.view.default(mm_508, [1, 384, 384, 1024]);  mm_508 = None
    split_tensor_277 = torch.ops.aten.split.Tensor(view_2643, 512, dim = -1);  view_2643 = None
    getitem_2549 = split_tensor_277[0]
    getitem_2550 = split_tensor_277[1];  split_tensor_277 = None
    silu_71 = torch.ops.aten.silu.default(getitem_2549);  getitem_2549 = None
    mul_344 = torch.ops.aten.mul.Tensor(silu_71, getitem_2550);  silu_71 = getitem_2550 = None
    _to_copy_1479 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_28_transition_pair_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_28_transition_pair_linear_out_weight = None
    t_548 = torch.ops.aten.t.default(_to_copy_1479);  _to_copy_1479 = None
    view_2645 = torch.ops.aten.view.default(mul_344, [147456, 512]);  mul_344 = None
    mm_509 = torch.ops.aten.mm.default(view_2645, t_548);  view_2645 = t_548 = None
    view_2646 = torch.ops.aten.view.default(mm_509, [1, 384, 384, 256]);  mm_509 = None
    add_283 = torch.ops.aten.add.Tensor(add_282, view_2646);  add_282 = view_2646 = None
    _to_copy_1480 = torch.ops.aten._to_copy.default(add_279, dtype = torch.float32)
    native_layer_norm_default_304 = torch.ops.aten.native_layer_norm.default(_to_copy_1480, [384], pairformer_stack_blocks_28_attention_pair_bias_single_layer_norm_weight, pairformer_stack_blocks_28_attention_pair_bias_single_layer_norm_bias, 1e-05);  _to_copy_1480 = pairformer_stack_blocks_28_attention_pair_bias_single_layer_norm_weight = pairformer_stack_blocks_28_attention_pair_bias_single_layer_norm_bias = None
    getitem_2551 = native_layer_norm_default_304[0];  native_layer_norm_default_304 = None
    _to_copy_1481 = torch.ops.aten._to_copy.default(add_275, dtype = torch.float32);  add_275 = None
    native_layer_norm_default_305 = torch.ops.aten.native_layer_norm.default(_to_copy_1481, [256], pairformer_stack_blocks_28_attention_pair_bias_pair_layer_norm_weight, pairformer_stack_blocks_28_attention_pair_bias_pair_layer_norm_bias, 1e-05);  _to_copy_1481 = pairformer_stack_blocks_28_attention_pair_bias_pair_layer_norm_weight = pairformer_stack_blocks_28_attention_pair_bias_pair_layer_norm_bias = None
    getitem_2554 = native_layer_norm_default_305[0];  native_layer_norm_default_305 = None
    _to_copy_1482 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_28_attention_pair_bias_pair_linear_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_28_attention_pair_bias_pair_linear_weight = None
    _to_copy_1483 = torch.ops.aten._to_copy.default(getitem_2554, dtype = torch.bfloat16);  getitem_2554 = None
    t_549 = torch.ops.aten.t.default(_to_copy_1482);  _to_copy_1482 = None
    view_2647 = torch.ops.aten.view.default(_to_copy_1483, [147456, 256]);  _to_copy_1483 = None
    mm_510 = torch.ops.aten.mm.default(view_2647, t_549);  view_2647 = t_549 = None
    view_2648 = torch.ops.aten.view.default(mm_510, [1, 384, 384, 16]);  mm_510 = None
    permute_1430 = torch.ops.aten.permute.default(view_2648, [0, 3, 1, 2]);  view_2648 = None
    view_2649 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384])
    bitwise_not_161 = torch.ops.aten.bitwise_not.default(view_2649);  view_2649 = None
    masked_fill_161 = torch.ops.aten.masked_fill.Scalar(permute_1430, bitwise_not_161, -10000);  permute_1430 = bitwise_not_161 = None
    _to_copy_1484 = torch.ops.aten._to_copy.default(getitem_2551, dtype = torch.bfloat16);  getitem_2551 = None
    _to_copy_1485 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_28_attention_pair_bias_attention_input2qkvg_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_28_attention_pair_bias_attention_input2qkvg_weight = None
    unsqueeze_883 = torch.ops.aten.unsqueeze.default(_to_copy_1484, 3);  _to_copy_1484 = None
    unsqueeze_884 = torch.ops.aten.unsqueeze.default(unsqueeze_883, 4);  unsqueeze_883 = None
    unsqueeze_885 = torch.ops.aten.unsqueeze.default(unsqueeze_884, 5);  unsqueeze_884 = None
    permute_1431 = torch.ops.aten.permute.default(unsqueeze_885, [3, 0, 4, 1, 5, 2]);  unsqueeze_885 = None
    unsqueeze_886 = torch.ops.aten.unsqueeze.default(_to_copy_1485, 4);  _to_copy_1485 = None
    unsqueeze_887 = torch.ops.aten.unsqueeze.default(unsqueeze_886, 5);  unsqueeze_886 = None
    permute_1432 = torch.ops.aten.permute.default(unsqueeze_887, [1, 4, 2, 5, 3, 0]);  unsqueeze_887 = None
    permute_1433 = torch.ops.aten.permute.default(permute_1431, [3, 5, 0, 1, 2, 4]);  permute_1431 = None
    view_2650 = torch.ops.aten.view.default(permute_1433, [1, 384, 384]);  permute_1433 = None
    permute_1434 = torch.ops.aten.permute.default(permute_1432, [5, 0, 1, 2, 4, 3]);  permute_1432 = None
    view_2651 = torch.ops.aten.view.default(permute_1434, [1, 384, 1536]);  permute_1434 = None
    bmm_215 = torch.ops.aten.bmm.default(view_2650, view_2651);  view_2650 = view_2651 = None
    view_2652 = torch.ops.aten.view.default(bmm_215, [384, 1, 4, 1, 16, 24]);  bmm_215 = None
    permute_1435 = torch.ops.aten.permute.default(view_2652, [2, 3, 4, 0, 5, 1]);  view_2652 = None
    view_2653 = torch.ops.aten.view.default(permute_1435, [4, 1, 16, 384, 24]);  permute_1435 = None
    unbind_int_120 = torch.ops.aten.unbind.int(view_2653);  view_2653 = None
    getitem_2557 = unbind_int_120[0]
    getitem_2558 = unbind_int_120[1]
    getitem_2559 = unbind_int_120[2]
    getitem_2560 = unbind_int_120[3];  unbind_int_120 = None
    view_2654 = torch.ops.aten.view.default(pairformer_stack_blocks_28_attention_pair_bias_attention_query_bias, [1, 16, 1, 24]);  pairformer_stack_blocks_28_attention_pair_bias_attention_query_bias = None
    add_284 = torch.ops.aten.add.Tensor(getitem_2557, view_2654);  getitem_2557 = view_2654 = None
    _to_copy_1486 = torch.ops.aten._to_copy.default(add_284, dtype = torch.bfloat16);  add_284 = None
    expand_171 = torch.ops.aten.expand.default(masked_fill_161, [1, 16, 384, 384]);  masked_fill_161 = None
    _scaled_dot_product_efficient_attention_default_98 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_1486, getitem_2558, getitem_2559, expand_171, False);  _to_copy_1486 = getitem_2558 = getitem_2559 = expand_171 = None
    getitem_2561 = _scaled_dot_product_efficient_attention_default_98[0];  _scaled_dot_product_efficient_attention_default_98 = None
    add_285 = torch.ops.aten.add.Tensor(getitem_2560, 1);  getitem_2560 = None
    sigmoid_209 = torch.ops.aten.sigmoid.default(add_285);  add_285 = None
    mul_345 = torch.ops.aten.mul.Tensor(getitem_2561, sigmoid_209);  getitem_2561 = sigmoid_209 = None
    _to_copy_1487 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_28_attention_pair_bias_attention_output_proj_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_28_attention_pair_bias_attention_output_proj_weight = None
    unsqueeze_888 = torch.ops.aten.unsqueeze.default(mul_345, 4);  mul_345 = None
    permute_1436 = torch.ops.aten.permute.default(unsqueeze_888, [0, 2, 4, 3, 1]);  unsqueeze_888 = None
    unsqueeze_889 = torch.ops.aten.unsqueeze.default(_to_copy_1487, 3);  _to_copy_1487 = None
    unsqueeze_890 = torch.ops.aten.unsqueeze.default(unsqueeze_889, 4);  unsqueeze_889 = None
    permute_1437 = torch.ops.aten.permute.default(unsqueeze_890, [3, 4, 2, 1, 0]);  unsqueeze_890 = None
    permute_1438 = torch.ops.aten.permute.default(permute_1436, [1, 3, 4, 0, 2]);  permute_1436 = None
    clone_232 = torch.ops.aten.clone.default(permute_1438, memory_format = torch.contiguous_format);  permute_1438 = None
    _unsafe_view_196 = torch.ops.aten._unsafe_view.default(clone_232, [1, 384, 384]);  clone_232 = None
    permute_1439 = torch.ops.aten.permute.default(permute_1437, [3, 4, 0, 2, 1]);  permute_1437 = None
    clone_233 = torch.ops.aten.clone.default(permute_1439, memory_format = torch.contiguous_format);  permute_1439 = None
    _unsafe_view_197 = torch.ops.aten._unsafe_view.default(clone_233, [1, 384, 384]);  clone_233 = None
    bmm_216 = torch.ops.aten.bmm.default(_unsafe_view_196, _unsafe_view_197);  _unsafe_view_196 = _unsafe_view_197 = None
    view_2655 = torch.ops.aten.view.default(bmm_216, [384, 1, 1, 1, 384]);  bmm_216 = None
    permute_1440 = torch.ops.aten.permute.default(view_2655, [3, 0, 4, 1, 2]);  view_2655 = None
    view_2656 = torch.ops.aten.view.default(permute_1440, [1, 384, 384]);  permute_1440 = None
    unsqueeze_891 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
    mul_346 = torch.ops.aten.mul.Tensor(view_2656, unsqueeze_891);  view_2656 = unsqueeze_891 = None
    add_286 = torch.ops.aten.add.Tensor(add_279, mul_346);  mul_346 = None
    split_tensor_278 = torch.ops.aten.split.Tensor(add_279, 384, dim = -2);  add_279 = None
    getitem_2565 = split_tensor_278[0];  split_tensor_278 = None
    _to_copy_1488 = torch.ops.aten._to_copy.default(getitem_2565, dtype = torch.float32);  getitem_2565 = None
    native_layer_norm_default_306 = torch.ops.aten.native_layer_norm.default(_to_copy_1488, [384], pairformer_stack_blocks_28_transition_single_layer_norm_weight, pairformer_stack_blocks_28_transition_single_layer_norm_bias, 1e-05);  _to_copy_1488 = pairformer_stack_blocks_28_transition_single_layer_norm_weight = pairformer_stack_blocks_28_transition_single_layer_norm_bias = None
    getitem_2566 = native_layer_norm_default_306[0];  native_layer_norm_default_306 = None
    _to_copy_1489 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_28_transition_single_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_28_transition_single_linear_no_bias_ab_weight = None
    _to_copy_1490 = torch.ops.aten._to_copy.default(getitem_2566, dtype = torch.bfloat16);  getitem_2566 = None
    t_550 = torch.ops.aten.t.default(_to_copy_1489);  _to_copy_1489 = None
    view_2657 = torch.ops.aten.view.default(_to_copy_1490, [384, 384]);  _to_copy_1490 = None
    mm_511 = torch.ops.aten.mm.default(view_2657, t_550);  view_2657 = t_550 = None
    view_2658 = torch.ops.aten.view.default(mm_511, [1, 384, 1536]);  mm_511 = None
    split_tensor_279 = torch.ops.aten.split.Tensor(view_2658, 768, dim = -1);  view_2658 = None
    getitem_2569 = split_tensor_279[0]
    getitem_2570 = split_tensor_279[1];  split_tensor_279 = None
    silu_72 = torch.ops.aten.silu.default(getitem_2569);  getitem_2569 = None
    mul_347 = torch.ops.aten.mul.Tensor(silu_72, getitem_2570);  silu_72 = getitem_2570 = None
    _to_copy_1491 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_28_transition_single_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_28_transition_single_linear_out_weight = None
    t_551 = torch.ops.aten.t.default(_to_copy_1491);  _to_copy_1491 = None
    view_2660 = torch.ops.aten.view.default(mul_347, [384, 768]);  mul_347 = None
    mm_512 = torch.ops.aten.mm.default(view_2660, t_551);  view_2660 = t_551 = None
    view_2661 = torch.ops.aten.view.default(mm_512, [1, 384, 384]);  mm_512 = None
    add_287 = torch.ops.aten.add.Tensor(add_286, view_2661);  add_286 = view_2661 = None
    _to_copy_1492 = torch.ops.aten._to_copy.default(add_283, dtype = torch.float32)
    native_layer_norm_default_307 = torch.ops.aten.native_layer_norm.default(_to_copy_1492, [256], pairformer_stack_blocks_29_triangle_multiplication_layernorm_z_in_weight, pairformer_stack_blocks_29_triangle_multiplication_layernorm_z_in_bias, 1e-05);  _to_copy_1492 = pairformer_stack_blocks_29_triangle_multiplication_layernorm_z_in_weight = pairformer_stack_blocks_29_triangle_multiplication_layernorm_z_in_bias = None
    getitem_2571 = native_layer_norm_default_307[0];  native_layer_norm_default_307 = None
    split_with_sizes_default_70 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_29_triangle_multiplication_merged_linear_p_weight, [512, 512]);  pairformer_stack_blocks_29_triangle_multiplication_merged_linear_p_weight = None
    getitem_2574 = split_with_sizes_default_70[0]
    getitem_2575 = split_with_sizes_default_70[1];  split_with_sizes_default_70 = None
    split_with_sizes_default_71 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_29_triangle_multiplication_merged_linear_g_weight, [512, 512, 256]);  pairformer_stack_blocks_29_triangle_multiplication_merged_linear_g_weight = None
    getitem_2576 = split_with_sizes_default_71[0]
    getitem_2577 = split_with_sizes_default_71[1]
    getitem_2578 = split_with_sizes_default_71[2];  split_with_sizes_default_71 = None
    _to_copy_1493 = torch.ops.aten._to_copy.default(getitem_2574, dtype = torch.bfloat16);  getitem_2574 = None
    _to_copy_1494 = torch.ops.aten._to_copy.default(getitem_2571, dtype = torch.bfloat16)
    t_552 = torch.ops.aten.t.default(_to_copy_1493);  _to_copy_1493 = None
    view_2662 = torch.ops.aten.view.default(_to_copy_1494, [147456, 256]);  _to_copy_1494 = None
    mm_513 = torch.ops.aten.mm.default(view_2662, t_552);  view_2662 = t_552 = None
    view_2663 = torch.ops.aten.view.default(mm_513, [1, 384, 384, 512]);  mm_513 = None
    _to_copy_1495 = torch.ops.aten._to_copy.default(getitem_2576, dtype = torch.bfloat16);  getitem_2576 = None
    _to_copy_1496 = torch.ops.aten._to_copy.default(getitem_2571, dtype = torch.bfloat16)
    t_553 = torch.ops.aten.t.default(_to_copy_1495);  _to_copy_1495 = None
    view_2664 = torch.ops.aten.view.default(_to_copy_1496, [147456, 256]);  _to_copy_1496 = None
    mm_514 = torch.ops.aten.mm.default(view_2664, t_553);  view_2664 = t_553 = None
    view_2665 = torch.ops.aten.view.default(mm_514, [1, 384, 384, 512]);  mm_514 = None
    sigmoid_210 = torch.ops.aten.sigmoid.default(view_2665);  view_2665 = None
    mul_348 = torch.ops.aten.mul.Tensor(view_2663, sigmoid_210);  view_2663 = sigmoid_210 = None
    unsqueeze_892 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
    bitwise_not_162 = torch.ops.aten.bitwise_not.default(unsqueeze_892);  unsqueeze_892 = None
    masked_fill_162 = torch.ops.aten.masked_fill.Scalar(mul_348, bitwise_not_162, 0);  mul_348 = bitwise_not_162 = None
    split_tensor_280 = torch.ops.aten.split.Tensor(masked_fill_162, 256, dim = -1)
    getitem_2581 = split_tensor_280[0];  split_tensor_280 = None
    unsqueeze_895 = torch.ops.aten.unsqueeze.default(getitem_2581, 4);  getitem_2581 = None
    permute_1445 = torch.ops.aten.permute.default(unsqueeze_895, [0, 1, 4, 3, 2]);  unsqueeze_895 = None
    permute_1446 = torch.ops.aten.permute.default(permute_1445, [3, 1, 4, 0, 2]);  permute_1445 = None
    view_2668 = torch.ops.aten.view.default(permute_1446, [256, 384, 384]);  permute_1446 = None
    split_tensor_281 = torch.ops.aten.split.Tensor(masked_fill_162, 256, dim = -1);  masked_fill_162 = None
    getitem_2584 = split_tensor_281[1];  split_tensor_281 = None
    unsqueeze_896 = torch.ops.aten.unsqueeze.default(getitem_2584, 4);  getitem_2584 = None
    permute_1447 = torch.ops.aten.permute.default(unsqueeze_896, [0, 4, 1, 3, 2]);  unsqueeze_896 = None
    permute_1448 = torch.ops.aten.permute.default(permute_1447, [3, 4, 0, 2, 1]);  permute_1447 = None
    view_2669 = torch.ops.aten.view.default(permute_1448, [256, 384, 384]);  permute_1448 = None
    bmm_217 = torch.ops.aten.bmm.default(view_2668, view_2669);  view_2668 = view_2669 = None
    view_2670 = torch.ops.aten.view.default(bmm_217, [256, 384, 1, 1, 384]);  bmm_217 = None
    permute_1449 = torch.ops.aten.permute.default(view_2670, [3, 1, 4, 0, 2]);  view_2670 = None
    view_2671 = torch.ops.aten.view.default(permute_1449, [1, 384, 384, 256]);  permute_1449 = None
    _to_copy_1497 = torch.ops.aten._to_copy.default(getitem_2575, dtype = torch.bfloat16);  getitem_2575 = None
    _to_copy_1498 = torch.ops.aten._to_copy.default(getitem_2571, dtype = torch.bfloat16)
    t_554 = torch.ops.aten.t.default(_to_copy_1497);  _to_copy_1497 = None
    view_2672 = torch.ops.aten.view.default(_to_copy_1498, [147456, 256]);  _to_copy_1498 = None
    mm_515 = torch.ops.aten.mm.default(view_2672, t_554);  view_2672 = t_554 = None
    view_2673 = torch.ops.aten.view.default(mm_515, [1, 384, 384, 512]);  mm_515 = None
    _to_copy_1499 = torch.ops.aten._to_copy.default(getitem_2577, dtype = torch.bfloat16);  getitem_2577 = None
    _to_copy_1500 = torch.ops.aten._to_copy.default(getitem_2571, dtype = torch.bfloat16)
    t_555 = torch.ops.aten.t.default(_to_copy_1499);  _to_copy_1499 = None
    view_2674 = torch.ops.aten.view.default(_to_copy_1500, [147456, 256]);  _to_copy_1500 = None
    mm_516 = torch.ops.aten.mm.default(view_2674, t_555);  view_2674 = t_555 = None
    view_2675 = torch.ops.aten.view.default(mm_516, [1, 384, 384, 512]);  mm_516 = None
    sigmoid_211 = torch.ops.aten.sigmoid.default(view_2675);  view_2675 = None
    mul_349 = torch.ops.aten.mul.Tensor(view_2673, sigmoid_211);  view_2673 = sigmoid_211 = None
    view_2676 = torch.ops.aten.view.default(mul_349, [147456, 512]);  mul_349 = None
    view_2677 = torch.ops.aten.view.default(view_2676, [1, 384, 384, 512]);  view_2676 = None
    transpose_70 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
    unsqueeze_897 = torch.ops.aten.unsqueeze.default(transpose_70, 3);  transpose_70 = None
    clone_234 = torch.ops.aten.clone.default(unsqueeze_897, memory_format = torch.contiguous_format);  unsqueeze_897 = None
    bitwise_not_163 = torch.ops.aten.bitwise_not.default(clone_234);  clone_234 = None
    masked_fill_163 = torch.ops.aten.masked_fill.Scalar(view_2677, bitwise_not_163, 0);  view_2677 = bitwise_not_163 = None
    view_2678 = torch.ops.aten.view.default(masked_fill_163, [147456, 512]);  masked_fill_163 = None
    view_2682 = torch.ops.aten.view.default(view_2678, [1, 384, 384, 512])
    split_tensor_282 = torch.ops.aten.split.Tensor(view_2682, 256, dim = -1);  view_2682 = None
    getitem_2587 = split_tensor_282[0];  split_tensor_282 = None
    unsqueeze_900 = torch.ops.aten.unsqueeze.default(getitem_2587, 4);  getitem_2587 = None
    permute_1454 = torch.ops.aten.permute.default(unsqueeze_900, [0, 2, 4, 3, 1]);  unsqueeze_900 = None
    permute_1455 = torch.ops.aten.permute.default(permute_1454, [3, 1, 4, 0, 2]);  permute_1454 = None
    view_2683 = torch.ops.aten.view.default(permute_1455, [256, 384, 384]);  permute_1455 = None
    view_2684 = torch.ops.aten.view.default(view_2678, [1, 384, 384, 512]);  view_2678 = None
    split_tensor_283 = torch.ops.aten.split.Tensor(view_2684, 256, dim = -1);  view_2684 = None
    getitem_2590 = split_tensor_283[1];  split_tensor_283 = None
    unsqueeze_901 = torch.ops.aten.unsqueeze.default(getitem_2590, 4);  getitem_2590 = None
    permute_1456 = torch.ops.aten.permute.default(unsqueeze_901, [0, 4, 2, 3, 1]);  unsqueeze_901 = None
    permute_1457 = torch.ops.aten.permute.default(permute_1456, [3, 4, 0, 2, 1]);  permute_1456 = None
    view_2685 = torch.ops.aten.view.default(permute_1457, [256, 384, 384]);  permute_1457 = None
    bmm_218 = torch.ops.aten.bmm.default(view_2683, view_2685);  view_2683 = view_2685 = None
    view_2686 = torch.ops.aten.view.default(bmm_218, [256, 384, 1, 1, 384]);  bmm_218 = None
    permute_1458 = torch.ops.aten.permute.default(view_2686, [3, 1, 4, 0, 2]);  view_2686 = None
    view_2687 = torch.ops.aten.view.default(permute_1458, [1, 384, 384, 256]);  permute_1458 = None
    _to_copy_1501 = torch.ops.aten._to_copy.default(view_2671, dtype = torch.float32);  view_2671 = None
    native_layer_norm_default_308 = torch.ops.aten.native_layer_norm.default(_to_copy_1501, [256], None, None, 1e-05);  _to_copy_1501 = None
    getitem_2591 = native_layer_norm_default_308[0];  native_layer_norm_default_308 = None
    _to_copy_1502 = torch.ops.aten._to_copy.default(view_2687, dtype = torch.float32);  view_2687 = None
    native_layer_norm_default_309 = torch.ops.aten.native_layer_norm.default(_to_copy_1502, [256], None, None, 1e-05);  _to_copy_1502 = None
    getitem_2594 = native_layer_norm_default_309[0];  native_layer_norm_default_309 = None
    add_288 = torch.ops.aten.add.Tensor(getitem_2591, getitem_2594);  getitem_2591 = getitem_2594 = None
    _to_copy_1503 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_29_triangle_multiplication_linear_z_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_29_triangle_multiplication_linear_z_out_weight = None
    _to_copy_1504 = torch.ops.aten._to_copy.default(add_288, dtype = torch.bfloat16);  add_288 = None
    t_556 = torch.ops.aten.t.default(_to_copy_1503);  _to_copy_1503 = None
    view_2688 = torch.ops.aten.view.default(_to_copy_1504, [147456, 256]);  _to_copy_1504 = None
    mm_517 = torch.ops.aten.mm.default(view_2688, t_556);  view_2688 = t_556 = None
    view_2689 = torch.ops.aten.view.default(mm_517, [1, 384, 384, 256]);  mm_517 = None
    _to_copy_1505 = torch.ops.aten._to_copy.default(getitem_2578, dtype = torch.bfloat16);  getitem_2578 = None
    _to_copy_1506 = torch.ops.aten._to_copy.default(getitem_2571, dtype = torch.bfloat16);  getitem_2571 = None
    t_557 = torch.ops.aten.t.default(_to_copy_1505);  _to_copy_1505 = None
    view_2690 = torch.ops.aten.view.default(_to_copy_1506, [147456, 256]);  _to_copy_1506 = None
    mm_518 = torch.ops.aten.mm.default(view_2690, t_557);  view_2690 = t_557 = None
    view_2691 = torch.ops.aten.view.default(mm_518, [1, 384, 384, 256]);  mm_518 = None
    sigmoid_212 = torch.ops.aten.sigmoid.default(view_2691);  view_2691 = None
    mul_350 = torch.ops.aten.mul.Tensor(view_2689, sigmoid_212);  view_2689 = sigmoid_212 = None
    add_289 = torch.ops.aten.add.Tensor(add_283, mul_350);  mul_350 = None
    _to_copy_1507 = torch.ops.aten._to_copy.default(add_283, dtype = torch.float32)
    native_layer_norm_default_310 = torch.ops.aten.native_layer_norm.default(_to_copy_1507, [256], None, None, 1e-05);  _to_copy_1507 = None
    getitem_2597 = native_layer_norm_default_310[0];  native_layer_norm_default_310 = None
    _to_copy_1508 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_29_triangle_attention_pair2b_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_29_triangle_attention_pair2b_weight = None
    _to_copy_1509 = torch.ops.aten._to_copy.default(getitem_2597, dtype = torch.bfloat16)
    t_558 = torch.ops.aten.t.default(_to_copy_1508);  _to_copy_1508 = None
    view_2692 = torch.ops.aten.view.default(_to_copy_1509, [147456, 256]);  _to_copy_1509 = None
    mm_519 = torch.ops.aten.mm.default(view_2692, t_558);  view_2692 = t_558 = None
    view_2693 = torch.ops.aten.view.default(mm_519, [1, 384, 384, 8]);  mm_519 = None
    view_2694 = torch.ops.aten.view.default(view_2693, [1, 384, 384, 2, 4]);  view_2693 = None
    permute_1459 = torch.ops.aten.permute.default(view_2694, [0, 3, 4, 1, 2]);  view_2694 = None
    view_2695 = torch.ops.aten.view.default(permute_1459, [1, 2, 4, 1, 384, 384]);  permute_1459 = None
    view_2696 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 384, 384])
    bitwise_not_164 = torch.ops.aten.bitwise_not.default(view_2696);  view_2696 = None
    masked_fill_164 = torch.ops.aten.masked_fill.Scalar(view_2695, bitwise_not_164, -10000);  view_2695 = bitwise_not_164 = None
    view_2697 = torch.ops.aten.view.default(masked_fill_164, [1, 2, 4, 384, 384]);  masked_fill_164 = None
    permute_1460 = torch.ops.aten.permute.default(view_2697, [1, 0, 2, 3, 4]);  view_2697 = None
    view_2698 = torch.ops.aten.view.default(permute_1460, [2, 4, 1, 384, 384]);  permute_1460 = None
    _to_copy_1510 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_29_triangle_attention_pair2qkvg1_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_29_triangle_attention_pair2qkvg1_weight = None
    _to_copy_1511 = torch.ops.aten._to_copy.default(getitem_2597, dtype = torch.bfloat16)
    t_559 = torch.ops.aten.t.default(_to_copy_1510);  _to_copy_1510 = None
    view_2699 = torch.ops.aten.view.default(_to_copy_1511, [147456, 256]);  _to_copy_1511 = None
    mm_520 = torch.ops.aten.mm.default(view_2699, t_559);  view_2699 = t_559 = None
    view_2700 = torch.ops.aten.view.default(mm_520, [1, 384, 384, 1024]);  mm_520 = None
    select_71 = torch.ops.aten.select.int(view_2698, 0, 0)
    view_2701 = torch.ops.aten.view.default(view_2700, [1, 384, 384, 4, 4, 64]);  view_2700 = None
    permute_1461 = torch.ops.aten.permute.default(view_2701, [4, 0, 3, 1, 2, 5]);  view_2701 = None
    view_2702 = torch.ops.aten.view.default(permute_1461, [4, 4, 384, 384, 64]);  permute_1461 = None
    unbind_int_121 = torch.ops.aten.unbind.int(view_2702);  view_2702 = None
    getitem_2600 = unbind_int_121[0]
    getitem_2601 = unbind_int_121[1]
    getitem_2602 = unbind_int_121[2]
    getitem_2603 = unbind_int_121[3];  unbind_int_121 = None
    expand_172 = torch.ops.aten.expand.default(select_71, [4, 384, 384, 384]);  select_71 = None
    _scaled_dot_product_efficient_attention_default_99 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2600, getitem_2601, getitem_2602, expand_172, False);  getitem_2600 = getitem_2601 = getitem_2602 = expand_172 = None
    getitem_2604 = _scaled_dot_product_efficient_attention_default_99[0];  _scaled_dot_product_efficient_attention_default_99 = None
    sigmoid_213 = torch.ops.aten.sigmoid.default(getitem_2603);  getitem_2603 = None
    mul_351 = torch.ops.aten.mul.Tensor(getitem_2604, sigmoid_213);  getitem_2604 = sigmoid_213 = None
    view_2703 = torch.ops.aten.view.default(mul_351, [1, 4, 384, 384, 64]);  mul_351 = None
    permute_1462 = torch.ops.aten.permute.default(view_2703, [0, 2, 3, 1, 4]);  view_2703 = None
    clone_235 = torch.ops.aten.clone.default(permute_1462, memory_format = torch.contiguous_format);  permute_1462 = None
    _unsafe_view_198 = torch.ops.aten._unsafe_view.default(clone_235, [1, 384, 384, 256]);  clone_235 = None
    transpose_71 = torch.ops.aten.transpose.int(getitem_2597, 1, 2);  getitem_2597 = None
    _to_copy_1512 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_29_triangle_attention_pair2qkvg2_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_29_triangle_attention_pair2qkvg2_weight = None
    _to_copy_1513 = torch.ops.aten._to_copy.default(transpose_71, dtype = torch.bfloat16);  transpose_71 = None
    t_560 = torch.ops.aten.t.default(_to_copy_1512);  _to_copy_1512 = None
    expand_173 = torch.ops.aten.expand.default(_to_copy_1513, [1, 384, 384, 256]);  _to_copy_1513 = None
    view_2704 = torch.ops.aten.view.default(expand_173, [384, 384, 256]);  expand_173 = None
    expand_174 = torch.ops.aten.expand.default(t_560, [1, 384, 256, 1024]);  t_560 = None
    view_2705 = torch.ops.aten.view.default(expand_174, [384, 256, 1024]);  expand_174 = None
    bmm_219 = torch.ops.aten.bmm.default(view_2704, view_2705);  view_2704 = view_2705 = None
    view_2706 = torch.ops.aten.view.default(bmm_219, [1, 384, 384, 1024]);  bmm_219 = None
    select_72 = torch.ops.aten.select.int(view_2698, 0, 1);  view_2698 = None
    view_2707 = torch.ops.aten.view.default(view_2706, [1, 384, 384, 4, 4, 64]);  view_2706 = None
    permute_1463 = torch.ops.aten.permute.default(view_2707, [4, 0, 3, 1, 2, 5]);  view_2707 = None
    view_2708 = torch.ops.aten.view.default(permute_1463, [4, 4, 384, 384, 64]);  permute_1463 = None
    unbind_int_122 = torch.ops.aten.unbind.int(view_2708);  view_2708 = None
    getitem_2608 = unbind_int_122[0]
    getitem_2609 = unbind_int_122[1]
    getitem_2610 = unbind_int_122[2]
    getitem_2611 = unbind_int_122[3];  unbind_int_122 = None
    expand_175 = torch.ops.aten.expand.default(select_72, [4, 384, 384, 384]);  select_72 = None
    _scaled_dot_product_efficient_attention_default_100 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2608, getitem_2609, getitem_2610, expand_175, False);  getitem_2608 = getitem_2609 = getitem_2610 = expand_175 = None
    getitem_2612 = _scaled_dot_product_efficient_attention_default_100[0];  _scaled_dot_product_efficient_attention_default_100 = None
    sigmoid_214 = torch.ops.aten.sigmoid.default(getitem_2611);  getitem_2611 = None
    mul_352 = torch.ops.aten.mul.Tensor(getitem_2612, sigmoid_214);  getitem_2612 = sigmoid_214 = None
    view_2709 = torch.ops.aten.view.default(mul_352, [1, 4, 384, 384, 64]);  mul_352 = None
    permute_1464 = torch.ops.aten.permute.default(view_2709, [0, 2, 3, 1, 4]);  view_2709 = None
    clone_236 = torch.ops.aten.clone.default(permute_1464, memory_format = torch.contiguous_format);  permute_1464 = None
    _unsafe_view_199 = torch.ops.aten._unsafe_view.default(clone_236, [1, 384, 384, 256]);  clone_236 = None
    cat_41 = torch.ops.aten.cat.default([_unsafe_view_198, _unsafe_view_199], dim = -1);  _unsafe_view_198 = _unsafe_view_199 = None
    slice_192 = torch.ops.aten.slice.Tensor(pairformer_stack_blocks_29_triangle_attention_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  pairformer_stack_blocks_29_triangle_attention_out_scalers = None
    unsqueeze_902 = torch.ops.aten.unsqueeze.default(slice_192, 1);  slice_192 = None
    mul_353 = torch.ops.aten.mul.Tensor(pairformer_stack_blocks_29_triangle_attention_linear_out_weight, unsqueeze_902);  pairformer_stack_blocks_29_triangle_attention_linear_out_weight = unsqueeze_902 = None
    _to_copy_1514 = torch.ops.aten._to_copy.default(mul_353, dtype = torch.bfloat16);  mul_353 = None
    t_561 = torch.ops.aten.t.default(_to_copy_1514);  _to_copy_1514 = None
    view_2710 = torch.ops.aten.view.default(cat_41, [147456, 512]);  cat_41 = None
    mm_521 = torch.ops.aten.mm.default(view_2710, t_561);  view_2710 = t_561 = None
    view_2711 = torch.ops.aten.view.default(mm_521, [1, 384, 384, 256]);  mm_521 = None
    add_290 = torch.ops.aten.add.Tensor(add_289, view_2711);  add_289 = view_2711 = None
    split_tensor_284 = torch.ops.aten.split.Tensor(add_283, 384, dim = -2)
    getitem_2616 = split_tensor_284[0];  split_tensor_284 = None
    _to_copy_1515 = torch.ops.aten._to_copy.default(getitem_2616, dtype = torch.float32);  getitem_2616 = None
    native_layer_norm_default_311 = torch.ops.aten.native_layer_norm.default(_to_copy_1515, [256], pairformer_stack_blocks_29_transition_pair_layer_norm_weight, pairformer_stack_blocks_29_transition_pair_layer_norm_bias, 1e-05);  _to_copy_1515 = pairformer_stack_blocks_29_transition_pair_layer_norm_weight = pairformer_stack_blocks_29_transition_pair_layer_norm_bias = None
    getitem_2617 = native_layer_norm_default_311[0];  native_layer_norm_default_311 = None
    _to_copy_1516 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_29_transition_pair_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_29_transition_pair_linear_no_bias_ab_weight = None
    _to_copy_1517 = torch.ops.aten._to_copy.default(getitem_2617, dtype = torch.bfloat16);  getitem_2617 = None
    t_562 = torch.ops.aten.t.default(_to_copy_1516);  _to_copy_1516 = None
    view_2712 = torch.ops.aten.view.default(_to_copy_1517, [147456, 256]);  _to_copy_1517 = None
    mm_522 = torch.ops.aten.mm.default(view_2712, t_562);  view_2712 = t_562 = None
    view_2713 = torch.ops.aten.view.default(mm_522, [1, 384, 384, 1024]);  mm_522 = None
    split_tensor_285 = torch.ops.aten.split.Tensor(view_2713, 512, dim = -1);  view_2713 = None
    getitem_2620 = split_tensor_285[0]
    getitem_2621 = split_tensor_285[1];  split_tensor_285 = None
    silu_73 = torch.ops.aten.silu.default(getitem_2620);  getitem_2620 = None
    mul_354 = torch.ops.aten.mul.Tensor(silu_73, getitem_2621);  silu_73 = getitem_2621 = None
    _to_copy_1518 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_29_transition_pair_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_29_transition_pair_linear_out_weight = None
    t_563 = torch.ops.aten.t.default(_to_copy_1518);  _to_copy_1518 = None
    view_2715 = torch.ops.aten.view.default(mul_354, [147456, 512]);  mul_354 = None
    mm_523 = torch.ops.aten.mm.default(view_2715, t_563);  view_2715 = t_563 = None
    view_2716 = torch.ops.aten.view.default(mm_523, [1, 384, 384, 256]);  mm_523 = None
    add_291 = torch.ops.aten.add.Tensor(add_290, view_2716);  add_290 = view_2716 = None
    _to_copy_1519 = torch.ops.aten._to_copy.default(add_287, dtype = torch.float32)
    native_layer_norm_default_312 = torch.ops.aten.native_layer_norm.default(_to_copy_1519, [384], pairformer_stack_blocks_29_attention_pair_bias_single_layer_norm_weight, pairformer_stack_blocks_29_attention_pair_bias_single_layer_norm_bias, 1e-05);  _to_copy_1519 = pairformer_stack_blocks_29_attention_pair_bias_single_layer_norm_weight = pairformer_stack_blocks_29_attention_pair_bias_single_layer_norm_bias = None
    getitem_2622 = native_layer_norm_default_312[0];  native_layer_norm_default_312 = None
    _to_copy_1520 = torch.ops.aten._to_copy.default(add_283, dtype = torch.float32);  add_283 = None
    native_layer_norm_default_313 = torch.ops.aten.native_layer_norm.default(_to_copy_1520, [256], pairformer_stack_blocks_29_attention_pair_bias_pair_layer_norm_weight, pairformer_stack_blocks_29_attention_pair_bias_pair_layer_norm_bias, 1e-05);  _to_copy_1520 = pairformer_stack_blocks_29_attention_pair_bias_pair_layer_norm_weight = pairformer_stack_blocks_29_attention_pair_bias_pair_layer_norm_bias = None
    getitem_2625 = native_layer_norm_default_313[0];  native_layer_norm_default_313 = None
    _to_copy_1521 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_29_attention_pair_bias_pair_linear_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_29_attention_pair_bias_pair_linear_weight = None
    _to_copy_1522 = torch.ops.aten._to_copy.default(getitem_2625, dtype = torch.bfloat16);  getitem_2625 = None
    t_564 = torch.ops.aten.t.default(_to_copy_1521);  _to_copy_1521 = None
    view_2717 = torch.ops.aten.view.default(_to_copy_1522, [147456, 256]);  _to_copy_1522 = None
    mm_524 = torch.ops.aten.mm.default(view_2717, t_564);  view_2717 = t_564 = None
    view_2718 = torch.ops.aten.view.default(mm_524, [1, 384, 384, 16]);  mm_524 = None
    permute_1465 = torch.ops.aten.permute.default(view_2718, [0, 3, 1, 2]);  view_2718 = None
    view_2719 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384])
    bitwise_not_165 = torch.ops.aten.bitwise_not.default(view_2719);  view_2719 = None
    masked_fill_165 = torch.ops.aten.masked_fill.Scalar(permute_1465, bitwise_not_165, -10000);  permute_1465 = bitwise_not_165 = None
    _to_copy_1523 = torch.ops.aten._to_copy.default(getitem_2622, dtype = torch.bfloat16);  getitem_2622 = None
    _to_copy_1524 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_29_attention_pair_bias_attention_input2qkvg_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_29_attention_pair_bias_attention_input2qkvg_weight = None
    unsqueeze_903 = torch.ops.aten.unsqueeze.default(_to_copy_1523, 3);  _to_copy_1523 = None
    unsqueeze_904 = torch.ops.aten.unsqueeze.default(unsqueeze_903, 4);  unsqueeze_903 = None
    unsqueeze_905 = torch.ops.aten.unsqueeze.default(unsqueeze_904, 5);  unsqueeze_904 = None
    permute_1466 = torch.ops.aten.permute.default(unsqueeze_905, [3, 0, 4, 1, 5, 2]);  unsqueeze_905 = None
    unsqueeze_906 = torch.ops.aten.unsqueeze.default(_to_copy_1524, 4);  _to_copy_1524 = None
    unsqueeze_907 = torch.ops.aten.unsqueeze.default(unsqueeze_906, 5);  unsqueeze_906 = None
    permute_1467 = torch.ops.aten.permute.default(unsqueeze_907, [1, 4, 2, 5, 3, 0]);  unsqueeze_907 = None
    permute_1468 = torch.ops.aten.permute.default(permute_1466, [3, 5, 0, 1, 2, 4]);  permute_1466 = None
    view_2720 = torch.ops.aten.view.default(permute_1468, [1, 384, 384]);  permute_1468 = None
    permute_1469 = torch.ops.aten.permute.default(permute_1467, [5, 0, 1, 2, 4, 3]);  permute_1467 = None
    view_2721 = torch.ops.aten.view.default(permute_1469, [1, 384, 1536]);  permute_1469 = None
    bmm_220 = torch.ops.aten.bmm.default(view_2720, view_2721);  view_2720 = view_2721 = None
    view_2722 = torch.ops.aten.view.default(bmm_220, [384, 1, 4, 1, 16, 24]);  bmm_220 = None
    permute_1470 = torch.ops.aten.permute.default(view_2722, [2, 3, 4, 0, 5, 1]);  view_2722 = None
    view_2723 = torch.ops.aten.view.default(permute_1470, [4, 1, 16, 384, 24]);  permute_1470 = None
    unbind_int_123 = torch.ops.aten.unbind.int(view_2723);  view_2723 = None
    getitem_2628 = unbind_int_123[0]
    getitem_2629 = unbind_int_123[1]
    getitem_2630 = unbind_int_123[2]
    getitem_2631 = unbind_int_123[3];  unbind_int_123 = None
    view_2724 = torch.ops.aten.view.default(pairformer_stack_blocks_29_attention_pair_bias_attention_query_bias, [1, 16, 1, 24]);  pairformer_stack_blocks_29_attention_pair_bias_attention_query_bias = None
    add_292 = torch.ops.aten.add.Tensor(getitem_2628, view_2724);  getitem_2628 = view_2724 = None
    _to_copy_1525 = torch.ops.aten._to_copy.default(add_292, dtype = torch.bfloat16);  add_292 = None
    expand_176 = torch.ops.aten.expand.default(masked_fill_165, [1, 16, 384, 384]);  masked_fill_165 = None
    _scaled_dot_product_efficient_attention_default_101 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_1525, getitem_2629, getitem_2630, expand_176, False);  _to_copy_1525 = getitem_2629 = getitem_2630 = expand_176 = None
    getitem_2632 = _scaled_dot_product_efficient_attention_default_101[0];  _scaled_dot_product_efficient_attention_default_101 = None
    add_293 = torch.ops.aten.add.Tensor(getitem_2631, 1);  getitem_2631 = None
    sigmoid_215 = torch.ops.aten.sigmoid.default(add_293);  add_293 = None
    mul_355 = torch.ops.aten.mul.Tensor(getitem_2632, sigmoid_215);  getitem_2632 = sigmoid_215 = None
    _to_copy_1526 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_29_attention_pair_bias_attention_output_proj_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_29_attention_pair_bias_attention_output_proj_weight = None
    unsqueeze_908 = torch.ops.aten.unsqueeze.default(mul_355, 4);  mul_355 = None
    permute_1471 = torch.ops.aten.permute.default(unsqueeze_908, [0, 2, 4, 3, 1]);  unsqueeze_908 = None
    unsqueeze_909 = torch.ops.aten.unsqueeze.default(_to_copy_1526, 3);  _to_copy_1526 = None
    unsqueeze_910 = torch.ops.aten.unsqueeze.default(unsqueeze_909, 4);  unsqueeze_909 = None
    permute_1472 = torch.ops.aten.permute.default(unsqueeze_910, [3, 4, 2, 1, 0]);  unsqueeze_910 = None
    permute_1473 = torch.ops.aten.permute.default(permute_1471, [1, 3, 4, 0, 2]);  permute_1471 = None
    clone_237 = torch.ops.aten.clone.default(permute_1473, memory_format = torch.contiguous_format);  permute_1473 = None
    _unsafe_view_200 = torch.ops.aten._unsafe_view.default(clone_237, [1, 384, 384]);  clone_237 = None
    permute_1474 = torch.ops.aten.permute.default(permute_1472, [3, 4, 0, 2, 1]);  permute_1472 = None
    clone_238 = torch.ops.aten.clone.default(permute_1474, memory_format = torch.contiguous_format);  permute_1474 = None
    _unsafe_view_201 = torch.ops.aten._unsafe_view.default(clone_238, [1, 384, 384]);  clone_238 = None
    bmm_221 = torch.ops.aten.bmm.default(_unsafe_view_200, _unsafe_view_201);  _unsafe_view_200 = _unsafe_view_201 = None
    view_2725 = torch.ops.aten.view.default(bmm_221, [384, 1, 1, 1, 384]);  bmm_221 = None
    permute_1475 = torch.ops.aten.permute.default(view_2725, [3, 0, 4, 1, 2]);  view_2725 = None
    view_2726 = torch.ops.aten.view.default(permute_1475, [1, 384, 384]);  permute_1475 = None
    unsqueeze_911 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
    mul_356 = torch.ops.aten.mul.Tensor(view_2726, unsqueeze_911);  view_2726 = unsqueeze_911 = None
    add_294 = torch.ops.aten.add.Tensor(add_287, mul_356);  mul_356 = None
    split_tensor_286 = torch.ops.aten.split.Tensor(add_287, 384, dim = -2);  add_287 = None
    getitem_2636 = split_tensor_286[0];  split_tensor_286 = None
    _to_copy_1527 = torch.ops.aten._to_copy.default(getitem_2636, dtype = torch.float32);  getitem_2636 = None
    native_layer_norm_default_314 = torch.ops.aten.native_layer_norm.default(_to_copy_1527, [384], pairformer_stack_blocks_29_transition_single_layer_norm_weight, pairformer_stack_blocks_29_transition_single_layer_norm_bias, 1e-05);  _to_copy_1527 = pairformer_stack_blocks_29_transition_single_layer_norm_weight = pairformer_stack_blocks_29_transition_single_layer_norm_bias = None
    getitem_2637 = native_layer_norm_default_314[0];  native_layer_norm_default_314 = None
    _to_copy_1528 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_29_transition_single_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_29_transition_single_linear_no_bias_ab_weight = None
    _to_copy_1529 = torch.ops.aten._to_copy.default(getitem_2637, dtype = torch.bfloat16);  getitem_2637 = None
    t_565 = torch.ops.aten.t.default(_to_copy_1528);  _to_copy_1528 = None
    view_2727 = torch.ops.aten.view.default(_to_copy_1529, [384, 384]);  _to_copy_1529 = None
    mm_525 = torch.ops.aten.mm.default(view_2727, t_565);  view_2727 = t_565 = None
    view_2728 = torch.ops.aten.view.default(mm_525, [1, 384, 1536]);  mm_525 = None
    split_tensor_287 = torch.ops.aten.split.Tensor(view_2728, 768, dim = -1);  view_2728 = None
    getitem_2640 = split_tensor_287[0]
    getitem_2641 = split_tensor_287[1];  split_tensor_287 = None
    silu_74 = torch.ops.aten.silu.default(getitem_2640);  getitem_2640 = None
    mul_357 = torch.ops.aten.mul.Tensor(silu_74, getitem_2641);  silu_74 = getitem_2641 = None
    _to_copy_1530 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_29_transition_single_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_29_transition_single_linear_out_weight = None
    t_566 = torch.ops.aten.t.default(_to_copy_1530);  _to_copy_1530 = None
    view_2730 = torch.ops.aten.view.default(mul_357, [384, 768]);  mul_357 = None
    mm_526 = torch.ops.aten.mm.default(view_2730, t_566);  view_2730 = t_566 = None
    view_2731 = torch.ops.aten.view.default(mm_526, [1, 384, 384]);  mm_526 = None
    add_295 = torch.ops.aten.add.Tensor(add_294, view_2731);  add_294 = view_2731 = None
    _to_copy_1531 = torch.ops.aten._to_copy.default(add_291, dtype = torch.float32)
    native_layer_norm_default_315 = torch.ops.aten.native_layer_norm.default(_to_copy_1531, [256], pairformer_stack_blocks_30_triangle_multiplication_layernorm_z_in_weight, pairformer_stack_blocks_30_triangle_multiplication_layernorm_z_in_bias, 1e-05);  _to_copy_1531 = pairformer_stack_blocks_30_triangle_multiplication_layernorm_z_in_weight = pairformer_stack_blocks_30_triangle_multiplication_layernorm_z_in_bias = None
    getitem_2642 = native_layer_norm_default_315[0];  native_layer_norm_default_315 = None
    split_with_sizes_default_72 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_30_triangle_multiplication_merged_linear_p_weight, [512, 512]);  pairformer_stack_blocks_30_triangle_multiplication_merged_linear_p_weight = None
    getitem_2645 = split_with_sizes_default_72[0]
    getitem_2646 = split_with_sizes_default_72[1];  split_with_sizes_default_72 = None
    split_with_sizes_default_73 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_30_triangle_multiplication_merged_linear_g_weight, [512, 512, 256]);  pairformer_stack_blocks_30_triangle_multiplication_merged_linear_g_weight = None
    getitem_2647 = split_with_sizes_default_73[0]
    getitem_2648 = split_with_sizes_default_73[1]
    getitem_2649 = split_with_sizes_default_73[2];  split_with_sizes_default_73 = None
    _to_copy_1532 = torch.ops.aten._to_copy.default(getitem_2645, dtype = torch.bfloat16);  getitem_2645 = None
    _to_copy_1533 = torch.ops.aten._to_copy.default(getitem_2642, dtype = torch.bfloat16)
    t_567 = torch.ops.aten.t.default(_to_copy_1532);  _to_copy_1532 = None
    view_2732 = torch.ops.aten.view.default(_to_copy_1533, [147456, 256]);  _to_copy_1533 = None
    mm_527 = torch.ops.aten.mm.default(view_2732, t_567);  view_2732 = t_567 = None
    view_2733 = torch.ops.aten.view.default(mm_527, [1, 384, 384, 512]);  mm_527 = None
    _to_copy_1534 = torch.ops.aten._to_copy.default(getitem_2647, dtype = torch.bfloat16);  getitem_2647 = None
    _to_copy_1535 = torch.ops.aten._to_copy.default(getitem_2642, dtype = torch.bfloat16)
    t_568 = torch.ops.aten.t.default(_to_copy_1534);  _to_copy_1534 = None
    view_2734 = torch.ops.aten.view.default(_to_copy_1535, [147456, 256]);  _to_copy_1535 = None
    mm_528 = torch.ops.aten.mm.default(view_2734, t_568);  view_2734 = t_568 = None
    view_2735 = torch.ops.aten.view.default(mm_528, [1, 384, 384, 512]);  mm_528 = None
    sigmoid_216 = torch.ops.aten.sigmoid.default(view_2735);  view_2735 = None
    mul_358 = torch.ops.aten.mul.Tensor(view_2733, sigmoid_216);  view_2733 = sigmoid_216 = None
    unsqueeze_912 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
    bitwise_not_166 = torch.ops.aten.bitwise_not.default(unsqueeze_912);  unsqueeze_912 = None
    masked_fill_166 = torch.ops.aten.masked_fill.Scalar(mul_358, bitwise_not_166, 0);  mul_358 = bitwise_not_166 = None
    split_tensor_288 = torch.ops.aten.split.Tensor(masked_fill_166, 256, dim = -1)
    getitem_2652 = split_tensor_288[0];  split_tensor_288 = None
    unsqueeze_915 = torch.ops.aten.unsqueeze.default(getitem_2652, 4);  getitem_2652 = None
    permute_1480 = torch.ops.aten.permute.default(unsqueeze_915, [0, 1, 4, 3, 2]);  unsqueeze_915 = None
    permute_1481 = torch.ops.aten.permute.default(permute_1480, [3, 1, 4, 0, 2]);  permute_1480 = None
    view_2738 = torch.ops.aten.view.default(permute_1481, [256, 384, 384]);  permute_1481 = None
    split_tensor_289 = torch.ops.aten.split.Tensor(masked_fill_166, 256, dim = -1);  masked_fill_166 = None
    getitem_2655 = split_tensor_289[1];  split_tensor_289 = None
    unsqueeze_916 = torch.ops.aten.unsqueeze.default(getitem_2655, 4);  getitem_2655 = None
    permute_1482 = torch.ops.aten.permute.default(unsqueeze_916, [0, 4, 1, 3, 2]);  unsqueeze_916 = None
    permute_1483 = torch.ops.aten.permute.default(permute_1482, [3, 4, 0, 2, 1]);  permute_1482 = None
    view_2739 = torch.ops.aten.view.default(permute_1483, [256, 384, 384]);  permute_1483 = None
    bmm_222 = torch.ops.aten.bmm.default(view_2738, view_2739);  view_2738 = view_2739 = None
    view_2740 = torch.ops.aten.view.default(bmm_222, [256, 384, 1, 1, 384]);  bmm_222 = None
    permute_1484 = torch.ops.aten.permute.default(view_2740, [3, 1, 4, 0, 2]);  view_2740 = None
    view_2741 = torch.ops.aten.view.default(permute_1484, [1, 384, 384, 256]);  permute_1484 = None
    _to_copy_1536 = torch.ops.aten._to_copy.default(getitem_2646, dtype = torch.bfloat16);  getitem_2646 = None
    _to_copy_1537 = torch.ops.aten._to_copy.default(getitem_2642, dtype = torch.bfloat16)
    t_569 = torch.ops.aten.t.default(_to_copy_1536);  _to_copy_1536 = None
    view_2742 = torch.ops.aten.view.default(_to_copy_1537, [147456, 256]);  _to_copy_1537 = None
    mm_529 = torch.ops.aten.mm.default(view_2742, t_569);  view_2742 = t_569 = None
    view_2743 = torch.ops.aten.view.default(mm_529, [1, 384, 384, 512]);  mm_529 = None
    _to_copy_1538 = torch.ops.aten._to_copy.default(getitem_2648, dtype = torch.bfloat16);  getitem_2648 = None
    _to_copy_1539 = torch.ops.aten._to_copy.default(getitem_2642, dtype = torch.bfloat16)
    t_570 = torch.ops.aten.t.default(_to_copy_1538);  _to_copy_1538 = None
    view_2744 = torch.ops.aten.view.default(_to_copy_1539, [147456, 256]);  _to_copy_1539 = None
    mm_530 = torch.ops.aten.mm.default(view_2744, t_570);  view_2744 = t_570 = None
    view_2745 = torch.ops.aten.view.default(mm_530, [1, 384, 384, 512]);  mm_530 = None
    sigmoid_217 = torch.ops.aten.sigmoid.default(view_2745);  view_2745 = None
    mul_359 = torch.ops.aten.mul.Tensor(view_2743, sigmoid_217);  view_2743 = sigmoid_217 = None
    view_2746 = torch.ops.aten.view.default(mul_359, [147456, 512]);  mul_359 = None
    view_2747 = torch.ops.aten.view.default(view_2746, [1, 384, 384, 512]);  view_2746 = None
    transpose_72 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
    unsqueeze_917 = torch.ops.aten.unsqueeze.default(transpose_72, 3);  transpose_72 = None
    clone_239 = torch.ops.aten.clone.default(unsqueeze_917, memory_format = torch.contiguous_format);  unsqueeze_917 = None
    bitwise_not_167 = torch.ops.aten.bitwise_not.default(clone_239);  clone_239 = None
    masked_fill_167 = torch.ops.aten.masked_fill.Scalar(view_2747, bitwise_not_167, 0);  view_2747 = bitwise_not_167 = None
    view_2748 = torch.ops.aten.view.default(masked_fill_167, [147456, 512]);  masked_fill_167 = None
    view_2752 = torch.ops.aten.view.default(view_2748, [1, 384, 384, 512])
    split_tensor_290 = torch.ops.aten.split.Tensor(view_2752, 256, dim = -1);  view_2752 = None
    getitem_2658 = split_tensor_290[0];  split_tensor_290 = None
    unsqueeze_920 = torch.ops.aten.unsqueeze.default(getitem_2658, 4);  getitem_2658 = None
    permute_1489 = torch.ops.aten.permute.default(unsqueeze_920, [0, 2, 4, 3, 1]);  unsqueeze_920 = None
    permute_1490 = torch.ops.aten.permute.default(permute_1489, [3, 1, 4, 0, 2]);  permute_1489 = None
    view_2753 = torch.ops.aten.view.default(permute_1490, [256, 384, 384]);  permute_1490 = None
    view_2754 = torch.ops.aten.view.default(view_2748, [1, 384, 384, 512]);  view_2748 = None
    split_tensor_291 = torch.ops.aten.split.Tensor(view_2754, 256, dim = -1);  view_2754 = None
    getitem_2661 = split_tensor_291[1];  split_tensor_291 = None
    unsqueeze_921 = torch.ops.aten.unsqueeze.default(getitem_2661, 4);  getitem_2661 = None
    permute_1491 = torch.ops.aten.permute.default(unsqueeze_921, [0, 4, 2, 3, 1]);  unsqueeze_921 = None
    permute_1492 = torch.ops.aten.permute.default(permute_1491, [3, 4, 0, 2, 1]);  permute_1491 = None
    view_2755 = torch.ops.aten.view.default(permute_1492, [256, 384, 384]);  permute_1492 = None
    bmm_223 = torch.ops.aten.bmm.default(view_2753, view_2755);  view_2753 = view_2755 = None
    view_2756 = torch.ops.aten.view.default(bmm_223, [256, 384, 1, 1, 384]);  bmm_223 = None
    permute_1493 = torch.ops.aten.permute.default(view_2756, [3, 1, 4, 0, 2]);  view_2756 = None
    view_2757 = torch.ops.aten.view.default(permute_1493, [1, 384, 384, 256]);  permute_1493 = None
    _to_copy_1540 = torch.ops.aten._to_copy.default(view_2741, dtype = torch.float32);  view_2741 = None
    native_layer_norm_default_316 = torch.ops.aten.native_layer_norm.default(_to_copy_1540, [256], None, None, 1e-05);  _to_copy_1540 = None
    getitem_2662 = native_layer_norm_default_316[0];  native_layer_norm_default_316 = None
    _to_copy_1541 = torch.ops.aten._to_copy.default(view_2757, dtype = torch.float32);  view_2757 = None
    native_layer_norm_default_317 = torch.ops.aten.native_layer_norm.default(_to_copy_1541, [256], None, None, 1e-05);  _to_copy_1541 = None
    getitem_2665 = native_layer_norm_default_317[0];  native_layer_norm_default_317 = None
    add_296 = torch.ops.aten.add.Tensor(getitem_2662, getitem_2665);  getitem_2662 = getitem_2665 = None
    _to_copy_1542 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_30_triangle_multiplication_linear_z_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_30_triangle_multiplication_linear_z_out_weight = None
    _to_copy_1543 = torch.ops.aten._to_copy.default(add_296, dtype = torch.bfloat16);  add_296 = None
    t_571 = torch.ops.aten.t.default(_to_copy_1542);  _to_copy_1542 = None
    view_2758 = torch.ops.aten.view.default(_to_copy_1543, [147456, 256]);  _to_copy_1543 = None
    mm_531 = torch.ops.aten.mm.default(view_2758, t_571);  view_2758 = t_571 = None
    view_2759 = torch.ops.aten.view.default(mm_531, [1, 384, 384, 256]);  mm_531 = None
    _to_copy_1544 = torch.ops.aten._to_copy.default(getitem_2649, dtype = torch.bfloat16);  getitem_2649 = None
    _to_copy_1545 = torch.ops.aten._to_copy.default(getitem_2642, dtype = torch.bfloat16);  getitem_2642 = None
    t_572 = torch.ops.aten.t.default(_to_copy_1544);  _to_copy_1544 = None
    view_2760 = torch.ops.aten.view.default(_to_copy_1545, [147456, 256]);  _to_copy_1545 = None
    mm_532 = torch.ops.aten.mm.default(view_2760, t_572);  view_2760 = t_572 = None
    view_2761 = torch.ops.aten.view.default(mm_532, [1, 384, 384, 256]);  mm_532 = None
    sigmoid_218 = torch.ops.aten.sigmoid.default(view_2761);  view_2761 = None
    mul_360 = torch.ops.aten.mul.Tensor(view_2759, sigmoid_218);  view_2759 = sigmoid_218 = None
    add_297 = torch.ops.aten.add.Tensor(add_291, mul_360);  mul_360 = None
    _to_copy_1546 = torch.ops.aten._to_copy.default(add_291, dtype = torch.float32)
    native_layer_norm_default_318 = torch.ops.aten.native_layer_norm.default(_to_copy_1546, [256], None, None, 1e-05);  _to_copy_1546 = None
    getitem_2668 = native_layer_norm_default_318[0];  native_layer_norm_default_318 = None
    _to_copy_1547 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_30_triangle_attention_pair2b_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_30_triangle_attention_pair2b_weight = None
    _to_copy_1548 = torch.ops.aten._to_copy.default(getitem_2668, dtype = torch.bfloat16)
    t_573 = torch.ops.aten.t.default(_to_copy_1547);  _to_copy_1547 = None
    view_2762 = torch.ops.aten.view.default(_to_copy_1548, [147456, 256]);  _to_copy_1548 = None
    mm_533 = torch.ops.aten.mm.default(view_2762, t_573);  view_2762 = t_573 = None
    view_2763 = torch.ops.aten.view.default(mm_533, [1, 384, 384, 8]);  mm_533 = None
    view_2764 = torch.ops.aten.view.default(view_2763, [1, 384, 384, 2, 4]);  view_2763 = None
    permute_1494 = torch.ops.aten.permute.default(view_2764, [0, 3, 4, 1, 2]);  view_2764 = None
    view_2765 = torch.ops.aten.view.default(permute_1494, [1, 2, 4, 1, 384, 384]);  permute_1494 = None
    view_2766 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 384, 384])
    bitwise_not_168 = torch.ops.aten.bitwise_not.default(view_2766);  view_2766 = None
    masked_fill_168 = torch.ops.aten.masked_fill.Scalar(view_2765, bitwise_not_168, -10000);  view_2765 = bitwise_not_168 = None
    view_2767 = torch.ops.aten.view.default(masked_fill_168, [1, 2, 4, 384, 384]);  masked_fill_168 = None
    permute_1495 = torch.ops.aten.permute.default(view_2767, [1, 0, 2, 3, 4]);  view_2767 = None
    view_2768 = torch.ops.aten.view.default(permute_1495, [2, 4, 1, 384, 384]);  permute_1495 = None
    _to_copy_1549 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_30_triangle_attention_pair2qkvg1_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_30_triangle_attention_pair2qkvg1_weight = None
    _to_copy_1550 = torch.ops.aten._to_copy.default(getitem_2668, dtype = torch.bfloat16)
    t_574 = torch.ops.aten.t.default(_to_copy_1549);  _to_copy_1549 = None
    view_2769 = torch.ops.aten.view.default(_to_copy_1550, [147456, 256]);  _to_copy_1550 = None
    mm_534 = torch.ops.aten.mm.default(view_2769, t_574);  view_2769 = t_574 = None
    view_2770 = torch.ops.aten.view.default(mm_534, [1, 384, 384, 1024]);  mm_534 = None
    select_73 = torch.ops.aten.select.int(view_2768, 0, 0)
    view_2771 = torch.ops.aten.view.default(view_2770, [1, 384, 384, 4, 4, 64]);  view_2770 = None
    permute_1496 = torch.ops.aten.permute.default(view_2771, [4, 0, 3, 1, 2, 5]);  view_2771 = None
    view_2772 = torch.ops.aten.view.default(permute_1496, [4, 4, 384, 384, 64]);  permute_1496 = None
    unbind_int_124 = torch.ops.aten.unbind.int(view_2772);  view_2772 = None
    getitem_2671 = unbind_int_124[0]
    getitem_2672 = unbind_int_124[1]
    getitem_2673 = unbind_int_124[2]
    getitem_2674 = unbind_int_124[3];  unbind_int_124 = None
    expand_177 = torch.ops.aten.expand.default(select_73, [4, 384, 384, 384]);  select_73 = None
    _scaled_dot_product_efficient_attention_default_102 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2671, getitem_2672, getitem_2673, expand_177, False);  getitem_2671 = getitem_2672 = getitem_2673 = expand_177 = None
    getitem_2675 = _scaled_dot_product_efficient_attention_default_102[0];  _scaled_dot_product_efficient_attention_default_102 = None
    sigmoid_219 = torch.ops.aten.sigmoid.default(getitem_2674);  getitem_2674 = None
    mul_361 = torch.ops.aten.mul.Tensor(getitem_2675, sigmoid_219);  getitem_2675 = sigmoid_219 = None
    view_2773 = torch.ops.aten.view.default(mul_361, [1, 4, 384, 384, 64]);  mul_361 = None
    permute_1497 = torch.ops.aten.permute.default(view_2773, [0, 2, 3, 1, 4]);  view_2773 = None
    clone_240 = torch.ops.aten.clone.default(permute_1497, memory_format = torch.contiguous_format);  permute_1497 = None
    _unsafe_view_202 = torch.ops.aten._unsafe_view.default(clone_240, [1, 384, 384, 256]);  clone_240 = None
    transpose_73 = torch.ops.aten.transpose.int(getitem_2668, 1, 2);  getitem_2668 = None
    _to_copy_1551 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_30_triangle_attention_pair2qkvg2_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_30_triangle_attention_pair2qkvg2_weight = None
    _to_copy_1552 = torch.ops.aten._to_copy.default(transpose_73, dtype = torch.bfloat16);  transpose_73 = None
    t_575 = torch.ops.aten.t.default(_to_copy_1551);  _to_copy_1551 = None
    expand_178 = torch.ops.aten.expand.default(_to_copy_1552, [1, 384, 384, 256]);  _to_copy_1552 = None
    view_2774 = torch.ops.aten.view.default(expand_178, [384, 384, 256]);  expand_178 = None
    expand_179 = torch.ops.aten.expand.default(t_575, [1, 384, 256, 1024]);  t_575 = None
    view_2775 = torch.ops.aten.view.default(expand_179, [384, 256, 1024]);  expand_179 = None
    bmm_224 = torch.ops.aten.bmm.default(view_2774, view_2775);  view_2774 = view_2775 = None
    view_2776 = torch.ops.aten.view.default(bmm_224, [1, 384, 384, 1024]);  bmm_224 = None
    select_74 = torch.ops.aten.select.int(view_2768, 0, 1);  view_2768 = None
    view_2777 = torch.ops.aten.view.default(view_2776, [1, 384, 384, 4, 4, 64]);  view_2776 = None
    permute_1498 = torch.ops.aten.permute.default(view_2777, [4, 0, 3, 1, 2, 5]);  view_2777 = None
    view_2778 = torch.ops.aten.view.default(permute_1498, [4, 4, 384, 384, 64]);  permute_1498 = None
    unbind_int_125 = torch.ops.aten.unbind.int(view_2778);  view_2778 = None
    getitem_2679 = unbind_int_125[0]
    getitem_2680 = unbind_int_125[1]
    getitem_2681 = unbind_int_125[2]
    getitem_2682 = unbind_int_125[3];  unbind_int_125 = None
    expand_180 = torch.ops.aten.expand.default(select_74, [4, 384, 384, 384]);  select_74 = None
    _scaled_dot_product_efficient_attention_default_103 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2679, getitem_2680, getitem_2681, expand_180, False);  getitem_2679 = getitem_2680 = getitem_2681 = expand_180 = None
    getitem_2683 = _scaled_dot_product_efficient_attention_default_103[0];  _scaled_dot_product_efficient_attention_default_103 = None
    sigmoid_220 = torch.ops.aten.sigmoid.default(getitem_2682);  getitem_2682 = None
    mul_362 = torch.ops.aten.mul.Tensor(getitem_2683, sigmoid_220);  getitem_2683 = sigmoid_220 = None
    view_2779 = torch.ops.aten.view.default(mul_362, [1, 4, 384, 384, 64]);  mul_362 = None
    permute_1499 = torch.ops.aten.permute.default(view_2779, [0, 2, 3, 1, 4]);  view_2779 = None
    clone_241 = torch.ops.aten.clone.default(permute_1499, memory_format = torch.contiguous_format);  permute_1499 = None
    _unsafe_view_203 = torch.ops.aten._unsafe_view.default(clone_241, [1, 384, 384, 256]);  clone_241 = None
    cat_42 = torch.ops.aten.cat.default([_unsafe_view_202, _unsafe_view_203], dim = -1);  _unsafe_view_202 = _unsafe_view_203 = None
    slice_193 = torch.ops.aten.slice.Tensor(pairformer_stack_blocks_30_triangle_attention_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  pairformer_stack_blocks_30_triangle_attention_out_scalers = None
    unsqueeze_922 = torch.ops.aten.unsqueeze.default(slice_193, 1);  slice_193 = None
    mul_363 = torch.ops.aten.mul.Tensor(pairformer_stack_blocks_30_triangle_attention_linear_out_weight, unsqueeze_922);  pairformer_stack_blocks_30_triangle_attention_linear_out_weight = unsqueeze_922 = None
    _to_copy_1553 = torch.ops.aten._to_copy.default(mul_363, dtype = torch.bfloat16);  mul_363 = None
    t_576 = torch.ops.aten.t.default(_to_copy_1553);  _to_copy_1553 = None
    view_2780 = torch.ops.aten.view.default(cat_42, [147456, 512]);  cat_42 = None
    mm_535 = torch.ops.aten.mm.default(view_2780, t_576);  view_2780 = t_576 = None
    view_2781 = torch.ops.aten.view.default(mm_535, [1, 384, 384, 256]);  mm_535 = None
    add_298 = torch.ops.aten.add.Tensor(add_297, view_2781);  add_297 = view_2781 = None
    split_tensor_292 = torch.ops.aten.split.Tensor(add_291, 384, dim = -2)
    getitem_2687 = split_tensor_292[0];  split_tensor_292 = None
    _to_copy_1554 = torch.ops.aten._to_copy.default(getitem_2687, dtype = torch.float32);  getitem_2687 = None
    native_layer_norm_default_319 = torch.ops.aten.native_layer_norm.default(_to_copy_1554, [256], pairformer_stack_blocks_30_transition_pair_layer_norm_weight, pairformer_stack_blocks_30_transition_pair_layer_norm_bias, 1e-05);  _to_copy_1554 = pairformer_stack_blocks_30_transition_pair_layer_norm_weight = pairformer_stack_blocks_30_transition_pair_layer_norm_bias = None
    getitem_2688 = native_layer_norm_default_319[0];  native_layer_norm_default_319 = None
    _to_copy_1555 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_30_transition_pair_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_30_transition_pair_linear_no_bias_ab_weight = None
    _to_copy_1556 = torch.ops.aten._to_copy.default(getitem_2688, dtype = torch.bfloat16);  getitem_2688 = None
    t_577 = torch.ops.aten.t.default(_to_copy_1555);  _to_copy_1555 = None
    view_2782 = torch.ops.aten.view.default(_to_copy_1556, [147456, 256]);  _to_copy_1556 = None
    mm_536 = torch.ops.aten.mm.default(view_2782, t_577);  view_2782 = t_577 = None
    view_2783 = torch.ops.aten.view.default(mm_536, [1, 384, 384, 1024]);  mm_536 = None
    split_tensor_293 = torch.ops.aten.split.Tensor(view_2783, 512, dim = -1);  view_2783 = None
    getitem_2691 = split_tensor_293[0]
    getitem_2692 = split_tensor_293[1];  split_tensor_293 = None
    silu_75 = torch.ops.aten.silu.default(getitem_2691);  getitem_2691 = None
    mul_364 = torch.ops.aten.mul.Tensor(silu_75, getitem_2692);  silu_75 = getitem_2692 = None
    _to_copy_1557 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_30_transition_pair_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_30_transition_pair_linear_out_weight = None
    t_578 = torch.ops.aten.t.default(_to_copy_1557);  _to_copy_1557 = None
    view_2785 = torch.ops.aten.view.default(mul_364, [147456, 512]);  mul_364 = None
    mm_537 = torch.ops.aten.mm.default(view_2785, t_578);  view_2785 = t_578 = None
    view_2786 = torch.ops.aten.view.default(mm_537, [1, 384, 384, 256]);  mm_537 = None
    add_299 = torch.ops.aten.add.Tensor(add_298, view_2786);  add_298 = view_2786 = None
    _to_copy_1558 = torch.ops.aten._to_copy.default(add_295, dtype = torch.float32)
    native_layer_norm_default_320 = torch.ops.aten.native_layer_norm.default(_to_copy_1558, [384], pairformer_stack_blocks_30_attention_pair_bias_single_layer_norm_weight, pairformer_stack_blocks_30_attention_pair_bias_single_layer_norm_bias, 1e-05);  _to_copy_1558 = pairformer_stack_blocks_30_attention_pair_bias_single_layer_norm_weight = pairformer_stack_blocks_30_attention_pair_bias_single_layer_norm_bias = None
    getitem_2693 = native_layer_norm_default_320[0];  native_layer_norm_default_320 = None
    _to_copy_1559 = torch.ops.aten._to_copy.default(add_291, dtype = torch.float32);  add_291 = None
    native_layer_norm_default_321 = torch.ops.aten.native_layer_norm.default(_to_copy_1559, [256], pairformer_stack_blocks_30_attention_pair_bias_pair_layer_norm_weight, pairformer_stack_blocks_30_attention_pair_bias_pair_layer_norm_bias, 1e-05);  _to_copy_1559 = pairformer_stack_blocks_30_attention_pair_bias_pair_layer_norm_weight = pairformer_stack_blocks_30_attention_pair_bias_pair_layer_norm_bias = None
    getitem_2696 = native_layer_norm_default_321[0];  native_layer_norm_default_321 = None
    _to_copy_1560 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_30_attention_pair_bias_pair_linear_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_30_attention_pair_bias_pair_linear_weight = None
    _to_copy_1561 = torch.ops.aten._to_copy.default(getitem_2696, dtype = torch.bfloat16);  getitem_2696 = None
    t_579 = torch.ops.aten.t.default(_to_copy_1560);  _to_copy_1560 = None
    view_2787 = torch.ops.aten.view.default(_to_copy_1561, [147456, 256]);  _to_copy_1561 = None
    mm_538 = torch.ops.aten.mm.default(view_2787, t_579);  view_2787 = t_579 = None
    view_2788 = torch.ops.aten.view.default(mm_538, [1, 384, 384, 16]);  mm_538 = None
    permute_1500 = torch.ops.aten.permute.default(view_2788, [0, 3, 1, 2]);  view_2788 = None
    view_2789 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384])
    bitwise_not_169 = torch.ops.aten.bitwise_not.default(view_2789);  view_2789 = None
    masked_fill_169 = torch.ops.aten.masked_fill.Scalar(permute_1500, bitwise_not_169, -10000);  permute_1500 = bitwise_not_169 = None
    _to_copy_1562 = torch.ops.aten._to_copy.default(getitem_2693, dtype = torch.bfloat16);  getitem_2693 = None
    _to_copy_1563 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_30_attention_pair_bias_attention_input2qkvg_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_30_attention_pair_bias_attention_input2qkvg_weight = None
    unsqueeze_923 = torch.ops.aten.unsqueeze.default(_to_copy_1562, 3);  _to_copy_1562 = None
    unsqueeze_924 = torch.ops.aten.unsqueeze.default(unsqueeze_923, 4);  unsqueeze_923 = None
    unsqueeze_925 = torch.ops.aten.unsqueeze.default(unsqueeze_924, 5);  unsqueeze_924 = None
    permute_1501 = torch.ops.aten.permute.default(unsqueeze_925, [3, 0, 4, 1, 5, 2]);  unsqueeze_925 = None
    unsqueeze_926 = torch.ops.aten.unsqueeze.default(_to_copy_1563, 4);  _to_copy_1563 = None
    unsqueeze_927 = torch.ops.aten.unsqueeze.default(unsqueeze_926, 5);  unsqueeze_926 = None
    permute_1502 = torch.ops.aten.permute.default(unsqueeze_927, [1, 4, 2, 5, 3, 0]);  unsqueeze_927 = None
    permute_1503 = torch.ops.aten.permute.default(permute_1501, [3, 5, 0, 1, 2, 4]);  permute_1501 = None
    view_2790 = torch.ops.aten.view.default(permute_1503, [1, 384, 384]);  permute_1503 = None
    permute_1504 = torch.ops.aten.permute.default(permute_1502, [5, 0, 1, 2, 4, 3]);  permute_1502 = None
    view_2791 = torch.ops.aten.view.default(permute_1504, [1, 384, 1536]);  permute_1504 = None
    bmm_225 = torch.ops.aten.bmm.default(view_2790, view_2791);  view_2790 = view_2791 = None
    view_2792 = torch.ops.aten.view.default(bmm_225, [384, 1, 4, 1, 16, 24]);  bmm_225 = None
    permute_1505 = torch.ops.aten.permute.default(view_2792, [2, 3, 4, 0, 5, 1]);  view_2792 = None
    view_2793 = torch.ops.aten.view.default(permute_1505, [4, 1, 16, 384, 24]);  permute_1505 = None
    unbind_int_126 = torch.ops.aten.unbind.int(view_2793);  view_2793 = None
    getitem_2699 = unbind_int_126[0]
    getitem_2700 = unbind_int_126[1]
    getitem_2701 = unbind_int_126[2]
    getitem_2702 = unbind_int_126[3];  unbind_int_126 = None
    view_2794 = torch.ops.aten.view.default(pairformer_stack_blocks_30_attention_pair_bias_attention_query_bias, [1, 16, 1, 24]);  pairformer_stack_blocks_30_attention_pair_bias_attention_query_bias = None
    add_300 = torch.ops.aten.add.Tensor(getitem_2699, view_2794);  getitem_2699 = view_2794 = None
    _to_copy_1564 = torch.ops.aten._to_copy.default(add_300, dtype = torch.bfloat16);  add_300 = None
    expand_181 = torch.ops.aten.expand.default(masked_fill_169, [1, 16, 384, 384]);  masked_fill_169 = None
    _scaled_dot_product_efficient_attention_default_104 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_1564, getitem_2700, getitem_2701, expand_181, False);  _to_copy_1564 = getitem_2700 = getitem_2701 = expand_181 = None
    getitem_2703 = _scaled_dot_product_efficient_attention_default_104[0];  _scaled_dot_product_efficient_attention_default_104 = None
    add_301 = torch.ops.aten.add.Tensor(getitem_2702, 1);  getitem_2702 = None
    sigmoid_221 = torch.ops.aten.sigmoid.default(add_301);  add_301 = None
    mul_365 = torch.ops.aten.mul.Tensor(getitem_2703, sigmoid_221);  getitem_2703 = sigmoid_221 = None
    _to_copy_1565 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_30_attention_pair_bias_attention_output_proj_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_30_attention_pair_bias_attention_output_proj_weight = None
    unsqueeze_928 = torch.ops.aten.unsqueeze.default(mul_365, 4);  mul_365 = None
    permute_1506 = torch.ops.aten.permute.default(unsqueeze_928, [0, 2, 4, 3, 1]);  unsqueeze_928 = None
    unsqueeze_929 = torch.ops.aten.unsqueeze.default(_to_copy_1565, 3);  _to_copy_1565 = None
    unsqueeze_930 = torch.ops.aten.unsqueeze.default(unsqueeze_929, 4);  unsqueeze_929 = None
    permute_1507 = torch.ops.aten.permute.default(unsqueeze_930, [3, 4, 2, 1, 0]);  unsqueeze_930 = None
    permute_1508 = torch.ops.aten.permute.default(permute_1506, [1, 3, 4, 0, 2]);  permute_1506 = None
    clone_242 = torch.ops.aten.clone.default(permute_1508, memory_format = torch.contiguous_format);  permute_1508 = None
    _unsafe_view_204 = torch.ops.aten._unsafe_view.default(clone_242, [1, 384, 384]);  clone_242 = None
    permute_1509 = torch.ops.aten.permute.default(permute_1507, [3, 4, 0, 2, 1]);  permute_1507 = None
    clone_243 = torch.ops.aten.clone.default(permute_1509, memory_format = torch.contiguous_format);  permute_1509 = None
    _unsafe_view_205 = torch.ops.aten._unsafe_view.default(clone_243, [1, 384, 384]);  clone_243 = None
    bmm_226 = torch.ops.aten.bmm.default(_unsafe_view_204, _unsafe_view_205);  _unsafe_view_204 = _unsafe_view_205 = None
    view_2795 = torch.ops.aten.view.default(bmm_226, [384, 1, 1, 1, 384]);  bmm_226 = None
    permute_1510 = torch.ops.aten.permute.default(view_2795, [3, 0, 4, 1, 2]);  view_2795 = None
    view_2796 = torch.ops.aten.view.default(permute_1510, [1, 384, 384]);  permute_1510 = None
    unsqueeze_931 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
    mul_366 = torch.ops.aten.mul.Tensor(view_2796, unsqueeze_931);  view_2796 = unsqueeze_931 = None
    add_302 = torch.ops.aten.add.Tensor(add_295, mul_366);  mul_366 = None
    split_tensor_294 = torch.ops.aten.split.Tensor(add_295, 384, dim = -2);  add_295 = None
    getitem_2707 = split_tensor_294[0];  split_tensor_294 = None
    _to_copy_1566 = torch.ops.aten._to_copy.default(getitem_2707, dtype = torch.float32);  getitem_2707 = None
    native_layer_norm_default_322 = torch.ops.aten.native_layer_norm.default(_to_copy_1566, [384], pairformer_stack_blocks_30_transition_single_layer_norm_weight, pairformer_stack_blocks_30_transition_single_layer_norm_bias, 1e-05);  _to_copy_1566 = pairformer_stack_blocks_30_transition_single_layer_norm_weight = pairformer_stack_blocks_30_transition_single_layer_norm_bias = None
    getitem_2708 = native_layer_norm_default_322[0];  native_layer_norm_default_322 = None
    _to_copy_1567 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_30_transition_single_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_30_transition_single_linear_no_bias_ab_weight = None
    _to_copy_1568 = torch.ops.aten._to_copy.default(getitem_2708, dtype = torch.bfloat16);  getitem_2708 = None
    t_580 = torch.ops.aten.t.default(_to_copy_1567);  _to_copy_1567 = None
    view_2797 = torch.ops.aten.view.default(_to_copy_1568, [384, 384]);  _to_copy_1568 = None
    mm_539 = torch.ops.aten.mm.default(view_2797, t_580);  view_2797 = t_580 = None
    view_2798 = torch.ops.aten.view.default(mm_539, [1, 384, 1536]);  mm_539 = None
    split_tensor_295 = torch.ops.aten.split.Tensor(view_2798, 768, dim = -1);  view_2798 = None
    getitem_2711 = split_tensor_295[0]
    getitem_2712 = split_tensor_295[1];  split_tensor_295 = None
    silu_76 = torch.ops.aten.silu.default(getitem_2711);  getitem_2711 = None
    mul_367 = torch.ops.aten.mul.Tensor(silu_76, getitem_2712);  silu_76 = getitem_2712 = None
    _to_copy_1569 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_30_transition_single_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_30_transition_single_linear_out_weight = None
    t_581 = torch.ops.aten.t.default(_to_copy_1569);  _to_copy_1569 = None
    view_2800 = torch.ops.aten.view.default(mul_367, [384, 768]);  mul_367 = None
    mm_540 = torch.ops.aten.mm.default(view_2800, t_581);  view_2800 = t_581 = None
    view_2801 = torch.ops.aten.view.default(mm_540, [1, 384, 384]);  mm_540 = None
    add_303 = torch.ops.aten.add.Tensor(add_302, view_2801);  add_302 = view_2801 = None
    _to_copy_1570 = torch.ops.aten._to_copy.default(add_299, dtype = torch.float32)
    native_layer_norm_default_323 = torch.ops.aten.native_layer_norm.default(_to_copy_1570, [256], pairformer_stack_blocks_31_triangle_multiplication_layernorm_z_in_weight, pairformer_stack_blocks_31_triangle_multiplication_layernorm_z_in_bias, 1e-05);  _to_copy_1570 = pairformer_stack_blocks_31_triangle_multiplication_layernorm_z_in_weight = pairformer_stack_blocks_31_triangle_multiplication_layernorm_z_in_bias = None
    getitem_2713 = native_layer_norm_default_323[0];  native_layer_norm_default_323 = None
    split_with_sizes_default_74 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_31_triangle_multiplication_merged_linear_p_weight, [512, 512]);  pairformer_stack_blocks_31_triangle_multiplication_merged_linear_p_weight = None
    getitem_2716 = split_with_sizes_default_74[0]
    getitem_2717 = split_with_sizes_default_74[1];  split_with_sizes_default_74 = None
    split_with_sizes_default_75 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_31_triangle_multiplication_merged_linear_g_weight, [512, 512, 256]);  pairformer_stack_blocks_31_triangle_multiplication_merged_linear_g_weight = None
    getitem_2718 = split_with_sizes_default_75[0]
    getitem_2719 = split_with_sizes_default_75[1]
    getitem_2720 = split_with_sizes_default_75[2];  split_with_sizes_default_75 = None
    _to_copy_1571 = torch.ops.aten._to_copy.default(getitem_2716, dtype = torch.bfloat16);  getitem_2716 = None
    _to_copy_1572 = torch.ops.aten._to_copy.default(getitem_2713, dtype = torch.bfloat16)
    t_582 = torch.ops.aten.t.default(_to_copy_1571);  _to_copy_1571 = None
    view_2802 = torch.ops.aten.view.default(_to_copy_1572, [147456, 256]);  _to_copy_1572 = None
    mm_541 = torch.ops.aten.mm.default(view_2802, t_582);  view_2802 = t_582 = None
    view_2803 = torch.ops.aten.view.default(mm_541, [1, 384, 384, 512]);  mm_541 = None
    _to_copy_1573 = torch.ops.aten._to_copy.default(getitem_2718, dtype = torch.bfloat16);  getitem_2718 = None
    _to_copy_1574 = torch.ops.aten._to_copy.default(getitem_2713, dtype = torch.bfloat16)
    t_583 = torch.ops.aten.t.default(_to_copy_1573);  _to_copy_1573 = None
    view_2804 = torch.ops.aten.view.default(_to_copy_1574, [147456, 256]);  _to_copy_1574 = None
    mm_542 = torch.ops.aten.mm.default(view_2804, t_583);  view_2804 = t_583 = None
    view_2805 = torch.ops.aten.view.default(mm_542, [1, 384, 384, 512]);  mm_542 = None
    sigmoid_222 = torch.ops.aten.sigmoid.default(view_2805);  view_2805 = None
    mul_368 = torch.ops.aten.mul.Tensor(view_2803, sigmoid_222);  view_2803 = sigmoid_222 = None
    unsqueeze_932 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
    bitwise_not_170 = torch.ops.aten.bitwise_not.default(unsqueeze_932);  unsqueeze_932 = None
    masked_fill_170 = torch.ops.aten.masked_fill.Scalar(mul_368, bitwise_not_170, 0);  mul_368 = bitwise_not_170 = None
    split_tensor_296 = torch.ops.aten.split.Tensor(masked_fill_170, 256, dim = -1)
    getitem_2723 = split_tensor_296[0];  split_tensor_296 = None
    unsqueeze_935 = torch.ops.aten.unsqueeze.default(getitem_2723, 4);  getitem_2723 = None
    permute_1515 = torch.ops.aten.permute.default(unsqueeze_935, [0, 1, 4, 3, 2]);  unsqueeze_935 = None
    permute_1516 = torch.ops.aten.permute.default(permute_1515, [3, 1, 4, 0, 2]);  permute_1515 = None
    view_2808 = torch.ops.aten.view.default(permute_1516, [256, 384, 384]);  permute_1516 = None
    split_tensor_297 = torch.ops.aten.split.Tensor(masked_fill_170, 256, dim = -1);  masked_fill_170 = None
    getitem_2726 = split_tensor_297[1];  split_tensor_297 = None
    unsqueeze_936 = torch.ops.aten.unsqueeze.default(getitem_2726, 4);  getitem_2726 = None
    permute_1517 = torch.ops.aten.permute.default(unsqueeze_936, [0, 4, 1, 3, 2]);  unsqueeze_936 = None
    permute_1518 = torch.ops.aten.permute.default(permute_1517, [3, 4, 0, 2, 1]);  permute_1517 = None
    view_2809 = torch.ops.aten.view.default(permute_1518, [256, 384, 384]);  permute_1518 = None
    bmm_227 = torch.ops.aten.bmm.default(view_2808, view_2809);  view_2808 = view_2809 = None
    view_2810 = torch.ops.aten.view.default(bmm_227, [256, 384, 1, 1, 384]);  bmm_227 = None
    permute_1519 = torch.ops.aten.permute.default(view_2810, [3, 1, 4, 0, 2]);  view_2810 = None
    view_2811 = torch.ops.aten.view.default(permute_1519, [1, 384, 384, 256]);  permute_1519 = None
    _to_copy_1575 = torch.ops.aten._to_copy.default(getitem_2717, dtype = torch.bfloat16);  getitem_2717 = None
    _to_copy_1576 = torch.ops.aten._to_copy.default(getitem_2713, dtype = torch.bfloat16)
    t_584 = torch.ops.aten.t.default(_to_copy_1575);  _to_copy_1575 = None
    view_2812 = torch.ops.aten.view.default(_to_copy_1576, [147456, 256]);  _to_copy_1576 = None
    mm_543 = torch.ops.aten.mm.default(view_2812, t_584);  view_2812 = t_584 = None
    view_2813 = torch.ops.aten.view.default(mm_543, [1, 384, 384, 512]);  mm_543 = None
    _to_copy_1577 = torch.ops.aten._to_copy.default(getitem_2719, dtype = torch.bfloat16);  getitem_2719 = None
    _to_copy_1578 = torch.ops.aten._to_copy.default(getitem_2713, dtype = torch.bfloat16)
    t_585 = torch.ops.aten.t.default(_to_copy_1577);  _to_copy_1577 = None
    view_2814 = torch.ops.aten.view.default(_to_copy_1578, [147456, 256]);  _to_copy_1578 = None
    mm_544 = torch.ops.aten.mm.default(view_2814, t_585);  view_2814 = t_585 = None
    view_2815 = torch.ops.aten.view.default(mm_544, [1, 384, 384, 512]);  mm_544 = None
    sigmoid_223 = torch.ops.aten.sigmoid.default(view_2815);  view_2815 = None
    mul_369 = torch.ops.aten.mul.Tensor(view_2813, sigmoid_223);  view_2813 = sigmoid_223 = None
    view_2816 = torch.ops.aten.view.default(mul_369, [147456, 512]);  mul_369 = None
    view_2817 = torch.ops.aten.view.default(view_2816, [1, 384, 384, 512]);  view_2816 = None
    transpose_74 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
    unsqueeze_937 = torch.ops.aten.unsqueeze.default(transpose_74, 3);  transpose_74 = None
    clone_244 = torch.ops.aten.clone.default(unsqueeze_937, memory_format = torch.contiguous_format);  unsqueeze_937 = None
    bitwise_not_171 = torch.ops.aten.bitwise_not.default(clone_244);  clone_244 = None
    masked_fill_171 = torch.ops.aten.masked_fill.Scalar(view_2817, bitwise_not_171, 0);  view_2817 = bitwise_not_171 = None
    view_2818 = torch.ops.aten.view.default(masked_fill_171, [147456, 512]);  masked_fill_171 = None
    view_2822 = torch.ops.aten.view.default(view_2818, [1, 384, 384, 512])
    split_tensor_298 = torch.ops.aten.split.Tensor(view_2822, 256, dim = -1);  view_2822 = None
    getitem_2729 = split_tensor_298[0];  split_tensor_298 = None
    unsqueeze_940 = torch.ops.aten.unsqueeze.default(getitem_2729, 4);  getitem_2729 = None
    permute_1524 = torch.ops.aten.permute.default(unsqueeze_940, [0, 2, 4, 3, 1]);  unsqueeze_940 = None
    permute_1525 = torch.ops.aten.permute.default(permute_1524, [3, 1, 4, 0, 2]);  permute_1524 = None
    view_2823 = torch.ops.aten.view.default(permute_1525, [256, 384, 384]);  permute_1525 = None
    view_2824 = torch.ops.aten.view.default(view_2818, [1, 384, 384, 512]);  view_2818 = None
    split_tensor_299 = torch.ops.aten.split.Tensor(view_2824, 256, dim = -1);  view_2824 = None
    getitem_2732 = split_tensor_299[1];  split_tensor_299 = None
    unsqueeze_941 = torch.ops.aten.unsqueeze.default(getitem_2732, 4);  getitem_2732 = None
    permute_1526 = torch.ops.aten.permute.default(unsqueeze_941, [0, 4, 2, 3, 1]);  unsqueeze_941 = None
    permute_1527 = torch.ops.aten.permute.default(permute_1526, [3, 4, 0, 2, 1]);  permute_1526 = None
    view_2825 = torch.ops.aten.view.default(permute_1527, [256, 384, 384]);  permute_1527 = None
    bmm_228 = torch.ops.aten.bmm.default(view_2823, view_2825);  view_2823 = view_2825 = None
    view_2826 = torch.ops.aten.view.default(bmm_228, [256, 384, 1, 1, 384]);  bmm_228 = None
    permute_1528 = torch.ops.aten.permute.default(view_2826, [3, 1, 4, 0, 2]);  view_2826 = None
    view_2827 = torch.ops.aten.view.default(permute_1528, [1, 384, 384, 256]);  permute_1528 = None
    _to_copy_1579 = torch.ops.aten._to_copy.default(view_2811, dtype = torch.float32);  view_2811 = None
    native_layer_norm_default_324 = torch.ops.aten.native_layer_norm.default(_to_copy_1579, [256], None, None, 1e-05);  _to_copy_1579 = None
    getitem_2733 = native_layer_norm_default_324[0];  native_layer_norm_default_324 = None
    _to_copy_1580 = torch.ops.aten._to_copy.default(view_2827, dtype = torch.float32);  view_2827 = None
    native_layer_norm_default_325 = torch.ops.aten.native_layer_norm.default(_to_copy_1580, [256], None, None, 1e-05);  _to_copy_1580 = None
    getitem_2736 = native_layer_norm_default_325[0];  native_layer_norm_default_325 = None
    add_304 = torch.ops.aten.add.Tensor(getitem_2733, getitem_2736);  getitem_2733 = getitem_2736 = None
    _to_copy_1581 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_31_triangle_multiplication_linear_z_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_31_triangle_multiplication_linear_z_out_weight = None
    _to_copy_1582 = torch.ops.aten._to_copy.default(add_304, dtype = torch.bfloat16);  add_304 = None
    t_586 = torch.ops.aten.t.default(_to_copy_1581);  _to_copy_1581 = None
    view_2828 = torch.ops.aten.view.default(_to_copy_1582, [147456, 256]);  _to_copy_1582 = None
    mm_545 = torch.ops.aten.mm.default(view_2828, t_586);  view_2828 = t_586 = None
    view_2829 = torch.ops.aten.view.default(mm_545, [1, 384, 384, 256]);  mm_545 = None
    _to_copy_1583 = torch.ops.aten._to_copy.default(getitem_2720, dtype = torch.bfloat16);  getitem_2720 = None
    _to_copy_1584 = torch.ops.aten._to_copy.default(getitem_2713, dtype = torch.bfloat16);  getitem_2713 = None
    t_587 = torch.ops.aten.t.default(_to_copy_1583);  _to_copy_1583 = None
    view_2830 = torch.ops.aten.view.default(_to_copy_1584, [147456, 256]);  _to_copy_1584 = None
    mm_546 = torch.ops.aten.mm.default(view_2830, t_587);  view_2830 = t_587 = None
    view_2831 = torch.ops.aten.view.default(mm_546, [1, 384, 384, 256]);  mm_546 = None
    sigmoid_224 = torch.ops.aten.sigmoid.default(view_2831);  view_2831 = None
    mul_370 = torch.ops.aten.mul.Tensor(view_2829, sigmoid_224);  view_2829 = sigmoid_224 = None
    add_305 = torch.ops.aten.add.Tensor(add_299, mul_370);  mul_370 = None
    _to_copy_1585 = torch.ops.aten._to_copy.default(add_299, dtype = torch.float32)
    native_layer_norm_default_326 = torch.ops.aten.native_layer_norm.default(_to_copy_1585, [256], None, None, 1e-05);  _to_copy_1585 = None
    getitem_2739 = native_layer_norm_default_326[0];  native_layer_norm_default_326 = None
    _to_copy_1586 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_31_triangle_attention_pair2b_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_31_triangle_attention_pair2b_weight = None
    _to_copy_1587 = torch.ops.aten._to_copy.default(getitem_2739, dtype = torch.bfloat16)
    t_588 = torch.ops.aten.t.default(_to_copy_1586);  _to_copy_1586 = None
    view_2832 = torch.ops.aten.view.default(_to_copy_1587, [147456, 256]);  _to_copy_1587 = None
    mm_547 = torch.ops.aten.mm.default(view_2832, t_588);  view_2832 = t_588 = None
    view_2833 = torch.ops.aten.view.default(mm_547, [1, 384, 384, 8]);  mm_547 = None
    view_2834 = torch.ops.aten.view.default(view_2833, [1, 384, 384, 2, 4]);  view_2833 = None
    permute_1529 = torch.ops.aten.permute.default(view_2834, [0, 3, 4, 1, 2]);  view_2834 = None
    view_2835 = torch.ops.aten.view.default(permute_1529, [1, 2, 4, 1, 384, 384]);  permute_1529 = None
    view_2836 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 384, 384])
    bitwise_not_172 = torch.ops.aten.bitwise_not.default(view_2836);  view_2836 = None
    masked_fill_172 = torch.ops.aten.masked_fill.Scalar(view_2835, bitwise_not_172, -10000);  view_2835 = bitwise_not_172 = None
    view_2837 = torch.ops.aten.view.default(masked_fill_172, [1, 2, 4, 384, 384]);  masked_fill_172 = None
    permute_1530 = torch.ops.aten.permute.default(view_2837, [1, 0, 2, 3, 4]);  view_2837 = None
    view_2838 = torch.ops.aten.view.default(permute_1530, [2, 4, 1, 384, 384]);  permute_1530 = None
    _to_copy_1588 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_31_triangle_attention_pair2qkvg1_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_31_triangle_attention_pair2qkvg1_weight = None
    _to_copy_1589 = torch.ops.aten._to_copy.default(getitem_2739, dtype = torch.bfloat16)
    t_589 = torch.ops.aten.t.default(_to_copy_1588);  _to_copy_1588 = None
    view_2839 = torch.ops.aten.view.default(_to_copy_1589, [147456, 256]);  _to_copy_1589 = None
    mm_548 = torch.ops.aten.mm.default(view_2839, t_589);  view_2839 = t_589 = None
    view_2840 = torch.ops.aten.view.default(mm_548, [1, 384, 384, 1024]);  mm_548 = None
    select_75 = torch.ops.aten.select.int(view_2838, 0, 0)
    view_2841 = torch.ops.aten.view.default(view_2840, [1, 384, 384, 4, 4, 64]);  view_2840 = None
    permute_1531 = torch.ops.aten.permute.default(view_2841, [4, 0, 3, 1, 2, 5]);  view_2841 = None
    view_2842 = torch.ops.aten.view.default(permute_1531, [4, 4, 384, 384, 64]);  permute_1531 = None
    unbind_int_127 = torch.ops.aten.unbind.int(view_2842);  view_2842 = None
    getitem_2742 = unbind_int_127[0]
    getitem_2743 = unbind_int_127[1]
    getitem_2744 = unbind_int_127[2]
    getitem_2745 = unbind_int_127[3];  unbind_int_127 = None
    expand_182 = torch.ops.aten.expand.default(select_75, [4, 384, 384, 384]);  select_75 = None
    _scaled_dot_product_efficient_attention_default_105 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2742, getitem_2743, getitem_2744, expand_182, False);  getitem_2742 = getitem_2743 = getitem_2744 = expand_182 = None
    getitem_2746 = _scaled_dot_product_efficient_attention_default_105[0];  _scaled_dot_product_efficient_attention_default_105 = None
    sigmoid_225 = torch.ops.aten.sigmoid.default(getitem_2745);  getitem_2745 = None
    mul_371 = torch.ops.aten.mul.Tensor(getitem_2746, sigmoid_225);  getitem_2746 = sigmoid_225 = None
    view_2843 = torch.ops.aten.view.default(mul_371, [1, 4, 384, 384, 64]);  mul_371 = None
    permute_1532 = torch.ops.aten.permute.default(view_2843, [0, 2, 3, 1, 4]);  view_2843 = None
    clone_245 = torch.ops.aten.clone.default(permute_1532, memory_format = torch.contiguous_format);  permute_1532 = None
    _unsafe_view_206 = torch.ops.aten._unsafe_view.default(clone_245, [1, 384, 384, 256]);  clone_245 = None
    transpose_75 = torch.ops.aten.transpose.int(getitem_2739, 1, 2);  getitem_2739 = None
    _to_copy_1590 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_31_triangle_attention_pair2qkvg2_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_31_triangle_attention_pair2qkvg2_weight = None
    _to_copy_1591 = torch.ops.aten._to_copy.default(transpose_75, dtype = torch.bfloat16);  transpose_75 = None
    t_590 = torch.ops.aten.t.default(_to_copy_1590);  _to_copy_1590 = None
    expand_183 = torch.ops.aten.expand.default(_to_copy_1591, [1, 384, 384, 256]);  _to_copy_1591 = None
    view_2844 = torch.ops.aten.view.default(expand_183, [384, 384, 256]);  expand_183 = None
    expand_184 = torch.ops.aten.expand.default(t_590, [1, 384, 256, 1024]);  t_590 = None
    view_2845 = torch.ops.aten.view.default(expand_184, [384, 256, 1024]);  expand_184 = None
    bmm_229 = torch.ops.aten.bmm.default(view_2844, view_2845);  view_2844 = view_2845 = None
    view_2846 = torch.ops.aten.view.default(bmm_229, [1, 384, 384, 1024]);  bmm_229 = None
    select_76 = torch.ops.aten.select.int(view_2838, 0, 1);  view_2838 = None
    view_2847 = torch.ops.aten.view.default(view_2846, [1, 384, 384, 4, 4, 64]);  view_2846 = None
    permute_1533 = torch.ops.aten.permute.default(view_2847, [4, 0, 3, 1, 2, 5]);  view_2847 = None
    view_2848 = torch.ops.aten.view.default(permute_1533, [4, 4, 384, 384, 64]);  permute_1533 = None
    unbind_int_128 = torch.ops.aten.unbind.int(view_2848);  view_2848 = None
    getitem_2750 = unbind_int_128[0]
    getitem_2751 = unbind_int_128[1]
    getitem_2752 = unbind_int_128[2]
    getitem_2753 = unbind_int_128[3];  unbind_int_128 = None
    expand_185 = torch.ops.aten.expand.default(select_76, [4, 384, 384, 384]);  select_76 = None
    _scaled_dot_product_efficient_attention_default_106 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2750, getitem_2751, getitem_2752, expand_185, False);  getitem_2750 = getitem_2751 = getitem_2752 = expand_185 = None
    getitem_2754 = _scaled_dot_product_efficient_attention_default_106[0];  _scaled_dot_product_efficient_attention_default_106 = None
    sigmoid_226 = torch.ops.aten.sigmoid.default(getitem_2753);  getitem_2753 = None
    mul_372 = torch.ops.aten.mul.Tensor(getitem_2754, sigmoid_226);  getitem_2754 = sigmoid_226 = None
    view_2849 = torch.ops.aten.view.default(mul_372, [1, 4, 384, 384, 64]);  mul_372 = None
    permute_1534 = torch.ops.aten.permute.default(view_2849, [0, 2, 3, 1, 4]);  view_2849 = None
    clone_246 = torch.ops.aten.clone.default(permute_1534, memory_format = torch.contiguous_format);  permute_1534 = None
    _unsafe_view_207 = torch.ops.aten._unsafe_view.default(clone_246, [1, 384, 384, 256]);  clone_246 = None
    cat_43 = torch.ops.aten.cat.default([_unsafe_view_206, _unsafe_view_207], dim = -1);  _unsafe_view_206 = _unsafe_view_207 = None
    slice_194 = torch.ops.aten.slice.Tensor(pairformer_stack_blocks_31_triangle_attention_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  pairformer_stack_blocks_31_triangle_attention_out_scalers = None
    unsqueeze_942 = torch.ops.aten.unsqueeze.default(slice_194, 1);  slice_194 = None
    mul_373 = torch.ops.aten.mul.Tensor(pairformer_stack_blocks_31_triangle_attention_linear_out_weight, unsqueeze_942);  pairformer_stack_blocks_31_triangle_attention_linear_out_weight = unsqueeze_942 = None
    _to_copy_1592 = torch.ops.aten._to_copy.default(mul_373, dtype = torch.bfloat16);  mul_373 = None
    t_591 = torch.ops.aten.t.default(_to_copy_1592);  _to_copy_1592 = None
    view_2850 = torch.ops.aten.view.default(cat_43, [147456, 512]);  cat_43 = None
    mm_549 = torch.ops.aten.mm.default(view_2850, t_591);  view_2850 = t_591 = None
    view_2851 = torch.ops.aten.view.default(mm_549, [1, 384, 384, 256]);  mm_549 = None
    add_306 = torch.ops.aten.add.Tensor(add_305, view_2851);  add_305 = view_2851 = None
    split_tensor_300 = torch.ops.aten.split.Tensor(add_299, 384, dim = -2)
    getitem_2758 = split_tensor_300[0];  split_tensor_300 = None
    _to_copy_1593 = torch.ops.aten._to_copy.default(getitem_2758, dtype = torch.float32);  getitem_2758 = None
    native_layer_norm_default_327 = torch.ops.aten.native_layer_norm.default(_to_copy_1593, [256], pairformer_stack_blocks_31_transition_pair_layer_norm_weight, pairformer_stack_blocks_31_transition_pair_layer_norm_bias, 1e-05);  _to_copy_1593 = pairformer_stack_blocks_31_transition_pair_layer_norm_weight = pairformer_stack_blocks_31_transition_pair_layer_norm_bias = None
    getitem_2759 = native_layer_norm_default_327[0];  native_layer_norm_default_327 = None
    _to_copy_1594 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_31_transition_pair_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_31_transition_pair_linear_no_bias_ab_weight = None
    _to_copy_1595 = torch.ops.aten._to_copy.default(getitem_2759, dtype = torch.bfloat16);  getitem_2759 = None
    t_592 = torch.ops.aten.t.default(_to_copy_1594);  _to_copy_1594 = None
    view_2852 = torch.ops.aten.view.default(_to_copy_1595, [147456, 256]);  _to_copy_1595 = None
    mm_550 = torch.ops.aten.mm.default(view_2852, t_592);  view_2852 = t_592 = None
    view_2853 = torch.ops.aten.view.default(mm_550, [1, 384, 384, 1024]);  mm_550 = None
    split_tensor_301 = torch.ops.aten.split.Tensor(view_2853, 512, dim = -1);  view_2853 = None
    getitem_2762 = split_tensor_301[0]
    getitem_2763 = split_tensor_301[1];  split_tensor_301 = None
    silu_77 = torch.ops.aten.silu.default(getitem_2762);  getitem_2762 = None
    mul_374 = torch.ops.aten.mul.Tensor(silu_77, getitem_2763);  silu_77 = getitem_2763 = None
    _to_copy_1596 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_31_transition_pair_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_31_transition_pair_linear_out_weight = None
    t_593 = torch.ops.aten.t.default(_to_copy_1596);  _to_copy_1596 = None
    view_2855 = torch.ops.aten.view.default(mul_374, [147456, 512]);  mul_374 = None
    mm_551 = torch.ops.aten.mm.default(view_2855, t_593);  view_2855 = t_593 = None
    view_2856 = torch.ops.aten.view.default(mm_551, [1, 384, 384, 256]);  mm_551 = None
    add_307 = torch.ops.aten.add.Tensor(add_306, view_2856);  add_306 = view_2856 = None
    _to_copy_1597 = torch.ops.aten._to_copy.default(add_303, dtype = torch.float32)
    native_layer_norm_default_328 = torch.ops.aten.native_layer_norm.default(_to_copy_1597, [384], pairformer_stack_blocks_31_attention_pair_bias_single_layer_norm_weight, pairformer_stack_blocks_31_attention_pair_bias_single_layer_norm_bias, 1e-05);  _to_copy_1597 = pairformer_stack_blocks_31_attention_pair_bias_single_layer_norm_weight = pairformer_stack_blocks_31_attention_pair_bias_single_layer_norm_bias = None
    getitem_2764 = native_layer_norm_default_328[0];  native_layer_norm_default_328 = None
    _to_copy_1598 = torch.ops.aten._to_copy.default(add_299, dtype = torch.float32);  add_299 = None
    native_layer_norm_default_329 = torch.ops.aten.native_layer_norm.default(_to_copy_1598, [256], pairformer_stack_blocks_31_attention_pair_bias_pair_layer_norm_weight, pairformer_stack_blocks_31_attention_pair_bias_pair_layer_norm_bias, 1e-05);  _to_copy_1598 = pairformer_stack_blocks_31_attention_pair_bias_pair_layer_norm_weight = pairformer_stack_blocks_31_attention_pair_bias_pair_layer_norm_bias = None
    getitem_2767 = native_layer_norm_default_329[0];  native_layer_norm_default_329 = None
    _to_copy_1599 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_31_attention_pair_bias_pair_linear_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_31_attention_pair_bias_pair_linear_weight = None
    _to_copy_1600 = torch.ops.aten._to_copy.default(getitem_2767, dtype = torch.bfloat16);  getitem_2767 = None
    t_594 = torch.ops.aten.t.default(_to_copy_1599);  _to_copy_1599 = None
    view_2857 = torch.ops.aten.view.default(_to_copy_1600, [147456, 256]);  _to_copy_1600 = None
    mm_552 = torch.ops.aten.mm.default(view_2857, t_594);  view_2857 = t_594 = None
    view_2858 = torch.ops.aten.view.default(mm_552, [1, 384, 384, 16]);  mm_552 = None
    permute_1535 = torch.ops.aten.permute.default(view_2858, [0, 3, 1, 2]);  view_2858 = None
    view_2859 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384])
    bitwise_not_173 = torch.ops.aten.bitwise_not.default(view_2859);  view_2859 = None
    masked_fill_173 = torch.ops.aten.masked_fill.Scalar(permute_1535, bitwise_not_173, -10000);  permute_1535 = bitwise_not_173 = None
    _to_copy_1601 = torch.ops.aten._to_copy.default(getitem_2764, dtype = torch.bfloat16);  getitem_2764 = None
    _to_copy_1602 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_31_attention_pair_bias_attention_input2qkvg_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_31_attention_pair_bias_attention_input2qkvg_weight = None
    unsqueeze_943 = torch.ops.aten.unsqueeze.default(_to_copy_1601, 3);  _to_copy_1601 = None
    unsqueeze_944 = torch.ops.aten.unsqueeze.default(unsqueeze_943, 4);  unsqueeze_943 = None
    unsqueeze_945 = torch.ops.aten.unsqueeze.default(unsqueeze_944, 5);  unsqueeze_944 = None
    permute_1536 = torch.ops.aten.permute.default(unsqueeze_945, [3, 0, 4, 1, 5, 2]);  unsqueeze_945 = None
    unsqueeze_946 = torch.ops.aten.unsqueeze.default(_to_copy_1602, 4);  _to_copy_1602 = None
    unsqueeze_947 = torch.ops.aten.unsqueeze.default(unsqueeze_946, 5);  unsqueeze_946 = None
    permute_1537 = torch.ops.aten.permute.default(unsqueeze_947, [1, 4, 2, 5, 3, 0]);  unsqueeze_947 = None
    permute_1538 = torch.ops.aten.permute.default(permute_1536, [3, 5, 0, 1, 2, 4]);  permute_1536 = None
    view_2860 = torch.ops.aten.view.default(permute_1538, [1, 384, 384]);  permute_1538 = None
    permute_1539 = torch.ops.aten.permute.default(permute_1537, [5, 0, 1, 2, 4, 3]);  permute_1537 = None
    view_2861 = torch.ops.aten.view.default(permute_1539, [1, 384, 1536]);  permute_1539 = None
    bmm_230 = torch.ops.aten.bmm.default(view_2860, view_2861);  view_2860 = view_2861 = None
    view_2862 = torch.ops.aten.view.default(bmm_230, [384, 1, 4, 1, 16, 24]);  bmm_230 = None
    permute_1540 = torch.ops.aten.permute.default(view_2862, [2, 3, 4, 0, 5, 1]);  view_2862 = None
    view_2863 = torch.ops.aten.view.default(permute_1540, [4, 1, 16, 384, 24]);  permute_1540 = None
    unbind_int_129 = torch.ops.aten.unbind.int(view_2863);  view_2863 = None
    getitem_2770 = unbind_int_129[0]
    getitem_2771 = unbind_int_129[1]
    getitem_2772 = unbind_int_129[2]
    getitem_2773 = unbind_int_129[3];  unbind_int_129 = None
    view_2864 = torch.ops.aten.view.default(pairformer_stack_blocks_31_attention_pair_bias_attention_query_bias, [1, 16, 1, 24]);  pairformer_stack_blocks_31_attention_pair_bias_attention_query_bias = None
    add_308 = torch.ops.aten.add.Tensor(getitem_2770, view_2864);  getitem_2770 = view_2864 = None
    _to_copy_1603 = torch.ops.aten._to_copy.default(add_308, dtype = torch.bfloat16);  add_308 = None
    expand_186 = torch.ops.aten.expand.default(masked_fill_173, [1, 16, 384, 384]);  masked_fill_173 = None
    _scaled_dot_product_efficient_attention_default_107 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_1603, getitem_2771, getitem_2772, expand_186, False);  _to_copy_1603 = getitem_2771 = getitem_2772 = expand_186 = None
    getitem_2774 = _scaled_dot_product_efficient_attention_default_107[0];  _scaled_dot_product_efficient_attention_default_107 = None
    add_309 = torch.ops.aten.add.Tensor(getitem_2773, 1);  getitem_2773 = None
    sigmoid_227 = torch.ops.aten.sigmoid.default(add_309);  add_309 = None
    mul_375 = torch.ops.aten.mul.Tensor(getitem_2774, sigmoid_227);  getitem_2774 = sigmoid_227 = None
    _to_copy_1604 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_31_attention_pair_bias_attention_output_proj_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_31_attention_pair_bias_attention_output_proj_weight = None
    unsqueeze_948 = torch.ops.aten.unsqueeze.default(mul_375, 4);  mul_375 = None
    permute_1541 = torch.ops.aten.permute.default(unsqueeze_948, [0, 2, 4, 3, 1]);  unsqueeze_948 = None
    unsqueeze_949 = torch.ops.aten.unsqueeze.default(_to_copy_1604, 3);  _to_copy_1604 = None
    unsqueeze_950 = torch.ops.aten.unsqueeze.default(unsqueeze_949, 4);  unsqueeze_949 = None
    permute_1542 = torch.ops.aten.permute.default(unsqueeze_950, [3, 4, 2, 1, 0]);  unsqueeze_950 = None
    permute_1543 = torch.ops.aten.permute.default(permute_1541, [1, 3, 4, 0, 2]);  permute_1541 = None
    clone_247 = torch.ops.aten.clone.default(permute_1543, memory_format = torch.contiguous_format);  permute_1543 = None
    _unsafe_view_208 = torch.ops.aten._unsafe_view.default(clone_247, [1, 384, 384]);  clone_247 = None
    permute_1544 = torch.ops.aten.permute.default(permute_1542, [3, 4, 0, 2, 1]);  permute_1542 = None
    clone_248 = torch.ops.aten.clone.default(permute_1544, memory_format = torch.contiguous_format);  permute_1544 = None
    _unsafe_view_209 = torch.ops.aten._unsafe_view.default(clone_248, [1, 384, 384]);  clone_248 = None
    bmm_231 = torch.ops.aten.bmm.default(_unsafe_view_208, _unsafe_view_209);  _unsafe_view_208 = _unsafe_view_209 = None
    view_2865 = torch.ops.aten.view.default(bmm_231, [384, 1, 1, 1, 384]);  bmm_231 = None
    permute_1545 = torch.ops.aten.permute.default(view_2865, [3, 0, 4, 1, 2]);  view_2865 = None
    view_2866 = torch.ops.aten.view.default(permute_1545, [1, 384, 384]);  permute_1545 = None
    unsqueeze_951 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
    mul_376 = torch.ops.aten.mul.Tensor(view_2866, unsqueeze_951);  view_2866 = unsqueeze_951 = None
    add_310 = torch.ops.aten.add.Tensor(add_303, mul_376);  mul_376 = None
    split_tensor_302 = torch.ops.aten.split.Tensor(add_303, 384, dim = -2);  add_303 = None
    getitem_2778 = split_tensor_302[0];  split_tensor_302 = None
    _to_copy_1605 = torch.ops.aten._to_copy.default(getitem_2778, dtype = torch.float32);  getitem_2778 = None
    native_layer_norm_default_330 = torch.ops.aten.native_layer_norm.default(_to_copy_1605, [384], pairformer_stack_blocks_31_transition_single_layer_norm_weight, pairformer_stack_blocks_31_transition_single_layer_norm_bias, 1e-05);  _to_copy_1605 = pairformer_stack_blocks_31_transition_single_layer_norm_weight = pairformer_stack_blocks_31_transition_single_layer_norm_bias = None
    getitem_2779 = native_layer_norm_default_330[0];  native_layer_norm_default_330 = None
    _to_copy_1606 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_31_transition_single_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_31_transition_single_linear_no_bias_ab_weight = None
    _to_copy_1607 = torch.ops.aten._to_copy.default(getitem_2779, dtype = torch.bfloat16);  getitem_2779 = None
    t_595 = torch.ops.aten.t.default(_to_copy_1606);  _to_copy_1606 = None
    view_2867 = torch.ops.aten.view.default(_to_copy_1607, [384, 384]);  _to_copy_1607 = None
    mm_553 = torch.ops.aten.mm.default(view_2867, t_595);  view_2867 = t_595 = None
    view_2868 = torch.ops.aten.view.default(mm_553, [1, 384, 1536]);  mm_553 = None
    split_tensor_303 = torch.ops.aten.split.Tensor(view_2868, 768, dim = -1);  view_2868 = None
    getitem_2782 = split_tensor_303[0]
    getitem_2783 = split_tensor_303[1];  split_tensor_303 = None
    silu_78 = torch.ops.aten.silu.default(getitem_2782);  getitem_2782 = None
    mul_377 = torch.ops.aten.mul.Tensor(silu_78, getitem_2783);  silu_78 = getitem_2783 = None
    _to_copy_1608 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_31_transition_single_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_31_transition_single_linear_out_weight = None
    t_596 = torch.ops.aten.t.default(_to_copy_1608);  _to_copy_1608 = None
    view_2870 = torch.ops.aten.view.default(mul_377, [384, 768]);  mul_377 = None
    mm_554 = torch.ops.aten.mm.default(view_2870, t_596);  view_2870 = t_596 = None
    view_2871 = torch.ops.aten.view.default(mm_554, [1, 384, 384]);  mm_554 = None
    add_311 = torch.ops.aten.add.Tensor(add_310, view_2871);  add_310 = view_2871 = None
    _to_copy_1609 = torch.ops.aten._to_copy.default(add_307, dtype = torch.float32)
    native_layer_norm_default_331 = torch.ops.aten.native_layer_norm.default(_to_copy_1609, [256], pairformer_stack_blocks_32_triangle_multiplication_layernorm_z_in_weight, pairformer_stack_blocks_32_triangle_multiplication_layernorm_z_in_bias, 1e-05);  _to_copy_1609 = pairformer_stack_blocks_32_triangle_multiplication_layernorm_z_in_weight = pairformer_stack_blocks_32_triangle_multiplication_layernorm_z_in_bias = None
    getitem_2784 = native_layer_norm_default_331[0];  native_layer_norm_default_331 = None
    split_with_sizes_default_76 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_32_triangle_multiplication_merged_linear_p_weight, [512, 512]);  pairformer_stack_blocks_32_triangle_multiplication_merged_linear_p_weight = None
    getitem_2787 = split_with_sizes_default_76[0]
    getitem_2788 = split_with_sizes_default_76[1];  split_with_sizes_default_76 = None
    split_with_sizes_default_77 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_32_triangle_multiplication_merged_linear_g_weight, [512, 512, 256]);  pairformer_stack_blocks_32_triangle_multiplication_merged_linear_g_weight = None
    getitem_2789 = split_with_sizes_default_77[0]
    getitem_2790 = split_with_sizes_default_77[1]
    getitem_2791 = split_with_sizes_default_77[2];  split_with_sizes_default_77 = None
    _to_copy_1610 = torch.ops.aten._to_copy.default(getitem_2787, dtype = torch.bfloat16);  getitem_2787 = None
    _to_copy_1611 = torch.ops.aten._to_copy.default(getitem_2784, dtype = torch.bfloat16)
    t_597 = torch.ops.aten.t.default(_to_copy_1610);  _to_copy_1610 = None
    view_2872 = torch.ops.aten.view.default(_to_copy_1611, [147456, 256]);  _to_copy_1611 = None
    mm_555 = torch.ops.aten.mm.default(view_2872, t_597);  view_2872 = t_597 = None
    view_2873 = torch.ops.aten.view.default(mm_555, [1, 384, 384, 512]);  mm_555 = None
    _to_copy_1612 = torch.ops.aten._to_copy.default(getitem_2789, dtype = torch.bfloat16);  getitem_2789 = None
    _to_copy_1613 = torch.ops.aten._to_copy.default(getitem_2784, dtype = torch.bfloat16)
    t_598 = torch.ops.aten.t.default(_to_copy_1612);  _to_copy_1612 = None
    view_2874 = torch.ops.aten.view.default(_to_copy_1613, [147456, 256]);  _to_copy_1613 = None
    mm_556 = torch.ops.aten.mm.default(view_2874, t_598);  view_2874 = t_598 = None
    view_2875 = torch.ops.aten.view.default(mm_556, [1, 384, 384, 512]);  mm_556 = None
    sigmoid_228 = torch.ops.aten.sigmoid.default(view_2875);  view_2875 = None
    mul_378 = torch.ops.aten.mul.Tensor(view_2873, sigmoid_228);  view_2873 = sigmoid_228 = None
    unsqueeze_952 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
    bitwise_not_174 = torch.ops.aten.bitwise_not.default(unsqueeze_952);  unsqueeze_952 = None
    masked_fill_174 = torch.ops.aten.masked_fill.Scalar(mul_378, bitwise_not_174, 0);  mul_378 = bitwise_not_174 = None
    split_tensor_304 = torch.ops.aten.split.Tensor(masked_fill_174, 256, dim = -1)
    getitem_2794 = split_tensor_304[0];  split_tensor_304 = None
    unsqueeze_955 = torch.ops.aten.unsqueeze.default(getitem_2794, 4);  getitem_2794 = None
    permute_1550 = torch.ops.aten.permute.default(unsqueeze_955, [0, 1, 4, 3, 2]);  unsqueeze_955 = None
    permute_1551 = torch.ops.aten.permute.default(permute_1550, [3, 1, 4, 0, 2]);  permute_1550 = None
    view_2878 = torch.ops.aten.view.default(permute_1551, [256, 384, 384]);  permute_1551 = None
    split_tensor_305 = torch.ops.aten.split.Tensor(masked_fill_174, 256, dim = -1);  masked_fill_174 = None
    getitem_2797 = split_tensor_305[1];  split_tensor_305 = None
    unsqueeze_956 = torch.ops.aten.unsqueeze.default(getitem_2797, 4);  getitem_2797 = None
    permute_1552 = torch.ops.aten.permute.default(unsqueeze_956, [0, 4, 1, 3, 2]);  unsqueeze_956 = None
    permute_1553 = torch.ops.aten.permute.default(permute_1552, [3, 4, 0, 2, 1]);  permute_1552 = None
    view_2879 = torch.ops.aten.view.default(permute_1553, [256, 384, 384]);  permute_1553 = None
    bmm_232 = torch.ops.aten.bmm.default(view_2878, view_2879);  view_2878 = view_2879 = None
    view_2880 = torch.ops.aten.view.default(bmm_232, [256, 384, 1, 1, 384]);  bmm_232 = None
    permute_1554 = torch.ops.aten.permute.default(view_2880, [3, 1, 4, 0, 2]);  view_2880 = None
    view_2881 = torch.ops.aten.view.default(permute_1554, [1, 384, 384, 256]);  permute_1554 = None
    _to_copy_1614 = torch.ops.aten._to_copy.default(getitem_2788, dtype = torch.bfloat16);  getitem_2788 = None
    _to_copy_1615 = torch.ops.aten._to_copy.default(getitem_2784, dtype = torch.bfloat16)
    t_599 = torch.ops.aten.t.default(_to_copy_1614);  _to_copy_1614 = None
    view_2882 = torch.ops.aten.view.default(_to_copy_1615, [147456, 256]);  _to_copy_1615 = None
    mm_557 = torch.ops.aten.mm.default(view_2882, t_599);  view_2882 = t_599 = None
    view_2883 = torch.ops.aten.view.default(mm_557, [1, 384, 384, 512]);  mm_557 = None
    _to_copy_1616 = torch.ops.aten._to_copy.default(getitem_2790, dtype = torch.bfloat16);  getitem_2790 = None
    _to_copy_1617 = torch.ops.aten._to_copy.default(getitem_2784, dtype = torch.bfloat16)
    t_600 = torch.ops.aten.t.default(_to_copy_1616);  _to_copy_1616 = None
    view_2884 = torch.ops.aten.view.default(_to_copy_1617, [147456, 256]);  _to_copy_1617 = None
    mm_558 = torch.ops.aten.mm.default(view_2884, t_600);  view_2884 = t_600 = None
    view_2885 = torch.ops.aten.view.default(mm_558, [1, 384, 384, 512]);  mm_558 = None
    sigmoid_229 = torch.ops.aten.sigmoid.default(view_2885);  view_2885 = None
    mul_379 = torch.ops.aten.mul.Tensor(view_2883, sigmoid_229);  view_2883 = sigmoid_229 = None
    view_2886 = torch.ops.aten.view.default(mul_379, [147456, 512]);  mul_379 = None
    view_2887 = torch.ops.aten.view.default(view_2886, [1, 384, 384, 512]);  view_2886 = None
    transpose_76 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
    unsqueeze_957 = torch.ops.aten.unsqueeze.default(transpose_76, 3);  transpose_76 = None
    clone_249 = torch.ops.aten.clone.default(unsqueeze_957, memory_format = torch.contiguous_format);  unsqueeze_957 = None
    bitwise_not_175 = torch.ops.aten.bitwise_not.default(clone_249);  clone_249 = None
    masked_fill_175 = torch.ops.aten.masked_fill.Scalar(view_2887, bitwise_not_175, 0);  view_2887 = bitwise_not_175 = None
    view_2888 = torch.ops.aten.view.default(masked_fill_175, [147456, 512]);  masked_fill_175 = None
    view_2892 = torch.ops.aten.view.default(view_2888, [1, 384, 384, 512])
    split_tensor_306 = torch.ops.aten.split.Tensor(view_2892, 256, dim = -1);  view_2892 = None
    getitem_2800 = split_tensor_306[0];  split_tensor_306 = None
    unsqueeze_960 = torch.ops.aten.unsqueeze.default(getitem_2800, 4);  getitem_2800 = None
    permute_1559 = torch.ops.aten.permute.default(unsqueeze_960, [0, 2, 4, 3, 1]);  unsqueeze_960 = None
    permute_1560 = torch.ops.aten.permute.default(permute_1559, [3, 1, 4, 0, 2]);  permute_1559 = None
    view_2893 = torch.ops.aten.view.default(permute_1560, [256, 384, 384]);  permute_1560 = None
    view_2894 = torch.ops.aten.view.default(view_2888, [1, 384, 384, 512]);  view_2888 = None
    split_tensor_307 = torch.ops.aten.split.Tensor(view_2894, 256, dim = -1);  view_2894 = None
    getitem_2803 = split_tensor_307[1];  split_tensor_307 = None
    unsqueeze_961 = torch.ops.aten.unsqueeze.default(getitem_2803, 4);  getitem_2803 = None
    permute_1561 = torch.ops.aten.permute.default(unsqueeze_961, [0, 4, 2, 3, 1]);  unsqueeze_961 = None
    permute_1562 = torch.ops.aten.permute.default(permute_1561, [3, 4, 0, 2, 1]);  permute_1561 = None
    view_2895 = torch.ops.aten.view.default(permute_1562, [256, 384, 384]);  permute_1562 = None
    bmm_233 = torch.ops.aten.bmm.default(view_2893, view_2895);  view_2893 = view_2895 = None
    view_2896 = torch.ops.aten.view.default(bmm_233, [256, 384, 1, 1, 384]);  bmm_233 = None
    permute_1563 = torch.ops.aten.permute.default(view_2896, [3, 1, 4, 0, 2]);  view_2896 = None
    view_2897 = torch.ops.aten.view.default(permute_1563, [1, 384, 384, 256]);  permute_1563 = None
    _to_copy_1618 = torch.ops.aten._to_copy.default(view_2881, dtype = torch.float32);  view_2881 = None
    native_layer_norm_default_332 = torch.ops.aten.native_layer_norm.default(_to_copy_1618, [256], None, None, 1e-05);  _to_copy_1618 = None
    getitem_2804 = native_layer_norm_default_332[0];  native_layer_norm_default_332 = None
    _to_copy_1619 = torch.ops.aten._to_copy.default(view_2897, dtype = torch.float32);  view_2897 = None
    native_layer_norm_default_333 = torch.ops.aten.native_layer_norm.default(_to_copy_1619, [256], None, None, 1e-05);  _to_copy_1619 = None
    getitem_2807 = native_layer_norm_default_333[0];  native_layer_norm_default_333 = None
    add_312 = torch.ops.aten.add.Tensor(getitem_2804, getitem_2807);  getitem_2804 = getitem_2807 = None
    _to_copy_1620 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_32_triangle_multiplication_linear_z_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_32_triangle_multiplication_linear_z_out_weight = None
    _to_copy_1621 = torch.ops.aten._to_copy.default(add_312, dtype = torch.bfloat16);  add_312 = None
    t_601 = torch.ops.aten.t.default(_to_copy_1620);  _to_copy_1620 = None
    view_2898 = torch.ops.aten.view.default(_to_copy_1621, [147456, 256]);  _to_copy_1621 = None
    mm_559 = torch.ops.aten.mm.default(view_2898, t_601);  view_2898 = t_601 = None
    view_2899 = torch.ops.aten.view.default(mm_559, [1, 384, 384, 256]);  mm_559 = None
    _to_copy_1622 = torch.ops.aten._to_copy.default(getitem_2791, dtype = torch.bfloat16);  getitem_2791 = None
    _to_copy_1623 = torch.ops.aten._to_copy.default(getitem_2784, dtype = torch.bfloat16);  getitem_2784 = None
    t_602 = torch.ops.aten.t.default(_to_copy_1622);  _to_copy_1622 = None
    view_2900 = torch.ops.aten.view.default(_to_copy_1623, [147456, 256]);  _to_copy_1623 = None
    mm_560 = torch.ops.aten.mm.default(view_2900, t_602);  view_2900 = t_602 = None
    view_2901 = torch.ops.aten.view.default(mm_560, [1, 384, 384, 256]);  mm_560 = None
    sigmoid_230 = torch.ops.aten.sigmoid.default(view_2901);  view_2901 = None
    mul_380 = torch.ops.aten.mul.Tensor(view_2899, sigmoid_230);  view_2899 = sigmoid_230 = None
    add_313 = torch.ops.aten.add.Tensor(add_307, mul_380);  mul_380 = None
    _to_copy_1624 = torch.ops.aten._to_copy.default(add_307, dtype = torch.float32)
    native_layer_norm_default_334 = torch.ops.aten.native_layer_norm.default(_to_copy_1624, [256], None, None, 1e-05);  _to_copy_1624 = None
    getitem_2810 = native_layer_norm_default_334[0];  native_layer_norm_default_334 = None
    _to_copy_1625 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_32_triangle_attention_pair2b_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_32_triangle_attention_pair2b_weight = None
    _to_copy_1626 = torch.ops.aten._to_copy.default(getitem_2810, dtype = torch.bfloat16)
    t_603 = torch.ops.aten.t.default(_to_copy_1625);  _to_copy_1625 = None
    view_2902 = torch.ops.aten.view.default(_to_copy_1626, [147456, 256]);  _to_copy_1626 = None
    mm_561 = torch.ops.aten.mm.default(view_2902, t_603);  view_2902 = t_603 = None
    view_2903 = torch.ops.aten.view.default(mm_561, [1, 384, 384, 8]);  mm_561 = None
    view_2904 = torch.ops.aten.view.default(view_2903, [1, 384, 384, 2, 4]);  view_2903 = None
    permute_1564 = torch.ops.aten.permute.default(view_2904, [0, 3, 4, 1, 2]);  view_2904 = None
    view_2905 = torch.ops.aten.view.default(permute_1564, [1, 2, 4, 1, 384, 384]);  permute_1564 = None
    view_2906 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 384, 384])
    bitwise_not_176 = torch.ops.aten.bitwise_not.default(view_2906);  view_2906 = None
    masked_fill_176 = torch.ops.aten.masked_fill.Scalar(view_2905, bitwise_not_176, -10000);  view_2905 = bitwise_not_176 = None
    view_2907 = torch.ops.aten.view.default(masked_fill_176, [1, 2, 4, 384, 384]);  masked_fill_176 = None
    permute_1565 = torch.ops.aten.permute.default(view_2907, [1, 0, 2, 3, 4]);  view_2907 = None
    view_2908 = torch.ops.aten.view.default(permute_1565, [2, 4, 1, 384, 384]);  permute_1565 = None
    _to_copy_1627 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_32_triangle_attention_pair2qkvg1_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_32_triangle_attention_pair2qkvg1_weight = None
    _to_copy_1628 = torch.ops.aten._to_copy.default(getitem_2810, dtype = torch.bfloat16)
    t_604 = torch.ops.aten.t.default(_to_copy_1627);  _to_copy_1627 = None
    view_2909 = torch.ops.aten.view.default(_to_copy_1628, [147456, 256]);  _to_copy_1628 = None
    mm_562 = torch.ops.aten.mm.default(view_2909, t_604);  view_2909 = t_604 = None
    view_2910 = torch.ops.aten.view.default(mm_562, [1, 384, 384, 1024]);  mm_562 = None
    select_77 = torch.ops.aten.select.int(view_2908, 0, 0)
    view_2911 = torch.ops.aten.view.default(view_2910, [1, 384, 384, 4, 4, 64]);  view_2910 = None
    permute_1566 = torch.ops.aten.permute.default(view_2911, [4, 0, 3, 1, 2, 5]);  view_2911 = None
    view_2912 = torch.ops.aten.view.default(permute_1566, [4, 4, 384, 384, 64]);  permute_1566 = None
    unbind_int_130 = torch.ops.aten.unbind.int(view_2912);  view_2912 = None
    getitem_2813 = unbind_int_130[0]
    getitem_2814 = unbind_int_130[1]
    getitem_2815 = unbind_int_130[2]
    getitem_2816 = unbind_int_130[3];  unbind_int_130 = None
    expand_187 = torch.ops.aten.expand.default(select_77, [4, 384, 384, 384]);  select_77 = None
    _scaled_dot_product_efficient_attention_default_108 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2813, getitem_2814, getitem_2815, expand_187, False);  getitem_2813 = getitem_2814 = getitem_2815 = expand_187 = None
    getitem_2817 = _scaled_dot_product_efficient_attention_default_108[0];  _scaled_dot_product_efficient_attention_default_108 = None
    sigmoid_231 = torch.ops.aten.sigmoid.default(getitem_2816);  getitem_2816 = None
    mul_381 = torch.ops.aten.mul.Tensor(getitem_2817, sigmoid_231);  getitem_2817 = sigmoid_231 = None
    view_2913 = torch.ops.aten.view.default(mul_381, [1, 4, 384, 384, 64]);  mul_381 = None
    permute_1567 = torch.ops.aten.permute.default(view_2913, [0, 2, 3, 1, 4]);  view_2913 = None
    clone_250 = torch.ops.aten.clone.default(permute_1567, memory_format = torch.contiguous_format);  permute_1567 = None
    _unsafe_view_210 = torch.ops.aten._unsafe_view.default(clone_250, [1, 384, 384, 256]);  clone_250 = None
    transpose_77 = torch.ops.aten.transpose.int(getitem_2810, 1, 2);  getitem_2810 = None
    _to_copy_1629 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_32_triangle_attention_pair2qkvg2_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_32_triangle_attention_pair2qkvg2_weight = None
    _to_copy_1630 = torch.ops.aten._to_copy.default(transpose_77, dtype = torch.bfloat16);  transpose_77 = None
    t_605 = torch.ops.aten.t.default(_to_copy_1629);  _to_copy_1629 = None
    expand_188 = torch.ops.aten.expand.default(_to_copy_1630, [1, 384, 384, 256]);  _to_copy_1630 = None
    view_2914 = torch.ops.aten.view.default(expand_188, [384, 384, 256]);  expand_188 = None
    expand_189 = torch.ops.aten.expand.default(t_605, [1, 384, 256, 1024]);  t_605 = None
    view_2915 = torch.ops.aten.view.default(expand_189, [384, 256, 1024]);  expand_189 = None
    bmm_234 = torch.ops.aten.bmm.default(view_2914, view_2915);  view_2914 = view_2915 = None
    view_2916 = torch.ops.aten.view.default(bmm_234, [1, 384, 384, 1024]);  bmm_234 = None
    select_78 = torch.ops.aten.select.int(view_2908, 0, 1);  view_2908 = None
    view_2917 = torch.ops.aten.view.default(view_2916, [1, 384, 384, 4, 4, 64]);  view_2916 = None
    permute_1568 = torch.ops.aten.permute.default(view_2917, [4, 0, 3, 1, 2, 5]);  view_2917 = None
    view_2918 = torch.ops.aten.view.default(permute_1568, [4, 4, 384, 384, 64]);  permute_1568 = None
    unbind_int_131 = torch.ops.aten.unbind.int(view_2918);  view_2918 = None
    getitem_2821 = unbind_int_131[0]
    getitem_2822 = unbind_int_131[1]
    getitem_2823 = unbind_int_131[2]
    getitem_2824 = unbind_int_131[3];  unbind_int_131 = None
    expand_190 = torch.ops.aten.expand.default(select_78, [4, 384, 384, 384]);  select_78 = None
    _scaled_dot_product_efficient_attention_default_109 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2821, getitem_2822, getitem_2823, expand_190, False);  getitem_2821 = getitem_2822 = getitem_2823 = expand_190 = None
    getitem_2825 = _scaled_dot_product_efficient_attention_default_109[0];  _scaled_dot_product_efficient_attention_default_109 = None
    sigmoid_232 = torch.ops.aten.sigmoid.default(getitem_2824);  getitem_2824 = None
    mul_382 = torch.ops.aten.mul.Tensor(getitem_2825, sigmoid_232);  getitem_2825 = sigmoid_232 = None
    view_2919 = torch.ops.aten.view.default(mul_382, [1, 4, 384, 384, 64]);  mul_382 = None
    permute_1569 = torch.ops.aten.permute.default(view_2919, [0, 2, 3, 1, 4]);  view_2919 = None
    clone_251 = torch.ops.aten.clone.default(permute_1569, memory_format = torch.contiguous_format);  permute_1569 = None
    _unsafe_view_211 = torch.ops.aten._unsafe_view.default(clone_251, [1, 384, 384, 256]);  clone_251 = None
    cat_44 = torch.ops.aten.cat.default([_unsafe_view_210, _unsafe_view_211], dim = -1);  _unsafe_view_210 = _unsafe_view_211 = None
    slice_195 = torch.ops.aten.slice.Tensor(pairformer_stack_blocks_32_triangle_attention_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  pairformer_stack_blocks_32_triangle_attention_out_scalers = None
    unsqueeze_962 = torch.ops.aten.unsqueeze.default(slice_195, 1);  slice_195 = None
    mul_383 = torch.ops.aten.mul.Tensor(pairformer_stack_blocks_32_triangle_attention_linear_out_weight, unsqueeze_962);  pairformer_stack_blocks_32_triangle_attention_linear_out_weight = unsqueeze_962 = None
    _to_copy_1631 = torch.ops.aten._to_copy.default(mul_383, dtype = torch.bfloat16);  mul_383 = None
    t_606 = torch.ops.aten.t.default(_to_copy_1631);  _to_copy_1631 = None
    view_2920 = torch.ops.aten.view.default(cat_44, [147456, 512]);  cat_44 = None
    mm_563 = torch.ops.aten.mm.default(view_2920, t_606);  view_2920 = t_606 = None
    view_2921 = torch.ops.aten.view.default(mm_563, [1, 384, 384, 256]);  mm_563 = None
    add_314 = torch.ops.aten.add.Tensor(add_313, view_2921);  add_313 = view_2921 = None
    split_tensor_308 = torch.ops.aten.split.Tensor(add_307, 384, dim = -2)
    getitem_2829 = split_tensor_308[0];  split_tensor_308 = None
    _to_copy_1632 = torch.ops.aten._to_copy.default(getitem_2829, dtype = torch.float32);  getitem_2829 = None
    native_layer_norm_default_335 = torch.ops.aten.native_layer_norm.default(_to_copy_1632, [256], pairformer_stack_blocks_32_transition_pair_layer_norm_weight, pairformer_stack_blocks_32_transition_pair_layer_norm_bias, 1e-05);  _to_copy_1632 = pairformer_stack_blocks_32_transition_pair_layer_norm_weight = pairformer_stack_blocks_32_transition_pair_layer_norm_bias = None
    getitem_2830 = native_layer_norm_default_335[0];  native_layer_norm_default_335 = None
    _to_copy_1633 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_32_transition_pair_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_32_transition_pair_linear_no_bias_ab_weight = None
    _to_copy_1634 = torch.ops.aten._to_copy.default(getitem_2830, dtype = torch.bfloat16);  getitem_2830 = None
    t_607 = torch.ops.aten.t.default(_to_copy_1633);  _to_copy_1633 = None
    view_2922 = torch.ops.aten.view.default(_to_copy_1634, [147456, 256]);  _to_copy_1634 = None
    mm_564 = torch.ops.aten.mm.default(view_2922, t_607);  view_2922 = t_607 = None
    view_2923 = torch.ops.aten.view.default(mm_564, [1, 384, 384, 1024]);  mm_564 = None
    split_tensor_309 = torch.ops.aten.split.Tensor(view_2923, 512, dim = -1);  view_2923 = None
    getitem_2833 = split_tensor_309[0]
    getitem_2834 = split_tensor_309[1];  split_tensor_309 = None
    silu_79 = torch.ops.aten.silu.default(getitem_2833);  getitem_2833 = None
    mul_384 = torch.ops.aten.mul.Tensor(silu_79, getitem_2834);  silu_79 = getitem_2834 = None
    _to_copy_1635 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_32_transition_pair_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_32_transition_pair_linear_out_weight = None
    t_608 = torch.ops.aten.t.default(_to_copy_1635);  _to_copy_1635 = None
    view_2925 = torch.ops.aten.view.default(mul_384, [147456, 512]);  mul_384 = None
    mm_565 = torch.ops.aten.mm.default(view_2925, t_608);  view_2925 = t_608 = None
    view_2926 = torch.ops.aten.view.default(mm_565, [1, 384, 384, 256]);  mm_565 = None
    add_315 = torch.ops.aten.add.Tensor(add_314, view_2926);  add_314 = view_2926 = None
    _to_copy_1636 = torch.ops.aten._to_copy.default(add_311, dtype = torch.float32)
    native_layer_norm_default_336 = torch.ops.aten.native_layer_norm.default(_to_copy_1636, [384], pairformer_stack_blocks_32_attention_pair_bias_single_layer_norm_weight, pairformer_stack_blocks_32_attention_pair_bias_single_layer_norm_bias, 1e-05);  _to_copy_1636 = pairformer_stack_blocks_32_attention_pair_bias_single_layer_norm_weight = pairformer_stack_blocks_32_attention_pair_bias_single_layer_norm_bias = None
    getitem_2835 = native_layer_norm_default_336[0];  native_layer_norm_default_336 = None
    _to_copy_1637 = torch.ops.aten._to_copy.default(add_307, dtype = torch.float32);  add_307 = None
    native_layer_norm_default_337 = torch.ops.aten.native_layer_norm.default(_to_copy_1637, [256], pairformer_stack_blocks_32_attention_pair_bias_pair_layer_norm_weight, pairformer_stack_blocks_32_attention_pair_bias_pair_layer_norm_bias, 1e-05);  _to_copy_1637 = pairformer_stack_blocks_32_attention_pair_bias_pair_layer_norm_weight = pairformer_stack_blocks_32_attention_pair_bias_pair_layer_norm_bias = None
    getitem_2838 = native_layer_norm_default_337[0];  native_layer_norm_default_337 = None
    _to_copy_1638 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_32_attention_pair_bias_pair_linear_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_32_attention_pair_bias_pair_linear_weight = None
    _to_copy_1639 = torch.ops.aten._to_copy.default(getitem_2838, dtype = torch.bfloat16);  getitem_2838 = None
    t_609 = torch.ops.aten.t.default(_to_copy_1638);  _to_copy_1638 = None
    view_2927 = torch.ops.aten.view.default(_to_copy_1639, [147456, 256]);  _to_copy_1639 = None
    mm_566 = torch.ops.aten.mm.default(view_2927, t_609);  view_2927 = t_609 = None
    view_2928 = torch.ops.aten.view.default(mm_566, [1, 384, 384, 16]);  mm_566 = None
    permute_1570 = torch.ops.aten.permute.default(view_2928, [0, 3, 1, 2]);  view_2928 = None
    view_2929 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384])
    bitwise_not_177 = torch.ops.aten.bitwise_not.default(view_2929);  view_2929 = None
    masked_fill_177 = torch.ops.aten.masked_fill.Scalar(permute_1570, bitwise_not_177, -10000);  permute_1570 = bitwise_not_177 = None
    _to_copy_1640 = torch.ops.aten._to_copy.default(getitem_2835, dtype = torch.bfloat16);  getitem_2835 = None
    _to_copy_1641 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_32_attention_pair_bias_attention_input2qkvg_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_32_attention_pair_bias_attention_input2qkvg_weight = None
    unsqueeze_963 = torch.ops.aten.unsqueeze.default(_to_copy_1640, 3);  _to_copy_1640 = None
    unsqueeze_964 = torch.ops.aten.unsqueeze.default(unsqueeze_963, 4);  unsqueeze_963 = None
    unsqueeze_965 = torch.ops.aten.unsqueeze.default(unsqueeze_964, 5);  unsqueeze_964 = None
    permute_1571 = torch.ops.aten.permute.default(unsqueeze_965, [3, 0, 4, 1, 5, 2]);  unsqueeze_965 = None
    unsqueeze_966 = torch.ops.aten.unsqueeze.default(_to_copy_1641, 4);  _to_copy_1641 = None
    unsqueeze_967 = torch.ops.aten.unsqueeze.default(unsqueeze_966, 5);  unsqueeze_966 = None
    permute_1572 = torch.ops.aten.permute.default(unsqueeze_967, [1, 4, 2, 5, 3, 0]);  unsqueeze_967 = None
    permute_1573 = torch.ops.aten.permute.default(permute_1571, [3, 5, 0, 1, 2, 4]);  permute_1571 = None
    view_2930 = torch.ops.aten.view.default(permute_1573, [1, 384, 384]);  permute_1573 = None
    permute_1574 = torch.ops.aten.permute.default(permute_1572, [5, 0, 1, 2, 4, 3]);  permute_1572 = None
    view_2931 = torch.ops.aten.view.default(permute_1574, [1, 384, 1536]);  permute_1574 = None
    bmm_235 = torch.ops.aten.bmm.default(view_2930, view_2931);  view_2930 = view_2931 = None
    view_2932 = torch.ops.aten.view.default(bmm_235, [384, 1, 4, 1, 16, 24]);  bmm_235 = None
    permute_1575 = torch.ops.aten.permute.default(view_2932, [2, 3, 4, 0, 5, 1]);  view_2932 = None
    view_2933 = torch.ops.aten.view.default(permute_1575, [4, 1, 16, 384, 24]);  permute_1575 = None
    unbind_int_132 = torch.ops.aten.unbind.int(view_2933);  view_2933 = None
    getitem_2841 = unbind_int_132[0]
    getitem_2842 = unbind_int_132[1]
    getitem_2843 = unbind_int_132[2]
    getitem_2844 = unbind_int_132[3];  unbind_int_132 = None
    view_2934 = torch.ops.aten.view.default(pairformer_stack_blocks_32_attention_pair_bias_attention_query_bias, [1, 16, 1, 24]);  pairformer_stack_blocks_32_attention_pair_bias_attention_query_bias = None
    add_316 = torch.ops.aten.add.Tensor(getitem_2841, view_2934);  getitem_2841 = view_2934 = None
    _to_copy_1642 = torch.ops.aten._to_copy.default(add_316, dtype = torch.bfloat16);  add_316 = None
    expand_191 = torch.ops.aten.expand.default(masked_fill_177, [1, 16, 384, 384]);  masked_fill_177 = None
    _scaled_dot_product_efficient_attention_default_110 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_1642, getitem_2842, getitem_2843, expand_191, False);  _to_copy_1642 = getitem_2842 = getitem_2843 = expand_191 = None
    getitem_2845 = _scaled_dot_product_efficient_attention_default_110[0];  _scaled_dot_product_efficient_attention_default_110 = None
    add_317 = torch.ops.aten.add.Tensor(getitem_2844, 1);  getitem_2844 = None
    sigmoid_233 = torch.ops.aten.sigmoid.default(add_317);  add_317 = None
    mul_385 = torch.ops.aten.mul.Tensor(getitem_2845, sigmoid_233);  getitem_2845 = sigmoid_233 = None
    _to_copy_1643 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_32_attention_pair_bias_attention_output_proj_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_32_attention_pair_bias_attention_output_proj_weight = None
    unsqueeze_968 = torch.ops.aten.unsqueeze.default(mul_385, 4);  mul_385 = None
    permute_1576 = torch.ops.aten.permute.default(unsqueeze_968, [0, 2, 4, 3, 1]);  unsqueeze_968 = None
    unsqueeze_969 = torch.ops.aten.unsqueeze.default(_to_copy_1643, 3);  _to_copy_1643 = None
    unsqueeze_970 = torch.ops.aten.unsqueeze.default(unsqueeze_969, 4);  unsqueeze_969 = None
    permute_1577 = torch.ops.aten.permute.default(unsqueeze_970, [3, 4, 2, 1, 0]);  unsqueeze_970 = None
    permute_1578 = torch.ops.aten.permute.default(permute_1576, [1, 3, 4, 0, 2]);  permute_1576 = None
    clone_252 = torch.ops.aten.clone.default(permute_1578, memory_format = torch.contiguous_format);  permute_1578 = None
    _unsafe_view_212 = torch.ops.aten._unsafe_view.default(clone_252, [1, 384, 384]);  clone_252 = None
    permute_1579 = torch.ops.aten.permute.default(permute_1577, [3, 4, 0, 2, 1]);  permute_1577 = None
    clone_253 = torch.ops.aten.clone.default(permute_1579, memory_format = torch.contiguous_format);  permute_1579 = None
    _unsafe_view_213 = torch.ops.aten._unsafe_view.default(clone_253, [1, 384, 384]);  clone_253 = None
    bmm_236 = torch.ops.aten.bmm.default(_unsafe_view_212, _unsafe_view_213);  _unsafe_view_212 = _unsafe_view_213 = None
    view_2935 = torch.ops.aten.view.default(bmm_236, [384, 1, 1, 1, 384]);  bmm_236 = None
    permute_1580 = torch.ops.aten.permute.default(view_2935, [3, 0, 4, 1, 2]);  view_2935 = None
    view_2936 = torch.ops.aten.view.default(permute_1580, [1, 384, 384]);  permute_1580 = None
    unsqueeze_971 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
    mul_386 = torch.ops.aten.mul.Tensor(view_2936, unsqueeze_971);  view_2936 = unsqueeze_971 = None
    add_318 = torch.ops.aten.add.Tensor(add_311, mul_386);  mul_386 = None
    split_tensor_310 = torch.ops.aten.split.Tensor(add_311, 384, dim = -2);  add_311 = None
    getitem_2849 = split_tensor_310[0];  split_tensor_310 = None
    _to_copy_1644 = torch.ops.aten._to_copy.default(getitem_2849, dtype = torch.float32);  getitem_2849 = None
    native_layer_norm_default_338 = torch.ops.aten.native_layer_norm.default(_to_copy_1644, [384], pairformer_stack_blocks_32_transition_single_layer_norm_weight, pairformer_stack_blocks_32_transition_single_layer_norm_bias, 1e-05);  _to_copy_1644 = pairformer_stack_blocks_32_transition_single_layer_norm_weight = pairformer_stack_blocks_32_transition_single_layer_norm_bias = None
    getitem_2850 = native_layer_norm_default_338[0];  native_layer_norm_default_338 = None
    _to_copy_1645 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_32_transition_single_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_32_transition_single_linear_no_bias_ab_weight = None
    _to_copy_1646 = torch.ops.aten._to_copy.default(getitem_2850, dtype = torch.bfloat16);  getitem_2850 = None
    t_610 = torch.ops.aten.t.default(_to_copy_1645);  _to_copy_1645 = None
    view_2937 = torch.ops.aten.view.default(_to_copy_1646, [384, 384]);  _to_copy_1646 = None
    mm_567 = torch.ops.aten.mm.default(view_2937, t_610);  view_2937 = t_610 = None
    view_2938 = torch.ops.aten.view.default(mm_567, [1, 384, 1536]);  mm_567 = None
    split_tensor_311 = torch.ops.aten.split.Tensor(view_2938, 768, dim = -1);  view_2938 = None
    getitem_2853 = split_tensor_311[0]
    getitem_2854 = split_tensor_311[1];  split_tensor_311 = None
    silu_80 = torch.ops.aten.silu.default(getitem_2853);  getitem_2853 = None
    mul_387 = torch.ops.aten.mul.Tensor(silu_80, getitem_2854);  silu_80 = getitem_2854 = None
    _to_copy_1647 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_32_transition_single_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_32_transition_single_linear_out_weight = None
    t_611 = torch.ops.aten.t.default(_to_copy_1647);  _to_copy_1647 = None
    view_2940 = torch.ops.aten.view.default(mul_387, [384, 768]);  mul_387 = None
    mm_568 = torch.ops.aten.mm.default(view_2940, t_611);  view_2940 = t_611 = None
    view_2941 = torch.ops.aten.view.default(mm_568, [1, 384, 384]);  mm_568 = None
    add_319 = torch.ops.aten.add.Tensor(add_318, view_2941);  add_318 = view_2941 = None
    _to_copy_1648 = torch.ops.aten._to_copy.default(add_315, dtype = torch.float32)
    native_layer_norm_default_339 = torch.ops.aten.native_layer_norm.default(_to_copy_1648, [256], pairformer_stack_blocks_33_triangle_multiplication_layernorm_z_in_weight, pairformer_stack_blocks_33_triangle_multiplication_layernorm_z_in_bias, 1e-05);  _to_copy_1648 = pairformer_stack_blocks_33_triangle_multiplication_layernorm_z_in_weight = pairformer_stack_blocks_33_triangle_multiplication_layernorm_z_in_bias = None
    getitem_2855 = native_layer_norm_default_339[0];  native_layer_norm_default_339 = None
    split_with_sizes_default_78 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_33_triangle_multiplication_merged_linear_p_weight, [512, 512]);  pairformer_stack_blocks_33_triangle_multiplication_merged_linear_p_weight = None
    getitem_2858 = split_with_sizes_default_78[0]
    getitem_2859 = split_with_sizes_default_78[1];  split_with_sizes_default_78 = None
    split_with_sizes_default_79 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_33_triangle_multiplication_merged_linear_g_weight, [512, 512, 256]);  pairformer_stack_blocks_33_triangle_multiplication_merged_linear_g_weight = None
    getitem_2860 = split_with_sizes_default_79[0]
    getitem_2861 = split_with_sizes_default_79[1]
    getitem_2862 = split_with_sizes_default_79[2];  split_with_sizes_default_79 = None
    _to_copy_1649 = torch.ops.aten._to_copy.default(getitem_2858, dtype = torch.bfloat16);  getitem_2858 = None
    _to_copy_1650 = torch.ops.aten._to_copy.default(getitem_2855, dtype = torch.bfloat16)
    t_612 = torch.ops.aten.t.default(_to_copy_1649);  _to_copy_1649 = None
    view_2942 = torch.ops.aten.view.default(_to_copy_1650, [147456, 256]);  _to_copy_1650 = None
    mm_569 = torch.ops.aten.mm.default(view_2942, t_612);  view_2942 = t_612 = None
    view_2943 = torch.ops.aten.view.default(mm_569, [1, 384, 384, 512]);  mm_569 = None
    _to_copy_1651 = torch.ops.aten._to_copy.default(getitem_2860, dtype = torch.bfloat16);  getitem_2860 = None
    _to_copy_1652 = torch.ops.aten._to_copy.default(getitem_2855, dtype = torch.bfloat16)
    t_613 = torch.ops.aten.t.default(_to_copy_1651);  _to_copy_1651 = None
    view_2944 = torch.ops.aten.view.default(_to_copy_1652, [147456, 256]);  _to_copy_1652 = None
    mm_570 = torch.ops.aten.mm.default(view_2944, t_613);  view_2944 = t_613 = None
    view_2945 = torch.ops.aten.view.default(mm_570, [1, 384, 384, 512]);  mm_570 = None
    sigmoid_234 = torch.ops.aten.sigmoid.default(view_2945);  view_2945 = None
    mul_388 = torch.ops.aten.mul.Tensor(view_2943, sigmoid_234);  view_2943 = sigmoid_234 = None
    unsqueeze_972 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
    bitwise_not_178 = torch.ops.aten.bitwise_not.default(unsqueeze_972);  unsqueeze_972 = None
    masked_fill_178 = torch.ops.aten.masked_fill.Scalar(mul_388, bitwise_not_178, 0);  mul_388 = bitwise_not_178 = None
    split_tensor_312 = torch.ops.aten.split.Tensor(masked_fill_178, 256, dim = -1)
    getitem_2865 = split_tensor_312[0];  split_tensor_312 = None
    unsqueeze_975 = torch.ops.aten.unsqueeze.default(getitem_2865, 4);  getitem_2865 = None
    permute_1585 = torch.ops.aten.permute.default(unsqueeze_975, [0, 1, 4, 3, 2]);  unsqueeze_975 = None
    permute_1586 = torch.ops.aten.permute.default(permute_1585, [3, 1, 4, 0, 2]);  permute_1585 = None
    view_2948 = torch.ops.aten.view.default(permute_1586, [256, 384, 384]);  permute_1586 = None
    split_tensor_313 = torch.ops.aten.split.Tensor(masked_fill_178, 256, dim = -1);  masked_fill_178 = None
    getitem_2868 = split_tensor_313[1];  split_tensor_313 = None
    unsqueeze_976 = torch.ops.aten.unsqueeze.default(getitem_2868, 4);  getitem_2868 = None
    permute_1587 = torch.ops.aten.permute.default(unsqueeze_976, [0, 4, 1, 3, 2]);  unsqueeze_976 = None
    permute_1588 = torch.ops.aten.permute.default(permute_1587, [3, 4, 0, 2, 1]);  permute_1587 = None
    view_2949 = torch.ops.aten.view.default(permute_1588, [256, 384, 384]);  permute_1588 = None
    bmm_237 = torch.ops.aten.bmm.default(view_2948, view_2949);  view_2948 = view_2949 = None
    view_2950 = torch.ops.aten.view.default(bmm_237, [256, 384, 1, 1, 384]);  bmm_237 = None
    permute_1589 = torch.ops.aten.permute.default(view_2950, [3, 1, 4, 0, 2]);  view_2950 = None
    view_2951 = torch.ops.aten.view.default(permute_1589, [1, 384, 384, 256]);  permute_1589 = None
    _to_copy_1653 = torch.ops.aten._to_copy.default(getitem_2859, dtype = torch.bfloat16);  getitem_2859 = None
    _to_copy_1654 = torch.ops.aten._to_copy.default(getitem_2855, dtype = torch.bfloat16)
    t_614 = torch.ops.aten.t.default(_to_copy_1653);  _to_copy_1653 = None
    view_2952 = torch.ops.aten.view.default(_to_copy_1654, [147456, 256]);  _to_copy_1654 = None
    mm_571 = torch.ops.aten.mm.default(view_2952, t_614);  view_2952 = t_614 = None
    view_2953 = torch.ops.aten.view.default(mm_571, [1, 384, 384, 512]);  mm_571 = None
    _to_copy_1655 = torch.ops.aten._to_copy.default(getitem_2861, dtype = torch.bfloat16);  getitem_2861 = None
    _to_copy_1656 = torch.ops.aten._to_copy.default(getitem_2855, dtype = torch.bfloat16)
    t_615 = torch.ops.aten.t.default(_to_copy_1655);  _to_copy_1655 = None
    view_2954 = torch.ops.aten.view.default(_to_copy_1656, [147456, 256]);  _to_copy_1656 = None
    mm_572 = torch.ops.aten.mm.default(view_2954, t_615);  view_2954 = t_615 = None
    view_2955 = torch.ops.aten.view.default(mm_572, [1, 384, 384, 512]);  mm_572 = None
    sigmoid_235 = torch.ops.aten.sigmoid.default(view_2955);  view_2955 = None
    mul_389 = torch.ops.aten.mul.Tensor(view_2953, sigmoid_235);  view_2953 = sigmoid_235 = None
    view_2956 = torch.ops.aten.view.default(mul_389, [147456, 512]);  mul_389 = None
    view_2957 = torch.ops.aten.view.default(view_2956, [1, 384, 384, 512]);  view_2956 = None
    transpose_78 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
    unsqueeze_977 = torch.ops.aten.unsqueeze.default(transpose_78, 3);  transpose_78 = None
    clone_254 = torch.ops.aten.clone.default(unsqueeze_977, memory_format = torch.contiguous_format);  unsqueeze_977 = None
    bitwise_not_179 = torch.ops.aten.bitwise_not.default(clone_254);  clone_254 = None
    masked_fill_179 = torch.ops.aten.masked_fill.Scalar(view_2957, bitwise_not_179, 0);  view_2957 = bitwise_not_179 = None
    view_2958 = torch.ops.aten.view.default(masked_fill_179, [147456, 512]);  masked_fill_179 = None
    view_2962 = torch.ops.aten.view.default(view_2958, [1, 384, 384, 512])
    split_tensor_314 = torch.ops.aten.split.Tensor(view_2962, 256, dim = -1);  view_2962 = None
    getitem_2871 = split_tensor_314[0];  split_tensor_314 = None
    unsqueeze_980 = torch.ops.aten.unsqueeze.default(getitem_2871, 4);  getitem_2871 = None
    permute_1594 = torch.ops.aten.permute.default(unsqueeze_980, [0, 2, 4, 3, 1]);  unsqueeze_980 = None
    permute_1595 = torch.ops.aten.permute.default(permute_1594, [3, 1, 4, 0, 2]);  permute_1594 = None
    view_2963 = torch.ops.aten.view.default(permute_1595, [256, 384, 384]);  permute_1595 = None
    view_2964 = torch.ops.aten.view.default(view_2958, [1, 384, 384, 512]);  view_2958 = None
    split_tensor_315 = torch.ops.aten.split.Tensor(view_2964, 256, dim = -1);  view_2964 = None
    getitem_2874 = split_tensor_315[1];  split_tensor_315 = None
    unsqueeze_981 = torch.ops.aten.unsqueeze.default(getitem_2874, 4);  getitem_2874 = None
    permute_1596 = torch.ops.aten.permute.default(unsqueeze_981, [0, 4, 2, 3, 1]);  unsqueeze_981 = None
    permute_1597 = torch.ops.aten.permute.default(permute_1596, [3, 4, 0, 2, 1]);  permute_1596 = None
    view_2965 = torch.ops.aten.view.default(permute_1597, [256, 384, 384]);  permute_1597 = None
    bmm_238 = torch.ops.aten.bmm.default(view_2963, view_2965);  view_2963 = view_2965 = None
    view_2966 = torch.ops.aten.view.default(bmm_238, [256, 384, 1, 1, 384]);  bmm_238 = None
    permute_1598 = torch.ops.aten.permute.default(view_2966, [3, 1, 4, 0, 2]);  view_2966 = None
    view_2967 = torch.ops.aten.view.default(permute_1598, [1, 384, 384, 256]);  permute_1598 = None
    _to_copy_1657 = torch.ops.aten._to_copy.default(view_2951, dtype = torch.float32);  view_2951 = None
    native_layer_norm_default_340 = torch.ops.aten.native_layer_norm.default(_to_copy_1657, [256], None, None, 1e-05);  _to_copy_1657 = None
    getitem_2875 = native_layer_norm_default_340[0];  native_layer_norm_default_340 = None
    _to_copy_1658 = torch.ops.aten._to_copy.default(view_2967, dtype = torch.float32);  view_2967 = None
    native_layer_norm_default_341 = torch.ops.aten.native_layer_norm.default(_to_copy_1658, [256], None, None, 1e-05);  _to_copy_1658 = None
    getitem_2878 = native_layer_norm_default_341[0];  native_layer_norm_default_341 = None
    add_320 = torch.ops.aten.add.Tensor(getitem_2875, getitem_2878);  getitem_2875 = getitem_2878 = None
    _to_copy_1659 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_33_triangle_multiplication_linear_z_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_33_triangle_multiplication_linear_z_out_weight = None
    _to_copy_1660 = torch.ops.aten._to_copy.default(add_320, dtype = torch.bfloat16);  add_320 = None
    t_616 = torch.ops.aten.t.default(_to_copy_1659);  _to_copy_1659 = None
    view_2968 = torch.ops.aten.view.default(_to_copy_1660, [147456, 256]);  _to_copy_1660 = None
    mm_573 = torch.ops.aten.mm.default(view_2968, t_616);  view_2968 = t_616 = None
    view_2969 = torch.ops.aten.view.default(mm_573, [1, 384, 384, 256]);  mm_573 = None
    _to_copy_1661 = torch.ops.aten._to_copy.default(getitem_2862, dtype = torch.bfloat16);  getitem_2862 = None
    _to_copy_1662 = torch.ops.aten._to_copy.default(getitem_2855, dtype = torch.bfloat16);  getitem_2855 = None
    t_617 = torch.ops.aten.t.default(_to_copy_1661);  _to_copy_1661 = None
    view_2970 = torch.ops.aten.view.default(_to_copy_1662, [147456, 256]);  _to_copy_1662 = None
    mm_574 = torch.ops.aten.mm.default(view_2970, t_617);  view_2970 = t_617 = None
    view_2971 = torch.ops.aten.view.default(mm_574, [1, 384, 384, 256]);  mm_574 = None
    sigmoid_236 = torch.ops.aten.sigmoid.default(view_2971);  view_2971 = None
    mul_390 = torch.ops.aten.mul.Tensor(view_2969, sigmoid_236);  view_2969 = sigmoid_236 = None
    add_321 = torch.ops.aten.add.Tensor(add_315, mul_390);  mul_390 = None
    _to_copy_1663 = torch.ops.aten._to_copy.default(add_315, dtype = torch.float32)
    native_layer_norm_default_342 = torch.ops.aten.native_layer_norm.default(_to_copy_1663, [256], None, None, 1e-05);  _to_copy_1663 = None
    getitem_2881 = native_layer_norm_default_342[0];  native_layer_norm_default_342 = None
    _to_copy_1664 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_33_triangle_attention_pair2b_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_33_triangle_attention_pair2b_weight = None
    _to_copy_1665 = torch.ops.aten._to_copy.default(getitem_2881, dtype = torch.bfloat16)
    t_618 = torch.ops.aten.t.default(_to_copy_1664);  _to_copy_1664 = None
    view_2972 = torch.ops.aten.view.default(_to_copy_1665, [147456, 256]);  _to_copy_1665 = None
    mm_575 = torch.ops.aten.mm.default(view_2972, t_618);  view_2972 = t_618 = None
    view_2973 = torch.ops.aten.view.default(mm_575, [1, 384, 384, 8]);  mm_575 = None
    view_2974 = torch.ops.aten.view.default(view_2973, [1, 384, 384, 2, 4]);  view_2973 = None
    permute_1599 = torch.ops.aten.permute.default(view_2974, [0, 3, 4, 1, 2]);  view_2974 = None
    view_2975 = torch.ops.aten.view.default(permute_1599, [1, 2, 4, 1, 384, 384]);  permute_1599 = None
    view_2976 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 384, 384])
    bitwise_not_180 = torch.ops.aten.bitwise_not.default(view_2976);  view_2976 = None
    masked_fill_180 = torch.ops.aten.masked_fill.Scalar(view_2975, bitwise_not_180, -10000);  view_2975 = bitwise_not_180 = None
    view_2977 = torch.ops.aten.view.default(masked_fill_180, [1, 2, 4, 384, 384]);  masked_fill_180 = None
    permute_1600 = torch.ops.aten.permute.default(view_2977, [1, 0, 2, 3, 4]);  view_2977 = None
    view_2978 = torch.ops.aten.view.default(permute_1600, [2, 4, 1, 384, 384]);  permute_1600 = None
    _to_copy_1666 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_33_triangle_attention_pair2qkvg1_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_33_triangle_attention_pair2qkvg1_weight = None
    _to_copy_1667 = torch.ops.aten._to_copy.default(getitem_2881, dtype = torch.bfloat16)
    t_619 = torch.ops.aten.t.default(_to_copy_1666);  _to_copy_1666 = None
    view_2979 = torch.ops.aten.view.default(_to_copy_1667, [147456, 256]);  _to_copy_1667 = None
    mm_576 = torch.ops.aten.mm.default(view_2979, t_619);  view_2979 = t_619 = None
    view_2980 = torch.ops.aten.view.default(mm_576, [1, 384, 384, 1024]);  mm_576 = None
    select_79 = torch.ops.aten.select.int(view_2978, 0, 0)
    view_2981 = torch.ops.aten.view.default(view_2980, [1, 384, 384, 4, 4, 64]);  view_2980 = None
    permute_1601 = torch.ops.aten.permute.default(view_2981, [4, 0, 3, 1, 2, 5]);  view_2981 = None
    view_2982 = torch.ops.aten.view.default(permute_1601, [4, 4, 384, 384, 64]);  permute_1601 = None
    unbind_int_133 = torch.ops.aten.unbind.int(view_2982);  view_2982 = None
    getitem_2884 = unbind_int_133[0]
    getitem_2885 = unbind_int_133[1]
    getitem_2886 = unbind_int_133[2]
    getitem_2887 = unbind_int_133[3];  unbind_int_133 = None
    expand_192 = torch.ops.aten.expand.default(select_79, [4, 384, 384, 384]);  select_79 = None
    _scaled_dot_product_efficient_attention_default_111 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2884, getitem_2885, getitem_2886, expand_192, False);  getitem_2884 = getitem_2885 = getitem_2886 = expand_192 = None
    getitem_2888 = _scaled_dot_product_efficient_attention_default_111[0];  _scaled_dot_product_efficient_attention_default_111 = None
    sigmoid_237 = torch.ops.aten.sigmoid.default(getitem_2887);  getitem_2887 = None
    mul_391 = torch.ops.aten.mul.Tensor(getitem_2888, sigmoid_237);  getitem_2888 = sigmoid_237 = None
    view_2983 = torch.ops.aten.view.default(mul_391, [1, 4, 384, 384, 64]);  mul_391 = None
    permute_1602 = torch.ops.aten.permute.default(view_2983, [0, 2, 3, 1, 4]);  view_2983 = None
    clone_255 = torch.ops.aten.clone.default(permute_1602, memory_format = torch.contiguous_format);  permute_1602 = None
    _unsafe_view_214 = torch.ops.aten._unsafe_view.default(clone_255, [1, 384, 384, 256]);  clone_255 = None
    transpose_79 = torch.ops.aten.transpose.int(getitem_2881, 1, 2);  getitem_2881 = None
    _to_copy_1668 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_33_triangle_attention_pair2qkvg2_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_33_triangle_attention_pair2qkvg2_weight = None
    _to_copy_1669 = torch.ops.aten._to_copy.default(transpose_79, dtype = torch.bfloat16);  transpose_79 = None
    t_620 = torch.ops.aten.t.default(_to_copy_1668);  _to_copy_1668 = None
    expand_193 = torch.ops.aten.expand.default(_to_copy_1669, [1, 384, 384, 256]);  _to_copy_1669 = None
    view_2984 = torch.ops.aten.view.default(expand_193, [384, 384, 256]);  expand_193 = None
    expand_194 = torch.ops.aten.expand.default(t_620, [1, 384, 256, 1024]);  t_620 = None
    view_2985 = torch.ops.aten.view.default(expand_194, [384, 256, 1024]);  expand_194 = None
    bmm_239 = torch.ops.aten.bmm.default(view_2984, view_2985);  view_2984 = view_2985 = None
    view_2986 = torch.ops.aten.view.default(bmm_239, [1, 384, 384, 1024]);  bmm_239 = None
    select_80 = torch.ops.aten.select.int(view_2978, 0, 1);  view_2978 = None
    view_2987 = torch.ops.aten.view.default(view_2986, [1, 384, 384, 4, 4, 64]);  view_2986 = None
    permute_1603 = torch.ops.aten.permute.default(view_2987, [4, 0, 3, 1, 2, 5]);  view_2987 = None
    view_2988 = torch.ops.aten.view.default(permute_1603, [4, 4, 384, 384, 64]);  permute_1603 = None
    unbind_int_134 = torch.ops.aten.unbind.int(view_2988);  view_2988 = None
    getitem_2892 = unbind_int_134[0]
    getitem_2893 = unbind_int_134[1]
    getitem_2894 = unbind_int_134[2]
    getitem_2895 = unbind_int_134[3];  unbind_int_134 = None
    expand_195 = torch.ops.aten.expand.default(select_80, [4, 384, 384, 384]);  select_80 = None
    _scaled_dot_product_efficient_attention_default_112 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2892, getitem_2893, getitem_2894, expand_195, False);  getitem_2892 = getitem_2893 = getitem_2894 = expand_195 = None
    getitem_2896 = _scaled_dot_product_efficient_attention_default_112[0];  _scaled_dot_product_efficient_attention_default_112 = None
    sigmoid_238 = torch.ops.aten.sigmoid.default(getitem_2895);  getitem_2895 = None
    mul_392 = torch.ops.aten.mul.Tensor(getitem_2896, sigmoid_238);  getitem_2896 = sigmoid_238 = None
    view_2989 = torch.ops.aten.view.default(mul_392, [1, 4, 384, 384, 64]);  mul_392 = None
    permute_1604 = torch.ops.aten.permute.default(view_2989, [0, 2, 3, 1, 4]);  view_2989 = None
    clone_256 = torch.ops.aten.clone.default(permute_1604, memory_format = torch.contiguous_format);  permute_1604 = None
    _unsafe_view_215 = torch.ops.aten._unsafe_view.default(clone_256, [1, 384, 384, 256]);  clone_256 = None
    cat_45 = torch.ops.aten.cat.default([_unsafe_view_214, _unsafe_view_215], dim = -1);  _unsafe_view_214 = _unsafe_view_215 = None
    slice_196 = torch.ops.aten.slice.Tensor(pairformer_stack_blocks_33_triangle_attention_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  pairformer_stack_blocks_33_triangle_attention_out_scalers = None
    unsqueeze_982 = torch.ops.aten.unsqueeze.default(slice_196, 1);  slice_196 = None
    mul_393 = torch.ops.aten.mul.Tensor(pairformer_stack_blocks_33_triangle_attention_linear_out_weight, unsqueeze_982);  pairformer_stack_blocks_33_triangle_attention_linear_out_weight = unsqueeze_982 = None
    _to_copy_1670 = torch.ops.aten._to_copy.default(mul_393, dtype = torch.bfloat16);  mul_393 = None
    t_621 = torch.ops.aten.t.default(_to_copy_1670);  _to_copy_1670 = None
    view_2990 = torch.ops.aten.view.default(cat_45, [147456, 512]);  cat_45 = None
    mm_577 = torch.ops.aten.mm.default(view_2990, t_621);  view_2990 = t_621 = None
    view_2991 = torch.ops.aten.view.default(mm_577, [1, 384, 384, 256]);  mm_577 = None
    add_322 = torch.ops.aten.add.Tensor(add_321, view_2991);  add_321 = view_2991 = None
    split_tensor_316 = torch.ops.aten.split.Tensor(add_315, 384, dim = -2)
    getitem_2900 = split_tensor_316[0];  split_tensor_316 = None
    _to_copy_1671 = torch.ops.aten._to_copy.default(getitem_2900, dtype = torch.float32);  getitem_2900 = None
    native_layer_norm_default_343 = torch.ops.aten.native_layer_norm.default(_to_copy_1671, [256], pairformer_stack_blocks_33_transition_pair_layer_norm_weight, pairformer_stack_blocks_33_transition_pair_layer_norm_bias, 1e-05);  _to_copy_1671 = pairformer_stack_blocks_33_transition_pair_layer_norm_weight = pairformer_stack_blocks_33_transition_pair_layer_norm_bias = None
    getitem_2901 = native_layer_norm_default_343[0];  native_layer_norm_default_343 = None
    _to_copy_1672 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_33_transition_pair_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_33_transition_pair_linear_no_bias_ab_weight = None
    _to_copy_1673 = torch.ops.aten._to_copy.default(getitem_2901, dtype = torch.bfloat16);  getitem_2901 = None
    t_622 = torch.ops.aten.t.default(_to_copy_1672);  _to_copy_1672 = None
    view_2992 = torch.ops.aten.view.default(_to_copy_1673, [147456, 256]);  _to_copy_1673 = None
    mm_578 = torch.ops.aten.mm.default(view_2992, t_622);  view_2992 = t_622 = None
    view_2993 = torch.ops.aten.view.default(mm_578, [1, 384, 384, 1024]);  mm_578 = None
    split_tensor_317 = torch.ops.aten.split.Tensor(view_2993, 512, dim = -1);  view_2993 = None
    getitem_2904 = split_tensor_317[0]
    getitem_2905 = split_tensor_317[1];  split_tensor_317 = None
    silu_81 = torch.ops.aten.silu.default(getitem_2904);  getitem_2904 = None
    mul_394 = torch.ops.aten.mul.Tensor(silu_81, getitem_2905);  silu_81 = getitem_2905 = None
    _to_copy_1674 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_33_transition_pair_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_33_transition_pair_linear_out_weight = None
    t_623 = torch.ops.aten.t.default(_to_copy_1674);  _to_copy_1674 = None
    view_2995 = torch.ops.aten.view.default(mul_394, [147456, 512]);  mul_394 = None
    mm_579 = torch.ops.aten.mm.default(view_2995, t_623);  view_2995 = t_623 = None
    view_2996 = torch.ops.aten.view.default(mm_579, [1, 384, 384, 256]);  mm_579 = None
    add_323 = torch.ops.aten.add.Tensor(add_322, view_2996);  add_322 = view_2996 = None
    _to_copy_1675 = torch.ops.aten._to_copy.default(add_319, dtype = torch.float32)
    native_layer_norm_default_344 = torch.ops.aten.native_layer_norm.default(_to_copy_1675, [384], pairformer_stack_blocks_33_attention_pair_bias_single_layer_norm_weight, pairformer_stack_blocks_33_attention_pair_bias_single_layer_norm_bias, 1e-05);  _to_copy_1675 = pairformer_stack_blocks_33_attention_pair_bias_single_layer_norm_weight = pairformer_stack_blocks_33_attention_pair_bias_single_layer_norm_bias = None
    getitem_2906 = native_layer_norm_default_344[0];  native_layer_norm_default_344 = None
    _to_copy_1676 = torch.ops.aten._to_copy.default(add_315, dtype = torch.float32);  add_315 = None
    native_layer_norm_default_345 = torch.ops.aten.native_layer_norm.default(_to_copy_1676, [256], pairformer_stack_blocks_33_attention_pair_bias_pair_layer_norm_weight, pairformer_stack_blocks_33_attention_pair_bias_pair_layer_norm_bias, 1e-05);  _to_copy_1676 = pairformer_stack_blocks_33_attention_pair_bias_pair_layer_norm_weight = pairformer_stack_blocks_33_attention_pair_bias_pair_layer_norm_bias = None
    getitem_2909 = native_layer_norm_default_345[0];  native_layer_norm_default_345 = None
    _to_copy_1677 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_33_attention_pair_bias_pair_linear_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_33_attention_pair_bias_pair_linear_weight = None
    _to_copy_1678 = torch.ops.aten._to_copy.default(getitem_2909, dtype = torch.bfloat16);  getitem_2909 = None
    t_624 = torch.ops.aten.t.default(_to_copy_1677);  _to_copy_1677 = None
    view_2997 = torch.ops.aten.view.default(_to_copy_1678, [147456, 256]);  _to_copy_1678 = None
    mm_580 = torch.ops.aten.mm.default(view_2997, t_624);  view_2997 = t_624 = None
    view_2998 = torch.ops.aten.view.default(mm_580, [1, 384, 384, 16]);  mm_580 = None
    permute_1605 = torch.ops.aten.permute.default(view_2998, [0, 3, 1, 2]);  view_2998 = None
    view_2999 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384])
    bitwise_not_181 = torch.ops.aten.bitwise_not.default(view_2999);  view_2999 = None
    masked_fill_181 = torch.ops.aten.masked_fill.Scalar(permute_1605, bitwise_not_181, -10000);  permute_1605 = bitwise_not_181 = None
    _to_copy_1679 = torch.ops.aten._to_copy.default(getitem_2906, dtype = torch.bfloat16);  getitem_2906 = None
    _to_copy_1680 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_33_attention_pair_bias_attention_input2qkvg_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_33_attention_pair_bias_attention_input2qkvg_weight = None
    unsqueeze_983 = torch.ops.aten.unsqueeze.default(_to_copy_1679, 3);  _to_copy_1679 = None
    unsqueeze_984 = torch.ops.aten.unsqueeze.default(unsqueeze_983, 4);  unsqueeze_983 = None
    unsqueeze_985 = torch.ops.aten.unsqueeze.default(unsqueeze_984, 5);  unsqueeze_984 = None
    permute_1606 = torch.ops.aten.permute.default(unsqueeze_985, [3, 0, 4, 1, 5, 2]);  unsqueeze_985 = None
    unsqueeze_986 = torch.ops.aten.unsqueeze.default(_to_copy_1680, 4);  _to_copy_1680 = None
    unsqueeze_987 = torch.ops.aten.unsqueeze.default(unsqueeze_986, 5);  unsqueeze_986 = None
    permute_1607 = torch.ops.aten.permute.default(unsqueeze_987, [1, 4, 2, 5, 3, 0]);  unsqueeze_987 = None
    permute_1608 = torch.ops.aten.permute.default(permute_1606, [3, 5, 0, 1, 2, 4]);  permute_1606 = None
    view_3000 = torch.ops.aten.view.default(permute_1608, [1, 384, 384]);  permute_1608 = None
    permute_1609 = torch.ops.aten.permute.default(permute_1607, [5, 0, 1, 2, 4, 3]);  permute_1607 = None
    view_3001 = torch.ops.aten.view.default(permute_1609, [1, 384, 1536]);  permute_1609 = None
    bmm_240 = torch.ops.aten.bmm.default(view_3000, view_3001);  view_3000 = view_3001 = None
    view_3002 = torch.ops.aten.view.default(bmm_240, [384, 1, 4, 1, 16, 24]);  bmm_240 = None
    permute_1610 = torch.ops.aten.permute.default(view_3002, [2, 3, 4, 0, 5, 1]);  view_3002 = None
    view_3003 = torch.ops.aten.view.default(permute_1610, [4, 1, 16, 384, 24]);  permute_1610 = None
    unbind_int_135 = torch.ops.aten.unbind.int(view_3003);  view_3003 = None
    getitem_2912 = unbind_int_135[0]
    getitem_2913 = unbind_int_135[1]
    getitem_2914 = unbind_int_135[2]
    getitem_2915 = unbind_int_135[3];  unbind_int_135 = None
    view_3004 = torch.ops.aten.view.default(pairformer_stack_blocks_33_attention_pair_bias_attention_query_bias, [1, 16, 1, 24]);  pairformer_stack_blocks_33_attention_pair_bias_attention_query_bias = None
    add_324 = torch.ops.aten.add.Tensor(getitem_2912, view_3004);  getitem_2912 = view_3004 = None
    _to_copy_1681 = torch.ops.aten._to_copy.default(add_324, dtype = torch.bfloat16);  add_324 = None
    expand_196 = torch.ops.aten.expand.default(masked_fill_181, [1, 16, 384, 384]);  masked_fill_181 = None
    _scaled_dot_product_efficient_attention_default_113 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_1681, getitem_2913, getitem_2914, expand_196, False);  _to_copy_1681 = getitem_2913 = getitem_2914 = expand_196 = None
    getitem_2916 = _scaled_dot_product_efficient_attention_default_113[0];  _scaled_dot_product_efficient_attention_default_113 = None
    add_325 = torch.ops.aten.add.Tensor(getitem_2915, 1);  getitem_2915 = None
    sigmoid_239 = torch.ops.aten.sigmoid.default(add_325);  add_325 = None
    mul_395 = torch.ops.aten.mul.Tensor(getitem_2916, sigmoid_239);  getitem_2916 = sigmoid_239 = None
    _to_copy_1682 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_33_attention_pair_bias_attention_output_proj_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_33_attention_pair_bias_attention_output_proj_weight = None
    unsqueeze_988 = torch.ops.aten.unsqueeze.default(mul_395, 4);  mul_395 = None
    permute_1611 = torch.ops.aten.permute.default(unsqueeze_988, [0, 2, 4, 3, 1]);  unsqueeze_988 = None
    unsqueeze_989 = torch.ops.aten.unsqueeze.default(_to_copy_1682, 3);  _to_copy_1682 = None
    unsqueeze_990 = torch.ops.aten.unsqueeze.default(unsqueeze_989, 4);  unsqueeze_989 = None
    permute_1612 = torch.ops.aten.permute.default(unsqueeze_990, [3, 4, 2, 1, 0]);  unsqueeze_990 = None
    permute_1613 = torch.ops.aten.permute.default(permute_1611, [1, 3, 4, 0, 2]);  permute_1611 = None
    clone_257 = torch.ops.aten.clone.default(permute_1613, memory_format = torch.contiguous_format);  permute_1613 = None
    _unsafe_view_216 = torch.ops.aten._unsafe_view.default(clone_257, [1, 384, 384]);  clone_257 = None
    permute_1614 = torch.ops.aten.permute.default(permute_1612, [3, 4, 0, 2, 1]);  permute_1612 = None
    clone_258 = torch.ops.aten.clone.default(permute_1614, memory_format = torch.contiguous_format);  permute_1614 = None
    _unsafe_view_217 = torch.ops.aten._unsafe_view.default(clone_258, [1, 384, 384]);  clone_258 = None
    bmm_241 = torch.ops.aten.bmm.default(_unsafe_view_216, _unsafe_view_217);  _unsafe_view_216 = _unsafe_view_217 = None
    view_3005 = torch.ops.aten.view.default(bmm_241, [384, 1, 1, 1, 384]);  bmm_241 = None
    permute_1615 = torch.ops.aten.permute.default(view_3005, [3, 0, 4, 1, 2]);  view_3005 = None
    view_3006 = torch.ops.aten.view.default(permute_1615, [1, 384, 384]);  permute_1615 = None
    unsqueeze_991 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
    mul_396 = torch.ops.aten.mul.Tensor(view_3006, unsqueeze_991);  view_3006 = unsqueeze_991 = None
    add_326 = torch.ops.aten.add.Tensor(add_319, mul_396);  mul_396 = None
    split_tensor_318 = torch.ops.aten.split.Tensor(add_319, 384, dim = -2);  add_319 = None
    getitem_2920 = split_tensor_318[0];  split_tensor_318 = None
    _to_copy_1683 = torch.ops.aten._to_copy.default(getitem_2920, dtype = torch.float32);  getitem_2920 = None
    native_layer_norm_default_346 = torch.ops.aten.native_layer_norm.default(_to_copy_1683, [384], pairformer_stack_blocks_33_transition_single_layer_norm_weight, pairformer_stack_blocks_33_transition_single_layer_norm_bias, 1e-05);  _to_copy_1683 = pairformer_stack_blocks_33_transition_single_layer_norm_weight = pairformer_stack_blocks_33_transition_single_layer_norm_bias = None
    getitem_2921 = native_layer_norm_default_346[0];  native_layer_norm_default_346 = None
    _to_copy_1684 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_33_transition_single_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_33_transition_single_linear_no_bias_ab_weight = None
    _to_copy_1685 = torch.ops.aten._to_copy.default(getitem_2921, dtype = torch.bfloat16);  getitem_2921 = None
    t_625 = torch.ops.aten.t.default(_to_copy_1684);  _to_copy_1684 = None
    view_3007 = torch.ops.aten.view.default(_to_copy_1685, [384, 384]);  _to_copy_1685 = None
    mm_581 = torch.ops.aten.mm.default(view_3007, t_625);  view_3007 = t_625 = None
    view_3008 = torch.ops.aten.view.default(mm_581, [1, 384, 1536]);  mm_581 = None
    split_tensor_319 = torch.ops.aten.split.Tensor(view_3008, 768, dim = -1);  view_3008 = None
    getitem_2924 = split_tensor_319[0]
    getitem_2925 = split_tensor_319[1];  split_tensor_319 = None
    silu_82 = torch.ops.aten.silu.default(getitem_2924);  getitem_2924 = None
    mul_397 = torch.ops.aten.mul.Tensor(silu_82, getitem_2925);  silu_82 = getitem_2925 = None
    _to_copy_1686 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_33_transition_single_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_33_transition_single_linear_out_weight = None
    t_626 = torch.ops.aten.t.default(_to_copy_1686);  _to_copy_1686 = None
    view_3010 = torch.ops.aten.view.default(mul_397, [384, 768]);  mul_397 = None
    mm_582 = torch.ops.aten.mm.default(view_3010, t_626);  view_3010 = t_626 = None
    view_3011 = torch.ops.aten.view.default(mm_582, [1, 384, 384]);  mm_582 = None
    add_327 = torch.ops.aten.add.Tensor(add_326, view_3011);  add_326 = view_3011 = None
    _to_copy_1687 = torch.ops.aten._to_copy.default(add_323, dtype = torch.float32)
    native_layer_norm_default_347 = torch.ops.aten.native_layer_norm.default(_to_copy_1687, [256], pairformer_stack_blocks_34_triangle_multiplication_layernorm_z_in_weight, pairformer_stack_blocks_34_triangle_multiplication_layernorm_z_in_bias, 1e-05);  _to_copy_1687 = pairformer_stack_blocks_34_triangle_multiplication_layernorm_z_in_weight = pairformer_stack_blocks_34_triangle_multiplication_layernorm_z_in_bias = None
    getitem_2926 = native_layer_norm_default_347[0];  native_layer_norm_default_347 = None
    split_with_sizes_default_80 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_34_triangle_multiplication_merged_linear_p_weight, [512, 512]);  pairformer_stack_blocks_34_triangle_multiplication_merged_linear_p_weight = None
    getitem_2929 = split_with_sizes_default_80[0]
    getitem_2930 = split_with_sizes_default_80[1];  split_with_sizes_default_80 = None
    split_with_sizes_default_81 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_34_triangle_multiplication_merged_linear_g_weight, [512, 512, 256]);  pairformer_stack_blocks_34_triangle_multiplication_merged_linear_g_weight = None
    getitem_2931 = split_with_sizes_default_81[0]
    getitem_2932 = split_with_sizes_default_81[1]
    getitem_2933 = split_with_sizes_default_81[2];  split_with_sizes_default_81 = None
    _to_copy_1688 = torch.ops.aten._to_copy.default(getitem_2929, dtype = torch.bfloat16);  getitem_2929 = None
    _to_copy_1689 = torch.ops.aten._to_copy.default(getitem_2926, dtype = torch.bfloat16)
    t_627 = torch.ops.aten.t.default(_to_copy_1688);  _to_copy_1688 = None
    view_3012 = torch.ops.aten.view.default(_to_copy_1689, [147456, 256]);  _to_copy_1689 = None
    mm_583 = torch.ops.aten.mm.default(view_3012, t_627);  view_3012 = t_627 = None
    view_3013 = torch.ops.aten.view.default(mm_583, [1, 384, 384, 512]);  mm_583 = None
    _to_copy_1690 = torch.ops.aten._to_copy.default(getitem_2931, dtype = torch.bfloat16);  getitem_2931 = None
    _to_copy_1691 = torch.ops.aten._to_copy.default(getitem_2926, dtype = torch.bfloat16)
    t_628 = torch.ops.aten.t.default(_to_copy_1690);  _to_copy_1690 = None
    view_3014 = torch.ops.aten.view.default(_to_copy_1691, [147456, 256]);  _to_copy_1691 = None
    mm_584 = torch.ops.aten.mm.default(view_3014, t_628);  view_3014 = t_628 = None
    view_3015 = torch.ops.aten.view.default(mm_584, [1, 384, 384, 512]);  mm_584 = None
    sigmoid_240 = torch.ops.aten.sigmoid.default(view_3015);  view_3015 = None
    mul_398 = torch.ops.aten.mul.Tensor(view_3013, sigmoid_240);  view_3013 = sigmoid_240 = None
    unsqueeze_992 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
    bitwise_not_182 = torch.ops.aten.bitwise_not.default(unsqueeze_992);  unsqueeze_992 = None
    masked_fill_182 = torch.ops.aten.masked_fill.Scalar(mul_398, bitwise_not_182, 0);  mul_398 = bitwise_not_182 = None
    split_tensor_320 = torch.ops.aten.split.Tensor(masked_fill_182, 256, dim = -1)
    getitem_2936 = split_tensor_320[0];  split_tensor_320 = None
    unsqueeze_995 = torch.ops.aten.unsqueeze.default(getitem_2936, 4);  getitem_2936 = None
    permute_1620 = torch.ops.aten.permute.default(unsqueeze_995, [0, 1, 4, 3, 2]);  unsqueeze_995 = None
    permute_1621 = torch.ops.aten.permute.default(permute_1620, [3, 1, 4, 0, 2]);  permute_1620 = None
    view_3018 = torch.ops.aten.view.default(permute_1621, [256, 384, 384]);  permute_1621 = None
    split_tensor_321 = torch.ops.aten.split.Tensor(masked_fill_182, 256, dim = -1);  masked_fill_182 = None
    getitem_2939 = split_tensor_321[1];  split_tensor_321 = None
    unsqueeze_996 = torch.ops.aten.unsqueeze.default(getitem_2939, 4);  getitem_2939 = None
    permute_1622 = torch.ops.aten.permute.default(unsqueeze_996, [0, 4, 1, 3, 2]);  unsqueeze_996 = None
    permute_1623 = torch.ops.aten.permute.default(permute_1622, [3, 4, 0, 2, 1]);  permute_1622 = None
    view_3019 = torch.ops.aten.view.default(permute_1623, [256, 384, 384]);  permute_1623 = None
    bmm_242 = torch.ops.aten.bmm.default(view_3018, view_3019);  view_3018 = view_3019 = None
    view_3020 = torch.ops.aten.view.default(bmm_242, [256, 384, 1, 1, 384]);  bmm_242 = None
    permute_1624 = torch.ops.aten.permute.default(view_3020, [3, 1, 4, 0, 2]);  view_3020 = None
    view_3021 = torch.ops.aten.view.default(permute_1624, [1, 384, 384, 256]);  permute_1624 = None
    _to_copy_1692 = torch.ops.aten._to_copy.default(getitem_2930, dtype = torch.bfloat16);  getitem_2930 = None
    _to_copy_1693 = torch.ops.aten._to_copy.default(getitem_2926, dtype = torch.bfloat16)
    t_629 = torch.ops.aten.t.default(_to_copy_1692);  _to_copy_1692 = None
    view_3022 = torch.ops.aten.view.default(_to_copy_1693, [147456, 256]);  _to_copy_1693 = None
    mm_585 = torch.ops.aten.mm.default(view_3022, t_629);  view_3022 = t_629 = None
    view_3023 = torch.ops.aten.view.default(mm_585, [1, 384, 384, 512]);  mm_585 = None
    _to_copy_1694 = torch.ops.aten._to_copy.default(getitem_2932, dtype = torch.bfloat16);  getitem_2932 = None
    _to_copy_1695 = torch.ops.aten._to_copy.default(getitem_2926, dtype = torch.bfloat16)
    t_630 = torch.ops.aten.t.default(_to_copy_1694);  _to_copy_1694 = None
    view_3024 = torch.ops.aten.view.default(_to_copy_1695, [147456, 256]);  _to_copy_1695 = None
    mm_586 = torch.ops.aten.mm.default(view_3024, t_630);  view_3024 = t_630 = None
    view_3025 = torch.ops.aten.view.default(mm_586, [1, 384, 384, 512]);  mm_586 = None
    sigmoid_241 = torch.ops.aten.sigmoid.default(view_3025);  view_3025 = None
    mul_399 = torch.ops.aten.mul.Tensor(view_3023, sigmoid_241);  view_3023 = sigmoid_241 = None
    view_3026 = torch.ops.aten.view.default(mul_399, [147456, 512]);  mul_399 = None
    view_3027 = torch.ops.aten.view.default(view_3026, [1, 384, 384, 512]);  view_3026 = None
    transpose_80 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
    unsqueeze_997 = torch.ops.aten.unsqueeze.default(transpose_80, 3);  transpose_80 = None
    clone_259 = torch.ops.aten.clone.default(unsqueeze_997, memory_format = torch.contiguous_format);  unsqueeze_997 = None
    bitwise_not_183 = torch.ops.aten.bitwise_not.default(clone_259);  clone_259 = None
    masked_fill_183 = torch.ops.aten.masked_fill.Scalar(view_3027, bitwise_not_183, 0);  view_3027 = bitwise_not_183 = None
    view_3028 = torch.ops.aten.view.default(masked_fill_183, [147456, 512]);  masked_fill_183 = None
    view_3032 = torch.ops.aten.view.default(view_3028, [1, 384, 384, 512])
    split_tensor_322 = torch.ops.aten.split.Tensor(view_3032, 256, dim = -1);  view_3032 = None
    getitem_2942 = split_tensor_322[0];  split_tensor_322 = None
    unsqueeze_1000 = torch.ops.aten.unsqueeze.default(getitem_2942, 4);  getitem_2942 = None
    permute_1629 = torch.ops.aten.permute.default(unsqueeze_1000, [0, 2, 4, 3, 1]);  unsqueeze_1000 = None
    permute_1630 = torch.ops.aten.permute.default(permute_1629, [3, 1, 4, 0, 2]);  permute_1629 = None
    view_3033 = torch.ops.aten.view.default(permute_1630, [256, 384, 384]);  permute_1630 = None
    view_3034 = torch.ops.aten.view.default(view_3028, [1, 384, 384, 512]);  view_3028 = None
    split_tensor_323 = torch.ops.aten.split.Tensor(view_3034, 256, dim = -1);  view_3034 = None
    getitem_2945 = split_tensor_323[1];  split_tensor_323 = None
    unsqueeze_1001 = torch.ops.aten.unsqueeze.default(getitem_2945, 4);  getitem_2945 = None
    permute_1631 = torch.ops.aten.permute.default(unsqueeze_1001, [0, 4, 2, 3, 1]);  unsqueeze_1001 = None
    permute_1632 = torch.ops.aten.permute.default(permute_1631, [3, 4, 0, 2, 1]);  permute_1631 = None
    view_3035 = torch.ops.aten.view.default(permute_1632, [256, 384, 384]);  permute_1632 = None
    bmm_243 = torch.ops.aten.bmm.default(view_3033, view_3035);  view_3033 = view_3035 = None
    view_3036 = torch.ops.aten.view.default(bmm_243, [256, 384, 1, 1, 384]);  bmm_243 = None
    permute_1633 = torch.ops.aten.permute.default(view_3036, [3, 1, 4, 0, 2]);  view_3036 = None
    view_3037 = torch.ops.aten.view.default(permute_1633, [1, 384, 384, 256]);  permute_1633 = None
    _to_copy_1696 = torch.ops.aten._to_copy.default(view_3021, dtype = torch.float32);  view_3021 = None
    native_layer_norm_default_348 = torch.ops.aten.native_layer_norm.default(_to_copy_1696, [256], None, None, 1e-05);  _to_copy_1696 = None
    getitem_2946 = native_layer_norm_default_348[0];  native_layer_norm_default_348 = None
    _to_copy_1697 = torch.ops.aten._to_copy.default(view_3037, dtype = torch.float32);  view_3037 = None
    native_layer_norm_default_349 = torch.ops.aten.native_layer_norm.default(_to_copy_1697, [256], None, None, 1e-05);  _to_copy_1697 = None
    getitem_2949 = native_layer_norm_default_349[0];  native_layer_norm_default_349 = None
    add_328 = torch.ops.aten.add.Tensor(getitem_2946, getitem_2949);  getitem_2946 = getitem_2949 = None
    _to_copy_1698 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_34_triangle_multiplication_linear_z_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_34_triangle_multiplication_linear_z_out_weight = None
    _to_copy_1699 = torch.ops.aten._to_copy.default(add_328, dtype = torch.bfloat16);  add_328 = None
    t_631 = torch.ops.aten.t.default(_to_copy_1698);  _to_copy_1698 = None
    view_3038 = torch.ops.aten.view.default(_to_copy_1699, [147456, 256]);  _to_copy_1699 = None
    mm_587 = torch.ops.aten.mm.default(view_3038, t_631);  view_3038 = t_631 = None
    view_3039 = torch.ops.aten.view.default(mm_587, [1, 384, 384, 256]);  mm_587 = None
    _to_copy_1700 = torch.ops.aten._to_copy.default(getitem_2933, dtype = torch.bfloat16);  getitem_2933 = None
    _to_copy_1701 = torch.ops.aten._to_copy.default(getitem_2926, dtype = torch.bfloat16);  getitem_2926 = None
    t_632 = torch.ops.aten.t.default(_to_copy_1700);  _to_copy_1700 = None
    view_3040 = torch.ops.aten.view.default(_to_copy_1701, [147456, 256]);  _to_copy_1701 = None
    mm_588 = torch.ops.aten.mm.default(view_3040, t_632);  view_3040 = t_632 = None
    view_3041 = torch.ops.aten.view.default(mm_588, [1, 384, 384, 256]);  mm_588 = None
    sigmoid_242 = torch.ops.aten.sigmoid.default(view_3041);  view_3041 = None
    mul_400 = torch.ops.aten.mul.Tensor(view_3039, sigmoid_242);  view_3039 = sigmoid_242 = None
    add_329 = torch.ops.aten.add.Tensor(add_323, mul_400);  mul_400 = None
    _to_copy_1702 = torch.ops.aten._to_copy.default(add_323, dtype = torch.float32)
    native_layer_norm_default_350 = torch.ops.aten.native_layer_norm.default(_to_copy_1702, [256], None, None, 1e-05);  _to_copy_1702 = None
    getitem_2952 = native_layer_norm_default_350[0];  native_layer_norm_default_350 = None
    _to_copy_1703 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_34_triangle_attention_pair2b_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_34_triangle_attention_pair2b_weight = None
    _to_copy_1704 = torch.ops.aten._to_copy.default(getitem_2952, dtype = torch.bfloat16)
    t_633 = torch.ops.aten.t.default(_to_copy_1703);  _to_copy_1703 = None
    view_3042 = torch.ops.aten.view.default(_to_copy_1704, [147456, 256]);  _to_copy_1704 = None
    mm_589 = torch.ops.aten.mm.default(view_3042, t_633);  view_3042 = t_633 = None
    view_3043 = torch.ops.aten.view.default(mm_589, [1, 384, 384, 8]);  mm_589 = None
    view_3044 = torch.ops.aten.view.default(view_3043, [1, 384, 384, 2, 4]);  view_3043 = None
    permute_1634 = torch.ops.aten.permute.default(view_3044, [0, 3, 4, 1, 2]);  view_3044 = None
    view_3045 = torch.ops.aten.view.default(permute_1634, [1, 2, 4, 1, 384, 384]);  permute_1634 = None
    view_3046 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 384, 384])
    bitwise_not_184 = torch.ops.aten.bitwise_not.default(view_3046);  view_3046 = None
    masked_fill_184 = torch.ops.aten.masked_fill.Scalar(view_3045, bitwise_not_184, -10000);  view_3045 = bitwise_not_184 = None
    view_3047 = torch.ops.aten.view.default(masked_fill_184, [1, 2, 4, 384, 384]);  masked_fill_184 = None
    permute_1635 = torch.ops.aten.permute.default(view_3047, [1, 0, 2, 3, 4]);  view_3047 = None
    view_3048 = torch.ops.aten.view.default(permute_1635, [2, 4, 1, 384, 384]);  permute_1635 = None
    _to_copy_1705 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_34_triangle_attention_pair2qkvg1_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_34_triangle_attention_pair2qkvg1_weight = None
    _to_copy_1706 = torch.ops.aten._to_copy.default(getitem_2952, dtype = torch.bfloat16)
    t_634 = torch.ops.aten.t.default(_to_copy_1705);  _to_copy_1705 = None
    view_3049 = torch.ops.aten.view.default(_to_copy_1706, [147456, 256]);  _to_copy_1706 = None
    mm_590 = torch.ops.aten.mm.default(view_3049, t_634);  view_3049 = t_634 = None
    view_3050 = torch.ops.aten.view.default(mm_590, [1, 384, 384, 1024]);  mm_590 = None
    select_81 = torch.ops.aten.select.int(view_3048, 0, 0)
    view_3051 = torch.ops.aten.view.default(view_3050, [1, 384, 384, 4, 4, 64]);  view_3050 = None
    permute_1636 = torch.ops.aten.permute.default(view_3051, [4, 0, 3, 1, 2, 5]);  view_3051 = None
    view_3052 = torch.ops.aten.view.default(permute_1636, [4, 4, 384, 384, 64]);  permute_1636 = None
    unbind_int_136 = torch.ops.aten.unbind.int(view_3052);  view_3052 = None
    getitem_2955 = unbind_int_136[0]
    getitem_2956 = unbind_int_136[1]
    getitem_2957 = unbind_int_136[2]
    getitem_2958 = unbind_int_136[3];  unbind_int_136 = None
    expand_197 = torch.ops.aten.expand.default(select_81, [4, 384, 384, 384]);  select_81 = None
    _scaled_dot_product_efficient_attention_default_114 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2955, getitem_2956, getitem_2957, expand_197, False);  getitem_2955 = getitem_2956 = getitem_2957 = expand_197 = None
    getitem_2959 = _scaled_dot_product_efficient_attention_default_114[0];  _scaled_dot_product_efficient_attention_default_114 = None
    sigmoid_243 = torch.ops.aten.sigmoid.default(getitem_2958);  getitem_2958 = None
    mul_401 = torch.ops.aten.mul.Tensor(getitem_2959, sigmoid_243);  getitem_2959 = sigmoid_243 = None
    view_3053 = torch.ops.aten.view.default(mul_401, [1, 4, 384, 384, 64]);  mul_401 = None
    permute_1637 = torch.ops.aten.permute.default(view_3053, [0, 2, 3, 1, 4]);  view_3053 = None
    clone_260 = torch.ops.aten.clone.default(permute_1637, memory_format = torch.contiguous_format);  permute_1637 = None
    _unsafe_view_218 = torch.ops.aten._unsafe_view.default(clone_260, [1, 384, 384, 256]);  clone_260 = None
    transpose_81 = torch.ops.aten.transpose.int(getitem_2952, 1, 2);  getitem_2952 = None
    _to_copy_1707 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_34_triangle_attention_pair2qkvg2_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_34_triangle_attention_pair2qkvg2_weight = None
    _to_copy_1708 = torch.ops.aten._to_copy.default(transpose_81, dtype = torch.bfloat16);  transpose_81 = None
    t_635 = torch.ops.aten.t.default(_to_copy_1707);  _to_copy_1707 = None
    expand_198 = torch.ops.aten.expand.default(_to_copy_1708, [1, 384, 384, 256]);  _to_copy_1708 = None
    view_3054 = torch.ops.aten.view.default(expand_198, [384, 384, 256]);  expand_198 = None
    expand_199 = torch.ops.aten.expand.default(t_635, [1, 384, 256, 1024]);  t_635 = None
    view_3055 = torch.ops.aten.view.default(expand_199, [384, 256, 1024]);  expand_199 = None
    bmm_244 = torch.ops.aten.bmm.default(view_3054, view_3055);  view_3054 = view_3055 = None
    view_3056 = torch.ops.aten.view.default(bmm_244, [1, 384, 384, 1024]);  bmm_244 = None
    select_82 = torch.ops.aten.select.int(view_3048, 0, 1);  view_3048 = None
    view_3057 = torch.ops.aten.view.default(view_3056, [1, 384, 384, 4, 4, 64]);  view_3056 = None
    permute_1638 = torch.ops.aten.permute.default(view_3057, [4, 0, 3, 1, 2, 5]);  view_3057 = None
    view_3058 = torch.ops.aten.view.default(permute_1638, [4, 4, 384, 384, 64]);  permute_1638 = None
    unbind_int_137 = torch.ops.aten.unbind.int(view_3058);  view_3058 = None
    getitem_2963 = unbind_int_137[0]
    getitem_2964 = unbind_int_137[1]
    getitem_2965 = unbind_int_137[2]
    getitem_2966 = unbind_int_137[3];  unbind_int_137 = None
    expand_200 = torch.ops.aten.expand.default(select_82, [4, 384, 384, 384]);  select_82 = None
    _scaled_dot_product_efficient_attention_default_115 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2963, getitem_2964, getitem_2965, expand_200, False);  getitem_2963 = getitem_2964 = getitem_2965 = expand_200 = None
    getitem_2967 = _scaled_dot_product_efficient_attention_default_115[0];  _scaled_dot_product_efficient_attention_default_115 = None
    sigmoid_244 = torch.ops.aten.sigmoid.default(getitem_2966);  getitem_2966 = None
    mul_402 = torch.ops.aten.mul.Tensor(getitem_2967, sigmoid_244);  getitem_2967 = sigmoid_244 = None
    view_3059 = torch.ops.aten.view.default(mul_402, [1, 4, 384, 384, 64]);  mul_402 = None
    permute_1639 = torch.ops.aten.permute.default(view_3059, [0, 2, 3, 1, 4]);  view_3059 = None
    clone_261 = torch.ops.aten.clone.default(permute_1639, memory_format = torch.contiguous_format);  permute_1639 = None
    _unsafe_view_219 = torch.ops.aten._unsafe_view.default(clone_261, [1, 384, 384, 256]);  clone_261 = None
    cat_46 = torch.ops.aten.cat.default([_unsafe_view_218, _unsafe_view_219], dim = -1);  _unsafe_view_218 = _unsafe_view_219 = None
    slice_197 = torch.ops.aten.slice.Tensor(pairformer_stack_blocks_34_triangle_attention_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  pairformer_stack_blocks_34_triangle_attention_out_scalers = None
    unsqueeze_1002 = torch.ops.aten.unsqueeze.default(slice_197, 1);  slice_197 = None
    mul_403 = torch.ops.aten.mul.Tensor(pairformer_stack_blocks_34_triangle_attention_linear_out_weight, unsqueeze_1002);  pairformer_stack_blocks_34_triangle_attention_linear_out_weight = unsqueeze_1002 = None
    _to_copy_1709 = torch.ops.aten._to_copy.default(mul_403, dtype = torch.bfloat16);  mul_403 = None
    t_636 = torch.ops.aten.t.default(_to_copy_1709);  _to_copy_1709 = None
    view_3060 = torch.ops.aten.view.default(cat_46, [147456, 512]);  cat_46 = None
    mm_591 = torch.ops.aten.mm.default(view_3060, t_636);  view_3060 = t_636 = None
    view_3061 = torch.ops.aten.view.default(mm_591, [1, 384, 384, 256]);  mm_591 = None
    add_330 = torch.ops.aten.add.Tensor(add_329, view_3061);  add_329 = view_3061 = None
    split_tensor_324 = torch.ops.aten.split.Tensor(add_323, 384, dim = -2)
    getitem_2971 = split_tensor_324[0];  split_tensor_324 = None
    _to_copy_1710 = torch.ops.aten._to_copy.default(getitem_2971, dtype = torch.float32);  getitem_2971 = None
    native_layer_norm_default_351 = torch.ops.aten.native_layer_norm.default(_to_copy_1710, [256], pairformer_stack_blocks_34_transition_pair_layer_norm_weight, pairformer_stack_blocks_34_transition_pair_layer_norm_bias, 1e-05);  _to_copy_1710 = pairformer_stack_blocks_34_transition_pair_layer_norm_weight = pairformer_stack_blocks_34_transition_pair_layer_norm_bias = None
    getitem_2972 = native_layer_norm_default_351[0];  native_layer_norm_default_351 = None
    _to_copy_1711 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_34_transition_pair_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_34_transition_pair_linear_no_bias_ab_weight = None
    _to_copy_1712 = torch.ops.aten._to_copy.default(getitem_2972, dtype = torch.bfloat16);  getitem_2972 = None
    t_637 = torch.ops.aten.t.default(_to_copy_1711);  _to_copy_1711 = None
    view_3062 = torch.ops.aten.view.default(_to_copy_1712, [147456, 256]);  _to_copy_1712 = None
    mm_592 = torch.ops.aten.mm.default(view_3062, t_637);  view_3062 = t_637 = None
    view_3063 = torch.ops.aten.view.default(mm_592, [1, 384, 384, 1024]);  mm_592 = None
    split_tensor_325 = torch.ops.aten.split.Tensor(view_3063, 512, dim = -1);  view_3063 = None
    getitem_2975 = split_tensor_325[0]
    getitem_2976 = split_tensor_325[1];  split_tensor_325 = None
    silu_83 = torch.ops.aten.silu.default(getitem_2975);  getitem_2975 = None
    mul_404 = torch.ops.aten.mul.Tensor(silu_83, getitem_2976);  silu_83 = getitem_2976 = None
    _to_copy_1713 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_34_transition_pair_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_34_transition_pair_linear_out_weight = None
    t_638 = torch.ops.aten.t.default(_to_copy_1713);  _to_copy_1713 = None
    view_3065 = torch.ops.aten.view.default(mul_404, [147456, 512]);  mul_404 = None
    mm_593 = torch.ops.aten.mm.default(view_3065, t_638);  view_3065 = t_638 = None
    view_3066 = torch.ops.aten.view.default(mm_593, [1, 384, 384, 256]);  mm_593 = None
    add_331 = torch.ops.aten.add.Tensor(add_330, view_3066);  add_330 = view_3066 = None
    _to_copy_1714 = torch.ops.aten._to_copy.default(add_327, dtype = torch.float32)
    native_layer_norm_default_352 = torch.ops.aten.native_layer_norm.default(_to_copy_1714, [384], pairformer_stack_blocks_34_attention_pair_bias_single_layer_norm_weight, pairformer_stack_blocks_34_attention_pair_bias_single_layer_norm_bias, 1e-05);  _to_copy_1714 = pairformer_stack_blocks_34_attention_pair_bias_single_layer_norm_weight = pairformer_stack_blocks_34_attention_pair_bias_single_layer_norm_bias = None
    getitem_2977 = native_layer_norm_default_352[0];  native_layer_norm_default_352 = None
    _to_copy_1715 = torch.ops.aten._to_copy.default(add_323, dtype = torch.float32);  add_323 = None
    native_layer_norm_default_353 = torch.ops.aten.native_layer_norm.default(_to_copy_1715, [256], pairformer_stack_blocks_34_attention_pair_bias_pair_layer_norm_weight, pairformer_stack_blocks_34_attention_pair_bias_pair_layer_norm_bias, 1e-05);  _to_copy_1715 = pairformer_stack_blocks_34_attention_pair_bias_pair_layer_norm_weight = pairformer_stack_blocks_34_attention_pair_bias_pair_layer_norm_bias = None
    getitem_2980 = native_layer_norm_default_353[0];  native_layer_norm_default_353 = None
    _to_copy_1716 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_34_attention_pair_bias_pair_linear_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_34_attention_pair_bias_pair_linear_weight = None
    _to_copy_1717 = torch.ops.aten._to_copy.default(getitem_2980, dtype = torch.bfloat16);  getitem_2980 = None
    t_639 = torch.ops.aten.t.default(_to_copy_1716);  _to_copy_1716 = None
    view_3067 = torch.ops.aten.view.default(_to_copy_1717, [147456, 256]);  _to_copy_1717 = None
    mm_594 = torch.ops.aten.mm.default(view_3067, t_639);  view_3067 = t_639 = None
    view_3068 = torch.ops.aten.view.default(mm_594, [1, 384, 384, 16]);  mm_594 = None
    permute_1640 = torch.ops.aten.permute.default(view_3068, [0, 3, 1, 2]);  view_3068 = None
    view_3069 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384])
    bitwise_not_185 = torch.ops.aten.bitwise_not.default(view_3069);  view_3069 = None
    masked_fill_185 = torch.ops.aten.masked_fill.Scalar(permute_1640, bitwise_not_185, -10000);  permute_1640 = bitwise_not_185 = None
    _to_copy_1718 = torch.ops.aten._to_copy.default(getitem_2977, dtype = torch.bfloat16);  getitem_2977 = None
    _to_copy_1719 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_34_attention_pair_bias_attention_input2qkvg_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_34_attention_pair_bias_attention_input2qkvg_weight = None
    unsqueeze_1003 = torch.ops.aten.unsqueeze.default(_to_copy_1718, 3);  _to_copy_1718 = None
    unsqueeze_1004 = torch.ops.aten.unsqueeze.default(unsqueeze_1003, 4);  unsqueeze_1003 = None
    unsqueeze_1005 = torch.ops.aten.unsqueeze.default(unsqueeze_1004, 5);  unsqueeze_1004 = None
    permute_1641 = torch.ops.aten.permute.default(unsqueeze_1005, [3, 0, 4, 1, 5, 2]);  unsqueeze_1005 = None
    unsqueeze_1006 = torch.ops.aten.unsqueeze.default(_to_copy_1719, 4);  _to_copy_1719 = None
    unsqueeze_1007 = torch.ops.aten.unsqueeze.default(unsqueeze_1006, 5);  unsqueeze_1006 = None
    permute_1642 = torch.ops.aten.permute.default(unsqueeze_1007, [1, 4, 2, 5, 3, 0]);  unsqueeze_1007 = None
    permute_1643 = torch.ops.aten.permute.default(permute_1641, [3, 5, 0, 1, 2, 4]);  permute_1641 = None
    view_3070 = torch.ops.aten.view.default(permute_1643, [1, 384, 384]);  permute_1643 = None
    permute_1644 = torch.ops.aten.permute.default(permute_1642, [5, 0, 1, 2, 4, 3]);  permute_1642 = None
    view_3071 = torch.ops.aten.view.default(permute_1644, [1, 384, 1536]);  permute_1644 = None
    bmm_245 = torch.ops.aten.bmm.default(view_3070, view_3071);  view_3070 = view_3071 = None
    view_3072 = torch.ops.aten.view.default(bmm_245, [384, 1, 4, 1, 16, 24]);  bmm_245 = None
    permute_1645 = torch.ops.aten.permute.default(view_3072, [2, 3, 4, 0, 5, 1]);  view_3072 = None
    view_3073 = torch.ops.aten.view.default(permute_1645, [4, 1, 16, 384, 24]);  permute_1645 = None
    unbind_int_138 = torch.ops.aten.unbind.int(view_3073);  view_3073 = None
    getitem_2983 = unbind_int_138[0]
    getitem_2984 = unbind_int_138[1]
    getitem_2985 = unbind_int_138[2]
    getitem_2986 = unbind_int_138[3];  unbind_int_138 = None
    view_3074 = torch.ops.aten.view.default(pairformer_stack_blocks_34_attention_pair_bias_attention_query_bias, [1, 16, 1, 24]);  pairformer_stack_blocks_34_attention_pair_bias_attention_query_bias = None
    add_332 = torch.ops.aten.add.Tensor(getitem_2983, view_3074);  getitem_2983 = view_3074 = None
    _to_copy_1720 = torch.ops.aten._to_copy.default(add_332, dtype = torch.bfloat16);  add_332 = None
    expand_201 = torch.ops.aten.expand.default(masked_fill_185, [1, 16, 384, 384]);  masked_fill_185 = None
    _scaled_dot_product_efficient_attention_default_116 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_1720, getitem_2984, getitem_2985, expand_201, False);  _to_copy_1720 = getitem_2984 = getitem_2985 = expand_201 = None
    getitem_2987 = _scaled_dot_product_efficient_attention_default_116[0];  _scaled_dot_product_efficient_attention_default_116 = None
    add_333 = torch.ops.aten.add.Tensor(getitem_2986, 1);  getitem_2986 = None
    sigmoid_245 = torch.ops.aten.sigmoid.default(add_333);  add_333 = None
    mul_405 = torch.ops.aten.mul.Tensor(getitem_2987, sigmoid_245);  getitem_2987 = sigmoid_245 = None
    _to_copy_1721 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_34_attention_pair_bias_attention_output_proj_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_34_attention_pair_bias_attention_output_proj_weight = None
    unsqueeze_1008 = torch.ops.aten.unsqueeze.default(mul_405, 4);  mul_405 = None
    permute_1646 = torch.ops.aten.permute.default(unsqueeze_1008, [0, 2, 4, 3, 1]);  unsqueeze_1008 = None
    unsqueeze_1009 = torch.ops.aten.unsqueeze.default(_to_copy_1721, 3);  _to_copy_1721 = None
    unsqueeze_1010 = torch.ops.aten.unsqueeze.default(unsqueeze_1009, 4);  unsqueeze_1009 = None
    permute_1647 = torch.ops.aten.permute.default(unsqueeze_1010, [3, 4, 2, 1, 0]);  unsqueeze_1010 = None
    permute_1648 = torch.ops.aten.permute.default(permute_1646, [1, 3, 4, 0, 2]);  permute_1646 = None
    clone_262 = torch.ops.aten.clone.default(permute_1648, memory_format = torch.contiguous_format);  permute_1648 = None
    _unsafe_view_220 = torch.ops.aten._unsafe_view.default(clone_262, [1, 384, 384]);  clone_262 = None
    permute_1649 = torch.ops.aten.permute.default(permute_1647, [3, 4, 0, 2, 1]);  permute_1647 = None
    clone_263 = torch.ops.aten.clone.default(permute_1649, memory_format = torch.contiguous_format);  permute_1649 = None
    _unsafe_view_221 = torch.ops.aten._unsafe_view.default(clone_263, [1, 384, 384]);  clone_263 = None
    bmm_246 = torch.ops.aten.bmm.default(_unsafe_view_220, _unsafe_view_221);  _unsafe_view_220 = _unsafe_view_221 = None
    view_3075 = torch.ops.aten.view.default(bmm_246, [384, 1, 1, 1, 384]);  bmm_246 = None
    permute_1650 = torch.ops.aten.permute.default(view_3075, [3, 0, 4, 1, 2]);  view_3075 = None
    view_3076 = torch.ops.aten.view.default(permute_1650, [1, 384, 384]);  permute_1650 = None
    unsqueeze_1011 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
    mul_406 = torch.ops.aten.mul.Tensor(view_3076, unsqueeze_1011);  view_3076 = unsqueeze_1011 = None
    add_334 = torch.ops.aten.add.Tensor(add_327, mul_406);  mul_406 = None
    split_tensor_326 = torch.ops.aten.split.Tensor(add_327, 384, dim = -2);  add_327 = None
    getitem_2991 = split_tensor_326[0];  split_tensor_326 = None
    _to_copy_1722 = torch.ops.aten._to_copy.default(getitem_2991, dtype = torch.float32);  getitem_2991 = None
    native_layer_norm_default_354 = torch.ops.aten.native_layer_norm.default(_to_copy_1722, [384], pairformer_stack_blocks_34_transition_single_layer_norm_weight, pairformer_stack_blocks_34_transition_single_layer_norm_bias, 1e-05);  _to_copy_1722 = pairformer_stack_blocks_34_transition_single_layer_norm_weight = pairformer_stack_blocks_34_transition_single_layer_norm_bias = None
    getitem_2992 = native_layer_norm_default_354[0];  native_layer_norm_default_354 = None
    _to_copy_1723 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_34_transition_single_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_34_transition_single_linear_no_bias_ab_weight = None
    _to_copy_1724 = torch.ops.aten._to_copy.default(getitem_2992, dtype = torch.bfloat16);  getitem_2992 = None
    t_640 = torch.ops.aten.t.default(_to_copy_1723);  _to_copy_1723 = None
    view_3077 = torch.ops.aten.view.default(_to_copy_1724, [384, 384]);  _to_copy_1724 = None
    mm_595 = torch.ops.aten.mm.default(view_3077, t_640);  view_3077 = t_640 = None
    view_3078 = torch.ops.aten.view.default(mm_595, [1, 384, 1536]);  mm_595 = None
    split_tensor_327 = torch.ops.aten.split.Tensor(view_3078, 768, dim = -1);  view_3078 = None
    getitem_2995 = split_tensor_327[0]
    getitem_2996 = split_tensor_327[1];  split_tensor_327 = None
    silu_84 = torch.ops.aten.silu.default(getitem_2995);  getitem_2995 = None
    mul_407 = torch.ops.aten.mul.Tensor(silu_84, getitem_2996);  silu_84 = getitem_2996 = None
    _to_copy_1725 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_34_transition_single_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_34_transition_single_linear_out_weight = None
    t_641 = torch.ops.aten.t.default(_to_copy_1725);  _to_copy_1725 = None
    view_3080 = torch.ops.aten.view.default(mul_407, [384, 768]);  mul_407 = None
    mm_596 = torch.ops.aten.mm.default(view_3080, t_641);  view_3080 = t_641 = None
    view_3081 = torch.ops.aten.view.default(mm_596, [1, 384, 384]);  mm_596 = None
    add_335 = torch.ops.aten.add.Tensor(add_334, view_3081);  add_334 = view_3081 = None
    _to_copy_1726 = torch.ops.aten._to_copy.default(add_331, dtype = torch.float32)
    native_layer_norm_default_355 = torch.ops.aten.native_layer_norm.default(_to_copy_1726, [256], pairformer_stack_blocks_35_triangle_multiplication_layernorm_z_in_weight, pairformer_stack_blocks_35_triangle_multiplication_layernorm_z_in_bias, 1e-05);  _to_copy_1726 = pairformer_stack_blocks_35_triangle_multiplication_layernorm_z_in_weight = pairformer_stack_blocks_35_triangle_multiplication_layernorm_z_in_bias = None
    getitem_2997 = native_layer_norm_default_355[0];  native_layer_norm_default_355 = None
    split_with_sizes_default_82 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_35_triangle_multiplication_merged_linear_p_weight, [512, 512]);  pairformer_stack_blocks_35_triangle_multiplication_merged_linear_p_weight = None
    getitem_3000 = split_with_sizes_default_82[0]
    getitem_3001 = split_with_sizes_default_82[1];  split_with_sizes_default_82 = None
    split_with_sizes_default_83 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_35_triangle_multiplication_merged_linear_g_weight, [512, 512, 256]);  pairformer_stack_blocks_35_triangle_multiplication_merged_linear_g_weight = None
    getitem_3002 = split_with_sizes_default_83[0]
    getitem_3003 = split_with_sizes_default_83[1]
    getitem_3004 = split_with_sizes_default_83[2];  split_with_sizes_default_83 = None
    _to_copy_1727 = torch.ops.aten._to_copy.default(getitem_3000, dtype = torch.bfloat16);  getitem_3000 = None
    _to_copy_1728 = torch.ops.aten._to_copy.default(getitem_2997, dtype = torch.bfloat16)
    t_642 = torch.ops.aten.t.default(_to_copy_1727);  _to_copy_1727 = None
    view_3082 = torch.ops.aten.view.default(_to_copy_1728, [147456, 256]);  _to_copy_1728 = None
    mm_597 = torch.ops.aten.mm.default(view_3082, t_642);  view_3082 = t_642 = None
    view_3083 = torch.ops.aten.view.default(mm_597, [1, 384, 384, 512]);  mm_597 = None
    _to_copy_1729 = torch.ops.aten._to_copy.default(getitem_3002, dtype = torch.bfloat16);  getitem_3002 = None
    _to_copy_1730 = torch.ops.aten._to_copy.default(getitem_2997, dtype = torch.bfloat16)
    t_643 = torch.ops.aten.t.default(_to_copy_1729);  _to_copy_1729 = None
    view_3084 = torch.ops.aten.view.default(_to_copy_1730, [147456, 256]);  _to_copy_1730 = None
    mm_598 = torch.ops.aten.mm.default(view_3084, t_643);  view_3084 = t_643 = None
    view_3085 = torch.ops.aten.view.default(mm_598, [1, 384, 384, 512]);  mm_598 = None
    sigmoid_246 = torch.ops.aten.sigmoid.default(view_3085);  view_3085 = None
    mul_408 = torch.ops.aten.mul.Tensor(view_3083, sigmoid_246);  view_3083 = sigmoid_246 = None
    unsqueeze_1012 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
    bitwise_not_186 = torch.ops.aten.bitwise_not.default(unsqueeze_1012);  unsqueeze_1012 = None
    masked_fill_186 = torch.ops.aten.masked_fill.Scalar(mul_408, bitwise_not_186, 0);  mul_408 = bitwise_not_186 = None
    split_tensor_328 = torch.ops.aten.split.Tensor(masked_fill_186, 256, dim = -1)
    getitem_3007 = split_tensor_328[0];  split_tensor_328 = None
    unsqueeze_1015 = torch.ops.aten.unsqueeze.default(getitem_3007, 4);  getitem_3007 = None
    permute_1655 = torch.ops.aten.permute.default(unsqueeze_1015, [0, 1, 4, 3, 2]);  unsqueeze_1015 = None
    permute_1656 = torch.ops.aten.permute.default(permute_1655, [3, 1, 4, 0, 2]);  permute_1655 = None
    view_3088 = torch.ops.aten.view.default(permute_1656, [256, 384, 384]);  permute_1656 = None
    split_tensor_329 = torch.ops.aten.split.Tensor(masked_fill_186, 256, dim = -1);  masked_fill_186 = None
    getitem_3010 = split_tensor_329[1];  split_tensor_329 = None
    unsqueeze_1016 = torch.ops.aten.unsqueeze.default(getitem_3010, 4);  getitem_3010 = None
    permute_1657 = torch.ops.aten.permute.default(unsqueeze_1016, [0, 4, 1, 3, 2]);  unsqueeze_1016 = None
    permute_1658 = torch.ops.aten.permute.default(permute_1657, [3, 4, 0, 2, 1]);  permute_1657 = None
    view_3089 = torch.ops.aten.view.default(permute_1658, [256, 384, 384]);  permute_1658 = None
    bmm_247 = torch.ops.aten.bmm.default(view_3088, view_3089);  view_3088 = view_3089 = None
    view_3090 = torch.ops.aten.view.default(bmm_247, [256, 384, 1, 1, 384]);  bmm_247 = None
    permute_1659 = torch.ops.aten.permute.default(view_3090, [3, 1, 4, 0, 2]);  view_3090 = None
    view_3091 = torch.ops.aten.view.default(permute_1659, [1, 384, 384, 256]);  permute_1659 = None
    _to_copy_1731 = torch.ops.aten._to_copy.default(getitem_3001, dtype = torch.bfloat16);  getitem_3001 = None
    _to_copy_1732 = torch.ops.aten._to_copy.default(getitem_2997, dtype = torch.bfloat16)
    t_644 = torch.ops.aten.t.default(_to_copy_1731);  _to_copy_1731 = None
    view_3092 = torch.ops.aten.view.default(_to_copy_1732, [147456, 256]);  _to_copy_1732 = None
    mm_599 = torch.ops.aten.mm.default(view_3092, t_644);  view_3092 = t_644 = None
    view_3093 = torch.ops.aten.view.default(mm_599, [1, 384, 384, 512]);  mm_599 = None
    _to_copy_1733 = torch.ops.aten._to_copy.default(getitem_3003, dtype = torch.bfloat16);  getitem_3003 = None
    _to_copy_1734 = torch.ops.aten._to_copy.default(getitem_2997, dtype = torch.bfloat16)
    t_645 = torch.ops.aten.t.default(_to_copy_1733);  _to_copy_1733 = None
    view_3094 = torch.ops.aten.view.default(_to_copy_1734, [147456, 256]);  _to_copy_1734 = None
    mm_600 = torch.ops.aten.mm.default(view_3094, t_645);  view_3094 = t_645 = None
    view_3095 = torch.ops.aten.view.default(mm_600, [1, 384, 384, 512]);  mm_600 = None
    sigmoid_247 = torch.ops.aten.sigmoid.default(view_3095);  view_3095 = None
    mul_409 = torch.ops.aten.mul.Tensor(view_3093, sigmoid_247);  view_3093 = sigmoid_247 = None
    view_3096 = torch.ops.aten.view.default(mul_409, [147456, 512]);  mul_409 = None
    view_3097 = torch.ops.aten.view.default(view_3096, [1, 384, 384, 512]);  view_3096 = None
    transpose_82 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
    unsqueeze_1017 = torch.ops.aten.unsqueeze.default(transpose_82, 3);  transpose_82 = None
    clone_264 = torch.ops.aten.clone.default(unsqueeze_1017, memory_format = torch.contiguous_format);  unsqueeze_1017 = None
    bitwise_not_187 = torch.ops.aten.bitwise_not.default(clone_264);  clone_264 = None
    masked_fill_187 = torch.ops.aten.masked_fill.Scalar(view_3097, bitwise_not_187, 0);  view_3097 = bitwise_not_187 = None
    view_3098 = torch.ops.aten.view.default(masked_fill_187, [147456, 512]);  masked_fill_187 = None
    view_3102 = torch.ops.aten.view.default(view_3098, [1, 384, 384, 512])
    split_tensor_330 = torch.ops.aten.split.Tensor(view_3102, 256, dim = -1);  view_3102 = None
    getitem_3013 = split_tensor_330[0];  split_tensor_330 = None
    unsqueeze_1020 = torch.ops.aten.unsqueeze.default(getitem_3013, 4);  getitem_3013 = None
    permute_1664 = torch.ops.aten.permute.default(unsqueeze_1020, [0, 2, 4, 3, 1]);  unsqueeze_1020 = None
    permute_1665 = torch.ops.aten.permute.default(permute_1664, [3, 1, 4, 0, 2]);  permute_1664 = None
    view_3103 = torch.ops.aten.view.default(permute_1665, [256, 384, 384]);  permute_1665 = None
    view_3104 = torch.ops.aten.view.default(view_3098, [1, 384, 384, 512]);  view_3098 = None
    split_tensor_331 = torch.ops.aten.split.Tensor(view_3104, 256, dim = -1);  view_3104 = None
    getitem_3016 = split_tensor_331[1];  split_tensor_331 = None
    unsqueeze_1021 = torch.ops.aten.unsqueeze.default(getitem_3016, 4);  getitem_3016 = None
    permute_1666 = torch.ops.aten.permute.default(unsqueeze_1021, [0, 4, 2, 3, 1]);  unsqueeze_1021 = None
    permute_1667 = torch.ops.aten.permute.default(permute_1666, [3, 4, 0, 2, 1]);  permute_1666 = None
    view_3105 = torch.ops.aten.view.default(permute_1667, [256, 384, 384]);  permute_1667 = None
    bmm_248 = torch.ops.aten.bmm.default(view_3103, view_3105);  view_3103 = view_3105 = None
    view_3106 = torch.ops.aten.view.default(bmm_248, [256, 384, 1, 1, 384]);  bmm_248 = None
    permute_1668 = torch.ops.aten.permute.default(view_3106, [3, 1, 4, 0, 2]);  view_3106 = None
    view_3107 = torch.ops.aten.view.default(permute_1668, [1, 384, 384, 256]);  permute_1668 = None
    _to_copy_1735 = torch.ops.aten._to_copy.default(view_3091, dtype = torch.float32);  view_3091 = None
    native_layer_norm_default_356 = torch.ops.aten.native_layer_norm.default(_to_copy_1735, [256], None, None, 1e-05);  _to_copy_1735 = None
    getitem_3017 = native_layer_norm_default_356[0];  native_layer_norm_default_356 = None
    _to_copy_1736 = torch.ops.aten._to_copy.default(view_3107, dtype = torch.float32);  view_3107 = None
    native_layer_norm_default_357 = torch.ops.aten.native_layer_norm.default(_to_copy_1736, [256], None, None, 1e-05);  _to_copy_1736 = None
    getitem_3020 = native_layer_norm_default_357[0];  native_layer_norm_default_357 = None
    add_336 = torch.ops.aten.add.Tensor(getitem_3017, getitem_3020);  getitem_3017 = getitem_3020 = None
    _to_copy_1737 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_35_triangle_multiplication_linear_z_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_35_triangle_multiplication_linear_z_out_weight = None
    _to_copy_1738 = torch.ops.aten._to_copy.default(add_336, dtype = torch.bfloat16);  add_336 = None
    t_646 = torch.ops.aten.t.default(_to_copy_1737);  _to_copy_1737 = None
    view_3108 = torch.ops.aten.view.default(_to_copy_1738, [147456, 256]);  _to_copy_1738 = None
    mm_601 = torch.ops.aten.mm.default(view_3108, t_646);  view_3108 = t_646 = None
    view_3109 = torch.ops.aten.view.default(mm_601, [1, 384, 384, 256]);  mm_601 = None
    _to_copy_1739 = torch.ops.aten._to_copy.default(getitem_3004, dtype = torch.bfloat16);  getitem_3004 = None
    _to_copy_1740 = torch.ops.aten._to_copy.default(getitem_2997, dtype = torch.bfloat16);  getitem_2997 = None
    t_647 = torch.ops.aten.t.default(_to_copy_1739);  _to_copy_1739 = None
    view_3110 = torch.ops.aten.view.default(_to_copy_1740, [147456, 256]);  _to_copy_1740 = None
    mm_602 = torch.ops.aten.mm.default(view_3110, t_647);  view_3110 = t_647 = None
    view_3111 = torch.ops.aten.view.default(mm_602, [1, 384, 384, 256]);  mm_602 = None
    sigmoid_248 = torch.ops.aten.sigmoid.default(view_3111);  view_3111 = None
    mul_410 = torch.ops.aten.mul.Tensor(view_3109, sigmoid_248);  view_3109 = sigmoid_248 = None
    add_337 = torch.ops.aten.add.Tensor(add_331, mul_410);  mul_410 = None
    _to_copy_1741 = torch.ops.aten._to_copy.default(add_331, dtype = torch.float32)
    native_layer_norm_default_358 = torch.ops.aten.native_layer_norm.default(_to_copy_1741, [256], None, None, 1e-05);  _to_copy_1741 = None
    getitem_3023 = native_layer_norm_default_358[0];  native_layer_norm_default_358 = None
    _to_copy_1742 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_35_triangle_attention_pair2b_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_35_triangle_attention_pair2b_weight = None
    _to_copy_1743 = torch.ops.aten._to_copy.default(getitem_3023, dtype = torch.bfloat16)
    t_648 = torch.ops.aten.t.default(_to_copy_1742);  _to_copy_1742 = None
    view_3112 = torch.ops.aten.view.default(_to_copy_1743, [147456, 256]);  _to_copy_1743 = None
    mm_603 = torch.ops.aten.mm.default(view_3112, t_648);  view_3112 = t_648 = None
    view_3113 = torch.ops.aten.view.default(mm_603, [1, 384, 384, 8]);  mm_603 = None
    view_3114 = torch.ops.aten.view.default(view_3113, [1, 384, 384, 2, 4]);  view_3113 = None
    permute_1669 = torch.ops.aten.permute.default(view_3114, [0, 3, 4, 1, 2]);  view_3114 = None
    view_3115 = torch.ops.aten.view.default(permute_1669, [1, 2, 4, 1, 384, 384]);  permute_1669 = None
    view_3116 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 384, 384])
    bitwise_not_188 = torch.ops.aten.bitwise_not.default(view_3116);  view_3116 = None
    masked_fill_188 = torch.ops.aten.masked_fill.Scalar(view_3115, bitwise_not_188, -10000);  view_3115 = bitwise_not_188 = None
    view_3117 = torch.ops.aten.view.default(masked_fill_188, [1, 2, 4, 384, 384]);  masked_fill_188 = None
    permute_1670 = torch.ops.aten.permute.default(view_3117, [1, 0, 2, 3, 4]);  view_3117 = None
    view_3118 = torch.ops.aten.view.default(permute_1670, [2, 4, 1, 384, 384]);  permute_1670 = None
    _to_copy_1744 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_35_triangle_attention_pair2qkvg1_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_35_triangle_attention_pair2qkvg1_weight = None
    _to_copy_1745 = torch.ops.aten._to_copy.default(getitem_3023, dtype = torch.bfloat16)
    t_649 = torch.ops.aten.t.default(_to_copy_1744);  _to_copy_1744 = None
    view_3119 = torch.ops.aten.view.default(_to_copy_1745, [147456, 256]);  _to_copy_1745 = None
    mm_604 = torch.ops.aten.mm.default(view_3119, t_649);  view_3119 = t_649 = None
    view_3120 = torch.ops.aten.view.default(mm_604, [1, 384, 384, 1024]);  mm_604 = None
    select_83 = torch.ops.aten.select.int(view_3118, 0, 0)
    view_3121 = torch.ops.aten.view.default(view_3120, [1, 384, 384, 4, 4, 64]);  view_3120 = None
    permute_1671 = torch.ops.aten.permute.default(view_3121, [4, 0, 3, 1, 2, 5]);  view_3121 = None
    view_3122 = torch.ops.aten.view.default(permute_1671, [4, 4, 384, 384, 64]);  permute_1671 = None
    unbind_int_139 = torch.ops.aten.unbind.int(view_3122);  view_3122 = None
    getitem_3026 = unbind_int_139[0]
    getitem_3027 = unbind_int_139[1]
    getitem_3028 = unbind_int_139[2]
    getitem_3029 = unbind_int_139[3];  unbind_int_139 = None
    expand_202 = torch.ops.aten.expand.default(select_83, [4, 384, 384, 384]);  select_83 = None
    _scaled_dot_product_efficient_attention_default_117 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3026, getitem_3027, getitem_3028, expand_202, False);  getitem_3026 = getitem_3027 = getitem_3028 = expand_202 = None
    getitem_3030 = _scaled_dot_product_efficient_attention_default_117[0];  _scaled_dot_product_efficient_attention_default_117 = None
    sigmoid_249 = torch.ops.aten.sigmoid.default(getitem_3029);  getitem_3029 = None
    mul_411 = torch.ops.aten.mul.Tensor(getitem_3030, sigmoid_249);  getitem_3030 = sigmoid_249 = None
    view_3123 = torch.ops.aten.view.default(mul_411, [1, 4, 384, 384, 64]);  mul_411 = None
    permute_1672 = torch.ops.aten.permute.default(view_3123, [0, 2, 3, 1, 4]);  view_3123 = None
    clone_265 = torch.ops.aten.clone.default(permute_1672, memory_format = torch.contiguous_format);  permute_1672 = None
    _unsafe_view_222 = torch.ops.aten._unsafe_view.default(clone_265, [1, 384, 384, 256]);  clone_265 = None
    transpose_83 = torch.ops.aten.transpose.int(getitem_3023, 1, 2);  getitem_3023 = None
    _to_copy_1746 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_35_triangle_attention_pair2qkvg2_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_35_triangle_attention_pair2qkvg2_weight = None
    _to_copy_1747 = torch.ops.aten._to_copy.default(transpose_83, dtype = torch.bfloat16);  transpose_83 = None
    t_650 = torch.ops.aten.t.default(_to_copy_1746);  _to_copy_1746 = None
    expand_203 = torch.ops.aten.expand.default(_to_copy_1747, [1, 384, 384, 256]);  _to_copy_1747 = None
    view_3124 = torch.ops.aten.view.default(expand_203, [384, 384, 256]);  expand_203 = None
    expand_204 = torch.ops.aten.expand.default(t_650, [1, 384, 256, 1024]);  t_650 = None
    view_3125 = torch.ops.aten.view.default(expand_204, [384, 256, 1024]);  expand_204 = None
    bmm_249 = torch.ops.aten.bmm.default(view_3124, view_3125);  view_3124 = view_3125 = None
    view_3126 = torch.ops.aten.view.default(bmm_249, [1, 384, 384, 1024]);  bmm_249 = None
    select_84 = torch.ops.aten.select.int(view_3118, 0, 1);  view_3118 = None
    view_3127 = torch.ops.aten.view.default(view_3126, [1, 384, 384, 4, 4, 64]);  view_3126 = None
    permute_1673 = torch.ops.aten.permute.default(view_3127, [4, 0, 3, 1, 2, 5]);  view_3127 = None
    view_3128 = torch.ops.aten.view.default(permute_1673, [4, 4, 384, 384, 64]);  permute_1673 = None
    unbind_int_140 = torch.ops.aten.unbind.int(view_3128);  view_3128 = None
    getitem_3034 = unbind_int_140[0]
    getitem_3035 = unbind_int_140[1]
    getitem_3036 = unbind_int_140[2]
    getitem_3037 = unbind_int_140[3];  unbind_int_140 = None
    expand_205 = torch.ops.aten.expand.default(select_84, [4, 384, 384, 384]);  select_84 = None
    _scaled_dot_product_efficient_attention_default_118 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3034, getitem_3035, getitem_3036, expand_205, False);  getitem_3034 = getitem_3035 = getitem_3036 = expand_205 = None
    getitem_3038 = _scaled_dot_product_efficient_attention_default_118[0];  _scaled_dot_product_efficient_attention_default_118 = None
    sigmoid_250 = torch.ops.aten.sigmoid.default(getitem_3037);  getitem_3037 = None
    mul_412 = torch.ops.aten.mul.Tensor(getitem_3038, sigmoid_250);  getitem_3038 = sigmoid_250 = None
    view_3129 = torch.ops.aten.view.default(mul_412, [1, 4, 384, 384, 64]);  mul_412 = None
    permute_1674 = torch.ops.aten.permute.default(view_3129, [0, 2, 3, 1, 4]);  view_3129 = None
    clone_266 = torch.ops.aten.clone.default(permute_1674, memory_format = torch.contiguous_format);  permute_1674 = None
    _unsafe_view_223 = torch.ops.aten._unsafe_view.default(clone_266, [1, 384, 384, 256]);  clone_266 = None
    cat_47 = torch.ops.aten.cat.default([_unsafe_view_222, _unsafe_view_223], dim = -1);  _unsafe_view_222 = _unsafe_view_223 = None
    slice_198 = torch.ops.aten.slice.Tensor(pairformer_stack_blocks_35_triangle_attention_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  pairformer_stack_blocks_35_triangle_attention_out_scalers = None
    unsqueeze_1022 = torch.ops.aten.unsqueeze.default(slice_198, 1);  slice_198 = None
    mul_413 = torch.ops.aten.mul.Tensor(pairformer_stack_blocks_35_triangle_attention_linear_out_weight, unsqueeze_1022);  pairformer_stack_blocks_35_triangle_attention_linear_out_weight = unsqueeze_1022 = None
    _to_copy_1748 = torch.ops.aten._to_copy.default(mul_413, dtype = torch.bfloat16);  mul_413 = None
    t_651 = torch.ops.aten.t.default(_to_copy_1748);  _to_copy_1748 = None
    view_3130 = torch.ops.aten.view.default(cat_47, [147456, 512]);  cat_47 = None
    mm_605 = torch.ops.aten.mm.default(view_3130, t_651);  view_3130 = t_651 = None
    view_3131 = torch.ops.aten.view.default(mm_605, [1, 384, 384, 256]);  mm_605 = None
    add_338 = torch.ops.aten.add.Tensor(add_337, view_3131);  add_337 = view_3131 = None
    split_tensor_332 = torch.ops.aten.split.Tensor(add_331, 384, dim = -2)
    getitem_3042 = split_tensor_332[0];  split_tensor_332 = None
    _to_copy_1749 = torch.ops.aten._to_copy.default(getitem_3042, dtype = torch.float32);  getitem_3042 = None
    native_layer_norm_default_359 = torch.ops.aten.native_layer_norm.default(_to_copy_1749, [256], pairformer_stack_blocks_35_transition_pair_layer_norm_weight, pairformer_stack_blocks_35_transition_pair_layer_norm_bias, 1e-05);  _to_copy_1749 = pairformer_stack_blocks_35_transition_pair_layer_norm_weight = pairformer_stack_blocks_35_transition_pair_layer_norm_bias = None
    getitem_3043 = native_layer_norm_default_359[0];  native_layer_norm_default_359 = None
    _to_copy_1750 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_35_transition_pair_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_35_transition_pair_linear_no_bias_ab_weight = None
    _to_copy_1751 = torch.ops.aten._to_copy.default(getitem_3043, dtype = torch.bfloat16);  getitem_3043 = None
    t_652 = torch.ops.aten.t.default(_to_copy_1750);  _to_copy_1750 = None
    view_3132 = torch.ops.aten.view.default(_to_copy_1751, [147456, 256]);  _to_copy_1751 = None
    mm_606 = torch.ops.aten.mm.default(view_3132, t_652);  view_3132 = t_652 = None
    view_3133 = torch.ops.aten.view.default(mm_606, [1, 384, 384, 1024]);  mm_606 = None
    split_tensor_333 = torch.ops.aten.split.Tensor(view_3133, 512, dim = -1);  view_3133 = None
    getitem_3046 = split_tensor_333[0]
    getitem_3047 = split_tensor_333[1];  split_tensor_333 = None
    silu_85 = torch.ops.aten.silu.default(getitem_3046);  getitem_3046 = None
    mul_414 = torch.ops.aten.mul.Tensor(silu_85, getitem_3047);  silu_85 = getitem_3047 = None
    _to_copy_1752 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_35_transition_pair_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_35_transition_pair_linear_out_weight = None
    t_653 = torch.ops.aten.t.default(_to_copy_1752);  _to_copy_1752 = None
    view_3135 = torch.ops.aten.view.default(mul_414, [147456, 512]);  mul_414 = None
    mm_607 = torch.ops.aten.mm.default(view_3135, t_653);  view_3135 = t_653 = None
    view_3136 = torch.ops.aten.view.default(mm_607, [1, 384, 384, 256]);  mm_607 = None
    add_339 = torch.ops.aten.add.Tensor(add_338, view_3136);  add_338 = view_3136 = None
    _to_copy_1753 = torch.ops.aten._to_copy.default(add_335, dtype = torch.float32)
    native_layer_norm_default_360 = torch.ops.aten.native_layer_norm.default(_to_copy_1753, [384], pairformer_stack_blocks_35_attention_pair_bias_single_layer_norm_weight, pairformer_stack_blocks_35_attention_pair_bias_single_layer_norm_bias, 1e-05);  _to_copy_1753 = pairformer_stack_blocks_35_attention_pair_bias_single_layer_norm_weight = pairformer_stack_blocks_35_attention_pair_bias_single_layer_norm_bias = None
    getitem_3048 = native_layer_norm_default_360[0];  native_layer_norm_default_360 = None
    _to_copy_1754 = torch.ops.aten._to_copy.default(add_331, dtype = torch.float32);  add_331 = None
    native_layer_norm_default_361 = torch.ops.aten.native_layer_norm.default(_to_copy_1754, [256], pairformer_stack_blocks_35_attention_pair_bias_pair_layer_norm_weight, pairformer_stack_blocks_35_attention_pair_bias_pair_layer_norm_bias, 1e-05);  _to_copy_1754 = pairformer_stack_blocks_35_attention_pair_bias_pair_layer_norm_weight = pairformer_stack_blocks_35_attention_pair_bias_pair_layer_norm_bias = None
    getitem_3051 = native_layer_norm_default_361[0];  native_layer_norm_default_361 = None
    _to_copy_1755 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_35_attention_pair_bias_pair_linear_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_35_attention_pair_bias_pair_linear_weight = None
    _to_copy_1756 = torch.ops.aten._to_copy.default(getitem_3051, dtype = torch.bfloat16);  getitem_3051 = None
    t_654 = torch.ops.aten.t.default(_to_copy_1755);  _to_copy_1755 = None
    view_3137 = torch.ops.aten.view.default(_to_copy_1756, [147456, 256]);  _to_copy_1756 = None
    mm_608 = torch.ops.aten.mm.default(view_3137, t_654);  view_3137 = t_654 = None
    view_3138 = torch.ops.aten.view.default(mm_608, [1, 384, 384, 16]);  mm_608 = None
    permute_1675 = torch.ops.aten.permute.default(view_3138, [0, 3, 1, 2]);  view_3138 = None
    view_3139 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384])
    bitwise_not_189 = torch.ops.aten.bitwise_not.default(view_3139);  view_3139 = None
    masked_fill_189 = torch.ops.aten.masked_fill.Scalar(permute_1675, bitwise_not_189, -10000);  permute_1675 = bitwise_not_189 = None
    _to_copy_1757 = torch.ops.aten._to_copy.default(getitem_3048, dtype = torch.bfloat16);  getitem_3048 = None
    _to_copy_1758 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_35_attention_pair_bias_attention_input2qkvg_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_35_attention_pair_bias_attention_input2qkvg_weight = None
    unsqueeze_1023 = torch.ops.aten.unsqueeze.default(_to_copy_1757, 3);  _to_copy_1757 = None
    unsqueeze_1024 = torch.ops.aten.unsqueeze.default(unsqueeze_1023, 4);  unsqueeze_1023 = None
    unsqueeze_1025 = torch.ops.aten.unsqueeze.default(unsqueeze_1024, 5);  unsqueeze_1024 = None
    permute_1676 = torch.ops.aten.permute.default(unsqueeze_1025, [3, 0, 4, 1, 5, 2]);  unsqueeze_1025 = None
    unsqueeze_1026 = torch.ops.aten.unsqueeze.default(_to_copy_1758, 4);  _to_copy_1758 = None
    unsqueeze_1027 = torch.ops.aten.unsqueeze.default(unsqueeze_1026, 5);  unsqueeze_1026 = None
    permute_1677 = torch.ops.aten.permute.default(unsqueeze_1027, [1, 4, 2, 5, 3, 0]);  unsqueeze_1027 = None
    permute_1678 = torch.ops.aten.permute.default(permute_1676, [3, 5, 0, 1, 2, 4]);  permute_1676 = None
    view_3140 = torch.ops.aten.view.default(permute_1678, [1, 384, 384]);  permute_1678 = None
    permute_1679 = torch.ops.aten.permute.default(permute_1677, [5, 0, 1, 2, 4, 3]);  permute_1677 = None
    view_3141 = torch.ops.aten.view.default(permute_1679, [1, 384, 1536]);  permute_1679 = None
    bmm_250 = torch.ops.aten.bmm.default(view_3140, view_3141);  view_3140 = view_3141 = None
    view_3142 = torch.ops.aten.view.default(bmm_250, [384, 1, 4, 1, 16, 24]);  bmm_250 = None
    permute_1680 = torch.ops.aten.permute.default(view_3142, [2, 3, 4, 0, 5, 1]);  view_3142 = None
    view_3143 = torch.ops.aten.view.default(permute_1680, [4, 1, 16, 384, 24]);  permute_1680 = None
    unbind_int_141 = torch.ops.aten.unbind.int(view_3143);  view_3143 = None
    getitem_3054 = unbind_int_141[0]
    getitem_3055 = unbind_int_141[1]
    getitem_3056 = unbind_int_141[2]
    getitem_3057 = unbind_int_141[3];  unbind_int_141 = None
    view_3144 = torch.ops.aten.view.default(pairformer_stack_blocks_35_attention_pair_bias_attention_query_bias, [1, 16, 1, 24]);  pairformer_stack_blocks_35_attention_pair_bias_attention_query_bias = None
    add_340 = torch.ops.aten.add.Tensor(getitem_3054, view_3144);  getitem_3054 = view_3144 = None
    _to_copy_1759 = torch.ops.aten._to_copy.default(add_340, dtype = torch.bfloat16);  add_340 = None
    expand_206 = torch.ops.aten.expand.default(masked_fill_189, [1, 16, 384, 384]);  masked_fill_189 = None
    _scaled_dot_product_efficient_attention_default_119 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_1759, getitem_3055, getitem_3056, expand_206, False);  _to_copy_1759 = getitem_3055 = getitem_3056 = expand_206 = None
    getitem_3058 = _scaled_dot_product_efficient_attention_default_119[0];  _scaled_dot_product_efficient_attention_default_119 = None
    add_341 = torch.ops.aten.add.Tensor(getitem_3057, 1);  getitem_3057 = None
    sigmoid_251 = torch.ops.aten.sigmoid.default(add_341);  add_341 = None
    mul_415 = torch.ops.aten.mul.Tensor(getitem_3058, sigmoid_251);  getitem_3058 = sigmoid_251 = None
    _to_copy_1760 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_35_attention_pair_bias_attention_output_proj_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_35_attention_pair_bias_attention_output_proj_weight = None
    unsqueeze_1028 = torch.ops.aten.unsqueeze.default(mul_415, 4);  mul_415 = None
    permute_1681 = torch.ops.aten.permute.default(unsqueeze_1028, [0, 2, 4, 3, 1]);  unsqueeze_1028 = None
    unsqueeze_1029 = torch.ops.aten.unsqueeze.default(_to_copy_1760, 3);  _to_copy_1760 = None
    unsqueeze_1030 = torch.ops.aten.unsqueeze.default(unsqueeze_1029, 4);  unsqueeze_1029 = None
    permute_1682 = torch.ops.aten.permute.default(unsqueeze_1030, [3, 4, 2, 1, 0]);  unsqueeze_1030 = None
    permute_1683 = torch.ops.aten.permute.default(permute_1681, [1, 3, 4, 0, 2]);  permute_1681 = None
    clone_267 = torch.ops.aten.clone.default(permute_1683, memory_format = torch.contiguous_format);  permute_1683 = None
    _unsafe_view_224 = torch.ops.aten._unsafe_view.default(clone_267, [1, 384, 384]);  clone_267 = None
    permute_1684 = torch.ops.aten.permute.default(permute_1682, [3, 4, 0, 2, 1]);  permute_1682 = None
    clone_268 = torch.ops.aten.clone.default(permute_1684, memory_format = torch.contiguous_format);  permute_1684 = None
    _unsafe_view_225 = torch.ops.aten._unsafe_view.default(clone_268, [1, 384, 384]);  clone_268 = None
    bmm_251 = torch.ops.aten.bmm.default(_unsafe_view_224, _unsafe_view_225);  _unsafe_view_224 = _unsafe_view_225 = None
    view_3145 = torch.ops.aten.view.default(bmm_251, [384, 1, 1, 1, 384]);  bmm_251 = None
    permute_1685 = torch.ops.aten.permute.default(view_3145, [3, 0, 4, 1, 2]);  view_3145 = None
    view_3146 = torch.ops.aten.view.default(permute_1685, [1, 384, 384]);  permute_1685 = None
    unsqueeze_1031 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
    mul_416 = torch.ops.aten.mul.Tensor(view_3146, unsqueeze_1031);  view_3146 = unsqueeze_1031 = None
    add_342 = torch.ops.aten.add.Tensor(add_335, mul_416);  mul_416 = None
    split_tensor_334 = torch.ops.aten.split.Tensor(add_335, 384, dim = -2);  add_335 = None
    getitem_3062 = split_tensor_334[0];  split_tensor_334 = None
    _to_copy_1761 = torch.ops.aten._to_copy.default(getitem_3062, dtype = torch.float32);  getitem_3062 = None
    native_layer_norm_default_362 = torch.ops.aten.native_layer_norm.default(_to_copy_1761, [384], pairformer_stack_blocks_35_transition_single_layer_norm_weight, pairformer_stack_blocks_35_transition_single_layer_norm_bias, 1e-05);  _to_copy_1761 = pairformer_stack_blocks_35_transition_single_layer_norm_weight = pairformer_stack_blocks_35_transition_single_layer_norm_bias = None
    getitem_3063 = native_layer_norm_default_362[0];  native_layer_norm_default_362 = None
    _to_copy_1762 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_35_transition_single_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_35_transition_single_linear_no_bias_ab_weight = None
    _to_copy_1763 = torch.ops.aten._to_copy.default(getitem_3063, dtype = torch.bfloat16);  getitem_3063 = None
    t_655 = torch.ops.aten.t.default(_to_copy_1762);  _to_copy_1762 = None
    view_3147 = torch.ops.aten.view.default(_to_copy_1763, [384, 384]);  _to_copy_1763 = None
    mm_609 = torch.ops.aten.mm.default(view_3147, t_655);  view_3147 = t_655 = None
    view_3148 = torch.ops.aten.view.default(mm_609, [1, 384, 1536]);  mm_609 = None
    split_tensor_335 = torch.ops.aten.split.Tensor(view_3148, 768, dim = -1);  view_3148 = None
    getitem_3066 = split_tensor_335[0]
    getitem_3067 = split_tensor_335[1];  split_tensor_335 = None
    silu_86 = torch.ops.aten.silu.default(getitem_3066);  getitem_3066 = None
    mul_417 = torch.ops.aten.mul.Tensor(silu_86, getitem_3067);  silu_86 = getitem_3067 = None
    _to_copy_1764 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_35_transition_single_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_35_transition_single_linear_out_weight = None
    t_656 = torch.ops.aten.t.default(_to_copy_1764);  _to_copy_1764 = None
    view_3150 = torch.ops.aten.view.default(mul_417, [384, 768]);  mul_417 = None
    mm_610 = torch.ops.aten.mm.default(view_3150, t_656);  view_3150 = t_656 = None
    view_3151 = torch.ops.aten.view.default(mm_610, [1, 384, 384]);  mm_610 = None
    add_343 = torch.ops.aten.add.Tensor(add_342, view_3151);  add_342 = view_3151 = None
    _to_copy_1765 = torch.ops.aten._to_copy.default(add_339, dtype = torch.float32)
    native_layer_norm_default_363 = torch.ops.aten.native_layer_norm.default(_to_copy_1765, [256], pairformer_stack_blocks_36_triangle_multiplication_layernorm_z_in_weight, pairformer_stack_blocks_36_triangle_multiplication_layernorm_z_in_bias, 1e-05);  _to_copy_1765 = pairformer_stack_blocks_36_triangle_multiplication_layernorm_z_in_weight = pairformer_stack_blocks_36_triangle_multiplication_layernorm_z_in_bias = None
    getitem_3068 = native_layer_norm_default_363[0];  native_layer_norm_default_363 = None
    split_with_sizes_default_84 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_36_triangle_multiplication_merged_linear_p_weight, [512, 512]);  pairformer_stack_blocks_36_triangle_multiplication_merged_linear_p_weight = None
    getitem_3071 = split_with_sizes_default_84[0]
    getitem_3072 = split_with_sizes_default_84[1];  split_with_sizes_default_84 = None
    split_with_sizes_default_85 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_36_triangle_multiplication_merged_linear_g_weight, [512, 512, 256]);  pairformer_stack_blocks_36_triangle_multiplication_merged_linear_g_weight = None
    getitem_3073 = split_with_sizes_default_85[0]
    getitem_3074 = split_with_sizes_default_85[1]
    getitem_3075 = split_with_sizes_default_85[2];  split_with_sizes_default_85 = None
    _to_copy_1766 = torch.ops.aten._to_copy.default(getitem_3071, dtype = torch.bfloat16);  getitem_3071 = None
    _to_copy_1767 = torch.ops.aten._to_copy.default(getitem_3068, dtype = torch.bfloat16)
    t_657 = torch.ops.aten.t.default(_to_copy_1766);  _to_copy_1766 = None
    view_3152 = torch.ops.aten.view.default(_to_copy_1767, [147456, 256]);  _to_copy_1767 = None
    mm_611 = torch.ops.aten.mm.default(view_3152, t_657);  view_3152 = t_657 = None
    view_3153 = torch.ops.aten.view.default(mm_611, [1, 384, 384, 512]);  mm_611 = None
    _to_copy_1768 = torch.ops.aten._to_copy.default(getitem_3073, dtype = torch.bfloat16);  getitem_3073 = None
    _to_copy_1769 = torch.ops.aten._to_copy.default(getitem_3068, dtype = torch.bfloat16)
    t_658 = torch.ops.aten.t.default(_to_copy_1768);  _to_copy_1768 = None
    view_3154 = torch.ops.aten.view.default(_to_copy_1769, [147456, 256]);  _to_copy_1769 = None
    mm_612 = torch.ops.aten.mm.default(view_3154, t_658);  view_3154 = t_658 = None
    view_3155 = torch.ops.aten.view.default(mm_612, [1, 384, 384, 512]);  mm_612 = None
    sigmoid_252 = torch.ops.aten.sigmoid.default(view_3155);  view_3155 = None
    mul_418 = torch.ops.aten.mul.Tensor(view_3153, sigmoid_252);  view_3153 = sigmoid_252 = None
    unsqueeze_1032 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
    bitwise_not_190 = torch.ops.aten.bitwise_not.default(unsqueeze_1032);  unsqueeze_1032 = None
    masked_fill_190 = torch.ops.aten.masked_fill.Scalar(mul_418, bitwise_not_190, 0);  mul_418 = bitwise_not_190 = None
    split_tensor_336 = torch.ops.aten.split.Tensor(masked_fill_190, 256, dim = -1)
    getitem_3078 = split_tensor_336[0];  split_tensor_336 = None
    unsqueeze_1035 = torch.ops.aten.unsqueeze.default(getitem_3078, 4);  getitem_3078 = None
    permute_1690 = torch.ops.aten.permute.default(unsqueeze_1035, [0, 1, 4, 3, 2]);  unsqueeze_1035 = None
    permute_1691 = torch.ops.aten.permute.default(permute_1690, [3, 1, 4, 0, 2]);  permute_1690 = None
    view_3158 = torch.ops.aten.view.default(permute_1691, [256, 384, 384]);  permute_1691 = None
    split_tensor_337 = torch.ops.aten.split.Tensor(masked_fill_190, 256, dim = -1);  masked_fill_190 = None
    getitem_3081 = split_tensor_337[1];  split_tensor_337 = None
    unsqueeze_1036 = torch.ops.aten.unsqueeze.default(getitem_3081, 4);  getitem_3081 = None
    permute_1692 = torch.ops.aten.permute.default(unsqueeze_1036, [0, 4, 1, 3, 2]);  unsqueeze_1036 = None
    permute_1693 = torch.ops.aten.permute.default(permute_1692, [3, 4, 0, 2, 1]);  permute_1692 = None
    view_3159 = torch.ops.aten.view.default(permute_1693, [256, 384, 384]);  permute_1693 = None
    bmm_252 = torch.ops.aten.bmm.default(view_3158, view_3159);  view_3158 = view_3159 = None
    view_3160 = torch.ops.aten.view.default(bmm_252, [256, 384, 1, 1, 384]);  bmm_252 = None
    permute_1694 = torch.ops.aten.permute.default(view_3160, [3, 1, 4, 0, 2]);  view_3160 = None
    view_3161 = torch.ops.aten.view.default(permute_1694, [1, 384, 384, 256]);  permute_1694 = None
    _to_copy_1770 = torch.ops.aten._to_copy.default(getitem_3072, dtype = torch.bfloat16);  getitem_3072 = None
    _to_copy_1771 = torch.ops.aten._to_copy.default(getitem_3068, dtype = torch.bfloat16)
    t_659 = torch.ops.aten.t.default(_to_copy_1770);  _to_copy_1770 = None
    view_3162 = torch.ops.aten.view.default(_to_copy_1771, [147456, 256]);  _to_copy_1771 = None
    mm_613 = torch.ops.aten.mm.default(view_3162, t_659);  view_3162 = t_659 = None
    view_3163 = torch.ops.aten.view.default(mm_613, [1, 384, 384, 512]);  mm_613 = None
    _to_copy_1772 = torch.ops.aten._to_copy.default(getitem_3074, dtype = torch.bfloat16);  getitem_3074 = None
    _to_copy_1773 = torch.ops.aten._to_copy.default(getitem_3068, dtype = torch.bfloat16)
    t_660 = torch.ops.aten.t.default(_to_copy_1772);  _to_copy_1772 = None
    view_3164 = torch.ops.aten.view.default(_to_copy_1773, [147456, 256]);  _to_copy_1773 = None
    mm_614 = torch.ops.aten.mm.default(view_3164, t_660);  view_3164 = t_660 = None
    view_3165 = torch.ops.aten.view.default(mm_614, [1, 384, 384, 512]);  mm_614 = None
    sigmoid_253 = torch.ops.aten.sigmoid.default(view_3165);  view_3165 = None
    mul_419 = torch.ops.aten.mul.Tensor(view_3163, sigmoid_253);  view_3163 = sigmoid_253 = None
    view_3166 = torch.ops.aten.view.default(mul_419, [147456, 512]);  mul_419 = None
    view_3167 = torch.ops.aten.view.default(view_3166, [1, 384, 384, 512]);  view_3166 = None
    transpose_84 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
    unsqueeze_1037 = torch.ops.aten.unsqueeze.default(transpose_84, 3);  transpose_84 = None
    clone_269 = torch.ops.aten.clone.default(unsqueeze_1037, memory_format = torch.contiguous_format);  unsqueeze_1037 = None
    bitwise_not_191 = torch.ops.aten.bitwise_not.default(clone_269);  clone_269 = None
    masked_fill_191 = torch.ops.aten.masked_fill.Scalar(view_3167, bitwise_not_191, 0);  view_3167 = bitwise_not_191 = None
    view_3168 = torch.ops.aten.view.default(masked_fill_191, [147456, 512]);  masked_fill_191 = None
    view_3172 = torch.ops.aten.view.default(view_3168, [1, 384, 384, 512])
    split_tensor_338 = torch.ops.aten.split.Tensor(view_3172, 256, dim = -1);  view_3172 = None
    getitem_3084 = split_tensor_338[0];  split_tensor_338 = None
    unsqueeze_1040 = torch.ops.aten.unsqueeze.default(getitem_3084, 4);  getitem_3084 = None
    permute_1699 = torch.ops.aten.permute.default(unsqueeze_1040, [0, 2, 4, 3, 1]);  unsqueeze_1040 = None
    permute_1700 = torch.ops.aten.permute.default(permute_1699, [3, 1, 4, 0, 2]);  permute_1699 = None
    view_3173 = torch.ops.aten.view.default(permute_1700, [256, 384, 384]);  permute_1700 = None
    view_3174 = torch.ops.aten.view.default(view_3168, [1, 384, 384, 512]);  view_3168 = None
    split_tensor_339 = torch.ops.aten.split.Tensor(view_3174, 256, dim = -1);  view_3174 = None
    getitem_3087 = split_tensor_339[1];  split_tensor_339 = None
    unsqueeze_1041 = torch.ops.aten.unsqueeze.default(getitem_3087, 4);  getitem_3087 = None
    permute_1701 = torch.ops.aten.permute.default(unsqueeze_1041, [0, 4, 2, 3, 1]);  unsqueeze_1041 = None
    permute_1702 = torch.ops.aten.permute.default(permute_1701, [3, 4, 0, 2, 1]);  permute_1701 = None
    view_3175 = torch.ops.aten.view.default(permute_1702, [256, 384, 384]);  permute_1702 = None
    bmm_253 = torch.ops.aten.bmm.default(view_3173, view_3175);  view_3173 = view_3175 = None
    view_3176 = torch.ops.aten.view.default(bmm_253, [256, 384, 1, 1, 384]);  bmm_253 = None
    permute_1703 = torch.ops.aten.permute.default(view_3176, [3, 1, 4, 0, 2]);  view_3176 = None
    view_3177 = torch.ops.aten.view.default(permute_1703, [1, 384, 384, 256]);  permute_1703 = None
    _to_copy_1774 = torch.ops.aten._to_copy.default(view_3161, dtype = torch.float32);  view_3161 = None
    native_layer_norm_default_364 = torch.ops.aten.native_layer_norm.default(_to_copy_1774, [256], None, None, 1e-05);  _to_copy_1774 = None
    getitem_3088 = native_layer_norm_default_364[0];  native_layer_norm_default_364 = None
    _to_copy_1775 = torch.ops.aten._to_copy.default(view_3177, dtype = torch.float32);  view_3177 = None
    native_layer_norm_default_365 = torch.ops.aten.native_layer_norm.default(_to_copy_1775, [256], None, None, 1e-05);  _to_copy_1775 = None
    getitem_3091 = native_layer_norm_default_365[0];  native_layer_norm_default_365 = None
    add_344 = torch.ops.aten.add.Tensor(getitem_3088, getitem_3091);  getitem_3088 = getitem_3091 = None
    _to_copy_1776 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_36_triangle_multiplication_linear_z_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_36_triangle_multiplication_linear_z_out_weight = None
    _to_copy_1777 = torch.ops.aten._to_copy.default(add_344, dtype = torch.bfloat16);  add_344 = None
    t_661 = torch.ops.aten.t.default(_to_copy_1776);  _to_copy_1776 = None
    view_3178 = torch.ops.aten.view.default(_to_copy_1777, [147456, 256]);  _to_copy_1777 = None
    mm_615 = torch.ops.aten.mm.default(view_3178, t_661);  view_3178 = t_661 = None
    view_3179 = torch.ops.aten.view.default(mm_615, [1, 384, 384, 256]);  mm_615 = None
    _to_copy_1778 = torch.ops.aten._to_copy.default(getitem_3075, dtype = torch.bfloat16);  getitem_3075 = None
    _to_copy_1779 = torch.ops.aten._to_copy.default(getitem_3068, dtype = torch.bfloat16);  getitem_3068 = None
    t_662 = torch.ops.aten.t.default(_to_copy_1778);  _to_copy_1778 = None
    view_3180 = torch.ops.aten.view.default(_to_copy_1779, [147456, 256]);  _to_copy_1779 = None
    mm_616 = torch.ops.aten.mm.default(view_3180, t_662);  view_3180 = t_662 = None
    view_3181 = torch.ops.aten.view.default(mm_616, [1, 384, 384, 256]);  mm_616 = None
    sigmoid_254 = torch.ops.aten.sigmoid.default(view_3181);  view_3181 = None
    mul_420 = torch.ops.aten.mul.Tensor(view_3179, sigmoid_254);  view_3179 = sigmoid_254 = None
    add_345 = torch.ops.aten.add.Tensor(add_339, mul_420);  mul_420 = None
    _to_copy_1780 = torch.ops.aten._to_copy.default(add_339, dtype = torch.float32)
    native_layer_norm_default_366 = torch.ops.aten.native_layer_norm.default(_to_copy_1780, [256], None, None, 1e-05);  _to_copy_1780 = None
    getitem_3094 = native_layer_norm_default_366[0];  native_layer_norm_default_366 = None
    _to_copy_1781 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_36_triangle_attention_pair2b_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_36_triangle_attention_pair2b_weight = None
    _to_copy_1782 = torch.ops.aten._to_copy.default(getitem_3094, dtype = torch.bfloat16)
    t_663 = torch.ops.aten.t.default(_to_copy_1781);  _to_copy_1781 = None
    view_3182 = torch.ops.aten.view.default(_to_copy_1782, [147456, 256]);  _to_copy_1782 = None
    mm_617 = torch.ops.aten.mm.default(view_3182, t_663);  view_3182 = t_663 = None
    view_3183 = torch.ops.aten.view.default(mm_617, [1, 384, 384, 8]);  mm_617 = None
    view_3184 = torch.ops.aten.view.default(view_3183, [1, 384, 384, 2, 4]);  view_3183 = None
    permute_1704 = torch.ops.aten.permute.default(view_3184, [0, 3, 4, 1, 2]);  view_3184 = None
    view_3185 = torch.ops.aten.view.default(permute_1704, [1, 2, 4, 1, 384, 384]);  permute_1704 = None
    view_3186 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 384, 384])
    bitwise_not_192 = torch.ops.aten.bitwise_not.default(view_3186);  view_3186 = None
    masked_fill_192 = torch.ops.aten.masked_fill.Scalar(view_3185, bitwise_not_192, -10000);  view_3185 = bitwise_not_192 = None
    view_3187 = torch.ops.aten.view.default(masked_fill_192, [1, 2, 4, 384, 384]);  masked_fill_192 = None
    permute_1705 = torch.ops.aten.permute.default(view_3187, [1, 0, 2, 3, 4]);  view_3187 = None
    view_3188 = torch.ops.aten.view.default(permute_1705, [2, 4, 1, 384, 384]);  permute_1705 = None
    _to_copy_1783 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_36_triangle_attention_pair2qkvg1_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_36_triangle_attention_pair2qkvg1_weight = None
    _to_copy_1784 = torch.ops.aten._to_copy.default(getitem_3094, dtype = torch.bfloat16)
    t_664 = torch.ops.aten.t.default(_to_copy_1783);  _to_copy_1783 = None
    view_3189 = torch.ops.aten.view.default(_to_copy_1784, [147456, 256]);  _to_copy_1784 = None
    mm_618 = torch.ops.aten.mm.default(view_3189, t_664);  view_3189 = t_664 = None
    view_3190 = torch.ops.aten.view.default(mm_618, [1, 384, 384, 1024]);  mm_618 = None
    select_85 = torch.ops.aten.select.int(view_3188, 0, 0)
    view_3191 = torch.ops.aten.view.default(view_3190, [1, 384, 384, 4, 4, 64]);  view_3190 = None
    permute_1706 = torch.ops.aten.permute.default(view_3191, [4, 0, 3, 1, 2, 5]);  view_3191 = None
    view_3192 = torch.ops.aten.view.default(permute_1706, [4, 4, 384, 384, 64]);  permute_1706 = None
    unbind_int_142 = torch.ops.aten.unbind.int(view_3192);  view_3192 = None
    getitem_3097 = unbind_int_142[0]
    getitem_3098 = unbind_int_142[1]
    getitem_3099 = unbind_int_142[2]
    getitem_3100 = unbind_int_142[3];  unbind_int_142 = None
    expand_207 = torch.ops.aten.expand.default(select_85, [4, 384, 384, 384]);  select_85 = None
    _scaled_dot_product_efficient_attention_default_120 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3097, getitem_3098, getitem_3099, expand_207, False);  getitem_3097 = getitem_3098 = getitem_3099 = expand_207 = None
    getitem_3101 = _scaled_dot_product_efficient_attention_default_120[0];  _scaled_dot_product_efficient_attention_default_120 = None
    sigmoid_255 = torch.ops.aten.sigmoid.default(getitem_3100);  getitem_3100 = None
    mul_421 = torch.ops.aten.mul.Tensor(getitem_3101, sigmoid_255);  getitem_3101 = sigmoid_255 = None
    view_3193 = torch.ops.aten.view.default(mul_421, [1, 4, 384, 384, 64]);  mul_421 = None
    permute_1707 = torch.ops.aten.permute.default(view_3193, [0, 2, 3, 1, 4]);  view_3193 = None
    clone_270 = torch.ops.aten.clone.default(permute_1707, memory_format = torch.contiguous_format);  permute_1707 = None
    _unsafe_view_226 = torch.ops.aten._unsafe_view.default(clone_270, [1, 384, 384, 256]);  clone_270 = None
    transpose_85 = torch.ops.aten.transpose.int(getitem_3094, 1, 2);  getitem_3094 = None
    _to_copy_1785 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_36_triangle_attention_pair2qkvg2_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_36_triangle_attention_pair2qkvg2_weight = None
    _to_copy_1786 = torch.ops.aten._to_copy.default(transpose_85, dtype = torch.bfloat16);  transpose_85 = None
    t_665 = torch.ops.aten.t.default(_to_copy_1785);  _to_copy_1785 = None
    expand_208 = torch.ops.aten.expand.default(_to_copy_1786, [1, 384, 384, 256]);  _to_copy_1786 = None
    view_3194 = torch.ops.aten.view.default(expand_208, [384, 384, 256]);  expand_208 = None
    expand_209 = torch.ops.aten.expand.default(t_665, [1, 384, 256, 1024]);  t_665 = None
    view_3195 = torch.ops.aten.view.default(expand_209, [384, 256, 1024]);  expand_209 = None
    bmm_254 = torch.ops.aten.bmm.default(view_3194, view_3195);  view_3194 = view_3195 = None
    view_3196 = torch.ops.aten.view.default(bmm_254, [1, 384, 384, 1024]);  bmm_254 = None
    select_86 = torch.ops.aten.select.int(view_3188, 0, 1);  view_3188 = None
    view_3197 = torch.ops.aten.view.default(view_3196, [1, 384, 384, 4, 4, 64]);  view_3196 = None
    permute_1708 = torch.ops.aten.permute.default(view_3197, [4, 0, 3, 1, 2, 5]);  view_3197 = None
    view_3198 = torch.ops.aten.view.default(permute_1708, [4, 4, 384, 384, 64]);  permute_1708 = None
    unbind_int_143 = torch.ops.aten.unbind.int(view_3198);  view_3198 = None
    getitem_3105 = unbind_int_143[0]
    getitem_3106 = unbind_int_143[1]
    getitem_3107 = unbind_int_143[2]
    getitem_3108 = unbind_int_143[3];  unbind_int_143 = None
    expand_210 = torch.ops.aten.expand.default(select_86, [4, 384, 384, 384]);  select_86 = None
    _scaled_dot_product_efficient_attention_default_121 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3105, getitem_3106, getitem_3107, expand_210, False);  getitem_3105 = getitem_3106 = getitem_3107 = expand_210 = None
    getitem_3109 = _scaled_dot_product_efficient_attention_default_121[0];  _scaled_dot_product_efficient_attention_default_121 = None
    sigmoid_256 = torch.ops.aten.sigmoid.default(getitem_3108);  getitem_3108 = None
    mul_422 = torch.ops.aten.mul.Tensor(getitem_3109, sigmoid_256);  getitem_3109 = sigmoid_256 = None
    view_3199 = torch.ops.aten.view.default(mul_422, [1, 4, 384, 384, 64]);  mul_422 = None
    permute_1709 = torch.ops.aten.permute.default(view_3199, [0, 2, 3, 1, 4]);  view_3199 = None
    clone_271 = torch.ops.aten.clone.default(permute_1709, memory_format = torch.contiguous_format);  permute_1709 = None
    _unsafe_view_227 = torch.ops.aten._unsafe_view.default(clone_271, [1, 384, 384, 256]);  clone_271 = None
    cat_48 = torch.ops.aten.cat.default([_unsafe_view_226, _unsafe_view_227], dim = -1);  _unsafe_view_226 = _unsafe_view_227 = None
    slice_199 = torch.ops.aten.slice.Tensor(pairformer_stack_blocks_36_triangle_attention_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  pairformer_stack_blocks_36_triangle_attention_out_scalers = None
    unsqueeze_1042 = torch.ops.aten.unsqueeze.default(slice_199, 1);  slice_199 = None
    mul_423 = torch.ops.aten.mul.Tensor(pairformer_stack_blocks_36_triangle_attention_linear_out_weight, unsqueeze_1042);  pairformer_stack_blocks_36_triangle_attention_linear_out_weight = unsqueeze_1042 = None
    _to_copy_1787 = torch.ops.aten._to_copy.default(mul_423, dtype = torch.bfloat16);  mul_423 = None
    t_666 = torch.ops.aten.t.default(_to_copy_1787);  _to_copy_1787 = None
    view_3200 = torch.ops.aten.view.default(cat_48, [147456, 512]);  cat_48 = None
    mm_619 = torch.ops.aten.mm.default(view_3200, t_666);  view_3200 = t_666 = None
    view_3201 = torch.ops.aten.view.default(mm_619, [1, 384, 384, 256]);  mm_619 = None
    add_346 = torch.ops.aten.add.Tensor(add_345, view_3201);  add_345 = view_3201 = None
    split_tensor_340 = torch.ops.aten.split.Tensor(add_339, 384, dim = -2)
    getitem_3113 = split_tensor_340[0];  split_tensor_340 = None
    _to_copy_1788 = torch.ops.aten._to_copy.default(getitem_3113, dtype = torch.float32);  getitem_3113 = None
    native_layer_norm_default_367 = torch.ops.aten.native_layer_norm.default(_to_copy_1788, [256], pairformer_stack_blocks_36_transition_pair_layer_norm_weight, pairformer_stack_blocks_36_transition_pair_layer_norm_bias, 1e-05);  _to_copy_1788 = pairformer_stack_blocks_36_transition_pair_layer_norm_weight = pairformer_stack_blocks_36_transition_pair_layer_norm_bias = None
    getitem_3114 = native_layer_norm_default_367[0];  native_layer_norm_default_367 = None
    _to_copy_1789 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_36_transition_pair_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_36_transition_pair_linear_no_bias_ab_weight = None
    _to_copy_1790 = torch.ops.aten._to_copy.default(getitem_3114, dtype = torch.bfloat16);  getitem_3114 = None
    t_667 = torch.ops.aten.t.default(_to_copy_1789);  _to_copy_1789 = None
    view_3202 = torch.ops.aten.view.default(_to_copy_1790, [147456, 256]);  _to_copy_1790 = None
    mm_620 = torch.ops.aten.mm.default(view_3202, t_667);  view_3202 = t_667 = None
    view_3203 = torch.ops.aten.view.default(mm_620, [1, 384, 384, 1024]);  mm_620 = None
    split_tensor_341 = torch.ops.aten.split.Tensor(view_3203, 512, dim = -1);  view_3203 = None
    getitem_3117 = split_tensor_341[0]
    getitem_3118 = split_tensor_341[1];  split_tensor_341 = None
    silu_87 = torch.ops.aten.silu.default(getitem_3117);  getitem_3117 = None
    mul_424 = torch.ops.aten.mul.Tensor(silu_87, getitem_3118);  silu_87 = getitem_3118 = None
    _to_copy_1791 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_36_transition_pair_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_36_transition_pair_linear_out_weight = None
    t_668 = torch.ops.aten.t.default(_to_copy_1791);  _to_copy_1791 = None
    view_3205 = torch.ops.aten.view.default(mul_424, [147456, 512]);  mul_424 = None
    mm_621 = torch.ops.aten.mm.default(view_3205, t_668);  view_3205 = t_668 = None
    view_3206 = torch.ops.aten.view.default(mm_621, [1, 384, 384, 256]);  mm_621 = None
    add_347 = torch.ops.aten.add.Tensor(add_346, view_3206);  add_346 = view_3206 = None
    _to_copy_1792 = torch.ops.aten._to_copy.default(add_343, dtype = torch.float32)
    native_layer_norm_default_368 = torch.ops.aten.native_layer_norm.default(_to_copy_1792, [384], pairformer_stack_blocks_36_attention_pair_bias_single_layer_norm_weight, pairformer_stack_blocks_36_attention_pair_bias_single_layer_norm_bias, 1e-05);  _to_copy_1792 = pairformer_stack_blocks_36_attention_pair_bias_single_layer_norm_weight = pairformer_stack_blocks_36_attention_pair_bias_single_layer_norm_bias = None
    getitem_3119 = native_layer_norm_default_368[0];  native_layer_norm_default_368 = None
    _to_copy_1793 = torch.ops.aten._to_copy.default(add_339, dtype = torch.float32);  add_339 = None
    native_layer_norm_default_369 = torch.ops.aten.native_layer_norm.default(_to_copy_1793, [256], pairformer_stack_blocks_36_attention_pair_bias_pair_layer_norm_weight, pairformer_stack_blocks_36_attention_pair_bias_pair_layer_norm_bias, 1e-05);  _to_copy_1793 = pairformer_stack_blocks_36_attention_pair_bias_pair_layer_norm_weight = pairformer_stack_blocks_36_attention_pair_bias_pair_layer_norm_bias = None
    getitem_3122 = native_layer_norm_default_369[0];  native_layer_norm_default_369 = None
    _to_copy_1794 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_36_attention_pair_bias_pair_linear_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_36_attention_pair_bias_pair_linear_weight = None
    _to_copy_1795 = torch.ops.aten._to_copy.default(getitem_3122, dtype = torch.bfloat16);  getitem_3122 = None
    t_669 = torch.ops.aten.t.default(_to_copy_1794);  _to_copy_1794 = None
    view_3207 = torch.ops.aten.view.default(_to_copy_1795, [147456, 256]);  _to_copy_1795 = None
    mm_622 = torch.ops.aten.mm.default(view_3207, t_669);  view_3207 = t_669 = None
    view_3208 = torch.ops.aten.view.default(mm_622, [1, 384, 384, 16]);  mm_622 = None
    permute_1710 = torch.ops.aten.permute.default(view_3208, [0, 3, 1, 2]);  view_3208 = None
    view_3209 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384])
    bitwise_not_193 = torch.ops.aten.bitwise_not.default(view_3209);  view_3209 = None
    masked_fill_193 = torch.ops.aten.masked_fill.Scalar(permute_1710, bitwise_not_193, -10000);  permute_1710 = bitwise_not_193 = None
    _to_copy_1796 = torch.ops.aten._to_copy.default(getitem_3119, dtype = torch.bfloat16);  getitem_3119 = None
    _to_copy_1797 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_36_attention_pair_bias_attention_input2qkvg_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_36_attention_pair_bias_attention_input2qkvg_weight = None
    unsqueeze_1043 = torch.ops.aten.unsqueeze.default(_to_copy_1796, 3);  _to_copy_1796 = None
    unsqueeze_1044 = torch.ops.aten.unsqueeze.default(unsqueeze_1043, 4);  unsqueeze_1043 = None
    unsqueeze_1045 = torch.ops.aten.unsqueeze.default(unsqueeze_1044, 5);  unsqueeze_1044 = None
    permute_1711 = torch.ops.aten.permute.default(unsqueeze_1045, [3, 0, 4, 1, 5, 2]);  unsqueeze_1045 = None
    unsqueeze_1046 = torch.ops.aten.unsqueeze.default(_to_copy_1797, 4);  _to_copy_1797 = None
    unsqueeze_1047 = torch.ops.aten.unsqueeze.default(unsqueeze_1046, 5);  unsqueeze_1046 = None
    permute_1712 = torch.ops.aten.permute.default(unsqueeze_1047, [1, 4, 2, 5, 3, 0]);  unsqueeze_1047 = None
    permute_1713 = torch.ops.aten.permute.default(permute_1711, [3, 5, 0, 1, 2, 4]);  permute_1711 = None
    view_3210 = torch.ops.aten.view.default(permute_1713, [1, 384, 384]);  permute_1713 = None
    permute_1714 = torch.ops.aten.permute.default(permute_1712, [5, 0, 1, 2, 4, 3]);  permute_1712 = None
    view_3211 = torch.ops.aten.view.default(permute_1714, [1, 384, 1536]);  permute_1714 = None
    bmm_255 = torch.ops.aten.bmm.default(view_3210, view_3211);  view_3210 = view_3211 = None
    view_3212 = torch.ops.aten.view.default(bmm_255, [384, 1, 4, 1, 16, 24]);  bmm_255 = None
    permute_1715 = torch.ops.aten.permute.default(view_3212, [2, 3, 4, 0, 5, 1]);  view_3212 = None
    view_3213 = torch.ops.aten.view.default(permute_1715, [4, 1, 16, 384, 24]);  permute_1715 = None
    unbind_int_144 = torch.ops.aten.unbind.int(view_3213);  view_3213 = None
    getitem_3125 = unbind_int_144[0]
    getitem_3126 = unbind_int_144[1]
    getitem_3127 = unbind_int_144[2]
    getitem_3128 = unbind_int_144[3];  unbind_int_144 = None
    view_3214 = torch.ops.aten.view.default(pairformer_stack_blocks_36_attention_pair_bias_attention_query_bias, [1, 16, 1, 24]);  pairformer_stack_blocks_36_attention_pair_bias_attention_query_bias = None
    add_348 = torch.ops.aten.add.Tensor(getitem_3125, view_3214);  getitem_3125 = view_3214 = None
    _to_copy_1798 = torch.ops.aten._to_copy.default(add_348, dtype = torch.bfloat16);  add_348 = None
    expand_211 = torch.ops.aten.expand.default(masked_fill_193, [1, 16, 384, 384]);  masked_fill_193 = None
    _scaled_dot_product_efficient_attention_default_122 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_1798, getitem_3126, getitem_3127, expand_211, False);  _to_copy_1798 = getitem_3126 = getitem_3127 = expand_211 = None
    getitem_3129 = _scaled_dot_product_efficient_attention_default_122[0];  _scaled_dot_product_efficient_attention_default_122 = None
    add_349 = torch.ops.aten.add.Tensor(getitem_3128, 1);  getitem_3128 = None
    sigmoid_257 = torch.ops.aten.sigmoid.default(add_349);  add_349 = None
    mul_425 = torch.ops.aten.mul.Tensor(getitem_3129, sigmoid_257);  getitem_3129 = sigmoid_257 = None
    _to_copy_1799 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_36_attention_pair_bias_attention_output_proj_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_36_attention_pair_bias_attention_output_proj_weight = None
    unsqueeze_1048 = torch.ops.aten.unsqueeze.default(mul_425, 4);  mul_425 = None
    permute_1716 = torch.ops.aten.permute.default(unsqueeze_1048, [0, 2, 4, 3, 1]);  unsqueeze_1048 = None
    unsqueeze_1049 = torch.ops.aten.unsqueeze.default(_to_copy_1799, 3);  _to_copy_1799 = None
    unsqueeze_1050 = torch.ops.aten.unsqueeze.default(unsqueeze_1049, 4);  unsqueeze_1049 = None
    permute_1717 = torch.ops.aten.permute.default(unsqueeze_1050, [3, 4, 2, 1, 0]);  unsqueeze_1050 = None
    permute_1718 = torch.ops.aten.permute.default(permute_1716, [1, 3, 4, 0, 2]);  permute_1716 = None
    clone_272 = torch.ops.aten.clone.default(permute_1718, memory_format = torch.contiguous_format);  permute_1718 = None
    _unsafe_view_228 = torch.ops.aten._unsafe_view.default(clone_272, [1, 384, 384]);  clone_272 = None
    permute_1719 = torch.ops.aten.permute.default(permute_1717, [3, 4, 0, 2, 1]);  permute_1717 = None
    clone_273 = torch.ops.aten.clone.default(permute_1719, memory_format = torch.contiguous_format);  permute_1719 = None
    _unsafe_view_229 = torch.ops.aten._unsafe_view.default(clone_273, [1, 384, 384]);  clone_273 = None
    bmm_256 = torch.ops.aten.bmm.default(_unsafe_view_228, _unsafe_view_229);  _unsafe_view_228 = _unsafe_view_229 = None
    view_3215 = torch.ops.aten.view.default(bmm_256, [384, 1, 1, 1, 384]);  bmm_256 = None
    permute_1720 = torch.ops.aten.permute.default(view_3215, [3, 0, 4, 1, 2]);  view_3215 = None
    view_3216 = torch.ops.aten.view.default(permute_1720, [1, 384, 384]);  permute_1720 = None
    unsqueeze_1051 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
    mul_426 = torch.ops.aten.mul.Tensor(view_3216, unsqueeze_1051);  view_3216 = unsqueeze_1051 = None
    add_350 = torch.ops.aten.add.Tensor(add_343, mul_426);  mul_426 = None
    split_tensor_342 = torch.ops.aten.split.Tensor(add_343, 384, dim = -2);  add_343 = None
    getitem_3133 = split_tensor_342[0];  split_tensor_342 = None
    _to_copy_1800 = torch.ops.aten._to_copy.default(getitem_3133, dtype = torch.float32);  getitem_3133 = None
    native_layer_norm_default_370 = torch.ops.aten.native_layer_norm.default(_to_copy_1800, [384], pairformer_stack_blocks_36_transition_single_layer_norm_weight, pairformer_stack_blocks_36_transition_single_layer_norm_bias, 1e-05);  _to_copy_1800 = pairformer_stack_blocks_36_transition_single_layer_norm_weight = pairformer_stack_blocks_36_transition_single_layer_norm_bias = None
    getitem_3134 = native_layer_norm_default_370[0];  native_layer_norm_default_370 = None
    _to_copy_1801 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_36_transition_single_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_36_transition_single_linear_no_bias_ab_weight = None
    _to_copy_1802 = torch.ops.aten._to_copy.default(getitem_3134, dtype = torch.bfloat16);  getitem_3134 = None
    t_670 = torch.ops.aten.t.default(_to_copy_1801);  _to_copy_1801 = None
    view_3217 = torch.ops.aten.view.default(_to_copy_1802, [384, 384]);  _to_copy_1802 = None
    mm_623 = torch.ops.aten.mm.default(view_3217, t_670);  view_3217 = t_670 = None
    view_3218 = torch.ops.aten.view.default(mm_623, [1, 384, 1536]);  mm_623 = None
    split_tensor_343 = torch.ops.aten.split.Tensor(view_3218, 768, dim = -1);  view_3218 = None
    getitem_3137 = split_tensor_343[0]
    getitem_3138 = split_tensor_343[1];  split_tensor_343 = None
    silu_88 = torch.ops.aten.silu.default(getitem_3137);  getitem_3137 = None
    mul_427 = torch.ops.aten.mul.Tensor(silu_88, getitem_3138);  silu_88 = getitem_3138 = None
    _to_copy_1803 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_36_transition_single_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_36_transition_single_linear_out_weight = None
    t_671 = torch.ops.aten.t.default(_to_copy_1803);  _to_copy_1803 = None
    view_3220 = torch.ops.aten.view.default(mul_427, [384, 768]);  mul_427 = None
    mm_624 = torch.ops.aten.mm.default(view_3220, t_671);  view_3220 = t_671 = None
    view_3221 = torch.ops.aten.view.default(mm_624, [1, 384, 384]);  mm_624 = None
    add_351 = torch.ops.aten.add.Tensor(add_350, view_3221);  add_350 = view_3221 = None
    _to_copy_1804 = torch.ops.aten._to_copy.default(add_347, dtype = torch.float32)
    native_layer_norm_default_371 = torch.ops.aten.native_layer_norm.default(_to_copy_1804, [256], pairformer_stack_blocks_37_triangle_multiplication_layernorm_z_in_weight, pairformer_stack_blocks_37_triangle_multiplication_layernorm_z_in_bias, 1e-05);  _to_copy_1804 = pairformer_stack_blocks_37_triangle_multiplication_layernorm_z_in_weight = pairformer_stack_blocks_37_triangle_multiplication_layernorm_z_in_bias = None
    getitem_3139 = native_layer_norm_default_371[0];  native_layer_norm_default_371 = None
    split_with_sizes_default_86 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_37_triangle_multiplication_merged_linear_p_weight, [512, 512]);  pairformer_stack_blocks_37_triangle_multiplication_merged_linear_p_weight = None
    getitem_3142 = split_with_sizes_default_86[0]
    getitem_3143 = split_with_sizes_default_86[1];  split_with_sizes_default_86 = None
    split_with_sizes_default_87 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_37_triangle_multiplication_merged_linear_g_weight, [512, 512, 256]);  pairformer_stack_blocks_37_triangle_multiplication_merged_linear_g_weight = None
    getitem_3144 = split_with_sizes_default_87[0]
    getitem_3145 = split_with_sizes_default_87[1]
    getitem_3146 = split_with_sizes_default_87[2];  split_with_sizes_default_87 = None
    _to_copy_1805 = torch.ops.aten._to_copy.default(getitem_3142, dtype = torch.bfloat16);  getitem_3142 = None
    _to_copy_1806 = torch.ops.aten._to_copy.default(getitem_3139, dtype = torch.bfloat16)
    t_672 = torch.ops.aten.t.default(_to_copy_1805);  _to_copy_1805 = None
    view_3222 = torch.ops.aten.view.default(_to_copy_1806, [147456, 256]);  _to_copy_1806 = None
    mm_625 = torch.ops.aten.mm.default(view_3222, t_672);  view_3222 = t_672 = None
    view_3223 = torch.ops.aten.view.default(mm_625, [1, 384, 384, 512]);  mm_625 = None
    _to_copy_1807 = torch.ops.aten._to_copy.default(getitem_3144, dtype = torch.bfloat16);  getitem_3144 = None
    _to_copy_1808 = torch.ops.aten._to_copy.default(getitem_3139, dtype = torch.bfloat16)
    t_673 = torch.ops.aten.t.default(_to_copy_1807);  _to_copy_1807 = None
    view_3224 = torch.ops.aten.view.default(_to_copy_1808, [147456, 256]);  _to_copy_1808 = None
    mm_626 = torch.ops.aten.mm.default(view_3224, t_673);  view_3224 = t_673 = None
    view_3225 = torch.ops.aten.view.default(mm_626, [1, 384, 384, 512]);  mm_626 = None
    sigmoid_258 = torch.ops.aten.sigmoid.default(view_3225);  view_3225 = None
    mul_428 = torch.ops.aten.mul.Tensor(view_3223, sigmoid_258);  view_3223 = sigmoid_258 = None
    unsqueeze_1052 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
    bitwise_not_194 = torch.ops.aten.bitwise_not.default(unsqueeze_1052);  unsqueeze_1052 = None
    masked_fill_194 = torch.ops.aten.masked_fill.Scalar(mul_428, bitwise_not_194, 0);  mul_428 = bitwise_not_194 = None
    split_tensor_344 = torch.ops.aten.split.Tensor(masked_fill_194, 256, dim = -1)
    getitem_3149 = split_tensor_344[0];  split_tensor_344 = None
    unsqueeze_1055 = torch.ops.aten.unsqueeze.default(getitem_3149, 4);  getitem_3149 = None
    permute_1725 = torch.ops.aten.permute.default(unsqueeze_1055, [0, 1, 4, 3, 2]);  unsqueeze_1055 = None
    permute_1726 = torch.ops.aten.permute.default(permute_1725, [3, 1, 4, 0, 2]);  permute_1725 = None
    view_3228 = torch.ops.aten.view.default(permute_1726, [256, 384, 384]);  permute_1726 = None
    split_tensor_345 = torch.ops.aten.split.Tensor(masked_fill_194, 256, dim = -1);  masked_fill_194 = None
    getitem_3152 = split_tensor_345[1];  split_tensor_345 = None
    unsqueeze_1056 = torch.ops.aten.unsqueeze.default(getitem_3152, 4);  getitem_3152 = None
    permute_1727 = torch.ops.aten.permute.default(unsqueeze_1056, [0, 4, 1, 3, 2]);  unsqueeze_1056 = None
    permute_1728 = torch.ops.aten.permute.default(permute_1727, [3, 4, 0, 2, 1]);  permute_1727 = None
    view_3229 = torch.ops.aten.view.default(permute_1728, [256, 384, 384]);  permute_1728 = None
    bmm_257 = torch.ops.aten.bmm.default(view_3228, view_3229);  view_3228 = view_3229 = None
    view_3230 = torch.ops.aten.view.default(bmm_257, [256, 384, 1, 1, 384]);  bmm_257 = None
    permute_1729 = torch.ops.aten.permute.default(view_3230, [3, 1, 4, 0, 2]);  view_3230 = None
    view_3231 = torch.ops.aten.view.default(permute_1729, [1, 384, 384, 256]);  permute_1729 = None
    _to_copy_1809 = torch.ops.aten._to_copy.default(getitem_3143, dtype = torch.bfloat16);  getitem_3143 = None
    _to_copy_1810 = torch.ops.aten._to_copy.default(getitem_3139, dtype = torch.bfloat16)
    t_674 = torch.ops.aten.t.default(_to_copy_1809);  _to_copy_1809 = None
    view_3232 = torch.ops.aten.view.default(_to_copy_1810, [147456, 256]);  _to_copy_1810 = None
    mm_627 = torch.ops.aten.mm.default(view_3232, t_674);  view_3232 = t_674 = None
    view_3233 = torch.ops.aten.view.default(mm_627, [1, 384, 384, 512]);  mm_627 = None
    _to_copy_1811 = torch.ops.aten._to_copy.default(getitem_3145, dtype = torch.bfloat16);  getitem_3145 = None
    _to_copy_1812 = torch.ops.aten._to_copy.default(getitem_3139, dtype = torch.bfloat16)
    t_675 = torch.ops.aten.t.default(_to_copy_1811);  _to_copy_1811 = None
    view_3234 = torch.ops.aten.view.default(_to_copy_1812, [147456, 256]);  _to_copy_1812 = None
    mm_628 = torch.ops.aten.mm.default(view_3234, t_675);  view_3234 = t_675 = None
    view_3235 = torch.ops.aten.view.default(mm_628, [1, 384, 384, 512]);  mm_628 = None
    sigmoid_259 = torch.ops.aten.sigmoid.default(view_3235);  view_3235 = None
    mul_429 = torch.ops.aten.mul.Tensor(view_3233, sigmoid_259);  view_3233 = sigmoid_259 = None
    view_3236 = torch.ops.aten.view.default(mul_429, [147456, 512]);  mul_429 = None
    view_3237 = torch.ops.aten.view.default(view_3236, [1, 384, 384, 512]);  view_3236 = None
    transpose_86 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
    unsqueeze_1057 = torch.ops.aten.unsqueeze.default(transpose_86, 3);  transpose_86 = None
    clone_274 = torch.ops.aten.clone.default(unsqueeze_1057, memory_format = torch.contiguous_format);  unsqueeze_1057 = None
    bitwise_not_195 = torch.ops.aten.bitwise_not.default(clone_274);  clone_274 = None
    masked_fill_195 = torch.ops.aten.masked_fill.Scalar(view_3237, bitwise_not_195, 0);  view_3237 = bitwise_not_195 = None
    view_3238 = torch.ops.aten.view.default(masked_fill_195, [147456, 512]);  masked_fill_195 = None
    view_3242 = torch.ops.aten.view.default(view_3238, [1, 384, 384, 512])
    split_tensor_346 = torch.ops.aten.split.Tensor(view_3242, 256, dim = -1);  view_3242 = None
    getitem_3155 = split_tensor_346[0];  split_tensor_346 = None
    unsqueeze_1060 = torch.ops.aten.unsqueeze.default(getitem_3155, 4);  getitem_3155 = None
    permute_1734 = torch.ops.aten.permute.default(unsqueeze_1060, [0, 2, 4, 3, 1]);  unsqueeze_1060 = None
    permute_1735 = torch.ops.aten.permute.default(permute_1734, [3, 1, 4, 0, 2]);  permute_1734 = None
    view_3243 = torch.ops.aten.view.default(permute_1735, [256, 384, 384]);  permute_1735 = None
    view_3244 = torch.ops.aten.view.default(view_3238, [1, 384, 384, 512]);  view_3238 = None
    split_tensor_347 = torch.ops.aten.split.Tensor(view_3244, 256, dim = -1);  view_3244 = None
    getitem_3158 = split_tensor_347[1];  split_tensor_347 = None
    unsqueeze_1061 = torch.ops.aten.unsqueeze.default(getitem_3158, 4);  getitem_3158 = None
    permute_1736 = torch.ops.aten.permute.default(unsqueeze_1061, [0, 4, 2, 3, 1]);  unsqueeze_1061 = None
    permute_1737 = torch.ops.aten.permute.default(permute_1736, [3, 4, 0, 2, 1]);  permute_1736 = None
    view_3245 = torch.ops.aten.view.default(permute_1737, [256, 384, 384]);  permute_1737 = None
    bmm_258 = torch.ops.aten.bmm.default(view_3243, view_3245);  view_3243 = view_3245 = None
    view_3246 = torch.ops.aten.view.default(bmm_258, [256, 384, 1, 1, 384]);  bmm_258 = None
    permute_1738 = torch.ops.aten.permute.default(view_3246, [3, 1, 4, 0, 2]);  view_3246 = None
    view_3247 = torch.ops.aten.view.default(permute_1738, [1, 384, 384, 256]);  permute_1738 = None
    _to_copy_1813 = torch.ops.aten._to_copy.default(view_3231, dtype = torch.float32);  view_3231 = None
    native_layer_norm_default_372 = torch.ops.aten.native_layer_norm.default(_to_copy_1813, [256], None, None, 1e-05);  _to_copy_1813 = None
    getitem_3159 = native_layer_norm_default_372[0];  native_layer_norm_default_372 = None
    _to_copy_1814 = torch.ops.aten._to_copy.default(view_3247, dtype = torch.float32);  view_3247 = None
    native_layer_norm_default_373 = torch.ops.aten.native_layer_norm.default(_to_copy_1814, [256], None, None, 1e-05);  _to_copy_1814 = None
    getitem_3162 = native_layer_norm_default_373[0];  native_layer_norm_default_373 = None
    add_352 = torch.ops.aten.add.Tensor(getitem_3159, getitem_3162);  getitem_3159 = getitem_3162 = None
    _to_copy_1815 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_37_triangle_multiplication_linear_z_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_37_triangle_multiplication_linear_z_out_weight = None
    _to_copy_1816 = torch.ops.aten._to_copy.default(add_352, dtype = torch.bfloat16);  add_352 = None
    t_676 = torch.ops.aten.t.default(_to_copy_1815);  _to_copy_1815 = None
    view_3248 = torch.ops.aten.view.default(_to_copy_1816, [147456, 256]);  _to_copy_1816 = None
    mm_629 = torch.ops.aten.mm.default(view_3248, t_676);  view_3248 = t_676 = None
    view_3249 = torch.ops.aten.view.default(mm_629, [1, 384, 384, 256]);  mm_629 = None
    _to_copy_1817 = torch.ops.aten._to_copy.default(getitem_3146, dtype = torch.bfloat16);  getitem_3146 = None
    _to_copy_1818 = torch.ops.aten._to_copy.default(getitem_3139, dtype = torch.bfloat16);  getitem_3139 = None
    t_677 = torch.ops.aten.t.default(_to_copy_1817);  _to_copy_1817 = None
    view_3250 = torch.ops.aten.view.default(_to_copy_1818, [147456, 256]);  _to_copy_1818 = None
    mm_630 = torch.ops.aten.mm.default(view_3250, t_677);  view_3250 = t_677 = None
    view_3251 = torch.ops.aten.view.default(mm_630, [1, 384, 384, 256]);  mm_630 = None
    sigmoid_260 = torch.ops.aten.sigmoid.default(view_3251);  view_3251 = None
    mul_430 = torch.ops.aten.mul.Tensor(view_3249, sigmoid_260);  view_3249 = sigmoid_260 = None
    add_353 = torch.ops.aten.add.Tensor(add_347, mul_430);  mul_430 = None
    _to_copy_1819 = torch.ops.aten._to_copy.default(add_347, dtype = torch.float32)
    native_layer_norm_default_374 = torch.ops.aten.native_layer_norm.default(_to_copy_1819, [256], None, None, 1e-05);  _to_copy_1819 = None
    getitem_3165 = native_layer_norm_default_374[0];  native_layer_norm_default_374 = None
    _to_copy_1820 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_37_triangle_attention_pair2b_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_37_triangle_attention_pair2b_weight = None
    _to_copy_1821 = torch.ops.aten._to_copy.default(getitem_3165, dtype = torch.bfloat16)
    t_678 = torch.ops.aten.t.default(_to_copy_1820);  _to_copy_1820 = None
    view_3252 = torch.ops.aten.view.default(_to_copy_1821, [147456, 256]);  _to_copy_1821 = None
    mm_631 = torch.ops.aten.mm.default(view_3252, t_678);  view_3252 = t_678 = None
    view_3253 = torch.ops.aten.view.default(mm_631, [1, 384, 384, 8]);  mm_631 = None
    view_3254 = torch.ops.aten.view.default(view_3253, [1, 384, 384, 2, 4]);  view_3253 = None
    permute_1739 = torch.ops.aten.permute.default(view_3254, [0, 3, 4, 1, 2]);  view_3254 = None
    view_3255 = torch.ops.aten.view.default(permute_1739, [1, 2, 4, 1, 384, 384]);  permute_1739 = None
    view_3256 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 384, 384])
    bitwise_not_196 = torch.ops.aten.bitwise_not.default(view_3256);  view_3256 = None
    masked_fill_196 = torch.ops.aten.masked_fill.Scalar(view_3255, bitwise_not_196, -10000);  view_3255 = bitwise_not_196 = None
    view_3257 = torch.ops.aten.view.default(masked_fill_196, [1, 2, 4, 384, 384]);  masked_fill_196 = None
    permute_1740 = torch.ops.aten.permute.default(view_3257, [1, 0, 2, 3, 4]);  view_3257 = None
    view_3258 = torch.ops.aten.view.default(permute_1740, [2, 4, 1, 384, 384]);  permute_1740 = None
    _to_copy_1822 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_37_triangle_attention_pair2qkvg1_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_37_triangle_attention_pair2qkvg1_weight = None
    _to_copy_1823 = torch.ops.aten._to_copy.default(getitem_3165, dtype = torch.bfloat16)
    t_679 = torch.ops.aten.t.default(_to_copy_1822);  _to_copy_1822 = None
    view_3259 = torch.ops.aten.view.default(_to_copy_1823, [147456, 256]);  _to_copy_1823 = None
    mm_632 = torch.ops.aten.mm.default(view_3259, t_679);  view_3259 = t_679 = None
    view_3260 = torch.ops.aten.view.default(mm_632, [1, 384, 384, 1024]);  mm_632 = None
    select_87 = torch.ops.aten.select.int(view_3258, 0, 0)
    view_3261 = torch.ops.aten.view.default(view_3260, [1, 384, 384, 4, 4, 64]);  view_3260 = None
    permute_1741 = torch.ops.aten.permute.default(view_3261, [4, 0, 3, 1, 2, 5]);  view_3261 = None
    view_3262 = torch.ops.aten.view.default(permute_1741, [4, 4, 384, 384, 64]);  permute_1741 = None
    unbind_int_145 = torch.ops.aten.unbind.int(view_3262);  view_3262 = None
    getitem_3168 = unbind_int_145[0]
    getitem_3169 = unbind_int_145[1]
    getitem_3170 = unbind_int_145[2]
    getitem_3171 = unbind_int_145[3];  unbind_int_145 = None
    expand_212 = torch.ops.aten.expand.default(select_87, [4, 384, 384, 384]);  select_87 = None
    _scaled_dot_product_efficient_attention_default_123 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3168, getitem_3169, getitem_3170, expand_212, False);  getitem_3168 = getitem_3169 = getitem_3170 = expand_212 = None
    getitem_3172 = _scaled_dot_product_efficient_attention_default_123[0];  _scaled_dot_product_efficient_attention_default_123 = None
    sigmoid_261 = torch.ops.aten.sigmoid.default(getitem_3171);  getitem_3171 = None
    mul_431 = torch.ops.aten.mul.Tensor(getitem_3172, sigmoid_261);  getitem_3172 = sigmoid_261 = None
    view_3263 = torch.ops.aten.view.default(mul_431, [1, 4, 384, 384, 64]);  mul_431 = None
    permute_1742 = torch.ops.aten.permute.default(view_3263, [0, 2, 3, 1, 4]);  view_3263 = None
    clone_275 = torch.ops.aten.clone.default(permute_1742, memory_format = torch.contiguous_format);  permute_1742 = None
    _unsafe_view_230 = torch.ops.aten._unsafe_view.default(clone_275, [1, 384, 384, 256]);  clone_275 = None
    transpose_87 = torch.ops.aten.transpose.int(getitem_3165, 1, 2);  getitem_3165 = None
    _to_copy_1824 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_37_triangle_attention_pair2qkvg2_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_37_triangle_attention_pair2qkvg2_weight = None
    _to_copy_1825 = torch.ops.aten._to_copy.default(transpose_87, dtype = torch.bfloat16);  transpose_87 = None
    t_680 = torch.ops.aten.t.default(_to_copy_1824);  _to_copy_1824 = None
    expand_213 = torch.ops.aten.expand.default(_to_copy_1825, [1, 384, 384, 256]);  _to_copy_1825 = None
    view_3264 = torch.ops.aten.view.default(expand_213, [384, 384, 256]);  expand_213 = None
    expand_214 = torch.ops.aten.expand.default(t_680, [1, 384, 256, 1024]);  t_680 = None
    view_3265 = torch.ops.aten.view.default(expand_214, [384, 256, 1024]);  expand_214 = None
    bmm_259 = torch.ops.aten.bmm.default(view_3264, view_3265);  view_3264 = view_3265 = None
    view_3266 = torch.ops.aten.view.default(bmm_259, [1, 384, 384, 1024]);  bmm_259 = None
    select_88 = torch.ops.aten.select.int(view_3258, 0, 1);  view_3258 = None
    view_3267 = torch.ops.aten.view.default(view_3266, [1, 384, 384, 4, 4, 64]);  view_3266 = None
    permute_1743 = torch.ops.aten.permute.default(view_3267, [4, 0, 3, 1, 2, 5]);  view_3267 = None
    view_3268 = torch.ops.aten.view.default(permute_1743, [4, 4, 384, 384, 64]);  permute_1743 = None
    unbind_int_146 = torch.ops.aten.unbind.int(view_3268);  view_3268 = None
    getitem_3176 = unbind_int_146[0]
    getitem_3177 = unbind_int_146[1]
    getitem_3178 = unbind_int_146[2]
    getitem_3179 = unbind_int_146[3];  unbind_int_146 = None
    expand_215 = torch.ops.aten.expand.default(select_88, [4, 384, 384, 384]);  select_88 = None
    _scaled_dot_product_efficient_attention_default_124 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3176, getitem_3177, getitem_3178, expand_215, False);  getitem_3176 = getitem_3177 = getitem_3178 = expand_215 = None
    getitem_3180 = _scaled_dot_product_efficient_attention_default_124[0];  _scaled_dot_product_efficient_attention_default_124 = None
    sigmoid_262 = torch.ops.aten.sigmoid.default(getitem_3179);  getitem_3179 = None
    mul_432 = torch.ops.aten.mul.Tensor(getitem_3180, sigmoid_262);  getitem_3180 = sigmoid_262 = None
    view_3269 = torch.ops.aten.view.default(mul_432, [1, 4, 384, 384, 64]);  mul_432 = None
    permute_1744 = torch.ops.aten.permute.default(view_3269, [0, 2, 3, 1, 4]);  view_3269 = None
    clone_276 = torch.ops.aten.clone.default(permute_1744, memory_format = torch.contiguous_format);  permute_1744 = None
    _unsafe_view_231 = torch.ops.aten._unsafe_view.default(clone_276, [1, 384, 384, 256]);  clone_276 = None
    cat_49 = torch.ops.aten.cat.default([_unsafe_view_230, _unsafe_view_231], dim = -1);  _unsafe_view_230 = _unsafe_view_231 = None
    slice_200 = torch.ops.aten.slice.Tensor(pairformer_stack_blocks_37_triangle_attention_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  pairformer_stack_blocks_37_triangle_attention_out_scalers = None
    unsqueeze_1062 = torch.ops.aten.unsqueeze.default(slice_200, 1);  slice_200 = None
    mul_433 = torch.ops.aten.mul.Tensor(pairformer_stack_blocks_37_triangle_attention_linear_out_weight, unsqueeze_1062);  pairformer_stack_blocks_37_triangle_attention_linear_out_weight = unsqueeze_1062 = None
    _to_copy_1826 = torch.ops.aten._to_copy.default(mul_433, dtype = torch.bfloat16);  mul_433 = None
    t_681 = torch.ops.aten.t.default(_to_copy_1826);  _to_copy_1826 = None
    view_3270 = torch.ops.aten.view.default(cat_49, [147456, 512]);  cat_49 = None
    mm_633 = torch.ops.aten.mm.default(view_3270, t_681);  view_3270 = t_681 = None
    view_3271 = torch.ops.aten.view.default(mm_633, [1, 384, 384, 256]);  mm_633 = None
    add_354 = torch.ops.aten.add.Tensor(add_353, view_3271);  add_353 = view_3271 = None
    split_tensor_348 = torch.ops.aten.split.Tensor(add_347, 384, dim = -2)
    getitem_3184 = split_tensor_348[0];  split_tensor_348 = None
    _to_copy_1827 = torch.ops.aten._to_copy.default(getitem_3184, dtype = torch.float32);  getitem_3184 = None
    native_layer_norm_default_375 = torch.ops.aten.native_layer_norm.default(_to_copy_1827, [256], pairformer_stack_blocks_37_transition_pair_layer_norm_weight, pairformer_stack_blocks_37_transition_pair_layer_norm_bias, 1e-05);  _to_copy_1827 = pairformer_stack_blocks_37_transition_pair_layer_norm_weight = pairformer_stack_blocks_37_transition_pair_layer_norm_bias = None
    getitem_3185 = native_layer_norm_default_375[0];  native_layer_norm_default_375 = None
    _to_copy_1828 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_37_transition_pair_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_37_transition_pair_linear_no_bias_ab_weight = None
    _to_copy_1829 = torch.ops.aten._to_copy.default(getitem_3185, dtype = torch.bfloat16);  getitem_3185 = None
    t_682 = torch.ops.aten.t.default(_to_copy_1828);  _to_copy_1828 = None
    view_3272 = torch.ops.aten.view.default(_to_copy_1829, [147456, 256]);  _to_copy_1829 = None
    mm_634 = torch.ops.aten.mm.default(view_3272, t_682);  view_3272 = t_682 = None
    view_3273 = torch.ops.aten.view.default(mm_634, [1, 384, 384, 1024]);  mm_634 = None
    split_tensor_349 = torch.ops.aten.split.Tensor(view_3273, 512, dim = -1);  view_3273 = None
    getitem_3188 = split_tensor_349[0]
    getitem_3189 = split_tensor_349[1];  split_tensor_349 = None
    silu_89 = torch.ops.aten.silu.default(getitem_3188);  getitem_3188 = None
    mul_434 = torch.ops.aten.mul.Tensor(silu_89, getitem_3189);  silu_89 = getitem_3189 = None
    _to_copy_1830 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_37_transition_pair_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_37_transition_pair_linear_out_weight = None
    t_683 = torch.ops.aten.t.default(_to_copy_1830);  _to_copy_1830 = None
    view_3275 = torch.ops.aten.view.default(mul_434, [147456, 512]);  mul_434 = None
    mm_635 = torch.ops.aten.mm.default(view_3275, t_683);  view_3275 = t_683 = None
    view_3276 = torch.ops.aten.view.default(mm_635, [1, 384, 384, 256]);  mm_635 = None
    add_355 = torch.ops.aten.add.Tensor(add_354, view_3276);  add_354 = view_3276 = None
    _to_copy_1831 = torch.ops.aten._to_copy.default(add_351, dtype = torch.float32)
    native_layer_norm_default_376 = torch.ops.aten.native_layer_norm.default(_to_copy_1831, [384], pairformer_stack_blocks_37_attention_pair_bias_single_layer_norm_weight, pairformer_stack_blocks_37_attention_pair_bias_single_layer_norm_bias, 1e-05);  _to_copy_1831 = pairformer_stack_blocks_37_attention_pair_bias_single_layer_norm_weight = pairformer_stack_blocks_37_attention_pair_bias_single_layer_norm_bias = None
    getitem_3190 = native_layer_norm_default_376[0];  native_layer_norm_default_376 = None
    _to_copy_1832 = torch.ops.aten._to_copy.default(add_347, dtype = torch.float32);  add_347 = None
    native_layer_norm_default_377 = torch.ops.aten.native_layer_norm.default(_to_copy_1832, [256], pairformer_stack_blocks_37_attention_pair_bias_pair_layer_norm_weight, pairformer_stack_blocks_37_attention_pair_bias_pair_layer_norm_bias, 1e-05);  _to_copy_1832 = pairformer_stack_blocks_37_attention_pair_bias_pair_layer_norm_weight = pairformer_stack_blocks_37_attention_pair_bias_pair_layer_norm_bias = None
    getitem_3193 = native_layer_norm_default_377[0];  native_layer_norm_default_377 = None
    _to_copy_1833 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_37_attention_pair_bias_pair_linear_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_37_attention_pair_bias_pair_linear_weight = None
    _to_copy_1834 = torch.ops.aten._to_copy.default(getitem_3193, dtype = torch.bfloat16);  getitem_3193 = None
    t_684 = torch.ops.aten.t.default(_to_copy_1833);  _to_copy_1833 = None
    view_3277 = torch.ops.aten.view.default(_to_copy_1834, [147456, 256]);  _to_copy_1834 = None
    mm_636 = torch.ops.aten.mm.default(view_3277, t_684);  view_3277 = t_684 = None
    view_3278 = torch.ops.aten.view.default(mm_636, [1, 384, 384, 16]);  mm_636 = None
    permute_1745 = torch.ops.aten.permute.default(view_3278, [0, 3, 1, 2]);  view_3278 = None
    view_3279 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384])
    bitwise_not_197 = torch.ops.aten.bitwise_not.default(view_3279);  view_3279 = None
    masked_fill_197 = torch.ops.aten.masked_fill.Scalar(permute_1745, bitwise_not_197, -10000);  permute_1745 = bitwise_not_197 = None
    _to_copy_1835 = torch.ops.aten._to_copy.default(getitem_3190, dtype = torch.bfloat16);  getitem_3190 = None
    _to_copy_1836 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_37_attention_pair_bias_attention_input2qkvg_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_37_attention_pair_bias_attention_input2qkvg_weight = None
    unsqueeze_1063 = torch.ops.aten.unsqueeze.default(_to_copy_1835, 3);  _to_copy_1835 = None
    unsqueeze_1064 = torch.ops.aten.unsqueeze.default(unsqueeze_1063, 4);  unsqueeze_1063 = None
    unsqueeze_1065 = torch.ops.aten.unsqueeze.default(unsqueeze_1064, 5);  unsqueeze_1064 = None
    permute_1746 = torch.ops.aten.permute.default(unsqueeze_1065, [3, 0, 4, 1, 5, 2]);  unsqueeze_1065 = None
    unsqueeze_1066 = torch.ops.aten.unsqueeze.default(_to_copy_1836, 4);  _to_copy_1836 = None
    unsqueeze_1067 = torch.ops.aten.unsqueeze.default(unsqueeze_1066, 5);  unsqueeze_1066 = None
    permute_1747 = torch.ops.aten.permute.default(unsqueeze_1067, [1, 4, 2, 5, 3, 0]);  unsqueeze_1067 = None
    permute_1748 = torch.ops.aten.permute.default(permute_1746, [3, 5, 0, 1, 2, 4]);  permute_1746 = None
    view_3280 = torch.ops.aten.view.default(permute_1748, [1, 384, 384]);  permute_1748 = None
    permute_1749 = torch.ops.aten.permute.default(permute_1747, [5, 0, 1, 2, 4, 3]);  permute_1747 = None
    view_3281 = torch.ops.aten.view.default(permute_1749, [1, 384, 1536]);  permute_1749 = None
    bmm_260 = torch.ops.aten.bmm.default(view_3280, view_3281);  view_3280 = view_3281 = None
    view_3282 = torch.ops.aten.view.default(bmm_260, [384, 1, 4, 1, 16, 24]);  bmm_260 = None
    permute_1750 = torch.ops.aten.permute.default(view_3282, [2, 3, 4, 0, 5, 1]);  view_3282 = None
    view_3283 = torch.ops.aten.view.default(permute_1750, [4, 1, 16, 384, 24]);  permute_1750 = None
    unbind_int_147 = torch.ops.aten.unbind.int(view_3283);  view_3283 = None
    getitem_3196 = unbind_int_147[0]
    getitem_3197 = unbind_int_147[1]
    getitem_3198 = unbind_int_147[2]
    getitem_3199 = unbind_int_147[3];  unbind_int_147 = None
    view_3284 = torch.ops.aten.view.default(pairformer_stack_blocks_37_attention_pair_bias_attention_query_bias, [1, 16, 1, 24]);  pairformer_stack_blocks_37_attention_pair_bias_attention_query_bias = None
    add_356 = torch.ops.aten.add.Tensor(getitem_3196, view_3284);  getitem_3196 = view_3284 = None
    _to_copy_1837 = torch.ops.aten._to_copy.default(add_356, dtype = torch.bfloat16);  add_356 = None
    expand_216 = torch.ops.aten.expand.default(masked_fill_197, [1, 16, 384, 384]);  masked_fill_197 = None
    _scaled_dot_product_efficient_attention_default_125 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_1837, getitem_3197, getitem_3198, expand_216, False);  _to_copy_1837 = getitem_3197 = getitem_3198 = expand_216 = None
    getitem_3200 = _scaled_dot_product_efficient_attention_default_125[0];  _scaled_dot_product_efficient_attention_default_125 = None
    add_357 = torch.ops.aten.add.Tensor(getitem_3199, 1);  getitem_3199 = None
    sigmoid_263 = torch.ops.aten.sigmoid.default(add_357);  add_357 = None
    mul_435 = torch.ops.aten.mul.Tensor(getitem_3200, sigmoid_263);  getitem_3200 = sigmoid_263 = None
    _to_copy_1838 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_37_attention_pair_bias_attention_output_proj_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_37_attention_pair_bias_attention_output_proj_weight = None
    unsqueeze_1068 = torch.ops.aten.unsqueeze.default(mul_435, 4);  mul_435 = None
    permute_1751 = torch.ops.aten.permute.default(unsqueeze_1068, [0, 2, 4, 3, 1]);  unsqueeze_1068 = None
    unsqueeze_1069 = torch.ops.aten.unsqueeze.default(_to_copy_1838, 3);  _to_copy_1838 = None
    unsqueeze_1070 = torch.ops.aten.unsqueeze.default(unsqueeze_1069, 4);  unsqueeze_1069 = None
    permute_1752 = torch.ops.aten.permute.default(unsqueeze_1070, [3, 4, 2, 1, 0]);  unsqueeze_1070 = None
    permute_1753 = torch.ops.aten.permute.default(permute_1751, [1, 3, 4, 0, 2]);  permute_1751 = None
    clone_277 = torch.ops.aten.clone.default(permute_1753, memory_format = torch.contiguous_format);  permute_1753 = None
    _unsafe_view_232 = torch.ops.aten._unsafe_view.default(clone_277, [1, 384, 384]);  clone_277 = None
    permute_1754 = torch.ops.aten.permute.default(permute_1752, [3, 4, 0, 2, 1]);  permute_1752 = None
    clone_278 = torch.ops.aten.clone.default(permute_1754, memory_format = torch.contiguous_format);  permute_1754 = None
    _unsafe_view_233 = torch.ops.aten._unsafe_view.default(clone_278, [1, 384, 384]);  clone_278 = None
    bmm_261 = torch.ops.aten.bmm.default(_unsafe_view_232, _unsafe_view_233);  _unsafe_view_232 = _unsafe_view_233 = None
    view_3285 = torch.ops.aten.view.default(bmm_261, [384, 1, 1, 1, 384]);  bmm_261 = None
    permute_1755 = torch.ops.aten.permute.default(view_3285, [3, 0, 4, 1, 2]);  view_3285 = None
    view_3286 = torch.ops.aten.view.default(permute_1755, [1, 384, 384]);  permute_1755 = None
    unsqueeze_1071 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
    mul_436 = torch.ops.aten.mul.Tensor(view_3286, unsqueeze_1071);  view_3286 = unsqueeze_1071 = None
    add_358 = torch.ops.aten.add.Tensor(add_351, mul_436);  mul_436 = None
    split_tensor_350 = torch.ops.aten.split.Tensor(add_351, 384, dim = -2);  add_351 = None
    getitem_3204 = split_tensor_350[0];  split_tensor_350 = None
    _to_copy_1839 = torch.ops.aten._to_copy.default(getitem_3204, dtype = torch.float32);  getitem_3204 = None
    native_layer_norm_default_378 = torch.ops.aten.native_layer_norm.default(_to_copy_1839, [384], pairformer_stack_blocks_37_transition_single_layer_norm_weight, pairformer_stack_blocks_37_transition_single_layer_norm_bias, 1e-05);  _to_copy_1839 = pairformer_stack_blocks_37_transition_single_layer_norm_weight = pairformer_stack_blocks_37_transition_single_layer_norm_bias = None
    getitem_3205 = native_layer_norm_default_378[0];  native_layer_norm_default_378 = None
    _to_copy_1840 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_37_transition_single_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_37_transition_single_linear_no_bias_ab_weight = None
    _to_copy_1841 = torch.ops.aten._to_copy.default(getitem_3205, dtype = torch.bfloat16);  getitem_3205 = None
    t_685 = torch.ops.aten.t.default(_to_copy_1840);  _to_copy_1840 = None
    view_3287 = torch.ops.aten.view.default(_to_copy_1841, [384, 384]);  _to_copy_1841 = None
    mm_637 = torch.ops.aten.mm.default(view_3287, t_685);  view_3287 = t_685 = None
    view_3288 = torch.ops.aten.view.default(mm_637, [1, 384, 1536]);  mm_637 = None
    split_tensor_351 = torch.ops.aten.split.Tensor(view_3288, 768, dim = -1);  view_3288 = None
    getitem_3208 = split_tensor_351[0]
    getitem_3209 = split_tensor_351[1];  split_tensor_351 = None
    silu_90 = torch.ops.aten.silu.default(getitem_3208);  getitem_3208 = None
    mul_437 = torch.ops.aten.mul.Tensor(silu_90, getitem_3209);  silu_90 = getitem_3209 = None
    _to_copy_1842 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_37_transition_single_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_37_transition_single_linear_out_weight = None
    t_686 = torch.ops.aten.t.default(_to_copy_1842);  _to_copy_1842 = None
    view_3290 = torch.ops.aten.view.default(mul_437, [384, 768]);  mul_437 = None
    mm_638 = torch.ops.aten.mm.default(view_3290, t_686);  view_3290 = t_686 = None
    view_3291 = torch.ops.aten.view.default(mm_638, [1, 384, 384]);  mm_638 = None
    add_359 = torch.ops.aten.add.Tensor(add_358, view_3291);  add_358 = view_3291 = None
    _to_copy_1843 = torch.ops.aten._to_copy.default(add_355, dtype = torch.float32)
    native_layer_norm_default_379 = torch.ops.aten.native_layer_norm.default(_to_copy_1843, [256], pairformer_stack_blocks_38_triangle_multiplication_layernorm_z_in_weight, pairformer_stack_blocks_38_triangle_multiplication_layernorm_z_in_bias, 1e-05);  _to_copy_1843 = pairformer_stack_blocks_38_triangle_multiplication_layernorm_z_in_weight = pairformer_stack_blocks_38_triangle_multiplication_layernorm_z_in_bias = None
    getitem_3210 = native_layer_norm_default_379[0];  native_layer_norm_default_379 = None
    split_with_sizes_default_88 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_38_triangle_multiplication_merged_linear_p_weight, [512, 512]);  pairformer_stack_blocks_38_triangle_multiplication_merged_linear_p_weight = None
    getitem_3213 = split_with_sizes_default_88[0]
    getitem_3214 = split_with_sizes_default_88[1];  split_with_sizes_default_88 = None
    split_with_sizes_default_89 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_38_triangle_multiplication_merged_linear_g_weight, [512, 512, 256]);  pairformer_stack_blocks_38_triangle_multiplication_merged_linear_g_weight = None
    getitem_3215 = split_with_sizes_default_89[0]
    getitem_3216 = split_with_sizes_default_89[1]
    getitem_3217 = split_with_sizes_default_89[2];  split_with_sizes_default_89 = None
    _to_copy_1844 = torch.ops.aten._to_copy.default(getitem_3213, dtype = torch.bfloat16);  getitem_3213 = None
    _to_copy_1845 = torch.ops.aten._to_copy.default(getitem_3210, dtype = torch.bfloat16)
    t_687 = torch.ops.aten.t.default(_to_copy_1844);  _to_copy_1844 = None
    view_3292 = torch.ops.aten.view.default(_to_copy_1845, [147456, 256]);  _to_copy_1845 = None
    mm_639 = torch.ops.aten.mm.default(view_3292, t_687);  view_3292 = t_687 = None
    view_3293 = torch.ops.aten.view.default(mm_639, [1, 384, 384, 512]);  mm_639 = None
    _to_copy_1846 = torch.ops.aten._to_copy.default(getitem_3215, dtype = torch.bfloat16);  getitem_3215 = None
    _to_copy_1847 = torch.ops.aten._to_copy.default(getitem_3210, dtype = torch.bfloat16)
    t_688 = torch.ops.aten.t.default(_to_copy_1846);  _to_copy_1846 = None
    view_3294 = torch.ops.aten.view.default(_to_copy_1847, [147456, 256]);  _to_copy_1847 = None
    mm_640 = torch.ops.aten.mm.default(view_3294, t_688);  view_3294 = t_688 = None
    view_3295 = torch.ops.aten.view.default(mm_640, [1, 384, 384, 512]);  mm_640 = None
    sigmoid_264 = torch.ops.aten.sigmoid.default(view_3295);  view_3295 = None
    mul_438 = torch.ops.aten.mul.Tensor(view_3293, sigmoid_264);  view_3293 = sigmoid_264 = None
    unsqueeze_1072 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
    bitwise_not_198 = torch.ops.aten.bitwise_not.default(unsqueeze_1072);  unsqueeze_1072 = None
    masked_fill_198 = torch.ops.aten.masked_fill.Scalar(mul_438, bitwise_not_198, 0);  mul_438 = bitwise_not_198 = None
    split_tensor_352 = torch.ops.aten.split.Tensor(masked_fill_198, 256, dim = -1)
    getitem_3220 = split_tensor_352[0];  split_tensor_352 = None
    unsqueeze_1075 = torch.ops.aten.unsqueeze.default(getitem_3220, 4);  getitem_3220 = None
    permute_1760 = torch.ops.aten.permute.default(unsqueeze_1075, [0, 1, 4, 3, 2]);  unsqueeze_1075 = None
    permute_1761 = torch.ops.aten.permute.default(permute_1760, [3, 1, 4, 0, 2]);  permute_1760 = None
    view_3298 = torch.ops.aten.view.default(permute_1761, [256, 384, 384]);  permute_1761 = None
    split_tensor_353 = torch.ops.aten.split.Tensor(masked_fill_198, 256, dim = -1);  masked_fill_198 = None
    getitem_3223 = split_tensor_353[1];  split_tensor_353 = None
    unsqueeze_1076 = torch.ops.aten.unsqueeze.default(getitem_3223, 4);  getitem_3223 = None
    permute_1762 = torch.ops.aten.permute.default(unsqueeze_1076, [0, 4, 1, 3, 2]);  unsqueeze_1076 = None
    permute_1763 = torch.ops.aten.permute.default(permute_1762, [3, 4, 0, 2, 1]);  permute_1762 = None
    view_3299 = torch.ops.aten.view.default(permute_1763, [256, 384, 384]);  permute_1763 = None
    bmm_262 = torch.ops.aten.bmm.default(view_3298, view_3299);  view_3298 = view_3299 = None
    view_3300 = torch.ops.aten.view.default(bmm_262, [256, 384, 1, 1, 384]);  bmm_262 = None
    permute_1764 = torch.ops.aten.permute.default(view_3300, [3, 1, 4, 0, 2]);  view_3300 = None
    view_3301 = torch.ops.aten.view.default(permute_1764, [1, 384, 384, 256]);  permute_1764 = None
    _to_copy_1848 = torch.ops.aten._to_copy.default(getitem_3214, dtype = torch.bfloat16);  getitem_3214 = None
    _to_copy_1849 = torch.ops.aten._to_copy.default(getitem_3210, dtype = torch.bfloat16)
    t_689 = torch.ops.aten.t.default(_to_copy_1848);  _to_copy_1848 = None
    view_3302 = torch.ops.aten.view.default(_to_copy_1849, [147456, 256]);  _to_copy_1849 = None
    mm_641 = torch.ops.aten.mm.default(view_3302, t_689);  view_3302 = t_689 = None
    view_3303 = torch.ops.aten.view.default(mm_641, [1, 384, 384, 512]);  mm_641 = None
    _to_copy_1850 = torch.ops.aten._to_copy.default(getitem_3216, dtype = torch.bfloat16);  getitem_3216 = None
    _to_copy_1851 = torch.ops.aten._to_copy.default(getitem_3210, dtype = torch.bfloat16)
    t_690 = torch.ops.aten.t.default(_to_copy_1850);  _to_copy_1850 = None
    view_3304 = torch.ops.aten.view.default(_to_copy_1851, [147456, 256]);  _to_copy_1851 = None
    mm_642 = torch.ops.aten.mm.default(view_3304, t_690);  view_3304 = t_690 = None
    view_3305 = torch.ops.aten.view.default(mm_642, [1, 384, 384, 512]);  mm_642 = None
    sigmoid_265 = torch.ops.aten.sigmoid.default(view_3305);  view_3305 = None
    mul_439 = torch.ops.aten.mul.Tensor(view_3303, sigmoid_265);  view_3303 = sigmoid_265 = None
    view_3306 = torch.ops.aten.view.default(mul_439, [147456, 512]);  mul_439 = None
    view_3307 = torch.ops.aten.view.default(view_3306, [1, 384, 384, 512]);  view_3306 = None
    transpose_88 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
    unsqueeze_1077 = torch.ops.aten.unsqueeze.default(transpose_88, 3);  transpose_88 = None
    clone_279 = torch.ops.aten.clone.default(unsqueeze_1077, memory_format = torch.contiguous_format);  unsqueeze_1077 = None
    bitwise_not_199 = torch.ops.aten.bitwise_not.default(clone_279);  clone_279 = None
    masked_fill_199 = torch.ops.aten.masked_fill.Scalar(view_3307, bitwise_not_199, 0);  view_3307 = bitwise_not_199 = None
    view_3308 = torch.ops.aten.view.default(masked_fill_199, [147456, 512]);  masked_fill_199 = None
    view_3312 = torch.ops.aten.view.default(view_3308, [1, 384, 384, 512])
    split_tensor_354 = torch.ops.aten.split.Tensor(view_3312, 256, dim = -1);  view_3312 = None
    getitem_3226 = split_tensor_354[0];  split_tensor_354 = None
    unsqueeze_1080 = torch.ops.aten.unsqueeze.default(getitem_3226, 4);  getitem_3226 = None
    permute_1769 = torch.ops.aten.permute.default(unsqueeze_1080, [0, 2, 4, 3, 1]);  unsqueeze_1080 = None
    permute_1770 = torch.ops.aten.permute.default(permute_1769, [3, 1, 4, 0, 2]);  permute_1769 = None
    view_3313 = torch.ops.aten.view.default(permute_1770, [256, 384, 384]);  permute_1770 = None
    view_3314 = torch.ops.aten.view.default(view_3308, [1, 384, 384, 512]);  view_3308 = None
    split_tensor_355 = torch.ops.aten.split.Tensor(view_3314, 256, dim = -1);  view_3314 = None
    getitem_3229 = split_tensor_355[1];  split_tensor_355 = None
    unsqueeze_1081 = torch.ops.aten.unsqueeze.default(getitem_3229, 4);  getitem_3229 = None
    permute_1771 = torch.ops.aten.permute.default(unsqueeze_1081, [0, 4, 2, 3, 1]);  unsqueeze_1081 = None
    permute_1772 = torch.ops.aten.permute.default(permute_1771, [3, 4, 0, 2, 1]);  permute_1771 = None
    view_3315 = torch.ops.aten.view.default(permute_1772, [256, 384, 384]);  permute_1772 = None
    bmm_263 = torch.ops.aten.bmm.default(view_3313, view_3315);  view_3313 = view_3315 = None
    view_3316 = torch.ops.aten.view.default(bmm_263, [256, 384, 1, 1, 384]);  bmm_263 = None
    permute_1773 = torch.ops.aten.permute.default(view_3316, [3, 1, 4, 0, 2]);  view_3316 = None
    view_3317 = torch.ops.aten.view.default(permute_1773, [1, 384, 384, 256]);  permute_1773 = None
    _to_copy_1852 = torch.ops.aten._to_copy.default(view_3301, dtype = torch.float32);  view_3301 = None
    native_layer_norm_default_380 = torch.ops.aten.native_layer_norm.default(_to_copy_1852, [256], None, None, 1e-05);  _to_copy_1852 = None
    getitem_3230 = native_layer_norm_default_380[0];  native_layer_norm_default_380 = None
    _to_copy_1853 = torch.ops.aten._to_copy.default(view_3317, dtype = torch.float32);  view_3317 = None
    native_layer_norm_default_381 = torch.ops.aten.native_layer_norm.default(_to_copy_1853, [256], None, None, 1e-05);  _to_copy_1853 = None
    getitem_3233 = native_layer_norm_default_381[0];  native_layer_norm_default_381 = None
    add_360 = torch.ops.aten.add.Tensor(getitem_3230, getitem_3233);  getitem_3230 = getitem_3233 = None
    _to_copy_1854 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_38_triangle_multiplication_linear_z_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_38_triangle_multiplication_linear_z_out_weight = None
    _to_copy_1855 = torch.ops.aten._to_copy.default(add_360, dtype = torch.bfloat16);  add_360 = None
    t_691 = torch.ops.aten.t.default(_to_copy_1854);  _to_copy_1854 = None
    view_3318 = torch.ops.aten.view.default(_to_copy_1855, [147456, 256]);  _to_copy_1855 = None
    mm_643 = torch.ops.aten.mm.default(view_3318, t_691);  view_3318 = t_691 = None
    view_3319 = torch.ops.aten.view.default(mm_643, [1, 384, 384, 256]);  mm_643 = None
    _to_copy_1856 = torch.ops.aten._to_copy.default(getitem_3217, dtype = torch.bfloat16);  getitem_3217 = None
    _to_copy_1857 = torch.ops.aten._to_copy.default(getitem_3210, dtype = torch.bfloat16);  getitem_3210 = None
    t_692 = torch.ops.aten.t.default(_to_copy_1856);  _to_copy_1856 = None
    view_3320 = torch.ops.aten.view.default(_to_copy_1857, [147456, 256]);  _to_copy_1857 = None
    mm_644 = torch.ops.aten.mm.default(view_3320, t_692);  view_3320 = t_692 = None
    view_3321 = torch.ops.aten.view.default(mm_644, [1, 384, 384, 256]);  mm_644 = None
    sigmoid_266 = torch.ops.aten.sigmoid.default(view_3321);  view_3321 = None
    mul_440 = torch.ops.aten.mul.Tensor(view_3319, sigmoid_266);  view_3319 = sigmoid_266 = None
    add_361 = torch.ops.aten.add.Tensor(add_355, mul_440);  mul_440 = None
    _to_copy_1858 = torch.ops.aten._to_copy.default(add_355, dtype = torch.float32)
    native_layer_norm_default_382 = torch.ops.aten.native_layer_norm.default(_to_copy_1858, [256], None, None, 1e-05);  _to_copy_1858 = None
    getitem_3236 = native_layer_norm_default_382[0];  native_layer_norm_default_382 = None
    _to_copy_1859 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_38_triangle_attention_pair2b_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_38_triangle_attention_pair2b_weight = None
    _to_copy_1860 = torch.ops.aten._to_copy.default(getitem_3236, dtype = torch.bfloat16)
    t_693 = torch.ops.aten.t.default(_to_copy_1859);  _to_copy_1859 = None
    view_3322 = torch.ops.aten.view.default(_to_copy_1860, [147456, 256]);  _to_copy_1860 = None
    mm_645 = torch.ops.aten.mm.default(view_3322, t_693);  view_3322 = t_693 = None
    view_3323 = torch.ops.aten.view.default(mm_645, [1, 384, 384, 8]);  mm_645 = None
    view_3324 = torch.ops.aten.view.default(view_3323, [1, 384, 384, 2, 4]);  view_3323 = None
    permute_1774 = torch.ops.aten.permute.default(view_3324, [0, 3, 4, 1, 2]);  view_3324 = None
    view_3325 = torch.ops.aten.view.default(permute_1774, [1, 2, 4, 1, 384, 384]);  permute_1774 = None
    view_3326 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 384, 384])
    bitwise_not_200 = torch.ops.aten.bitwise_not.default(view_3326);  view_3326 = None
    masked_fill_200 = torch.ops.aten.masked_fill.Scalar(view_3325, bitwise_not_200, -10000);  view_3325 = bitwise_not_200 = None
    view_3327 = torch.ops.aten.view.default(masked_fill_200, [1, 2, 4, 384, 384]);  masked_fill_200 = None
    permute_1775 = torch.ops.aten.permute.default(view_3327, [1, 0, 2, 3, 4]);  view_3327 = None
    view_3328 = torch.ops.aten.view.default(permute_1775, [2, 4, 1, 384, 384]);  permute_1775 = None
    _to_copy_1861 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_38_triangle_attention_pair2qkvg1_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_38_triangle_attention_pair2qkvg1_weight = None
    _to_copy_1862 = torch.ops.aten._to_copy.default(getitem_3236, dtype = torch.bfloat16)
    t_694 = torch.ops.aten.t.default(_to_copy_1861);  _to_copy_1861 = None
    view_3329 = torch.ops.aten.view.default(_to_copy_1862, [147456, 256]);  _to_copy_1862 = None
    mm_646 = torch.ops.aten.mm.default(view_3329, t_694);  view_3329 = t_694 = None
    view_3330 = torch.ops.aten.view.default(mm_646, [1, 384, 384, 1024]);  mm_646 = None
    select_89 = torch.ops.aten.select.int(view_3328, 0, 0)
    view_3331 = torch.ops.aten.view.default(view_3330, [1, 384, 384, 4, 4, 64]);  view_3330 = None
    permute_1776 = torch.ops.aten.permute.default(view_3331, [4, 0, 3, 1, 2, 5]);  view_3331 = None
    view_3332 = torch.ops.aten.view.default(permute_1776, [4, 4, 384, 384, 64]);  permute_1776 = None
    unbind_int_148 = torch.ops.aten.unbind.int(view_3332);  view_3332 = None
    getitem_3239 = unbind_int_148[0]
    getitem_3240 = unbind_int_148[1]
    getitem_3241 = unbind_int_148[2]
    getitem_3242 = unbind_int_148[3];  unbind_int_148 = None
    expand_217 = torch.ops.aten.expand.default(select_89, [4, 384, 384, 384]);  select_89 = None
    _scaled_dot_product_efficient_attention_default_126 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3239, getitem_3240, getitem_3241, expand_217, False);  getitem_3239 = getitem_3240 = getitem_3241 = expand_217 = None
    getitem_3243 = _scaled_dot_product_efficient_attention_default_126[0];  _scaled_dot_product_efficient_attention_default_126 = None
    sigmoid_267 = torch.ops.aten.sigmoid.default(getitem_3242);  getitem_3242 = None
    mul_441 = torch.ops.aten.mul.Tensor(getitem_3243, sigmoid_267);  getitem_3243 = sigmoid_267 = None
    view_3333 = torch.ops.aten.view.default(mul_441, [1, 4, 384, 384, 64]);  mul_441 = None
    permute_1777 = torch.ops.aten.permute.default(view_3333, [0, 2, 3, 1, 4]);  view_3333 = None
    clone_280 = torch.ops.aten.clone.default(permute_1777, memory_format = torch.contiguous_format);  permute_1777 = None
    _unsafe_view_234 = torch.ops.aten._unsafe_view.default(clone_280, [1, 384, 384, 256]);  clone_280 = None
    transpose_89 = torch.ops.aten.transpose.int(getitem_3236, 1, 2);  getitem_3236 = None
    _to_copy_1863 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_38_triangle_attention_pair2qkvg2_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_38_triangle_attention_pair2qkvg2_weight = None
    _to_copy_1864 = torch.ops.aten._to_copy.default(transpose_89, dtype = torch.bfloat16);  transpose_89 = None
    t_695 = torch.ops.aten.t.default(_to_copy_1863);  _to_copy_1863 = None
    expand_218 = torch.ops.aten.expand.default(_to_copy_1864, [1, 384, 384, 256]);  _to_copy_1864 = None
    view_3334 = torch.ops.aten.view.default(expand_218, [384, 384, 256]);  expand_218 = None
    expand_219 = torch.ops.aten.expand.default(t_695, [1, 384, 256, 1024]);  t_695 = None
    view_3335 = torch.ops.aten.view.default(expand_219, [384, 256, 1024]);  expand_219 = None
    bmm_264 = torch.ops.aten.bmm.default(view_3334, view_3335);  view_3334 = view_3335 = None
    view_3336 = torch.ops.aten.view.default(bmm_264, [1, 384, 384, 1024]);  bmm_264 = None
    select_90 = torch.ops.aten.select.int(view_3328, 0, 1);  view_3328 = None
    view_3337 = torch.ops.aten.view.default(view_3336, [1, 384, 384, 4, 4, 64]);  view_3336 = None
    permute_1778 = torch.ops.aten.permute.default(view_3337, [4, 0, 3, 1, 2, 5]);  view_3337 = None
    view_3338 = torch.ops.aten.view.default(permute_1778, [4, 4, 384, 384, 64]);  permute_1778 = None
    unbind_int_149 = torch.ops.aten.unbind.int(view_3338);  view_3338 = None
    getitem_3247 = unbind_int_149[0]
    getitem_3248 = unbind_int_149[1]
    getitem_3249 = unbind_int_149[2]
    getitem_3250 = unbind_int_149[3];  unbind_int_149 = None
    expand_220 = torch.ops.aten.expand.default(select_90, [4, 384, 384, 384]);  select_90 = None
    _scaled_dot_product_efficient_attention_default_127 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3247, getitem_3248, getitem_3249, expand_220, False);  getitem_3247 = getitem_3248 = getitem_3249 = expand_220 = None
    getitem_3251 = _scaled_dot_product_efficient_attention_default_127[0];  _scaled_dot_product_efficient_attention_default_127 = None
    sigmoid_268 = torch.ops.aten.sigmoid.default(getitem_3250);  getitem_3250 = None
    mul_442 = torch.ops.aten.mul.Tensor(getitem_3251, sigmoid_268);  getitem_3251 = sigmoid_268 = None
    view_3339 = torch.ops.aten.view.default(mul_442, [1, 4, 384, 384, 64]);  mul_442 = None
    permute_1779 = torch.ops.aten.permute.default(view_3339, [0, 2, 3, 1, 4]);  view_3339 = None
    clone_281 = torch.ops.aten.clone.default(permute_1779, memory_format = torch.contiguous_format);  permute_1779 = None
    _unsafe_view_235 = torch.ops.aten._unsafe_view.default(clone_281, [1, 384, 384, 256]);  clone_281 = None
    cat_50 = torch.ops.aten.cat.default([_unsafe_view_234, _unsafe_view_235], dim = -1);  _unsafe_view_234 = _unsafe_view_235 = None
    slice_201 = torch.ops.aten.slice.Tensor(pairformer_stack_blocks_38_triangle_attention_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  pairformer_stack_blocks_38_triangle_attention_out_scalers = None
    unsqueeze_1082 = torch.ops.aten.unsqueeze.default(slice_201, 1);  slice_201 = None
    mul_443 = torch.ops.aten.mul.Tensor(pairformer_stack_blocks_38_triangle_attention_linear_out_weight, unsqueeze_1082);  pairformer_stack_blocks_38_triangle_attention_linear_out_weight = unsqueeze_1082 = None
    _to_copy_1865 = torch.ops.aten._to_copy.default(mul_443, dtype = torch.bfloat16);  mul_443 = None
    t_696 = torch.ops.aten.t.default(_to_copy_1865);  _to_copy_1865 = None
    view_3340 = torch.ops.aten.view.default(cat_50, [147456, 512]);  cat_50 = None
    mm_647 = torch.ops.aten.mm.default(view_3340, t_696);  view_3340 = t_696 = None
    view_3341 = torch.ops.aten.view.default(mm_647, [1, 384, 384, 256]);  mm_647 = None
    add_362 = torch.ops.aten.add.Tensor(add_361, view_3341);  add_361 = view_3341 = None
    split_tensor_356 = torch.ops.aten.split.Tensor(add_355, 384, dim = -2)
    getitem_3255 = split_tensor_356[0];  split_tensor_356 = None
    _to_copy_1866 = torch.ops.aten._to_copy.default(getitem_3255, dtype = torch.float32);  getitem_3255 = None
    native_layer_norm_default_383 = torch.ops.aten.native_layer_norm.default(_to_copy_1866, [256], pairformer_stack_blocks_38_transition_pair_layer_norm_weight, pairformer_stack_blocks_38_transition_pair_layer_norm_bias, 1e-05);  _to_copy_1866 = pairformer_stack_blocks_38_transition_pair_layer_norm_weight = pairformer_stack_blocks_38_transition_pair_layer_norm_bias = None
    getitem_3256 = native_layer_norm_default_383[0];  native_layer_norm_default_383 = None
    _to_copy_1867 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_38_transition_pair_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_38_transition_pair_linear_no_bias_ab_weight = None
    _to_copy_1868 = torch.ops.aten._to_copy.default(getitem_3256, dtype = torch.bfloat16);  getitem_3256 = None
    t_697 = torch.ops.aten.t.default(_to_copy_1867);  _to_copy_1867 = None
    view_3342 = torch.ops.aten.view.default(_to_copy_1868, [147456, 256]);  _to_copy_1868 = None
    mm_648 = torch.ops.aten.mm.default(view_3342, t_697);  view_3342 = t_697 = None
    view_3343 = torch.ops.aten.view.default(mm_648, [1, 384, 384, 1024]);  mm_648 = None
    split_tensor_357 = torch.ops.aten.split.Tensor(view_3343, 512, dim = -1);  view_3343 = None
    getitem_3259 = split_tensor_357[0]
    getitem_3260 = split_tensor_357[1];  split_tensor_357 = None
    silu_91 = torch.ops.aten.silu.default(getitem_3259);  getitem_3259 = None
    mul_444 = torch.ops.aten.mul.Tensor(silu_91, getitem_3260);  silu_91 = getitem_3260 = None
    _to_copy_1869 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_38_transition_pair_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_38_transition_pair_linear_out_weight = None
    t_698 = torch.ops.aten.t.default(_to_copy_1869);  _to_copy_1869 = None
    view_3345 = torch.ops.aten.view.default(mul_444, [147456, 512]);  mul_444 = None
    mm_649 = torch.ops.aten.mm.default(view_3345, t_698);  view_3345 = t_698 = None
    view_3346 = torch.ops.aten.view.default(mm_649, [1, 384, 384, 256]);  mm_649 = None
    add_363 = torch.ops.aten.add.Tensor(add_362, view_3346);  add_362 = view_3346 = None
    _to_copy_1870 = torch.ops.aten._to_copy.default(add_359, dtype = torch.float32)
    native_layer_norm_default_384 = torch.ops.aten.native_layer_norm.default(_to_copy_1870, [384], pairformer_stack_blocks_38_attention_pair_bias_single_layer_norm_weight, pairformer_stack_blocks_38_attention_pair_bias_single_layer_norm_bias, 1e-05);  _to_copy_1870 = pairformer_stack_blocks_38_attention_pair_bias_single_layer_norm_weight = pairformer_stack_blocks_38_attention_pair_bias_single_layer_norm_bias = None
    getitem_3261 = native_layer_norm_default_384[0];  native_layer_norm_default_384 = None
    _to_copy_1871 = torch.ops.aten._to_copy.default(add_355, dtype = torch.float32);  add_355 = None
    native_layer_norm_default_385 = torch.ops.aten.native_layer_norm.default(_to_copy_1871, [256], pairformer_stack_blocks_38_attention_pair_bias_pair_layer_norm_weight, pairformer_stack_blocks_38_attention_pair_bias_pair_layer_norm_bias, 1e-05);  _to_copy_1871 = pairformer_stack_blocks_38_attention_pair_bias_pair_layer_norm_weight = pairformer_stack_blocks_38_attention_pair_bias_pair_layer_norm_bias = None
    getitem_3264 = native_layer_norm_default_385[0];  native_layer_norm_default_385 = None
    _to_copy_1872 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_38_attention_pair_bias_pair_linear_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_38_attention_pair_bias_pair_linear_weight = None
    _to_copy_1873 = torch.ops.aten._to_copy.default(getitem_3264, dtype = torch.bfloat16);  getitem_3264 = None
    t_699 = torch.ops.aten.t.default(_to_copy_1872);  _to_copy_1872 = None
    view_3347 = torch.ops.aten.view.default(_to_copy_1873, [147456, 256]);  _to_copy_1873 = None
    mm_650 = torch.ops.aten.mm.default(view_3347, t_699);  view_3347 = t_699 = None
    view_3348 = torch.ops.aten.view.default(mm_650, [1, 384, 384, 16]);  mm_650 = None
    permute_1780 = torch.ops.aten.permute.default(view_3348, [0, 3, 1, 2]);  view_3348 = None
    view_3349 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384])
    bitwise_not_201 = torch.ops.aten.bitwise_not.default(view_3349);  view_3349 = None
    masked_fill_201 = torch.ops.aten.masked_fill.Scalar(permute_1780, bitwise_not_201, -10000);  permute_1780 = bitwise_not_201 = None
    _to_copy_1874 = torch.ops.aten._to_copy.default(getitem_3261, dtype = torch.bfloat16);  getitem_3261 = None
    _to_copy_1875 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_38_attention_pair_bias_attention_input2qkvg_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_38_attention_pair_bias_attention_input2qkvg_weight = None
    unsqueeze_1083 = torch.ops.aten.unsqueeze.default(_to_copy_1874, 3);  _to_copy_1874 = None
    unsqueeze_1084 = torch.ops.aten.unsqueeze.default(unsqueeze_1083, 4);  unsqueeze_1083 = None
    unsqueeze_1085 = torch.ops.aten.unsqueeze.default(unsqueeze_1084, 5);  unsqueeze_1084 = None
    permute_1781 = torch.ops.aten.permute.default(unsqueeze_1085, [3, 0, 4, 1, 5, 2]);  unsqueeze_1085 = None
    unsqueeze_1086 = torch.ops.aten.unsqueeze.default(_to_copy_1875, 4);  _to_copy_1875 = None
    unsqueeze_1087 = torch.ops.aten.unsqueeze.default(unsqueeze_1086, 5);  unsqueeze_1086 = None
    permute_1782 = torch.ops.aten.permute.default(unsqueeze_1087, [1, 4, 2, 5, 3, 0]);  unsqueeze_1087 = None
    permute_1783 = torch.ops.aten.permute.default(permute_1781, [3, 5, 0, 1, 2, 4]);  permute_1781 = None
    view_3350 = torch.ops.aten.view.default(permute_1783, [1, 384, 384]);  permute_1783 = None
    permute_1784 = torch.ops.aten.permute.default(permute_1782, [5, 0, 1, 2, 4, 3]);  permute_1782 = None
    view_3351 = torch.ops.aten.view.default(permute_1784, [1, 384, 1536]);  permute_1784 = None
    bmm_265 = torch.ops.aten.bmm.default(view_3350, view_3351);  view_3350 = view_3351 = None
    view_3352 = torch.ops.aten.view.default(bmm_265, [384, 1, 4, 1, 16, 24]);  bmm_265 = None
    permute_1785 = torch.ops.aten.permute.default(view_3352, [2, 3, 4, 0, 5, 1]);  view_3352 = None
    view_3353 = torch.ops.aten.view.default(permute_1785, [4, 1, 16, 384, 24]);  permute_1785 = None
    unbind_int_150 = torch.ops.aten.unbind.int(view_3353);  view_3353 = None
    getitem_3267 = unbind_int_150[0]
    getitem_3268 = unbind_int_150[1]
    getitem_3269 = unbind_int_150[2]
    getitem_3270 = unbind_int_150[3];  unbind_int_150 = None
    view_3354 = torch.ops.aten.view.default(pairformer_stack_blocks_38_attention_pair_bias_attention_query_bias, [1, 16, 1, 24]);  pairformer_stack_blocks_38_attention_pair_bias_attention_query_bias = None
    add_364 = torch.ops.aten.add.Tensor(getitem_3267, view_3354);  getitem_3267 = view_3354 = None
    _to_copy_1876 = torch.ops.aten._to_copy.default(add_364, dtype = torch.bfloat16);  add_364 = None
    expand_221 = torch.ops.aten.expand.default(masked_fill_201, [1, 16, 384, 384]);  masked_fill_201 = None
    _scaled_dot_product_efficient_attention_default_128 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_1876, getitem_3268, getitem_3269, expand_221, False);  _to_copy_1876 = getitem_3268 = getitem_3269 = expand_221 = None
    getitem_3271 = _scaled_dot_product_efficient_attention_default_128[0];  _scaled_dot_product_efficient_attention_default_128 = None
    add_365 = torch.ops.aten.add.Tensor(getitem_3270, 1);  getitem_3270 = None
    sigmoid_269 = torch.ops.aten.sigmoid.default(add_365);  add_365 = None
    mul_445 = torch.ops.aten.mul.Tensor(getitem_3271, sigmoid_269);  getitem_3271 = sigmoid_269 = None
    _to_copy_1877 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_38_attention_pair_bias_attention_output_proj_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_38_attention_pair_bias_attention_output_proj_weight = None
    unsqueeze_1088 = torch.ops.aten.unsqueeze.default(mul_445, 4);  mul_445 = None
    permute_1786 = torch.ops.aten.permute.default(unsqueeze_1088, [0, 2, 4, 3, 1]);  unsqueeze_1088 = None
    unsqueeze_1089 = torch.ops.aten.unsqueeze.default(_to_copy_1877, 3);  _to_copy_1877 = None
    unsqueeze_1090 = torch.ops.aten.unsqueeze.default(unsqueeze_1089, 4);  unsqueeze_1089 = None
    permute_1787 = torch.ops.aten.permute.default(unsqueeze_1090, [3, 4, 2, 1, 0]);  unsqueeze_1090 = None
    permute_1788 = torch.ops.aten.permute.default(permute_1786, [1, 3, 4, 0, 2]);  permute_1786 = None
    clone_282 = torch.ops.aten.clone.default(permute_1788, memory_format = torch.contiguous_format);  permute_1788 = None
    _unsafe_view_236 = torch.ops.aten._unsafe_view.default(clone_282, [1, 384, 384]);  clone_282 = None
    permute_1789 = torch.ops.aten.permute.default(permute_1787, [3, 4, 0, 2, 1]);  permute_1787 = None
    clone_283 = torch.ops.aten.clone.default(permute_1789, memory_format = torch.contiguous_format);  permute_1789 = None
    _unsafe_view_237 = torch.ops.aten._unsafe_view.default(clone_283, [1, 384, 384]);  clone_283 = None
    bmm_266 = torch.ops.aten.bmm.default(_unsafe_view_236, _unsafe_view_237);  _unsafe_view_236 = _unsafe_view_237 = None
    view_3355 = torch.ops.aten.view.default(bmm_266, [384, 1, 1, 1, 384]);  bmm_266 = None
    permute_1790 = torch.ops.aten.permute.default(view_3355, [3, 0, 4, 1, 2]);  view_3355 = None
    view_3356 = torch.ops.aten.view.default(permute_1790, [1, 384, 384]);  permute_1790 = None
    unsqueeze_1091 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
    mul_446 = torch.ops.aten.mul.Tensor(view_3356, unsqueeze_1091);  view_3356 = unsqueeze_1091 = None
    add_366 = torch.ops.aten.add.Tensor(add_359, mul_446);  mul_446 = None
    split_tensor_358 = torch.ops.aten.split.Tensor(add_359, 384, dim = -2);  add_359 = None
    getitem_3275 = split_tensor_358[0];  split_tensor_358 = None
    _to_copy_1878 = torch.ops.aten._to_copy.default(getitem_3275, dtype = torch.float32);  getitem_3275 = None
    native_layer_norm_default_386 = torch.ops.aten.native_layer_norm.default(_to_copy_1878, [384], pairformer_stack_blocks_38_transition_single_layer_norm_weight, pairformer_stack_blocks_38_transition_single_layer_norm_bias, 1e-05);  _to_copy_1878 = pairformer_stack_blocks_38_transition_single_layer_norm_weight = pairformer_stack_blocks_38_transition_single_layer_norm_bias = None
    getitem_3276 = native_layer_norm_default_386[0];  native_layer_norm_default_386 = None
    _to_copy_1879 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_38_transition_single_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_38_transition_single_linear_no_bias_ab_weight = None
    _to_copy_1880 = torch.ops.aten._to_copy.default(getitem_3276, dtype = torch.bfloat16);  getitem_3276 = None
    t_700 = torch.ops.aten.t.default(_to_copy_1879);  _to_copy_1879 = None
    view_3357 = torch.ops.aten.view.default(_to_copy_1880, [384, 384]);  _to_copy_1880 = None
    mm_651 = torch.ops.aten.mm.default(view_3357, t_700);  view_3357 = t_700 = None
    view_3358 = torch.ops.aten.view.default(mm_651, [1, 384, 1536]);  mm_651 = None
    split_tensor_359 = torch.ops.aten.split.Tensor(view_3358, 768, dim = -1);  view_3358 = None
    getitem_3279 = split_tensor_359[0]
    getitem_3280 = split_tensor_359[1];  split_tensor_359 = None
    silu_92 = torch.ops.aten.silu.default(getitem_3279);  getitem_3279 = None
    mul_447 = torch.ops.aten.mul.Tensor(silu_92, getitem_3280);  silu_92 = getitem_3280 = None
    _to_copy_1881 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_38_transition_single_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_38_transition_single_linear_out_weight = None
    t_701 = torch.ops.aten.t.default(_to_copy_1881);  _to_copy_1881 = None
    view_3360 = torch.ops.aten.view.default(mul_447, [384, 768]);  mul_447 = None
    mm_652 = torch.ops.aten.mm.default(view_3360, t_701);  view_3360 = t_701 = None
    view_3361 = torch.ops.aten.view.default(mm_652, [1, 384, 384]);  mm_652 = None
    add_367 = torch.ops.aten.add.Tensor(add_366, view_3361);  add_366 = view_3361 = None
    _to_copy_1882 = torch.ops.aten._to_copy.default(add_363, dtype = torch.float32)
    native_layer_norm_default_387 = torch.ops.aten.native_layer_norm.default(_to_copy_1882, [256], pairformer_stack_blocks_39_triangle_multiplication_layernorm_z_in_weight, pairformer_stack_blocks_39_triangle_multiplication_layernorm_z_in_bias, 1e-05);  _to_copy_1882 = pairformer_stack_blocks_39_triangle_multiplication_layernorm_z_in_weight = pairformer_stack_blocks_39_triangle_multiplication_layernorm_z_in_bias = None
    getitem_3281 = native_layer_norm_default_387[0];  native_layer_norm_default_387 = None
    split_with_sizes_default_90 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_39_triangle_multiplication_merged_linear_p_weight, [512, 512]);  pairformer_stack_blocks_39_triangle_multiplication_merged_linear_p_weight = None
    getitem_3284 = split_with_sizes_default_90[0]
    getitem_3285 = split_with_sizes_default_90[1];  split_with_sizes_default_90 = None
    split_with_sizes_default_91 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_39_triangle_multiplication_merged_linear_g_weight, [512, 512, 256]);  pairformer_stack_blocks_39_triangle_multiplication_merged_linear_g_weight = None
    getitem_3286 = split_with_sizes_default_91[0]
    getitem_3287 = split_with_sizes_default_91[1]
    getitem_3288 = split_with_sizes_default_91[2];  split_with_sizes_default_91 = None
    _to_copy_1883 = torch.ops.aten._to_copy.default(getitem_3284, dtype = torch.bfloat16);  getitem_3284 = None
    _to_copy_1884 = torch.ops.aten._to_copy.default(getitem_3281, dtype = torch.bfloat16)
    t_702 = torch.ops.aten.t.default(_to_copy_1883);  _to_copy_1883 = None
    view_3362 = torch.ops.aten.view.default(_to_copy_1884, [147456, 256]);  _to_copy_1884 = None
    mm_653 = torch.ops.aten.mm.default(view_3362, t_702);  view_3362 = t_702 = None
    view_3363 = torch.ops.aten.view.default(mm_653, [1, 384, 384, 512]);  mm_653 = None
    _to_copy_1885 = torch.ops.aten._to_copy.default(getitem_3286, dtype = torch.bfloat16);  getitem_3286 = None
    _to_copy_1886 = torch.ops.aten._to_copy.default(getitem_3281, dtype = torch.bfloat16)
    t_703 = torch.ops.aten.t.default(_to_copy_1885);  _to_copy_1885 = None
    view_3364 = torch.ops.aten.view.default(_to_copy_1886, [147456, 256]);  _to_copy_1886 = None
    mm_654 = torch.ops.aten.mm.default(view_3364, t_703);  view_3364 = t_703 = None
    view_3365 = torch.ops.aten.view.default(mm_654, [1, 384, 384, 512]);  mm_654 = None
    sigmoid_270 = torch.ops.aten.sigmoid.default(view_3365);  view_3365 = None
    mul_448 = torch.ops.aten.mul.Tensor(view_3363, sigmoid_270);  view_3363 = sigmoid_270 = None
    unsqueeze_1092 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
    bitwise_not_202 = torch.ops.aten.bitwise_not.default(unsqueeze_1092);  unsqueeze_1092 = None
    masked_fill_202 = torch.ops.aten.masked_fill.Scalar(mul_448, bitwise_not_202, 0);  mul_448 = bitwise_not_202 = None
    split_tensor_360 = torch.ops.aten.split.Tensor(masked_fill_202, 256, dim = -1)
    getitem_3291 = split_tensor_360[0];  split_tensor_360 = None
    unsqueeze_1095 = torch.ops.aten.unsqueeze.default(getitem_3291, 4);  getitem_3291 = None
    permute_1795 = torch.ops.aten.permute.default(unsqueeze_1095, [0, 1, 4, 3, 2]);  unsqueeze_1095 = None
    permute_1796 = torch.ops.aten.permute.default(permute_1795, [3, 1, 4, 0, 2]);  permute_1795 = None
    view_3368 = torch.ops.aten.view.default(permute_1796, [256, 384, 384]);  permute_1796 = None
    split_tensor_361 = torch.ops.aten.split.Tensor(masked_fill_202, 256, dim = -1);  masked_fill_202 = None
    getitem_3294 = split_tensor_361[1];  split_tensor_361 = None
    unsqueeze_1096 = torch.ops.aten.unsqueeze.default(getitem_3294, 4);  getitem_3294 = None
    permute_1797 = torch.ops.aten.permute.default(unsqueeze_1096, [0, 4, 1, 3, 2]);  unsqueeze_1096 = None
    permute_1798 = torch.ops.aten.permute.default(permute_1797, [3, 4, 0, 2, 1]);  permute_1797 = None
    view_3369 = torch.ops.aten.view.default(permute_1798, [256, 384, 384]);  permute_1798 = None
    bmm_267 = torch.ops.aten.bmm.default(view_3368, view_3369);  view_3368 = view_3369 = None
    view_3370 = torch.ops.aten.view.default(bmm_267, [256, 384, 1, 1, 384]);  bmm_267 = None
    permute_1799 = torch.ops.aten.permute.default(view_3370, [3, 1, 4, 0, 2]);  view_3370 = None
    view_3371 = torch.ops.aten.view.default(permute_1799, [1, 384, 384, 256]);  permute_1799 = None
    _to_copy_1887 = torch.ops.aten._to_copy.default(getitem_3285, dtype = torch.bfloat16);  getitem_3285 = None
    _to_copy_1888 = torch.ops.aten._to_copy.default(getitem_3281, dtype = torch.bfloat16)
    t_704 = torch.ops.aten.t.default(_to_copy_1887);  _to_copy_1887 = None
    view_3372 = torch.ops.aten.view.default(_to_copy_1888, [147456, 256]);  _to_copy_1888 = None
    mm_655 = torch.ops.aten.mm.default(view_3372, t_704);  view_3372 = t_704 = None
    view_3373 = torch.ops.aten.view.default(mm_655, [1, 384, 384, 512]);  mm_655 = None
    _to_copy_1889 = torch.ops.aten._to_copy.default(getitem_3287, dtype = torch.bfloat16);  getitem_3287 = None
    _to_copy_1890 = torch.ops.aten._to_copy.default(getitem_3281, dtype = torch.bfloat16)
    t_705 = torch.ops.aten.t.default(_to_copy_1889);  _to_copy_1889 = None
    view_3374 = torch.ops.aten.view.default(_to_copy_1890, [147456, 256]);  _to_copy_1890 = None
    mm_656 = torch.ops.aten.mm.default(view_3374, t_705);  view_3374 = t_705 = None
    view_3375 = torch.ops.aten.view.default(mm_656, [1, 384, 384, 512]);  mm_656 = None
    sigmoid_271 = torch.ops.aten.sigmoid.default(view_3375);  view_3375 = None
    mul_449 = torch.ops.aten.mul.Tensor(view_3373, sigmoid_271);  view_3373 = sigmoid_271 = None
    view_3376 = torch.ops.aten.view.default(mul_449, [147456, 512]);  mul_449 = None
    view_3377 = torch.ops.aten.view.default(view_3376, [1, 384, 384, 512]);  view_3376 = None
    transpose_90 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
    unsqueeze_1097 = torch.ops.aten.unsqueeze.default(transpose_90, 3);  transpose_90 = None
    clone_284 = torch.ops.aten.clone.default(unsqueeze_1097, memory_format = torch.contiguous_format);  unsqueeze_1097 = None
    bitwise_not_203 = torch.ops.aten.bitwise_not.default(clone_284);  clone_284 = None
    masked_fill_203 = torch.ops.aten.masked_fill.Scalar(view_3377, bitwise_not_203, 0);  view_3377 = bitwise_not_203 = None
    view_3378 = torch.ops.aten.view.default(masked_fill_203, [147456, 512]);  masked_fill_203 = None
    view_3382 = torch.ops.aten.view.default(view_3378, [1, 384, 384, 512])
    split_tensor_362 = torch.ops.aten.split.Tensor(view_3382, 256, dim = -1);  view_3382 = None
    getitem_3297 = split_tensor_362[0];  split_tensor_362 = None
    unsqueeze_1100 = torch.ops.aten.unsqueeze.default(getitem_3297, 4);  getitem_3297 = None
    permute_1804 = torch.ops.aten.permute.default(unsqueeze_1100, [0, 2, 4, 3, 1]);  unsqueeze_1100 = None
    permute_1805 = torch.ops.aten.permute.default(permute_1804, [3, 1, 4, 0, 2]);  permute_1804 = None
    view_3383 = torch.ops.aten.view.default(permute_1805, [256, 384, 384]);  permute_1805 = None
    view_3384 = torch.ops.aten.view.default(view_3378, [1, 384, 384, 512]);  view_3378 = None
    split_tensor_363 = torch.ops.aten.split.Tensor(view_3384, 256, dim = -1);  view_3384 = None
    getitem_3300 = split_tensor_363[1];  split_tensor_363 = None
    unsqueeze_1101 = torch.ops.aten.unsqueeze.default(getitem_3300, 4);  getitem_3300 = None
    permute_1806 = torch.ops.aten.permute.default(unsqueeze_1101, [0, 4, 2, 3, 1]);  unsqueeze_1101 = None
    permute_1807 = torch.ops.aten.permute.default(permute_1806, [3, 4, 0, 2, 1]);  permute_1806 = None
    view_3385 = torch.ops.aten.view.default(permute_1807, [256, 384, 384]);  permute_1807 = None
    bmm_268 = torch.ops.aten.bmm.default(view_3383, view_3385);  view_3383 = view_3385 = None
    view_3386 = torch.ops.aten.view.default(bmm_268, [256, 384, 1, 1, 384]);  bmm_268 = None
    permute_1808 = torch.ops.aten.permute.default(view_3386, [3, 1, 4, 0, 2]);  view_3386 = None
    view_3387 = torch.ops.aten.view.default(permute_1808, [1, 384, 384, 256]);  permute_1808 = None
    _to_copy_1891 = torch.ops.aten._to_copy.default(view_3371, dtype = torch.float32);  view_3371 = None
    native_layer_norm_default_388 = torch.ops.aten.native_layer_norm.default(_to_copy_1891, [256], None, None, 1e-05);  _to_copy_1891 = None
    getitem_3301 = native_layer_norm_default_388[0];  native_layer_norm_default_388 = None
    _to_copy_1892 = torch.ops.aten._to_copy.default(view_3387, dtype = torch.float32);  view_3387 = None
    native_layer_norm_default_389 = torch.ops.aten.native_layer_norm.default(_to_copy_1892, [256], None, None, 1e-05);  _to_copy_1892 = None
    getitem_3304 = native_layer_norm_default_389[0];  native_layer_norm_default_389 = None
    add_368 = torch.ops.aten.add.Tensor(getitem_3301, getitem_3304);  getitem_3301 = getitem_3304 = None
    _to_copy_1893 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_39_triangle_multiplication_linear_z_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_39_triangle_multiplication_linear_z_out_weight = None
    _to_copy_1894 = torch.ops.aten._to_copy.default(add_368, dtype = torch.bfloat16);  add_368 = None
    t_706 = torch.ops.aten.t.default(_to_copy_1893);  _to_copy_1893 = None
    view_3388 = torch.ops.aten.view.default(_to_copy_1894, [147456, 256]);  _to_copy_1894 = None
    mm_657 = torch.ops.aten.mm.default(view_3388, t_706);  view_3388 = t_706 = None
    view_3389 = torch.ops.aten.view.default(mm_657, [1, 384, 384, 256]);  mm_657 = None
    _to_copy_1895 = torch.ops.aten._to_copy.default(getitem_3288, dtype = torch.bfloat16);  getitem_3288 = None
    _to_copy_1896 = torch.ops.aten._to_copy.default(getitem_3281, dtype = torch.bfloat16);  getitem_3281 = None
    t_707 = torch.ops.aten.t.default(_to_copy_1895);  _to_copy_1895 = None
    view_3390 = torch.ops.aten.view.default(_to_copy_1896, [147456, 256]);  _to_copy_1896 = None
    mm_658 = torch.ops.aten.mm.default(view_3390, t_707);  view_3390 = t_707 = None
    view_3391 = torch.ops.aten.view.default(mm_658, [1, 384, 384, 256]);  mm_658 = None
    sigmoid_272 = torch.ops.aten.sigmoid.default(view_3391);  view_3391 = None
    mul_450 = torch.ops.aten.mul.Tensor(view_3389, sigmoid_272);  view_3389 = sigmoid_272 = None
    add_369 = torch.ops.aten.add.Tensor(add_363, mul_450);  mul_450 = None
    _to_copy_1897 = torch.ops.aten._to_copy.default(add_363, dtype = torch.float32)
    native_layer_norm_default_390 = torch.ops.aten.native_layer_norm.default(_to_copy_1897, [256], None, None, 1e-05);  _to_copy_1897 = None
    getitem_3307 = native_layer_norm_default_390[0];  native_layer_norm_default_390 = None
    _to_copy_1898 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_39_triangle_attention_pair2b_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_39_triangle_attention_pair2b_weight = None
    _to_copy_1899 = torch.ops.aten._to_copy.default(getitem_3307, dtype = torch.bfloat16)
    t_708 = torch.ops.aten.t.default(_to_copy_1898);  _to_copy_1898 = None
    view_3392 = torch.ops.aten.view.default(_to_copy_1899, [147456, 256]);  _to_copy_1899 = None
    mm_659 = torch.ops.aten.mm.default(view_3392, t_708);  view_3392 = t_708 = None
    view_3393 = torch.ops.aten.view.default(mm_659, [1, 384, 384, 8]);  mm_659 = None
    view_3394 = torch.ops.aten.view.default(view_3393, [1, 384, 384, 2, 4]);  view_3393 = None
    permute_1809 = torch.ops.aten.permute.default(view_3394, [0, 3, 4, 1, 2]);  view_3394 = None
    view_3395 = torch.ops.aten.view.default(permute_1809, [1, 2, 4, 1, 384, 384]);  permute_1809 = None
    view_3396 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 384, 384])
    bitwise_not_204 = torch.ops.aten.bitwise_not.default(view_3396);  view_3396 = None
    masked_fill_204 = torch.ops.aten.masked_fill.Scalar(view_3395, bitwise_not_204, -10000);  view_3395 = bitwise_not_204 = None
    view_3397 = torch.ops.aten.view.default(masked_fill_204, [1, 2, 4, 384, 384]);  masked_fill_204 = None
    permute_1810 = torch.ops.aten.permute.default(view_3397, [1, 0, 2, 3, 4]);  view_3397 = None
    view_3398 = torch.ops.aten.view.default(permute_1810, [2, 4, 1, 384, 384]);  permute_1810 = None
    _to_copy_1900 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_39_triangle_attention_pair2qkvg1_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_39_triangle_attention_pair2qkvg1_weight = None
    _to_copy_1901 = torch.ops.aten._to_copy.default(getitem_3307, dtype = torch.bfloat16)
    t_709 = torch.ops.aten.t.default(_to_copy_1900);  _to_copy_1900 = None
    view_3399 = torch.ops.aten.view.default(_to_copy_1901, [147456, 256]);  _to_copy_1901 = None
    mm_660 = torch.ops.aten.mm.default(view_3399, t_709);  view_3399 = t_709 = None
    view_3400 = torch.ops.aten.view.default(mm_660, [1, 384, 384, 1024]);  mm_660 = None
    select_91 = torch.ops.aten.select.int(view_3398, 0, 0)
    view_3401 = torch.ops.aten.view.default(view_3400, [1, 384, 384, 4, 4, 64]);  view_3400 = None
    permute_1811 = torch.ops.aten.permute.default(view_3401, [4, 0, 3, 1, 2, 5]);  view_3401 = None
    view_3402 = torch.ops.aten.view.default(permute_1811, [4, 4, 384, 384, 64]);  permute_1811 = None
    unbind_int_151 = torch.ops.aten.unbind.int(view_3402);  view_3402 = None
    getitem_3310 = unbind_int_151[0]
    getitem_3311 = unbind_int_151[1]
    getitem_3312 = unbind_int_151[2]
    getitem_3313 = unbind_int_151[3];  unbind_int_151 = None
    expand_222 = torch.ops.aten.expand.default(select_91, [4, 384, 384, 384]);  select_91 = None
    _scaled_dot_product_efficient_attention_default_129 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3310, getitem_3311, getitem_3312, expand_222, False);  getitem_3310 = getitem_3311 = getitem_3312 = expand_222 = None
    getitem_3314 = _scaled_dot_product_efficient_attention_default_129[0];  _scaled_dot_product_efficient_attention_default_129 = None
    sigmoid_273 = torch.ops.aten.sigmoid.default(getitem_3313);  getitem_3313 = None
    mul_451 = torch.ops.aten.mul.Tensor(getitem_3314, sigmoid_273);  getitem_3314 = sigmoid_273 = None
    view_3403 = torch.ops.aten.view.default(mul_451, [1, 4, 384, 384, 64]);  mul_451 = None
    permute_1812 = torch.ops.aten.permute.default(view_3403, [0, 2, 3, 1, 4]);  view_3403 = None
    clone_285 = torch.ops.aten.clone.default(permute_1812, memory_format = torch.contiguous_format);  permute_1812 = None
    _unsafe_view_238 = torch.ops.aten._unsafe_view.default(clone_285, [1, 384, 384, 256]);  clone_285 = None
    transpose_91 = torch.ops.aten.transpose.int(getitem_3307, 1, 2);  getitem_3307 = None
    _to_copy_1902 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_39_triangle_attention_pair2qkvg2_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_39_triangle_attention_pair2qkvg2_weight = None
    _to_copy_1903 = torch.ops.aten._to_copy.default(transpose_91, dtype = torch.bfloat16);  transpose_91 = None
    t_710 = torch.ops.aten.t.default(_to_copy_1902);  _to_copy_1902 = None
    expand_223 = torch.ops.aten.expand.default(_to_copy_1903, [1, 384, 384, 256]);  _to_copy_1903 = None
    view_3404 = torch.ops.aten.view.default(expand_223, [384, 384, 256]);  expand_223 = None
    expand_224 = torch.ops.aten.expand.default(t_710, [1, 384, 256, 1024]);  t_710 = None
    view_3405 = torch.ops.aten.view.default(expand_224, [384, 256, 1024]);  expand_224 = None
    bmm_269 = torch.ops.aten.bmm.default(view_3404, view_3405);  view_3404 = view_3405 = None
    view_3406 = torch.ops.aten.view.default(bmm_269, [1, 384, 384, 1024]);  bmm_269 = None
    select_92 = torch.ops.aten.select.int(view_3398, 0, 1);  view_3398 = None
    view_3407 = torch.ops.aten.view.default(view_3406, [1, 384, 384, 4, 4, 64]);  view_3406 = None
    permute_1813 = torch.ops.aten.permute.default(view_3407, [4, 0, 3, 1, 2, 5]);  view_3407 = None
    view_3408 = torch.ops.aten.view.default(permute_1813, [4, 4, 384, 384, 64]);  permute_1813 = None
    unbind_int_152 = torch.ops.aten.unbind.int(view_3408);  view_3408 = None
    getitem_3318 = unbind_int_152[0]
    getitem_3319 = unbind_int_152[1]
    getitem_3320 = unbind_int_152[2]
    getitem_3321 = unbind_int_152[3];  unbind_int_152 = None
    expand_225 = torch.ops.aten.expand.default(select_92, [4, 384, 384, 384]);  select_92 = None
    _scaled_dot_product_efficient_attention_default_130 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3318, getitem_3319, getitem_3320, expand_225, False);  getitem_3318 = getitem_3319 = getitem_3320 = expand_225 = None
    getitem_3322 = _scaled_dot_product_efficient_attention_default_130[0];  _scaled_dot_product_efficient_attention_default_130 = None
    sigmoid_274 = torch.ops.aten.sigmoid.default(getitem_3321);  getitem_3321 = None
    mul_452 = torch.ops.aten.mul.Tensor(getitem_3322, sigmoid_274);  getitem_3322 = sigmoid_274 = None
    view_3409 = torch.ops.aten.view.default(mul_452, [1, 4, 384, 384, 64]);  mul_452 = None
    permute_1814 = torch.ops.aten.permute.default(view_3409, [0, 2, 3, 1, 4]);  view_3409 = None
    clone_286 = torch.ops.aten.clone.default(permute_1814, memory_format = torch.contiguous_format);  permute_1814 = None
    _unsafe_view_239 = torch.ops.aten._unsafe_view.default(clone_286, [1, 384, 384, 256]);  clone_286 = None
    cat_51 = torch.ops.aten.cat.default([_unsafe_view_238, _unsafe_view_239], dim = -1);  _unsafe_view_238 = _unsafe_view_239 = None
    slice_202 = torch.ops.aten.slice.Tensor(pairformer_stack_blocks_39_triangle_attention_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  pairformer_stack_blocks_39_triangle_attention_out_scalers = None
    unsqueeze_1102 = torch.ops.aten.unsqueeze.default(slice_202, 1);  slice_202 = None
    mul_453 = torch.ops.aten.mul.Tensor(pairformer_stack_blocks_39_triangle_attention_linear_out_weight, unsqueeze_1102);  pairformer_stack_blocks_39_triangle_attention_linear_out_weight = unsqueeze_1102 = None
    _to_copy_1904 = torch.ops.aten._to_copy.default(mul_453, dtype = torch.bfloat16);  mul_453 = None
    t_711 = torch.ops.aten.t.default(_to_copy_1904);  _to_copy_1904 = None
    view_3410 = torch.ops.aten.view.default(cat_51, [147456, 512]);  cat_51 = None
    mm_661 = torch.ops.aten.mm.default(view_3410, t_711);  view_3410 = t_711 = None
    view_3411 = torch.ops.aten.view.default(mm_661, [1, 384, 384, 256]);  mm_661 = None
    add_370 = torch.ops.aten.add.Tensor(add_369, view_3411);  add_369 = view_3411 = None
    split_tensor_364 = torch.ops.aten.split.Tensor(add_363, 384, dim = -2)
    getitem_3326 = split_tensor_364[0];  split_tensor_364 = None
    _to_copy_1905 = torch.ops.aten._to_copy.default(getitem_3326, dtype = torch.float32);  getitem_3326 = None
    native_layer_norm_default_391 = torch.ops.aten.native_layer_norm.default(_to_copy_1905, [256], pairformer_stack_blocks_39_transition_pair_layer_norm_weight, pairformer_stack_blocks_39_transition_pair_layer_norm_bias, 1e-05);  _to_copy_1905 = pairformer_stack_blocks_39_transition_pair_layer_norm_weight = pairformer_stack_blocks_39_transition_pair_layer_norm_bias = None
    getitem_3327 = native_layer_norm_default_391[0];  native_layer_norm_default_391 = None
    _to_copy_1906 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_39_transition_pair_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_39_transition_pair_linear_no_bias_ab_weight = None
    _to_copy_1907 = torch.ops.aten._to_copy.default(getitem_3327, dtype = torch.bfloat16);  getitem_3327 = None
    t_712 = torch.ops.aten.t.default(_to_copy_1906);  _to_copy_1906 = None
    view_3412 = torch.ops.aten.view.default(_to_copy_1907, [147456, 256]);  _to_copy_1907 = None
    mm_662 = torch.ops.aten.mm.default(view_3412, t_712);  view_3412 = t_712 = None
    view_3413 = torch.ops.aten.view.default(mm_662, [1, 384, 384, 1024]);  mm_662 = None
    split_tensor_365 = torch.ops.aten.split.Tensor(view_3413, 512, dim = -1);  view_3413 = None
    getitem_3330 = split_tensor_365[0]
    getitem_3331 = split_tensor_365[1];  split_tensor_365 = None
    silu_93 = torch.ops.aten.silu.default(getitem_3330);  getitem_3330 = None
    mul_454 = torch.ops.aten.mul.Tensor(silu_93, getitem_3331);  silu_93 = getitem_3331 = None
    _to_copy_1908 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_39_transition_pair_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_39_transition_pair_linear_out_weight = None
    t_713 = torch.ops.aten.t.default(_to_copy_1908);  _to_copy_1908 = None
    view_3415 = torch.ops.aten.view.default(mul_454, [147456, 512]);  mul_454 = None
    mm_663 = torch.ops.aten.mm.default(view_3415, t_713);  view_3415 = t_713 = None
    view_3416 = torch.ops.aten.view.default(mm_663, [1, 384, 384, 256]);  mm_663 = None
    add_371 = torch.ops.aten.add.Tensor(add_370, view_3416);  add_370 = view_3416 = None
    _to_copy_1909 = torch.ops.aten._to_copy.default(add_367, dtype = torch.float32)
    native_layer_norm_default_392 = torch.ops.aten.native_layer_norm.default(_to_copy_1909, [384], pairformer_stack_blocks_39_attention_pair_bias_single_layer_norm_weight, pairformer_stack_blocks_39_attention_pair_bias_single_layer_norm_bias, 1e-05);  _to_copy_1909 = pairformer_stack_blocks_39_attention_pair_bias_single_layer_norm_weight = pairformer_stack_blocks_39_attention_pair_bias_single_layer_norm_bias = None
    getitem_3332 = native_layer_norm_default_392[0];  native_layer_norm_default_392 = None
    _to_copy_1910 = torch.ops.aten._to_copy.default(add_363, dtype = torch.float32);  add_363 = None
    native_layer_norm_default_393 = torch.ops.aten.native_layer_norm.default(_to_copy_1910, [256], pairformer_stack_blocks_39_attention_pair_bias_pair_layer_norm_weight, pairformer_stack_blocks_39_attention_pair_bias_pair_layer_norm_bias, 1e-05);  _to_copy_1910 = pairformer_stack_blocks_39_attention_pair_bias_pair_layer_norm_weight = pairformer_stack_blocks_39_attention_pair_bias_pair_layer_norm_bias = None
    getitem_3335 = native_layer_norm_default_393[0];  native_layer_norm_default_393 = None
    _to_copy_1911 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_39_attention_pair_bias_pair_linear_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_39_attention_pair_bias_pair_linear_weight = None
    _to_copy_1912 = torch.ops.aten._to_copy.default(getitem_3335, dtype = torch.bfloat16);  getitem_3335 = None
    t_714 = torch.ops.aten.t.default(_to_copy_1911);  _to_copy_1911 = None
    view_3417 = torch.ops.aten.view.default(_to_copy_1912, [147456, 256]);  _to_copy_1912 = None
    mm_664 = torch.ops.aten.mm.default(view_3417, t_714);  view_3417 = t_714 = None
    view_3418 = torch.ops.aten.view.default(mm_664, [1, 384, 384, 16]);  mm_664 = None
    permute_1815 = torch.ops.aten.permute.default(view_3418, [0, 3, 1, 2]);  view_3418 = None
    view_3419 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384])
    bitwise_not_205 = torch.ops.aten.bitwise_not.default(view_3419);  view_3419 = None
    masked_fill_205 = torch.ops.aten.masked_fill.Scalar(permute_1815, bitwise_not_205, -10000);  permute_1815 = bitwise_not_205 = None
    _to_copy_1913 = torch.ops.aten._to_copy.default(getitem_3332, dtype = torch.bfloat16);  getitem_3332 = None
    _to_copy_1914 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_39_attention_pair_bias_attention_input2qkvg_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_39_attention_pair_bias_attention_input2qkvg_weight = None
    unsqueeze_1103 = torch.ops.aten.unsqueeze.default(_to_copy_1913, 3);  _to_copy_1913 = None
    unsqueeze_1104 = torch.ops.aten.unsqueeze.default(unsqueeze_1103, 4);  unsqueeze_1103 = None
    unsqueeze_1105 = torch.ops.aten.unsqueeze.default(unsqueeze_1104, 5);  unsqueeze_1104 = None
    permute_1816 = torch.ops.aten.permute.default(unsqueeze_1105, [3, 0, 4, 1, 5, 2]);  unsqueeze_1105 = None
    unsqueeze_1106 = torch.ops.aten.unsqueeze.default(_to_copy_1914, 4);  _to_copy_1914 = None
    unsqueeze_1107 = torch.ops.aten.unsqueeze.default(unsqueeze_1106, 5);  unsqueeze_1106 = None
    permute_1817 = torch.ops.aten.permute.default(unsqueeze_1107, [1, 4, 2, 5, 3, 0]);  unsqueeze_1107 = None
    permute_1818 = torch.ops.aten.permute.default(permute_1816, [3, 5, 0, 1, 2, 4]);  permute_1816 = None
    view_3420 = torch.ops.aten.view.default(permute_1818, [1, 384, 384]);  permute_1818 = None
    permute_1819 = torch.ops.aten.permute.default(permute_1817, [5, 0, 1, 2, 4, 3]);  permute_1817 = None
    view_3421 = torch.ops.aten.view.default(permute_1819, [1, 384, 1536]);  permute_1819 = None
    bmm_270 = torch.ops.aten.bmm.default(view_3420, view_3421);  view_3420 = view_3421 = None
    view_3422 = torch.ops.aten.view.default(bmm_270, [384, 1, 4, 1, 16, 24]);  bmm_270 = None
    permute_1820 = torch.ops.aten.permute.default(view_3422, [2, 3, 4, 0, 5, 1]);  view_3422 = None
    view_3423 = torch.ops.aten.view.default(permute_1820, [4, 1, 16, 384, 24]);  permute_1820 = None
    unbind_int_153 = torch.ops.aten.unbind.int(view_3423);  view_3423 = None
    getitem_3338 = unbind_int_153[0]
    getitem_3339 = unbind_int_153[1]
    getitem_3340 = unbind_int_153[2]
    getitem_3341 = unbind_int_153[3];  unbind_int_153 = None
    view_3424 = torch.ops.aten.view.default(pairformer_stack_blocks_39_attention_pair_bias_attention_query_bias, [1, 16, 1, 24]);  pairformer_stack_blocks_39_attention_pair_bias_attention_query_bias = None
    add_372 = torch.ops.aten.add.Tensor(getitem_3338, view_3424);  getitem_3338 = view_3424 = None
    _to_copy_1915 = torch.ops.aten._to_copy.default(add_372, dtype = torch.bfloat16);  add_372 = None
    expand_226 = torch.ops.aten.expand.default(masked_fill_205, [1, 16, 384, 384]);  masked_fill_205 = None
    _scaled_dot_product_efficient_attention_default_131 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_1915, getitem_3339, getitem_3340, expand_226, False);  _to_copy_1915 = getitem_3339 = getitem_3340 = expand_226 = None
    getitem_3342 = _scaled_dot_product_efficient_attention_default_131[0];  _scaled_dot_product_efficient_attention_default_131 = None
    add_373 = torch.ops.aten.add.Tensor(getitem_3341, 1);  getitem_3341 = None
    sigmoid_275 = torch.ops.aten.sigmoid.default(add_373);  add_373 = None
    mul_455 = torch.ops.aten.mul.Tensor(getitem_3342, sigmoid_275);  getitem_3342 = sigmoid_275 = None
    _to_copy_1916 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_39_attention_pair_bias_attention_output_proj_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_39_attention_pair_bias_attention_output_proj_weight = None
    unsqueeze_1108 = torch.ops.aten.unsqueeze.default(mul_455, 4);  mul_455 = None
    permute_1821 = torch.ops.aten.permute.default(unsqueeze_1108, [0, 2, 4, 3, 1]);  unsqueeze_1108 = None
    unsqueeze_1109 = torch.ops.aten.unsqueeze.default(_to_copy_1916, 3);  _to_copy_1916 = None
    unsqueeze_1110 = torch.ops.aten.unsqueeze.default(unsqueeze_1109, 4);  unsqueeze_1109 = None
    permute_1822 = torch.ops.aten.permute.default(unsqueeze_1110, [3, 4, 2, 1, 0]);  unsqueeze_1110 = None
    permute_1823 = torch.ops.aten.permute.default(permute_1821, [1, 3, 4, 0, 2]);  permute_1821 = None
    clone_287 = torch.ops.aten.clone.default(permute_1823, memory_format = torch.contiguous_format);  permute_1823 = None
    _unsafe_view_240 = torch.ops.aten._unsafe_view.default(clone_287, [1, 384, 384]);  clone_287 = None
    permute_1824 = torch.ops.aten.permute.default(permute_1822, [3, 4, 0, 2, 1]);  permute_1822 = None
    clone_288 = torch.ops.aten.clone.default(permute_1824, memory_format = torch.contiguous_format);  permute_1824 = None
    _unsafe_view_241 = torch.ops.aten._unsafe_view.default(clone_288, [1, 384, 384]);  clone_288 = None
    bmm_271 = torch.ops.aten.bmm.default(_unsafe_view_240, _unsafe_view_241);  _unsafe_view_240 = _unsafe_view_241 = None
    view_3425 = torch.ops.aten.view.default(bmm_271, [384, 1, 1, 1, 384]);  bmm_271 = None
    permute_1825 = torch.ops.aten.permute.default(view_3425, [3, 0, 4, 1, 2]);  view_3425 = None
    view_3426 = torch.ops.aten.view.default(permute_1825, [1, 384, 384]);  permute_1825 = None
    unsqueeze_1111 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
    mul_456 = torch.ops.aten.mul.Tensor(view_3426, unsqueeze_1111);  view_3426 = unsqueeze_1111 = None
    add_374 = torch.ops.aten.add.Tensor(add_367, mul_456);  mul_456 = None
    split_tensor_366 = torch.ops.aten.split.Tensor(add_367, 384, dim = -2);  add_367 = None
    getitem_3346 = split_tensor_366[0];  split_tensor_366 = None
    _to_copy_1917 = torch.ops.aten._to_copy.default(getitem_3346, dtype = torch.float32);  getitem_3346 = None
    native_layer_norm_default_394 = torch.ops.aten.native_layer_norm.default(_to_copy_1917, [384], pairformer_stack_blocks_39_transition_single_layer_norm_weight, pairformer_stack_blocks_39_transition_single_layer_norm_bias, 1e-05);  _to_copy_1917 = pairformer_stack_blocks_39_transition_single_layer_norm_weight = pairformer_stack_blocks_39_transition_single_layer_norm_bias = None
    getitem_3347 = native_layer_norm_default_394[0];  native_layer_norm_default_394 = None
    _to_copy_1918 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_39_transition_single_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_39_transition_single_linear_no_bias_ab_weight = None
    _to_copy_1919 = torch.ops.aten._to_copy.default(getitem_3347, dtype = torch.bfloat16);  getitem_3347 = None
    t_715 = torch.ops.aten.t.default(_to_copy_1918);  _to_copy_1918 = None
    view_3427 = torch.ops.aten.view.default(_to_copy_1919, [384, 384]);  _to_copy_1919 = None
    mm_665 = torch.ops.aten.mm.default(view_3427, t_715);  view_3427 = t_715 = None
    view_3428 = torch.ops.aten.view.default(mm_665, [1, 384, 1536]);  mm_665 = None
    split_tensor_367 = torch.ops.aten.split.Tensor(view_3428, 768, dim = -1);  view_3428 = None
    getitem_3350 = split_tensor_367[0]
    getitem_3351 = split_tensor_367[1];  split_tensor_367 = None
    silu_94 = torch.ops.aten.silu.default(getitem_3350);  getitem_3350 = None
    mul_457 = torch.ops.aten.mul.Tensor(silu_94, getitem_3351);  silu_94 = getitem_3351 = None
    _to_copy_1920 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_39_transition_single_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_39_transition_single_linear_out_weight = None
    t_716 = torch.ops.aten.t.default(_to_copy_1920);  _to_copy_1920 = None
    view_3430 = torch.ops.aten.view.default(mul_457, [384, 768]);  mul_457 = None
    mm_666 = torch.ops.aten.mm.default(view_3430, t_716);  view_3430 = t_716 = None
    view_3431 = torch.ops.aten.view.default(mm_666, [1, 384, 384]);  mm_666 = None
    add_375 = torch.ops.aten.add.Tensor(add_374, view_3431);  add_374 = view_3431 = None
    _to_copy_1921 = torch.ops.aten._to_copy.default(add_371, dtype = torch.float32)
    native_layer_norm_default_395 = torch.ops.aten.native_layer_norm.default(_to_copy_1921, [256], pairformer_stack_blocks_40_triangle_multiplication_layernorm_z_in_weight, pairformer_stack_blocks_40_triangle_multiplication_layernorm_z_in_bias, 1e-05);  _to_copy_1921 = pairformer_stack_blocks_40_triangle_multiplication_layernorm_z_in_weight = pairformer_stack_blocks_40_triangle_multiplication_layernorm_z_in_bias = None
    getitem_3352 = native_layer_norm_default_395[0];  native_layer_norm_default_395 = None
    split_with_sizes_default_92 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_40_triangle_multiplication_merged_linear_p_weight, [512, 512]);  pairformer_stack_blocks_40_triangle_multiplication_merged_linear_p_weight = None
    getitem_3355 = split_with_sizes_default_92[0]
    getitem_3356 = split_with_sizes_default_92[1];  split_with_sizes_default_92 = None
    split_with_sizes_default_93 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_40_triangle_multiplication_merged_linear_g_weight, [512, 512, 256]);  pairformer_stack_blocks_40_triangle_multiplication_merged_linear_g_weight = None
    getitem_3357 = split_with_sizes_default_93[0]
    getitem_3358 = split_with_sizes_default_93[1]
    getitem_3359 = split_with_sizes_default_93[2];  split_with_sizes_default_93 = None
    _to_copy_1922 = torch.ops.aten._to_copy.default(getitem_3355, dtype = torch.bfloat16);  getitem_3355 = None
    _to_copy_1923 = torch.ops.aten._to_copy.default(getitem_3352, dtype = torch.bfloat16)
    t_717 = torch.ops.aten.t.default(_to_copy_1922);  _to_copy_1922 = None
    view_3432 = torch.ops.aten.view.default(_to_copy_1923, [147456, 256]);  _to_copy_1923 = None
    mm_667 = torch.ops.aten.mm.default(view_3432, t_717);  view_3432 = t_717 = None
    view_3433 = torch.ops.aten.view.default(mm_667, [1, 384, 384, 512]);  mm_667 = None
    _to_copy_1924 = torch.ops.aten._to_copy.default(getitem_3357, dtype = torch.bfloat16);  getitem_3357 = None
    _to_copy_1925 = torch.ops.aten._to_copy.default(getitem_3352, dtype = torch.bfloat16)
    t_718 = torch.ops.aten.t.default(_to_copy_1924);  _to_copy_1924 = None
    view_3434 = torch.ops.aten.view.default(_to_copy_1925, [147456, 256]);  _to_copy_1925 = None
    mm_668 = torch.ops.aten.mm.default(view_3434, t_718);  view_3434 = t_718 = None
    view_3435 = torch.ops.aten.view.default(mm_668, [1, 384, 384, 512]);  mm_668 = None
    sigmoid_276 = torch.ops.aten.sigmoid.default(view_3435);  view_3435 = None
    mul_458 = torch.ops.aten.mul.Tensor(view_3433, sigmoid_276);  view_3433 = sigmoid_276 = None
    unsqueeze_1112 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
    bitwise_not_206 = torch.ops.aten.bitwise_not.default(unsqueeze_1112);  unsqueeze_1112 = None
    masked_fill_206 = torch.ops.aten.masked_fill.Scalar(mul_458, bitwise_not_206, 0);  mul_458 = bitwise_not_206 = None
    split_tensor_368 = torch.ops.aten.split.Tensor(masked_fill_206, 256, dim = -1)
    getitem_3362 = split_tensor_368[0];  split_tensor_368 = None
    unsqueeze_1115 = torch.ops.aten.unsqueeze.default(getitem_3362, 4);  getitem_3362 = None
    permute_1830 = torch.ops.aten.permute.default(unsqueeze_1115, [0, 1, 4, 3, 2]);  unsqueeze_1115 = None
    permute_1831 = torch.ops.aten.permute.default(permute_1830, [3, 1, 4, 0, 2]);  permute_1830 = None
    view_3438 = torch.ops.aten.view.default(permute_1831, [256, 384, 384]);  permute_1831 = None
    split_tensor_369 = torch.ops.aten.split.Tensor(masked_fill_206, 256, dim = -1);  masked_fill_206 = None
    getitem_3365 = split_tensor_369[1];  split_tensor_369 = None
    unsqueeze_1116 = torch.ops.aten.unsqueeze.default(getitem_3365, 4);  getitem_3365 = None
    permute_1832 = torch.ops.aten.permute.default(unsqueeze_1116, [0, 4, 1, 3, 2]);  unsqueeze_1116 = None
    permute_1833 = torch.ops.aten.permute.default(permute_1832, [3, 4, 0, 2, 1]);  permute_1832 = None
    view_3439 = torch.ops.aten.view.default(permute_1833, [256, 384, 384]);  permute_1833 = None
    bmm_272 = torch.ops.aten.bmm.default(view_3438, view_3439);  view_3438 = view_3439 = None
    view_3440 = torch.ops.aten.view.default(bmm_272, [256, 384, 1, 1, 384]);  bmm_272 = None
    permute_1834 = torch.ops.aten.permute.default(view_3440, [3, 1, 4, 0, 2]);  view_3440 = None
    view_3441 = torch.ops.aten.view.default(permute_1834, [1, 384, 384, 256]);  permute_1834 = None
    _to_copy_1926 = torch.ops.aten._to_copy.default(getitem_3356, dtype = torch.bfloat16);  getitem_3356 = None
    _to_copy_1927 = torch.ops.aten._to_copy.default(getitem_3352, dtype = torch.bfloat16)
    t_719 = torch.ops.aten.t.default(_to_copy_1926);  _to_copy_1926 = None
    view_3442 = torch.ops.aten.view.default(_to_copy_1927, [147456, 256]);  _to_copy_1927 = None
    mm_669 = torch.ops.aten.mm.default(view_3442, t_719);  view_3442 = t_719 = None
    view_3443 = torch.ops.aten.view.default(mm_669, [1, 384, 384, 512]);  mm_669 = None
    _to_copy_1928 = torch.ops.aten._to_copy.default(getitem_3358, dtype = torch.bfloat16);  getitem_3358 = None
    _to_copy_1929 = torch.ops.aten._to_copy.default(getitem_3352, dtype = torch.bfloat16)
    t_720 = torch.ops.aten.t.default(_to_copy_1928);  _to_copy_1928 = None
    view_3444 = torch.ops.aten.view.default(_to_copy_1929, [147456, 256]);  _to_copy_1929 = None
    mm_670 = torch.ops.aten.mm.default(view_3444, t_720);  view_3444 = t_720 = None
    view_3445 = torch.ops.aten.view.default(mm_670, [1, 384, 384, 512]);  mm_670 = None
    sigmoid_277 = torch.ops.aten.sigmoid.default(view_3445);  view_3445 = None
    mul_459 = torch.ops.aten.mul.Tensor(view_3443, sigmoid_277);  view_3443 = sigmoid_277 = None
    view_3446 = torch.ops.aten.view.default(mul_459, [147456, 512]);  mul_459 = None
    view_3447 = torch.ops.aten.view.default(view_3446, [1, 384, 384, 512]);  view_3446 = None
    transpose_92 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
    unsqueeze_1117 = torch.ops.aten.unsqueeze.default(transpose_92, 3);  transpose_92 = None
    clone_289 = torch.ops.aten.clone.default(unsqueeze_1117, memory_format = torch.contiguous_format);  unsqueeze_1117 = None
    bitwise_not_207 = torch.ops.aten.bitwise_not.default(clone_289);  clone_289 = None
    masked_fill_207 = torch.ops.aten.masked_fill.Scalar(view_3447, bitwise_not_207, 0);  view_3447 = bitwise_not_207 = None
    view_3448 = torch.ops.aten.view.default(masked_fill_207, [147456, 512]);  masked_fill_207 = None
    view_3452 = torch.ops.aten.view.default(view_3448, [1, 384, 384, 512])
    split_tensor_370 = torch.ops.aten.split.Tensor(view_3452, 256, dim = -1);  view_3452 = None
    getitem_3368 = split_tensor_370[0];  split_tensor_370 = None
    unsqueeze_1120 = torch.ops.aten.unsqueeze.default(getitem_3368, 4);  getitem_3368 = None
    permute_1839 = torch.ops.aten.permute.default(unsqueeze_1120, [0, 2, 4, 3, 1]);  unsqueeze_1120 = None
    permute_1840 = torch.ops.aten.permute.default(permute_1839, [3, 1, 4, 0, 2]);  permute_1839 = None
    view_3453 = torch.ops.aten.view.default(permute_1840, [256, 384, 384]);  permute_1840 = None
    view_3454 = torch.ops.aten.view.default(view_3448, [1, 384, 384, 512]);  view_3448 = None
    split_tensor_371 = torch.ops.aten.split.Tensor(view_3454, 256, dim = -1);  view_3454 = None
    getitem_3371 = split_tensor_371[1];  split_tensor_371 = None
    unsqueeze_1121 = torch.ops.aten.unsqueeze.default(getitem_3371, 4);  getitem_3371 = None
    permute_1841 = torch.ops.aten.permute.default(unsqueeze_1121, [0, 4, 2, 3, 1]);  unsqueeze_1121 = None
    permute_1842 = torch.ops.aten.permute.default(permute_1841, [3, 4, 0, 2, 1]);  permute_1841 = None
    view_3455 = torch.ops.aten.view.default(permute_1842, [256, 384, 384]);  permute_1842 = None
    bmm_273 = torch.ops.aten.bmm.default(view_3453, view_3455);  view_3453 = view_3455 = None
    view_3456 = torch.ops.aten.view.default(bmm_273, [256, 384, 1, 1, 384]);  bmm_273 = None
    permute_1843 = torch.ops.aten.permute.default(view_3456, [3, 1, 4, 0, 2]);  view_3456 = None
    view_3457 = torch.ops.aten.view.default(permute_1843, [1, 384, 384, 256]);  permute_1843 = None
    _to_copy_1930 = torch.ops.aten._to_copy.default(view_3441, dtype = torch.float32);  view_3441 = None
    native_layer_norm_default_396 = torch.ops.aten.native_layer_norm.default(_to_copy_1930, [256], None, None, 1e-05);  _to_copy_1930 = None
    getitem_3372 = native_layer_norm_default_396[0];  native_layer_norm_default_396 = None
    _to_copy_1931 = torch.ops.aten._to_copy.default(view_3457, dtype = torch.float32);  view_3457 = None
    native_layer_norm_default_397 = torch.ops.aten.native_layer_norm.default(_to_copy_1931, [256], None, None, 1e-05);  _to_copy_1931 = None
    getitem_3375 = native_layer_norm_default_397[0];  native_layer_norm_default_397 = None
    add_376 = torch.ops.aten.add.Tensor(getitem_3372, getitem_3375);  getitem_3372 = getitem_3375 = None
    _to_copy_1932 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_40_triangle_multiplication_linear_z_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_40_triangle_multiplication_linear_z_out_weight = None
    _to_copy_1933 = torch.ops.aten._to_copy.default(add_376, dtype = torch.bfloat16);  add_376 = None
    t_721 = torch.ops.aten.t.default(_to_copy_1932);  _to_copy_1932 = None
    view_3458 = torch.ops.aten.view.default(_to_copy_1933, [147456, 256]);  _to_copy_1933 = None
    mm_671 = torch.ops.aten.mm.default(view_3458, t_721);  view_3458 = t_721 = None
    view_3459 = torch.ops.aten.view.default(mm_671, [1, 384, 384, 256]);  mm_671 = None
    _to_copy_1934 = torch.ops.aten._to_copy.default(getitem_3359, dtype = torch.bfloat16);  getitem_3359 = None
    _to_copy_1935 = torch.ops.aten._to_copy.default(getitem_3352, dtype = torch.bfloat16);  getitem_3352 = None
    t_722 = torch.ops.aten.t.default(_to_copy_1934);  _to_copy_1934 = None
    view_3460 = torch.ops.aten.view.default(_to_copy_1935, [147456, 256]);  _to_copy_1935 = None
    mm_672 = torch.ops.aten.mm.default(view_3460, t_722);  view_3460 = t_722 = None
    view_3461 = torch.ops.aten.view.default(mm_672, [1, 384, 384, 256]);  mm_672 = None
    sigmoid_278 = torch.ops.aten.sigmoid.default(view_3461);  view_3461 = None
    mul_460 = torch.ops.aten.mul.Tensor(view_3459, sigmoid_278);  view_3459 = sigmoid_278 = None
    add_377 = torch.ops.aten.add.Tensor(add_371, mul_460);  mul_460 = None
    _to_copy_1936 = torch.ops.aten._to_copy.default(add_371, dtype = torch.float32)
    native_layer_norm_default_398 = torch.ops.aten.native_layer_norm.default(_to_copy_1936, [256], None, None, 1e-05);  _to_copy_1936 = None
    getitem_3378 = native_layer_norm_default_398[0];  native_layer_norm_default_398 = None
    _to_copy_1937 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_40_triangle_attention_pair2b_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_40_triangle_attention_pair2b_weight = None
    _to_copy_1938 = torch.ops.aten._to_copy.default(getitem_3378, dtype = torch.bfloat16)
    t_723 = torch.ops.aten.t.default(_to_copy_1937);  _to_copy_1937 = None
    view_3462 = torch.ops.aten.view.default(_to_copy_1938, [147456, 256]);  _to_copy_1938 = None
    mm_673 = torch.ops.aten.mm.default(view_3462, t_723);  view_3462 = t_723 = None
    view_3463 = torch.ops.aten.view.default(mm_673, [1, 384, 384, 8]);  mm_673 = None
    view_3464 = torch.ops.aten.view.default(view_3463, [1, 384, 384, 2, 4]);  view_3463 = None
    permute_1844 = torch.ops.aten.permute.default(view_3464, [0, 3, 4, 1, 2]);  view_3464 = None
    view_3465 = torch.ops.aten.view.default(permute_1844, [1, 2, 4, 1, 384, 384]);  permute_1844 = None
    view_3466 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 384, 384])
    bitwise_not_208 = torch.ops.aten.bitwise_not.default(view_3466);  view_3466 = None
    masked_fill_208 = torch.ops.aten.masked_fill.Scalar(view_3465, bitwise_not_208, -10000);  view_3465 = bitwise_not_208 = None
    view_3467 = torch.ops.aten.view.default(masked_fill_208, [1, 2, 4, 384, 384]);  masked_fill_208 = None
    permute_1845 = torch.ops.aten.permute.default(view_3467, [1, 0, 2, 3, 4]);  view_3467 = None
    view_3468 = torch.ops.aten.view.default(permute_1845, [2, 4, 1, 384, 384]);  permute_1845 = None
    _to_copy_1939 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_40_triangle_attention_pair2qkvg1_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_40_triangle_attention_pair2qkvg1_weight = None
    _to_copy_1940 = torch.ops.aten._to_copy.default(getitem_3378, dtype = torch.bfloat16)
    t_724 = torch.ops.aten.t.default(_to_copy_1939);  _to_copy_1939 = None
    view_3469 = torch.ops.aten.view.default(_to_copy_1940, [147456, 256]);  _to_copy_1940 = None
    mm_674 = torch.ops.aten.mm.default(view_3469, t_724);  view_3469 = t_724 = None
    view_3470 = torch.ops.aten.view.default(mm_674, [1, 384, 384, 1024]);  mm_674 = None
    select_93 = torch.ops.aten.select.int(view_3468, 0, 0)
    view_3471 = torch.ops.aten.view.default(view_3470, [1, 384, 384, 4, 4, 64]);  view_3470 = None
    permute_1846 = torch.ops.aten.permute.default(view_3471, [4, 0, 3, 1, 2, 5]);  view_3471 = None
    view_3472 = torch.ops.aten.view.default(permute_1846, [4, 4, 384, 384, 64]);  permute_1846 = None
    unbind_int_154 = torch.ops.aten.unbind.int(view_3472);  view_3472 = None
    getitem_3381 = unbind_int_154[0]
    getitem_3382 = unbind_int_154[1]
    getitem_3383 = unbind_int_154[2]
    getitem_3384 = unbind_int_154[3];  unbind_int_154 = None
    expand_227 = torch.ops.aten.expand.default(select_93, [4, 384, 384, 384]);  select_93 = None
    _scaled_dot_product_efficient_attention_default_132 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3381, getitem_3382, getitem_3383, expand_227, False);  getitem_3381 = getitem_3382 = getitem_3383 = expand_227 = None
    getitem_3385 = _scaled_dot_product_efficient_attention_default_132[0];  _scaled_dot_product_efficient_attention_default_132 = None
    sigmoid_279 = torch.ops.aten.sigmoid.default(getitem_3384);  getitem_3384 = None
    mul_461 = torch.ops.aten.mul.Tensor(getitem_3385, sigmoid_279);  getitem_3385 = sigmoid_279 = None
    view_3473 = torch.ops.aten.view.default(mul_461, [1, 4, 384, 384, 64]);  mul_461 = None
    permute_1847 = torch.ops.aten.permute.default(view_3473, [0, 2, 3, 1, 4]);  view_3473 = None
    clone_290 = torch.ops.aten.clone.default(permute_1847, memory_format = torch.contiguous_format);  permute_1847 = None
    _unsafe_view_242 = torch.ops.aten._unsafe_view.default(clone_290, [1, 384, 384, 256]);  clone_290 = None
    transpose_93 = torch.ops.aten.transpose.int(getitem_3378, 1, 2);  getitem_3378 = None
    _to_copy_1941 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_40_triangle_attention_pair2qkvg2_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_40_triangle_attention_pair2qkvg2_weight = None
    _to_copy_1942 = torch.ops.aten._to_copy.default(transpose_93, dtype = torch.bfloat16);  transpose_93 = None
    t_725 = torch.ops.aten.t.default(_to_copy_1941);  _to_copy_1941 = None
    expand_228 = torch.ops.aten.expand.default(_to_copy_1942, [1, 384, 384, 256]);  _to_copy_1942 = None
    view_3474 = torch.ops.aten.view.default(expand_228, [384, 384, 256]);  expand_228 = None
    expand_229 = torch.ops.aten.expand.default(t_725, [1, 384, 256, 1024]);  t_725 = None
    view_3475 = torch.ops.aten.view.default(expand_229, [384, 256, 1024]);  expand_229 = None
    bmm_274 = torch.ops.aten.bmm.default(view_3474, view_3475);  view_3474 = view_3475 = None
    view_3476 = torch.ops.aten.view.default(bmm_274, [1, 384, 384, 1024]);  bmm_274 = None
    select_94 = torch.ops.aten.select.int(view_3468, 0, 1);  view_3468 = None
    view_3477 = torch.ops.aten.view.default(view_3476, [1, 384, 384, 4, 4, 64]);  view_3476 = None
    permute_1848 = torch.ops.aten.permute.default(view_3477, [4, 0, 3, 1, 2, 5]);  view_3477 = None
    view_3478 = torch.ops.aten.view.default(permute_1848, [4, 4, 384, 384, 64]);  permute_1848 = None
    unbind_int_155 = torch.ops.aten.unbind.int(view_3478);  view_3478 = None
    getitem_3389 = unbind_int_155[0]
    getitem_3390 = unbind_int_155[1]
    getitem_3391 = unbind_int_155[2]
    getitem_3392 = unbind_int_155[3];  unbind_int_155 = None
    expand_230 = torch.ops.aten.expand.default(select_94, [4, 384, 384, 384]);  select_94 = None
    _scaled_dot_product_efficient_attention_default_133 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3389, getitem_3390, getitem_3391, expand_230, False);  getitem_3389 = getitem_3390 = getitem_3391 = expand_230 = None
    getitem_3393 = _scaled_dot_product_efficient_attention_default_133[0];  _scaled_dot_product_efficient_attention_default_133 = None
    sigmoid_280 = torch.ops.aten.sigmoid.default(getitem_3392);  getitem_3392 = None
    mul_462 = torch.ops.aten.mul.Tensor(getitem_3393, sigmoid_280);  getitem_3393 = sigmoid_280 = None
    view_3479 = torch.ops.aten.view.default(mul_462, [1, 4, 384, 384, 64]);  mul_462 = None
    permute_1849 = torch.ops.aten.permute.default(view_3479, [0, 2, 3, 1, 4]);  view_3479 = None
    clone_291 = torch.ops.aten.clone.default(permute_1849, memory_format = torch.contiguous_format);  permute_1849 = None
    _unsafe_view_243 = torch.ops.aten._unsafe_view.default(clone_291, [1, 384, 384, 256]);  clone_291 = None
    cat_52 = torch.ops.aten.cat.default([_unsafe_view_242, _unsafe_view_243], dim = -1);  _unsafe_view_242 = _unsafe_view_243 = None
    slice_203 = torch.ops.aten.slice.Tensor(pairformer_stack_blocks_40_triangle_attention_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  pairformer_stack_blocks_40_triangle_attention_out_scalers = None
    unsqueeze_1122 = torch.ops.aten.unsqueeze.default(slice_203, 1);  slice_203 = None
    mul_463 = torch.ops.aten.mul.Tensor(pairformer_stack_blocks_40_triangle_attention_linear_out_weight, unsqueeze_1122);  pairformer_stack_blocks_40_triangle_attention_linear_out_weight = unsqueeze_1122 = None
    _to_copy_1943 = torch.ops.aten._to_copy.default(mul_463, dtype = torch.bfloat16);  mul_463 = None
    t_726 = torch.ops.aten.t.default(_to_copy_1943);  _to_copy_1943 = None
    view_3480 = torch.ops.aten.view.default(cat_52, [147456, 512]);  cat_52 = None
    mm_675 = torch.ops.aten.mm.default(view_3480, t_726);  view_3480 = t_726 = None
    view_3481 = torch.ops.aten.view.default(mm_675, [1, 384, 384, 256]);  mm_675 = None
    add_378 = torch.ops.aten.add.Tensor(add_377, view_3481);  add_377 = view_3481 = None
    split_tensor_372 = torch.ops.aten.split.Tensor(add_371, 384, dim = -2)
    getitem_3397 = split_tensor_372[0];  split_tensor_372 = None
    _to_copy_1944 = torch.ops.aten._to_copy.default(getitem_3397, dtype = torch.float32);  getitem_3397 = None
    native_layer_norm_default_399 = torch.ops.aten.native_layer_norm.default(_to_copy_1944, [256], pairformer_stack_blocks_40_transition_pair_layer_norm_weight, pairformer_stack_blocks_40_transition_pair_layer_norm_bias, 1e-05);  _to_copy_1944 = pairformer_stack_blocks_40_transition_pair_layer_norm_weight = pairformer_stack_blocks_40_transition_pair_layer_norm_bias = None
    getitem_3398 = native_layer_norm_default_399[0];  native_layer_norm_default_399 = None
    _to_copy_1945 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_40_transition_pair_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_40_transition_pair_linear_no_bias_ab_weight = None
    _to_copy_1946 = torch.ops.aten._to_copy.default(getitem_3398, dtype = torch.bfloat16);  getitem_3398 = None
    t_727 = torch.ops.aten.t.default(_to_copy_1945);  _to_copy_1945 = None
    view_3482 = torch.ops.aten.view.default(_to_copy_1946, [147456, 256]);  _to_copy_1946 = None
    mm_676 = torch.ops.aten.mm.default(view_3482, t_727);  view_3482 = t_727 = None
    view_3483 = torch.ops.aten.view.default(mm_676, [1, 384, 384, 1024]);  mm_676 = None
    split_tensor_373 = torch.ops.aten.split.Tensor(view_3483, 512, dim = -1);  view_3483 = None
    getitem_3401 = split_tensor_373[0]
    getitem_3402 = split_tensor_373[1];  split_tensor_373 = None
    silu_95 = torch.ops.aten.silu.default(getitem_3401);  getitem_3401 = None
    mul_464 = torch.ops.aten.mul.Tensor(silu_95, getitem_3402);  silu_95 = getitem_3402 = None
    _to_copy_1947 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_40_transition_pair_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_40_transition_pair_linear_out_weight = None
    t_728 = torch.ops.aten.t.default(_to_copy_1947);  _to_copy_1947 = None
    view_3485 = torch.ops.aten.view.default(mul_464, [147456, 512]);  mul_464 = None
    mm_677 = torch.ops.aten.mm.default(view_3485, t_728);  view_3485 = t_728 = None
    view_3486 = torch.ops.aten.view.default(mm_677, [1, 384, 384, 256]);  mm_677 = None
    add_379 = torch.ops.aten.add.Tensor(add_378, view_3486);  add_378 = view_3486 = None
    _to_copy_1948 = torch.ops.aten._to_copy.default(add_375, dtype = torch.float32)
    native_layer_norm_default_400 = torch.ops.aten.native_layer_norm.default(_to_copy_1948, [384], pairformer_stack_blocks_40_attention_pair_bias_single_layer_norm_weight, pairformer_stack_blocks_40_attention_pair_bias_single_layer_norm_bias, 1e-05);  _to_copy_1948 = pairformer_stack_blocks_40_attention_pair_bias_single_layer_norm_weight = pairformer_stack_blocks_40_attention_pair_bias_single_layer_norm_bias = None
    getitem_3403 = native_layer_norm_default_400[0];  native_layer_norm_default_400 = None
    _to_copy_1949 = torch.ops.aten._to_copy.default(add_371, dtype = torch.float32);  add_371 = None
    native_layer_norm_default_401 = torch.ops.aten.native_layer_norm.default(_to_copy_1949, [256], pairformer_stack_blocks_40_attention_pair_bias_pair_layer_norm_weight, pairformer_stack_blocks_40_attention_pair_bias_pair_layer_norm_bias, 1e-05);  _to_copy_1949 = pairformer_stack_blocks_40_attention_pair_bias_pair_layer_norm_weight = pairformer_stack_blocks_40_attention_pair_bias_pair_layer_norm_bias = None
    getitem_3406 = native_layer_norm_default_401[0];  native_layer_norm_default_401 = None
    _to_copy_1950 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_40_attention_pair_bias_pair_linear_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_40_attention_pair_bias_pair_linear_weight = None
    _to_copy_1951 = torch.ops.aten._to_copy.default(getitem_3406, dtype = torch.bfloat16);  getitem_3406 = None
    t_729 = torch.ops.aten.t.default(_to_copy_1950);  _to_copy_1950 = None
    view_3487 = torch.ops.aten.view.default(_to_copy_1951, [147456, 256]);  _to_copy_1951 = None
    mm_678 = torch.ops.aten.mm.default(view_3487, t_729);  view_3487 = t_729 = None
    view_3488 = torch.ops.aten.view.default(mm_678, [1, 384, 384, 16]);  mm_678 = None
    permute_1850 = torch.ops.aten.permute.default(view_3488, [0, 3, 1, 2]);  view_3488 = None
    view_3489 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384])
    bitwise_not_209 = torch.ops.aten.bitwise_not.default(view_3489);  view_3489 = None
    masked_fill_209 = torch.ops.aten.masked_fill.Scalar(permute_1850, bitwise_not_209, -10000);  permute_1850 = bitwise_not_209 = None
    _to_copy_1952 = torch.ops.aten._to_copy.default(getitem_3403, dtype = torch.bfloat16);  getitem_3403 = None
    _to_copy_1953 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_40_attention_pair_bias_attention_input2qkvg_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_40_attention_pair_bias_attention_input2qkvg_weight = None
    unsqueeze_1123 = torch.ops.aten.unsqueeze.default(_to_copy_1952, 3);  _to_copy_1952 = None
    unsqueeze_1124 = torch.ops.aten.unsqueeze.default(unsqueeze_1123, 4);  unsqueeze_1123 = None
    unsqueeze_1125 = torch.ops.aten.unsqueeze.default(unsqueeze_1124, 5);  unsqueeze_1124 = None
    permute_1851 = torch.ops.aten.permute.default(unsqueeze_1125, [3, 0, 4, 1, 5, 2]);  unsqueeze_1125 = None
    unsqueeze_1126 = torch.ops.aten.unsqueeze.default(_to_copy_1953, 4);  _to_copy_1953 = None
    unsqueeze_1127 = torch.ops.aten.unsqueeze.default(unsqueeze_1126, 5);  unsqueeze_1126 = None
    permute_1852 = torch.ops.aten.permute.default(unsqueeze_1127, [1, 4, 2, 5, 3, 0]);  unsqueeze_1127 = None
    permute_1853 = torch.ops.aten.permute.default(permute_1851, [3, 5, 0, 1, 2, 4]);  permute_1851 = None
    view_3490 = torch.ops.aten.view.default(permute_1853, [1, 384, 384]);  permute_1853 = None
    permute_1854 = torch.ops.aten.permute.default(permute_1852, [5, 0, 1, 2, 4, 3]);  permute_1852 = None
    view_3491 = torch.ops.aten.view.default(permute_1854, [1, 384, 1536]);  permute_1854 = None
    bmm_275 = torch.ops.aten.bmm.default(view_3490, view_3491);  view_3490 = view_3491 = None
    view_3492 = torch.ops.aten.view.default(bmm_275, [384, 1, 4, 1, 16, 24]);  bmm_275 = None
    permute_1855 = torch.ops.aten.permute.default(view_3492, [2, 3, 4, 0, 5, 1]);  view_3492 = None
    view_3493 = torch.ops.aten.view.default(permute_1855, [4, 1, 16, 384, 24]);  permute_1855 = None
    unbind_int_156 = torch.ops.aten.unbind.int(view_3493);  view_3493 = None
    getitem_3409 = unbind_int_156[0]
    getitem_3410 = unbind_int_156[1]
    getitem_3411 = unbind_int_156[2]
    getitem_3412 = unbind_int_156[3];  unbind_int_156 = None
    view_3494 = torch.ops.aten.view.default(pairformer_stack_blocks_40_attention_pair_bias_attention_query_bias, [1, 16, 1, 24]);  pairformer_stack_blocks_40_attention_pair_bias_attention_query_bias = None
    add_380 = torch.ops.aten.add.Tensor(getitem_3409, view_3494);  getitem_3409 = view_3494 = None
    _to_copy_1954 = torch.ops.aten._to_copy.default(add_380, dtype = torch.bfloat16);  add_380 = None
    expand_231 = torch.ops.aten.expand.default(masked_fill_209, [1, 16, 384, 384]);  masked_fill_209 = None
    _scaled_dot_product_efficient_attention_default_134 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_1954, getitem_3410, getitem_3411, expand_231, False);  _to_copy_1954 = getitem_3410 = getitem_3411 = expand_231 = None
    getitem_3413 = _scaled_dot_product_efficient_attention_default_134[0];  _scaled_dot_product_efficient_attention_default_134 = None
    add_381 = torch.ops.aten.add.Tensor(getitem_3412, 1);  getitem_3412 = None
    sigmoid_281 = torch.ops.aten.sigmoid.default(add_381);  add_381 = None
    mul_465 = torch.ops.aten.mul.Tensor(getitem_3413, sigmoid_281);  getitem_3413 = sigmoid_281 = None
    _to_copy_1955 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_40_attention_pair_bias_attention_output_proj_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_40_attention_pair_bias_attention_output_proj_weight = None
    unsqueeze_1128 = torch.ops.aten.unsqueeze.default(mul_465, 4);  mul_465 = None
    permute_1856 = torch.ops.aten.permute.default(unsqueeze_1128, [0, 2, 4, 3, 1]);  unsqueeze_1128 = None
    unsqueeze_1129 = torch.ops.aten.unsqueeze.default(_to_copy_1955, 3);  _to_copy_1955 = None
    unsqueeze_1130 = torch.ops.aten.unsqueeze.default(unsqueeze_1129, 4);  unsqueeze_1129 = None
    permute_1857 = torch.ops.aten.permute.default(unsqueeze_1130, [3, 4, 2, 1, 0]);  unsqueeze_1130 = None
    permute_1858 = torch.ops.aten.permute.default(permute_1856, [1, 3, 4, 0, 2]);  permute_1856 = None
    clone_292 = torch.ops.aten.clone.default(permute_1858, memory_format = torch.contiguous_format);  permute_1858 = None
    _unsafe_view_244 = torch.ops.aten._unsafe_view.default(clone_292, [1, 384, 384]);  clone_292 = None
    permute_1859 = torch.ops.aten.permute.default(permute_1857, [3, 4, 0, 2, 1]);  permute_1857 = None
    clone_293 = torch.ops.aten.clone.default(permute_1859, memory_format = torch.contiguous_format);  permute_1859 = None
    _unsafe_view_245 = torch.ops.aten._unsafe_view.default(clone_293, [1, 384, 384]);  clone_293 = None
    bmm_276 = torch.ops.aten.bmm.default(_unsafe_view_244, _unsafe_view_245);  _unsafe_view_244 = _unsafe_view_245 = None
    view_3495 = torch.ops.aten.view.default(bmm_276, [384, 1, 1, 1, 384]);  bmm_276 = None
    permute_1860 = torch.ops.aten.permute.default(view_3495, [3, 0, 4, 1, 2]);  view_3495 = None
    view_3496 = torch.ops.aten.view.default(permute_1860, [1, 384, 384]);  permute_1860 = None
    unsqueeze_1131 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
    mul_466 = torch.ops.aten.mul.Tensor(view_3496, unsqueeze_1131);  view_3496 = unsqueeze_1131 = None
    add_382 = torch.ops.aten.add.Tensor(add_375, mul_466);  mul_466 = None
    split_tensor_374 = torch.ops.aten.split.Tensor(add_375, 384, dim = -2);  add_375 = None
    getitem_3417 = split_tensor_374[0];  split_tensor_374 = None
    _to_copy_1956 = torch.ops.aten._to_copy.default(getitem_3417, dtype = torch.float32);  getitem_3417 = None
    native_layer_norm_default_402 = torch.ops.aten.native_layer_norm.default(_to_copy_1956, [384], pairformer_stack_blocks_40_transition_single_layer_norm_weight, pairformer_stack_blocks_40_transition_single_layer_norm_bias, 1e-05);  _to_copy_1956 = pairformer_stack_blocks_40_transition_single_layer_norm_weight = pairformer_stack_blocks_40_transition_single_layer_norm_bias = None
    getitem_3418 = native_layer_norm_default_402[0];  native_layer_norm_default_402 = None
    _to_copy_1957 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_40_transition_single_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_40_transition_single_linear_no_bias_ab_weight = None
    _to_copy_1958 = torch.ops.aten._to_copy.default(getitem_3418, dtype = torch.bfloat16);  getitem_3418 = None
    t_730 = torch.ops.aten.t.default(_to_copy_1957);  _to_copy_1957 = None
    view_3497 = torch.ops.aten.view.default(_to_copy_1958, [384, 384]);  _to_copy_1958 = None
    mm_679 = torch.ops.aten.mm.default(view_3497, t_730);  view_3497 = t_730 = None
    view_3498 = torch.ops.aten.view.default(mm_679, [1, 384, 1536]);  mm_679 = None
    split_tensor_375 = torch.ops.aten.split.Tensor(view_3498, 768, dim = -1);  view_3498 = None
    getitem_3421 = split_tensor_375[0]
    getitem_3422 = split_tensor_375[1];  split_tensor_375 = None
    silu_96 = torch.ops.aten.silu.default(getitem_3421);  getitem_3421 = None
    mul_467 = torch.ops.aten.mul.Tensor(silu_96, getitem_3422);  silu_96 = getitem_3422 = None
    _to_copy_1959 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_40_transition_single_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_40_transition_single_linear_out_weight = None
    t_731 = torch.ops.aten.t.default(_to_copy_1959);  _to_copy_1959 = None
    view_3500 = torch.ops.aten.view.default(mul_467, [384, 768]);  mul_467 = None
    mm_680 = torch.ops.aten.mm.default(view_3500, t_731);  view_3500 = t_731 = None
    view_3501 = torch.ops.aten.view.default(mm_680, [1, 384, 384]);  mm_680 = None
    add_383 = torch.ops.aten.add.Tensor(add_382, view_3501);  add_382 = view_3501 = None
    _to_copy_1960 = torch.ops.aten._to_copy.default(add_379, dtype = torch.float32)
    native_layer_norm_default_403 = torch.ops.aten.native_layer_norm.default(_to_copy_1960, [256], pairformer_stack_blocks_41_triangle_multiplication_layernorm_z_in_weight, pairformer_stack_blocks_41_triangle_multiplication_layernorm_z_in_bias, 1e-05);  _to_copy_1960 = pairformer_stack_blocks_41_triangle_multiplication_layernorm_z_in_weight = pairformer_stack_blocks_41_triangle_multiplication_layernorm_z_in_bias = None
    getitem_3423 = native_layer_norm_default_403[0];  native_layer_norm_default_403 = None
    split_with_sizes_default_94 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_41_triangle_multiplication_merged_linear_p_weight, [512, 512]);  pairformer_stack_blocks_41_triangle_multiplication_merged_linear_p_weight = None
    getitem_3426 = split_with_sizes_default_94[0]
    getitem_3427 = split_with_sizes_default_94[1];  split_with_sizes_default_94 = None
    split_with_sizes_default_95 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_41_triangle_multiplication_merged_linear_g_weight, [512, 512, 256]);  pairformer_stack_blocks_41_triangle_multiplication_merged_linear_g_weight = None
    getitem_3428 = split_with_sizes_default_95[0]
    getitem_3429 = split_with_sizes_default_95[1]
    getitem_3430 = split_with_sizes_default_95[2];  split_with_sizes_default_95 = None
    _to_copy_1961 = torch.ops.aten._to_copy.default(getitem_3426, dtype = torch.bfloat16);  getitem_3426 = None
    _to_copy_1962 = torch.ops.aten._to_copy.default(getitem_3423, dtype = torch.bfloat16)
    t_732 = torch.ops.aten.t.default(_to_copy_1961);  _to_copy_1961 = None
    view_3502 = torch.ops.aten.view.default(_to_copy_1962, [147456, 256]);  _to_copy_1962 = None
    mm_681 = torch.ops.aten.mm.default(view_3502, t_732);  view_3502 = t_732 = None
    view_3503 = torch.ops.aten.view.default(mm_681, [1, 384, 384, 512]);  mm_681 = None
    _to_copy_1963 = torch.ops.aten._to_copy.default(getitem_3428, dtype = torch.bfloat16);  getitem_3428 = None
    _to_copy_1964 = torch.ops.aten._to_copy.default(getitem_3423, dtype = torch.bfloat16)
    t_733 = torch.ops.aten.t.default(_to_copy_1963);  _to_copy_1963 = None
    view_3504 = torch.ops.aten.view.default(_to_copy_1964, [147456, 256]);  _to_copy_1964 = None
    mm_682 = torch.ops.aten.mm.default(view_3504, t_733);  view_3504 = t_733 = None
    view_3505 = torch.ops.aten.view.default(mm_682, [1, 384, 384, 512]);  mm_682 = None
    sigmoid_282 = torch.ops.aten.sigmoid.default(view_3505);  view_3505 = None
    mul_468 = torch.ops.aten.mul.Tensor(view_3503, sigmoid_282);  view_3503 = sigmoid_282 = None
    unsqueeze_1132 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
    bitwise_not_210 = torch.ops.aten.bitwise_not.default(unsqueeze_1132);  unsqueeze_1132 = None
    masked_fill_210 = torch.ops.aten.masked_fill.Scalar(mul_468, bitwise_not_210, 0);  mul_468 = bitwise_not_210 = None
    split_tensor_376 = torch.ops.aten.split.Tensor(masked_fill_210, 256, dim = -1)
    getitem_3433 = split_tensor_376[0];  split_tensor_376 = None
    unsqueeze_1135 = torch.ops.aten.unsqueeze.default(getitem_3433, 4);  getitem_3433 = None
    permute_1865 = torch.ops.aten.permute.default(unsqueeze_1135, [0, 1, 4, 3, 2]);  unsqueeze_1135 = None
    permute_1866 = torch.ops.aten.permute.default(permute_1865, [3, 1, 4, 0, 2]);  permute_1865 = None
    view_3508 = torch.ops.aten.view.default(permute_1866, [256, 384, 384]);  permute_1866 = None
    split_tensor_377 = torch.ops.aten.split.Tensor(masked_fill_210, 256, dim = -1);  masked_fill_210 = None
    getitem_3436 = split_tensor_377[1];  split_tensor_377 = None
    unsqueeze_1136 = torch.ops.aten.unsqueeze.default(getitem_3436, 4);  getitem_3436 = None
    permute_1867 = torch.ops.aten.permute.default(unsqueeze_1136, [0, 4, 1, 3, 2]);  unsqueeze_1136 = None
    permute_1868 = torch.ops.aten.permute.default(permute_1867, [3, 4, 0, 2, 1]);  permute_1867 = None
    view_3509 = torch.ops.aten.view.default(permute_1868, [256, 384, 384]);  permute_1868 = None
    bmm_277 = torch.ops.aten.bmm.default(view_3508, view_3509);  view_3508 = view_3509 = None
    view_3510 = torch.ops.aten.view.default(bmm_277, [256, 384, 1, 1, 384]);  bmm_277 = None
    permute_1869 = torch.ops.aten.permute.default(view_3510, [3, 1, 4, 0, 2]);  view_3510 = None
    view_3511 = torch.ops.aten.view.default(permute_1869, [1, 384, 384, 256]);  permute_1869 = None
    _to_copy_1965 = torch.ops.aten._to_copy.default(getitem_3427, dtype = torch.bfloat16);  getitem_3427 = None
    _to_copy_1966 = torch.ops.aten._to_copy.default(getitem_3423, dtype = torch.bfloat16)
    t_734 = torch.ops.aten.t.default(_to_copy_1965);  _to_copy_1965 = None
    view_3512 = torch.ops.aten.view.default(_to_copy_1966, [147456, 256]);  _to_copy_1966 = None
    mm_683 = torch.ops.aten.mm.default(view_3512, t_734);  view_3512 = t_734 = None
    view_3513 = torch.ops.aten.view.default(mm_683, [1, 384, 384, 512]);  mm_683 = None
    _to_copy_1967 = torch.ops.aten._to_copy.default(getitem_3429, dtype = torch.bfloat16);  getitem_3429 = None
    _to_copy_1968 = torch.ops.aten._to_copy.default(getitem_3423, dtype = torch.bfloat16)
    t_735 = torch.ops.aten.t.default(_to_copy_1967);  _to_copy_1967 = None
    view_3514 = torch.ops.aten.view.default(_to_copy_1968, [147456, 256]);  _to_copy_1968 = None
    mm_684 = torch.ops.aten.mm.default(view_3514, t_735);  view_3514 = t_735 = None
    view_3515 = torch.ops.aten.view.default(mm_684, [1, 384, 384, 512]);  mm_684 = None
    sigmoid_283 = torch.ops.aten.sigmoid.default(view_3515);  view_3515 = None
    mul_469 = torch.ops.aten.mul.Tensor(view_3513, sigmoid_283);  view_3513 = sigmoid_283 = None
    view_3516 = torch.ops.aten.view.default(mul_469, [147456, 512]);  mul_469 = None
    view_3517 = torch.ops.aten.view.default(view_3516, [1, 384, 384, 512]);  view_3516 = None
    transpose_94 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
    unsqueeze_1137 = torch.ops.aten.unsqueeze.default(transpose_94, 3);  transpose_94 = None
    clone_294 = torch.ops.aten.clone.default(unsqueeze_1137, memory_format = torch.contiguous_format);  unsqueeze_1137 = None
    bitwise_not_211 = torch.ops.aten.bitwise_not.default(clone_294);  clone_294 = None
    masked_fill_211 = torch.ops.aten.masked_fill.Scalar(view_3517, bitwise_not_211, 0);  view_3517 = bitwise_not_211 = None
    view_3518 = torch.ops.aten.view.default(masked_fill_211, [147456, 512]);  masked_fill_211 = None
    view_3522 = torch.ops.aten.view.default(view_3518, [1, 384, 384, 512])
    split_tensor_378 = torch.ops.aten.split.Tensor(view_3522, 256, dim = -1);  view_3522 = None
    getitem_3439 = split_tensor_378[0];  split_tensor_378 = None
    unsqueeze_1140 = torch.ops.aten.unsqueeze.default(getitem_3439, 4);  getitem_3439 = None
    permute_1874 = torch.ops.aten.permute.default(unsqueeze_1140, [0, 2, 4, 3, 1]);  unsqueeze_1140 = None
    permute_1875 = torch.ops.aten.permute.default(permute_1874, [3, 1, 4, 0, 2]);  permute_1874 = None
    view_3523 = torch.ops.aten.view.default(permute_1875, [256, 384, 384]);  permute_1875 = None
    view_3524 = torch.ops.aten.view.default(view_3518, [1, 384, 384, 512]);  view_3518 = None
    split_tensor_379 = torch.ops.aten.split.Tensor(view_3524, 256, dim = -1);  view_3524 = None
    getitem_3442 = split_tensor_379[1];  split_tensor_379 = None
    unsqueeze_1141 = torch.ops.aten.unsqueeze.default(getitem_3442, 4);  getitem_3442 = None
    permute_1876 = torch.ops.aten.permute.default(unsqueeze_1141, [0, 4, 2, 3, 1]);  unsqueeze_1141 = None
    permute_1877 = torch.ops.aten.permute.default(permute_1876, [3, 4, 0, 2, 1]);  permute_1876 = None
    view_3525 = torch.ops.aten.view.default(permute_1877, [256, 384, 384]);  permute_1877 = None
    bmm_278 = torch.ops.aten.bmm.default(view_3523, view_3525);  view_3523 = view_3525 = None
    view_3526 = torch.ops.aten.view.default(bmm_278, [256, 384, 1, 1, 384]);  bmm_278 = None
    permute_1878 = torch.ops.aten.permute.default(view_3526, [3, 1, 4, 0, 2]);  view_3526 = None
    view_3527 = torch.ops.aten.view.default(permute_1878, [1, 384, 384, 256]);  permute_1878 = None
    _to_copy_1969 = torch.ops.aten._to_copy.default(view_3511, dtype = torch.float32);  view_3511 = None
    native_layer_norm_default_404 = torch.ops.aten.native_layer_norm.default(_to_copy_1969, [256], None, None, 1e-05);  _to_copy_1969 = None
    getitem_3443 = native_layer_norm_default_404[0];  native_layer_norm_default_404 = None
    _to_copy_1970 = torch.ops.aten._to_copy.default(view_3527, dtype = torch.float32);  view_3527 = None
    native_layer_norm_default_405 = torch.ops.aten.native_layer_norm.default(_to_copy_1970, [256], None, None, 1e-05);  _to_copy_1970 = None
    getitem_3446 = native_layer_norm_default_405[0];  native_layer_norm_default_405 = None
    add_384 = torch.ops.aten.add.Tensor(getitem_3443, getitem_3446);  getitem_3443 = getitem_3446 = None
    _to_copy_1971 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_41_triangle_multiplication_linear_z_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_41_triangle_multiplication_linear_z_out_weight = None
    _to_copy_1972 = torch.ops.aten._to_copy.default(add_384, dtype = torch.bfloat16);  add_384 = None
    t_736 = torch.ops.aten.t.default(_to_copy_1971);  _to_copy_1971 = None
    view_3528 = torch.ops.aten.view.default(_to_copy_1972, [147456, 256]);  _to_copy_1972 = None
    mm_685 = torch.ops.aten.mm.default(view_3528, t_736);  view_3528 = t_736 = None
    view_3529 = torch.ops.aten.view.default(mm_685, [1, 384, 384, 256]);  mm_685 = None
    _to_copy_1973 = torch.ops.aten._to_copy.default(getitem_3430, dtype = torch.bfloat16);  getitem_3430 = None
    _to_copy_1974 = torch.ops.aten._to_copy.default(getitem_3423, dtype = torch.bfloat16);  getitem_3423 = None
    t_737 = torch.ops.aten.t.default(_to_copy_1973);  _to_copy_1973 = None
    view_3530 = torch.ops.aten.view.default(_to_copy_1974, [147456, 256]);  _to_copy_1974 = None
    mm_686 = torch.ops.aten.mm.default(view_3530, t_737);  view_3530 = t_737 = None
    view_3531 = torch.ops.aten.view.default(mm_686, [1, 384, 384, 256]);  mm_686 = None
    sigmoid_284 = torch.ops.aten.sigmoid.default(view_3531);  view_3531 = None
    mul_470 = torch.ops.aten.mul.Tensor(view_3529, sigmoid_284);  view_3529 = sigmoid_284 = None
    add_385 = torch.ops.aten.add.Tensor(add_379, mul_470);  mul_470 = None
    _to_copy_1975 = torch.ops.aten._to_copy.default(add_379, dtype = torch.float32)
    native_layer_norm_default_406 = torch.ops.aten.native_layer_norm.default(_to_copy_1975, [256], None, None, 1e-05);  _to_copy_1975 = None
    getitem_3449 = native_layer_norm_default_406[0];  native_layer_norm_default_406 = None
    _to_copy_1976 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_41_triangle_attention_pair2b_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_41_triangle_attention_pair2b_weight = None
    _to_copy_1977 = torch.ops.aten._to_copy.default(getitem_3449, dtype = torch.bfloat16)
    t_738 = torch.ops.aten.t.default(_to_copy_1976);  _to_copy_1976 = None
    view_3532 = torch.ops.aten.view.default(_to_copy_1977, [147456, 256]);  _to_copy_1977 = None
    mm_687 = torch.ops.aten.mm.default(view_3532, t_738);  view_3532 = t_738 = None
    view_3533 = torch.ops.aten.view.default(mm_687, [1, 384, 384, 8]);  mm_687 = None
    view_3534 = torch.ops.aten.view.default(view_3533, [1, 384, 384, 2, 4]);  view_3533 = None
    permute_1879 = torch.ops.aten.permute.default(view_3534, [0, 3, 4, 1, 2]);  view_3534 = None
    view_3535 = torch.ops.aten.view.default(permute_1879, [1, 2, 4, 1, 384, 384]);  permute_1879 = None
    view_3536 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 384, 384])
    bitwise_not_212 = torch.ops.aten.bitwise_not.default(view_3536);  view_3536 = None
    masked_fill_212 = torch.ops.aten.masked_fill.Scalar(view_3535, bitwise_not_212, -10000);  view_3535 = bitwise_not_212 = None
    view_3537 = torch.ops.aten.view.default(masked_fill_212, [1, 2, 4, 384, 384]);  masked_fill_212 = None
    permute_1880 = torch.ops.aten.permute.default(view_3537, [1, 0, 2, 3, 4]);  view_3537 = None
    view_3538 = torch.ops.aten.view.default(permute_1880, [2, 4, 1, 384, 384]);  permute_1880 = None
    _to_copy_1978 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_41_triangle_attention_pair2qkvg1_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_41_triangle_attention_pair2qkvg1_weight = None
    _to_copy_1979 = torch.ops.aten._to_copy.default(getitem_3449, dtype = torch.bfloat16)
    t_739 = torch.ops.aten.t.default(_to_copy_1978);  _to_copy_1978 = None
    view_3539 = torch.ops.aten.view.default(_to_copy_1979, [147456, 256]);  _to_copy_1979 = None
    mm_688 = torch.ops.aten.mm.default(view_3539, t_739);  view_3539 = t_739 = None
    view_3540 = torch.ops.aten.view.default(mm_688, [1, 384, 384, 1024]);  mm_688 = None
    select_95 = torch.ops.aten.select.int(view_3538, 0, 0)
    view_3541 = torch.ops.aten.view.default(view_3540, [1, 384, 384, 4, 4, 64]);  view_3540 = None
    permute_1881 = torch.ops.aten.permute.default(view_3541, [4, 0, 3, 1, 2, 5]);  view_3541 = None
    view_3542 = torch.ops.aten.view.default(permute_1881, [4, 4, 384, 384, 64]);  permute_1881 = None
    unbind_int_157 = torch.ops.aten.unbind.int(view_3542);  view_3542 = None
    getitem_3452 = unbind_int_157[0]
    getitem_3453 = unbind_int_157[1]
    getitem_3454 = unbind_int_157[2]
    getitem_3455 = unbind_int_157[3];  unbind_int_157 = None
    expand_232 = torch.ops.aten.expand.default(select_95, [4, 384, 384, 384]);  select_95 = None
    _scaled_dot_product_efficient_attention_default_135 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3452, getitem_3453, getitem_3454, expand_232, False);  getitem_3452 = getitem_3453 = getitem_3454 = expand_232 = None
    getitem_3456 = _scaled_dot_product_efficient_attention_default_135[0];  _scaled_dot_product_efficient_attention_default_135 = None
    sigmoid_285 = torch.ops.aten.sigmoid.default(getitem_3455);  getitem_3455 = None
    mul_471 = torch.ops.aten.mul.Tensor(getitem_3456, sigmoid_285);  getitem_3456 = sigmoid_285 = None
    view_3543 = torch.ops.aten.view.default(mul_471, [1, 4, 384, 384, 64]);  mul_471 = None
    permute_1882 = torch.ops.aten.permute.default(view_3543, [0, 2, 3, 1, 4]);  view_3543 = None
    clone_295 = torch.ops.aten.clone.default(permute_1882, memory_format = torch.contiguous_format);  permute_1882 = None
    _unsafe_view_246 = torch.ops.aten._unsafe_view.default(clone_295, [1, 384, 384, 256]);  clone_295 = None
    transpose_95 = torch.ops.aten.transpose.int(getitem_3449, 1, 2);  getitem_3449 = None
    _to_copy_1980 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_41_triangle_attention_pair2qkvg2_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_41_triangle_attention_pair2qkvg2_weight = None
    _to_copy_1981 = torch.ops.aten._to_copy.default(transpose_95, dtype = torch.bfloat16);  transpose_95 = None
    t_740 = torch.ops.aten.t.default(_to_copy_1980);  _to_copy_1980 = None
    expand_233 = torch.ops.aten.expand.default(_to_copy_1981, [1, 384, 384, 256]);  _to_copy_1981 = None
    view_3544 = torch.ops.aten.view.default(expand_233, [384, 384, 256]);  expand_233 = None
    expand_234 = torch.ops.aten.expand.default(t_740, [1, 384, 256, 1024]);  t_740 = None
    view_3545 = torch.ops.aten.view.default(expand_234, [384, 256, 1024]);  expand_234 = None
    bmm_279 = torch.ops.aten.bmm.default(view_3544, view_3545);  view_3544 = view_3545 = None
    view_3546 = torch.ops.aten.view.default(bmm_279, [1, 384, 384, 1024]);  bmm_279 = None
    select_96 = torch.ops.aten.select.int(view_3538, 0, 1);  view_3538 = None
    view_3547 = torch.ops.aten.view.default(view_3546, [1, 384, 384, 4, 4, 64]);  view_3546 = None
    permute_1883 = torch.ops.aten.permute.default(view_3547, [4, 0, 3, 1, 2, 5]);  view_3547 = None
    view_3548 = torch.ops.aten.view.default(permute_1883, [4, 4, 384, 384, 64]);  permute_1883 = None
    unbind_int_158 = torch.ops.aten.unbind.int(view_3548);  view_3548 = None
    getitem_3460 = unbind_int_158[0]
    getitem_3461 = unbind_int_158[1]
    getitem_3462 = unbind_int_158[2]
    getitem_3463 = unbind_int_158[3];  unbind_int_158 = None
    expand_235 = torch.ops.aten.expand.default(select_96, [4, 384, 384, 384]);  select_96 = None
    _scaled_dot_product_efficient_attention_default_136 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3460, getitem_3461, getitem_3462, expand_235, False);  getitem_3460 = getitem_3461 = getitem_3462 = expand_235 = None
    getitem_3464 = _scaled_dot_product_efficient_attention_default_136[0];  _scaled_dot_product_efficient_attention_default_136 = None
    sigmoid_286 = torch.ops.aten.sigmoid.default(getitem_3463);  getitem_3463 = None
    mul_472 = torch.ops.aten.mul.Tensor(getitem_3464, sigmoid_286);  getitem_3464 = sigmoid_286 = None
    view_3549 = torch.ops.aten.view.default(mul_472, [1, 4, 384, 384, 64]);  mul_472 = None
    permute_1884 = torch.ops.aten.permute.default(view_3549, [0, 2, 3, 1, 4]);  view_3549 = None
    clone_296 = torch.ops.aten.clone.default(permute_1884, memory_format = torch.contiguous_format);  permute_1884 = None
    _unsafe_view_247 = torch.ops.aten._unsafe_view.default(clone_296, [1, 384, 384, 256]);  clone_296 = None
    cat_53 = torch.ops.aten.cat.default([_unsafe_view_246, _unsafe_view_247], dim = -1);  _unsafe_view_246 = _unsafe_view_247 = None
    slice_204 = torch.ops.aten.slice.Tensor(pairformer_stack_blocks_41_triangle_attention_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  pairformer_stack_blocks_41_triangle_attention_out_scalers = None
    unsqueeze_1142 = torch.ops.aten.unsqueeze.default(slice_204, 1);  slice_204 = None
    mul_473 = torch.ops.aten.mul.Tensor(pairformer_stack_blocks_41_triangle_attention_linear_out_weight, unsqueeze_1142);  pairformer_stack_blocks_41_triangle_attention_linear_out_weight = unsqueeze_1142 = None
    _to_copy_1982 = torch.ops.aten._to_copy.default(mul_473, dtype = torch.bfloat16);  mul_473 = None
    t_741 = torch.ops.aten.t.default(_to_copy_1982);  _to_copy_1982 = None
    view_3550 = torch.ops.aten.view.default(cat_53, [147456, 512]);  cat_53 = None
    mm_689 = torch.ops.aten.mm.default(view_3550, t_741);  view_3550 = t_741 = None
    view_3551 = torch.ops.aten.view.default(mm_689, [1, 384, 384, 256]);  mm_689 = None
    add_386 = torch.ops.aten.add.Tensor(add_385, view_3551);  add_385 = view_3551 = None
    split_tensor_380 = torch.ops.aten.split.Tensor(add_379, 384, dim = -2)
    getitem_3468 = split_tensor_380[0];  split_tensor_380 = None
    _to_copy_1983 = torch.ops.aten._to_copy.default(getitem_3468, dtype = torch.float32);  getitem_3468 = None
    native_layer_norm_default_407 = torch.ops.aten.native_layer_norm.default(_to_copy_1983, [256], pairformer_stack_blocks_41_transition_pair_layer_norm_weight, pairformer_stack_blocks_41_transition_pair_layer_norm_bias, 1e-05);  _to_copy_1983 = pairformer_stack_blocks_41_transition_pair_layer_norm_weight = pairformer_stack_blocks_41_transition_pair_layer_norm_bias = None
    getitem_3469 = native_layer_norm_default_407[0];  native_layer_norm_default_407 = None
    _to_copy_1984 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_41_transition_pair_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_41_transition_pair_linear_no_bias_ab_weight = None
    _to_copy_1985 = torch.ops.aten._to_copy.default(getitem_3469, dtype = torch.bfloat16);  getitem_3469 = None
    t_742 = torch.ops.aten.t.default(_to_copy_1984);  _to_copy_1984 = None
    view_3552 = torch.ops.aten.view.default(_to_copy_1985, [147456, 256]);  _to_copy_1985 = None
    mm_690 = torch.ops.aten.mm.default(view_3552, t_742);  view_3552 = t_742 = None
    view_3553 = torch.ops.aten.view.default(mm_690, [1, 384, 384, 1024]);  mm_690 = None
    split_tensor_381 = torch.ops.aten.split.Tensor(view_3553, 512, dim = -1);  view_3553 = None
    getitem_3472 = split_tensor_381[0]
    getitem_3473 = split_tensor_381[1];  split_tensor_381 = None
    silu_97 = torch.ops.aten.silu.default(getitem_3472);  getitem_3472 = None
    mul_474 = torch.ops.aten.mul.Tensor(silu_97, getitem_3473);  silu_97 = getitem_3473 = None
    _to_copy_1986 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_41_transition_pair_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_41_transition_pair_linear_out_weight = None
    t_743 = torch.ops.aten.t.default(_to_copy_1986);  _to_copy_1986 = None
    view_3555 = torch.ops.aten.view.default(mul_474, [147456, 512]);  mul_474 = None
    mm_691 = torch.ops.aten.mm.default(view_3555, t_743);  view_3555 = t_743 = None
    view_3556 = torch.ops.aten.view.default(mm_691, [1, 384, 384, 256]);  mm_691 = None
    add_387 = torch.ops.aten.add.Tensor(add_386, view_3556);  add_386 = view_3556 = None
    _to_copy_1987 = torch.ops.aten._to_copy.default(add_383, dtype = torch.float32)
    native_layer_norm_default_408 = torch.ops.aten.native_layer_norm.default(_to_copy_1987, [384], pairformer_stack_blocks_41_attention_pair_bias_single_layer_norm_weight, pairformer_stack_blocks_41_attention_pair_bias_single_layer_norm_bias, 1e-05);  _to_copy_1987 = pairformer_stack_blocks_41_attention_pair_bias_single_layer_norm_weight = pairformer_stack_blocks_41_attention_pair_bias_single_layer_norm_bias = None
    getitem_3474 = native_layer_norm_default_408[0];  native_layer_norm_default_408 = None
    _to_copy_1988 = torch.ops.aten._to_copy.default(add_379, dtype = torch.float32);  add_379 = None
    native_layer_norm_default_409 = torch.ops.aten.native_layer_norm.default(_to_copy_1988, [256], pairformer_stack_blocks_41_attention_pair_bias_pair_layer_norm_weight, pairformer_stack_blocks_41_attention_pair_bias_pair_layer_norm_bias, 1e-05);  _to_copy_1988 = pairformer_stack_blocks_41_attention_pair_bias_pair_layer_norm_weight = pairformer_stack_blocks_41_attention_pair_bias_pair_layer_norm_bias = None
    getitem_3477 = native_layer_norm_default_409[0];  native_layer_norm_default_409 = None
    _to_copy_1989 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_41_attention_pair_bias_pair_linear_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_41_attention_pair_bias_pair_linear_weight = None
    _to_copy_1990 = torch.ops.aten._to_copy.default(getitem_3477, dtype = torch.bfloat16);  getitem_3477 = None
    t_744 = torch.ops.aten.t.default(_to_copy_1989);  _to_copy_1989 = None
    view_3557 = torch.ops.aten.view.default(_to_copy_1990, [147456, 256]);  _to_copy_1990 = None
    mm_692 = torch.ops.aten.mm.default(view_3557, t_744);  view_3557 = t_744 = None
    view_3558 = torch.ops.aten.view.default(mm_692, [1, 384, 384, 16]);  mm_692 = None
    permute_1885 = torch.ops.aten.permute.default(view_3558, [0, 3, 1, 2]);  view_3558 = None
    view_3559 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384])
    bitwise_not_213 = torch.ops.aten.bitwise_not.default(view_3559);  view_3559 = None
    masked_fill_213 = torch.ops.aten.masked_fill.Scalar(permute_1885, bitwise_not_213, -10000);  permute_1885 = bitwise_not_213 = None
    _to_copy_1991 = torch.ops.aten._to_copy.default(getitem_3474, dtype = torch.bfloat16);  getitem_3474 = None
    _to_copy_1992 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_41_attention_pair_bias_attention_input2qkvg_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_41_attention_pair_bias_attention_input2qkvg_weight = None
    unsqueeze_1143 = torch.ops.aten.unsqueeze.default(_to_copy_1991, 3);  _to_copy_1991 = None
    unsqueeze_1144 = torch.ops.aten.unsqueeze.default(unsqueeze_1143, 4);  unsqueeze_1143 = None
    unsqueeze_1145 = torch.ops.aten.unsqueeze.default(unsqueeze_1144, 5);  unsqueeze_1144 = None
    permute_1886 = torch.ops.aten.permute.default(unsqueeze_1145, [3, 0, 4, 1, 5, 2]);  unsqueeze_1145 = None
    unsqueeze_1146 = torch.ops.aten.unsqueeze.default(_to_copy_1992, 4);  _to_copy_1992 = None
    unsqueeze_1147 = torch.ops.aten.unsqueeze.default(unsqueeze_1146, 5);  unsqueeze_1146 = None
    permute_1887 = torch.ops.aten.permute.default(unsqueeze_1147, [1, 4, 2, 5, 3, 0]);  unsqueeze_1147 = None
    permute_1888 = torch.ops.aten.permute.default(permute_1886, [3, 5, 0, 1, 2, 4]);  permute_1886 = None
    view_3560 = torch.ops.aten.view.default(permute_1888, [1, 384, 384]);  permute_1888 = None
    permute_1889 = torch.ops.aten.permute.default(permute_1887, [5, 0, 1, 2, 4, 3]);  permute_1887 = None
    view_3561 = torch.ops.aten.view.default(permute_1889, [1, 384, 1536]);  permute_1889 = None
    bmm_280 = torch.ops.aten.bmm.default(view_3560, view_3561);  view_3560 = view_3561 = None
    view_3562 = torch.ops.aten.view.default(bmm_280, [384, 1, 4, 1, 16, 24]);  bmm_280 = None
    permute_1890 = torch.ops.aten.permute.default(view_3562, [2, 3, 4, 0, 5, 1]);  view_3562 = None
    view_3563 = torch.ops.aten.view.default(permute_1890, [4, 1, 16, 384, 24]);  permute_1890 = None
    unbind_int_159 = torch.ops.aten.unbind.int(view_3563);  view_3563 = None
    getitem_3480 = unbind_int_159[0]
    getitem_3481 = unbind_int_159[1]
    getitem_3482 = unbind_int_159[2]
    getitem_3483 = unbind_int_159[3];  unbind_int_159 = None
    view_3564 = torch.ops.aten.view.default(pairformer_stack_blocks_41_attention_pair_bias_attention_query_bias, [1, 16, 1, 24]);  pairformer_stack_blocks_41_attention_pair_bias_attention_query_bias = None
    add_388 = torch.ops.aten.add.Tensor(getitem_3480, view_3564);  getitem_3480 = view_3564 = None
    _to_copy_1993 = torch.ops.aten._to_copy.default(add_388, dtype = torch.bfloat16);  add_388 = None
    expand_236 = torch.ops.aten.expand.default(masked_fill_213, [1, 16, 384, 384]);  masked_fill_213 = None
    _scaled_dot_product_efficient_attention_default_137 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_1993, getitem_3481, getitem_3482, expand_236, False);  _to_copy_1993 = getitem_3481 = getitem_3482 = expand_236 = None
    getitem_3484 = _scaled_dot_product_efficient_attention_default_137[0];  _scaled_dot_product_efficient_attention_default_137 = None
    add_389 = torch.ops.aten.add.Tensor(getitem_3483, 1);  getitem_3483 = None
    sigmoid_287 = torch.ops.aten.sigmoid.default(add_389);  add_389 = None
    mul_475 = torch.ops.aten.mul.Tensor(getitem_3484, sigmoid_287);  getitem_3484 = sigmoid_287 = None
    _to_copy_1994 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_41_attention_pair_bias_attention_output_proj_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_41_attention_pair_bias_attention_output_proj_weight = None
    unsqueeze_1148 = torch.ops.aten.unsqueeze.default(mul_475, 4);  mul_475 = None
    permute_1891 = torch.ops.aten.permute.default(unsqueeze_1148, [0, 2, 4, 3, 1]);  unsqueeze_1148 = None
    unsqueeze_1149 = torch.ops.aten.unsqueeze.default(_to_copy_1994, 3);  _to_copy_1994 = None
    unsqueeze_1150 = torch.ops.aten.unsqueeze.default(unsqueeze_1149, 4);  unsqueeze_1149 = None
    permute_1892 = torch.ops.aten.permute.default(unsqueeze_1150, [3, 4, 2, 1, 0]);  unsqueeze_1150 = None
    permute_1893 = torch.ops.aten.permute.default(permute_1891, [1, 3, 4, 0, 2]);  permute_1891 = None
    clone_297 = torch.ops.aten.clone.default(permute_1893, memory_format = torch.contiguous_format);  permute_1893 = None
    _unsafe_view_248 = torch.ops.aten._unsafe_view.default(clone_297, [1, 384, 384]);  clone_297 = None
    permute_1894 = torch.ops.aten.permute.default(permute_1892, [3, 4, 0, 2, 1]);  permute_1892 = None
    clone_298 = torch.ops.aten.clone.default(permute_1894, memory_format = torch.contiguous_format);  permute_1894 = None
    _unsafe_view_249 = torch.ops.aten._unsafe_view.default(clone_298, [1, 384, 384]);  clone_298 = None
    bmm_281 = torch.ops.aten.bmm.default(_unsafe_view_248, _unsafe_view_249);  _unsafe_view_248 = _unsafe_view_249 = None
    view_3565 = torch.ops.aten.view.default(bmm_281, [384, 1, 1, 1, 384]);  bmm_281 = None
    permute_1895 = torch.ops.aten.permute.default(view_3565, [3, 0, 4, 1, 2]);  view_3565 = None
    view_3566 = torch.ops.aten.view.default(permute_1895, [1, 384, 384]);  permute_1895 = None
    unsqueeze_1151 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
    mul_476 = torch.ops.aten.mul.Tensor(view_3566, unsqueeze_1151);  view_3566 = unsqueeze_1151 = None
    add_390 = torch.ops.aten.add.Tensor(add_383, mul_476);  mul_476 = None
    split_tensor_382 = torch.ops.aten.split.Tensor(add_383, 384, dim = -2);  add_383 = None
    getitem_3488 = split_tensor_382[0];  split_tensor_382 = None
    _to_copy_1995 = torch.ops.aten._to_copy.default(getitem_3488, dtype = torch.float32);  getitem_3488 = None
    native_layer_norm_default_410 = torch.ops.aten.native_layer_norm.default(_to_copy_1995, [384], pairformer_stack_blocks_41_transition_single_layer_norm_weight, pairformer_stack_blocks_41_transition_single_layer_norm_bias, 1e-05);  _to_copy_1995 = pairformer_stack_blocks_41_transition_single_layer_norm_weight = pairformer_stack_blocks_41_transition_single_layer_norm_bias = None
    getitem_3489 = native_layer_norm_default_410[0];  native_layer_norm_default_410 = None
    _to_copy_1996 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_41_transition_single_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_41_transition_single_linear_no_bias_ab_weight = None
    _to_copy_1997 = torch.ops.aten._to_copy.default(getitem_3489, dtype = torch.bfloat16);  getitem_3489 = None
    t_745 = torch.ops.aten.t.default(_to_copy_1996);  _to_copy_1996 = None
    view_3567 = torch.ops.aten.view.default(_to_copy_1997, [384, 384]);  _to_copy_1997 = None
    mm_693 = torch.ops.aten.mm.default(view_3567, t_745);  view_3567 = t_745 = None
    view_3568 = torch.ops.aten.view.default(mm_693, [1, 384, 1536]);  mm_693 = None
    split_tensor_383 = torch.ops.aten.split.Tensor(view_3568, 768, dim = -1);  view_3568 = None
    getitem_3492 = split_tensor_383[0]
    getitem_3493 = split_tensor_383[1];  split_tensor_383 = None
    silu_98 = torch.ops.aten.silu.default(getitem_3492);  getitem_3492 = None
    mul_477 = torch.ops.aten.mul.Tensor(silu_98, getitem_3493);  silu_98 = getitem_3493 = None
    _to_copy_1998 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_41_transition_single_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_41_transition_single_linear_out_weight = None
    t_746 = torch.ops.aten.t.default(_to_copy_1998);  _to_copy_1998 = None
    view_3570 = torch.ops.aten.view.default(mul_477, [384, 768]);  mul_477 = None
    mm_694 = torch.ops.aten.mm.default(view_3570, t_746);  view_3570 = t_746 = None
    view_3571 = torch.ops.aten.view.default(mm_694, [1, 384, 384]);  mm_694 = None
    add_391 = torch.ops.aten.add.Tensor(add_390, view_3571);  add_390 = view_3571 = None
    _to_copy_1999 = torch.ops.aten._to_copy.default(add_387, dtype = torch.float32)
    native_layer_norm_default_411 = torch.ops.aten.native_layer_norm.default(_to_copy_1999, [256], pairformer_stack_blocks_42_triangle_multiplication_layernorm_z_in_weight, pairformer_stack_blocks_42_triangle_multiplication_layernorm_z_in_bias, 1e-05);  _to_copy_1999 = pairformer_stack_blocks_42_triangle_multiplication_layernorm_z_in_weight = pairformer_stack_blocks_42_triangle_multiplication_layernorm_z_in_bias = None
    getitem_3494 = native_layer_norm_default_411[0];  native_layer_norm_default_411 = None
    split_with_sizes_default_96 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_42_triangle_multiplication_merged_linear_p_weight, [512, 512]);  pairformer_stack_blocks_42_triangle_multiplication_merged_linear_p_weight = None
    getitem_3497 = split_with_sizes_default_96[0]
    getitem_3498 = split_with_sizes_default_96[1];  split_with_sizes_default_96 = None
    split_with_sizes_default_97 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_42_triangle_multiplication_merged_linear_g_weight, [512, 512, 256]);  pairformer_stack_blocks_42_triangle_multiplication_merged_linear_g_weight = None
    getitem_3499 = split_with_sizes_default_97[0]
    getitem_3500 = split_with_sizes_default_97[1]
    getitem_3501 = split_with_sizes_default_97[2];  split_with_sizes_default_97 = None
    _to_copy_2000 = torch.ops.aten._to_copy.default(getitem_3497, dtype = torch.bfloat16);  getitem_3497 = None
    _to_copy_2001 = torch.ops.aten._to_copy.default(getitem_3494, dtype = torch.bfloat16)
    t_747 = torch.ops.aten.t.default(_to_copy_2000);  _to_copy_2000 = None
    view_3572 = torch.ops.aten.view.default(_to_copy_2001, [147456, 256]);  _to_copy_2001 = None
    mm_695 = torch.ops.aten.mm.default(view_3572, t_747);  view_3572 = t_747 = None
    view_3573 = torch.ops.aten.view.default(mm_695, [1, 384, 384, 512]);  mm_695 = None
    _to_copy_2002 = torch.ops.aten._to_copy.default(getitem_3499, dtype = torch.bfloat16);  getitem_3499 = None
    _to_copy_2003 = torch.ops.aten._to_copy.default(getitem_3494, dtype = torch.bfloat16)
    t_748 = torch.ops.aten.t.default(_to_copy_2002);  _to_copy_2002 = None
    view_3574 = torch.ops.aten.view.default(_to_copy_2003, [147456, 256]);  _to_copy_2003 = None
    mm_696 = torch.ops.aten.mm.default(view_3574, t_748);  view_3574 = t_748 = None
    view_3575 = torch.ops.aten.view.default(mm_696, [1, 384, 384, 512]);  mm_696 = None
    sigmoid_288 = torch.ops.aten.sigmoid.default(view_3575);  view_3575 = None
    mul_478 = torch.ops.aten.mul.Tensor(view_3573, sigmoid_288);  view_3573 = sigmoid_288 = None
    unsqueeze_1152 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
    bitwise_not_214 = torch.ops.aten.bitwise_not.default(unsqueeze_1152);  unsqueeze_1152 = None
    masked_fill_214 = torch.ops.aten.masked_fill.Scalar(mul_478, bitwise_not_214, 0);  mul_478 = bitwise_not_214 = None
    split_tensor_384 = torch.ops.aten.split.Tensor(masked_fill_214, 256, dim = -1)
    getitem_3504 = split_tensor_384[0];  split_tensor_384 = None
    unsqueeze_1155 = torch.ops.aten.unsqueeze.default(getitem_3504, 4);  getitem_3504 = None
    permute_1900 = torch.ops.aten.permute.default(unsqueeze_1155, [0, 1, 4, 3, 2]);  unsqueeze_1155 = None
    permute_1901 = torch.ops.aten.permute.default(permute_1900, [3, 1, 4, 0, 2]);  permute_1900 = None
    view_3578 = torch.ops.aten.view.default(permute_1901, [256, 384, 384]);  permute_1901 = None
    split_tensor_385 = torch.ops.aten.split.Tensor(masked_fill_214, 256, dim = -1);  masked_fill_214 = None
    getitem_3507 = split_tensor_385[1];  split_tensor_385 = None
    unsqueeze_1156 = torch.ops.aten.unsqueeze.default(getitem_3507, 4);  getitem_3507 = None
    permute_1902 = torch.ops.aten.permute.default(unsqueeze_1156, [0, 4, 1, 3, 2]);  unsqueeze_1156 = None
    permute_1903 = torch.ops.aten.permute.default(permute_1902, [3, 4, 0, 2, 1]);  permute_1902 = None
    view_3579 = torch.ops.aten.view.default(permute_1903, [256, 384, 384]);  permute_1903 = None
    bmm_282 = torch.ops.aten.bmm.default(view_3578, view_3579);  view_3578 = view_3579 = None
    view_3580 = torch.ops.aten.view.default(bmm_282, [256, 384, 1, 1, 384]);  bmm_282 = None
    permute_1904 = torch.ops.aten.permute.default(view_3580, [3, 1, 4, 0, 2]);  view_3580 = None
    view_3581 = torch.ops.aten.view.default(permute_1904, [1, 384, 384, 256]);  permute_1904 = None
    _to_copy_2004 = torch.ops.aten._to_copy.default(getitem_3498, dtype = torch.bfloat16);  getitem_3498 = None
    _to_copy_2005 = torch.ops.aten._to_copy.default(getitem_3494, dtype = torch.bfloat16)
    t_749 = torch.ops.aten.t.default(_to_copy_2004);  _to_copy_2004 = None
    view_3582 = torch.ops.aten.view.default(_to_copy_2005, [147456, 256]);  _to_copy_2005 = None
    mm_697 = torch.ops.aten.mm.default(view_3582, t_749);  view_3582 = t_749 = None
    view_3583 = torch.ops.aten.view.default(mm_697, [1, 384, 384, 512]);  mm_697 = None
    _to_copy_2006 = torch.ops.aten._to_copy.default(getitem_3500, dtype = torch.bfloat16);  getitem_3500 = None
    _to_copy_2007 = torch.ops.aten._to_copy.default(getitem_3494, dtype = torch.bfloat16)
    t_750 = torch.ops.aten.t.default(_to_copy_2006);  _to_copy_2006 = None
    view_3584 = torch.ops.aten.view.default(_to_copy_2007, [147456, 256]);  _to_copy_2007 = None
    mm_698 = torch.ops.aten.mm.default(view_3584, t_750);  view_3584 = t_750 = None
    view_3585 = torch.ops.aten.view.default(mm_698, [1, 384, 384, 512]);  mm_698 = None
    sigmoid_289 = torch.ops.aten.sigmoid.default(view_3585);  view_3585 = None
    mul_479 = torch.ops.aten.mul.Tensor(view_3583, sigmoid_289);  view_3583 = sigmoid_289 = None
    view_3586 = torch.ops.aten.view.default(mul_479, [147456, 512]);  mul_479 = None
    view_3587 = torch.ops.aten.view.default(view_3586, [1, 384, 384, 512]);  view_3586 = None
    transpose_96 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
    unsqueeze_1157 = torch.ops.aten.unsqueeze.default(transpose_96, 3);  transpose_96 = None
    clone_299 = torch.ops.aten.clone.default(unsqueeze_1157, memory_format = torch.contiguous_format);  unsqueeze_1157 = None
    bitwise_not_215 = torch.ops.aten.bitwise_not.default(clone_299);  clone_299 = None
    masked_fill_215 = torch.ops.aten.masked_fill.Scalar(view_3587, bitwise_not_215, 0);  view_3587 = bitwise_not_215 = None
    view_3588 = torch.ops.aten.view.default(masked_fill_215, [147456, 512]);  masked_fill_215 = None
    view_3592 = torch.ops.aten.view.default(view_3588, [1, 384, 384, 512])
    split_tensor_386 = torch.ops.aten.split.Tensor(view_3592, 256, dim = -1);  view_3592 = None
    getitem_3510 = split_tensor_386[0];  split_tensor_386 = None
    unsqueeze_1160 = torch.ops.aten.unsqueeze.default(getitem_3510, 4);  getitem_3510 = None
    permute_1909 = torch.ops.aten.permute.default(unsqueeze_1160, [0, 2, 4, 3, 1]);  unsqueeze_1160 = None
    permute_1910 = torch.ops.aten.permute.default(permute_1909, [3, 1, 4, 0, 2]);  permute_1909 = None
    view_3593 = torch.ops.aten.view.default(permute_1910, [256, 384, 384]);  permute_1910 = None
    view_3594 = torch.ops.aten.view.default(view_3588, [1, 384, 384, 512]);  view_3588 = None
    split_tensor_387 = torch.ops.aten.split.Tensor(view_3594, 256, dim = -1);  view_3594 = None
    getitem_3513 = split_tensor_387[1];  split_tensor_387 = None
    unsqueeze_1161 = torch.ops.aten.unsqueeze.default(getitem_3513, 4);  getitem_3513 = None
    permute_1911 = torch.ops.aten.permute.default(unsqueeze_1161, [0, 4, 2, 3, 1]);  unsqueeze_1161 = None
    permute_1912 = torch.ops.aten.permute.default(permute_1911, [3, 4, 0, 2, 1]);  permute_1911 = None
    view_3595 = torch.ops.aten.view.default(permute_1912, [256, 384, 384]);  permute_1912 = None
    bmm_283 = torch.ops.aten.bmm.default(view_3593, view_3595);  view_3593 = view_3595 = None
    view_3596 = torch.ops.aten.view.default(bmm_283, [256, 384, 1, 1, 384]);  bmm_283 = None
    permute_1913 = torch.ops.aten.permute.default(view_3596, [3, 1, 4, 0, 2]);  view_3596 = None
    view_3597 = torch.ops.aten.view.default(permute_1913, [1, 384, 384, 256]);  permute_1913 = None
    _to_copy_2008 = torch.ops.aten._to_copy.default(view_3581, dtype = torch.float32);  view_3581 = None
    native_layer_norm_default_412 = torch.ops.aten.native_layer_norm.default(_to_copy_2008, [256], None, None, 1e-05);  _to_copy_2008 = None
    getitem_3514 = native_layer_norm_default_412[0];  native_layer_norm_default_412 = None
    _to_copy_2009 = torch.ops.aten._to_copy.default(view_3597, dtype = torch.float32);  view_3597 = None
    native_layer_norm_default_413 = torch.ops.aten.native_layer_norm.default(_to_copy_2009, [256], None, None, 1e-05);  _to_copy_2009 = None
    getitem_3517 = native_layer_norm_default_413[0];  native_layer_norm_default_413 = None
    add_392 = torch.ops.aten.add.Tensor(getitem_3514, getitem_3517);  getitem_3514 = getitem_3517 = None
    _to_copy_2010 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_42_triangle_multiplication_linear_z_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_42_triangle_multiplication_linear_z_out_weight = None
    _to_copy_2011 = torch.ops.aten._to_copy.default(add_392, dtype = torch.bfloat16);  add_392 = None
    t_751 = torch.ops.aten.t.default(_to_copy_2010);  _to_copy_2010 = None
    view_3598 = torch.ops.aten.view.default(_to_copy_2011, [147456, 256]);  _to_copy_2011 = None
    mm_699 = torch.ops.aten.mm.default(view_3598, t_751);  view_3598 = t_751 = None
    view_3599 = torch.ops.aten.view.default(mm_699, [1, 384, 384, 256]);  mm_699 = None
    _to_copy_2012 = torch.ops.aten._to_copy.default(getitem_3501, dtype = torch.bfloat16);  getitem_3501 = None
    _to_copy_2013 = torch.ops.aten._to_copy.default(getitem_3494, dtype = torch.bfloat16);  getitem_3494 = None
    t_752 = torch.ops.aten.t.default(_to_copy_2012);  _to_copy_2012 = None
    view_3600 = torch.ops.aten.view.default(_to_copy_2013, [147456, 256]);  _to_copy_2013 = None
    mm_700 = torch.ops.aten.mm.default(view_3600, t_752);  view_3600 = t_752 = None
    view_3601 = torch.ops.aten.view.default(mm_700, [1, 384, 384, 256]);  mm_700 = None
    sigmoid_290 = torch.ops.aten.sigmoid.default(view_3601);  view_3601 = None
    mul_480 = torch.ops.aten.mul.Tensor(view_3599, sigmoid_290);  view_3599 = sigmoid_290 = None
    add_393 = torch.ops.aten.add.Tensor(add_387, mul_480);  mul_480 = None
    _to_copy_2014 = torch.ops.aten._to_copy.default(add_387, dtype = torch.float32)
    native_layer_norm_default_414 = torch.ops.aten.native_layer_norm.default(_to_copy_2014, [256], None, None, 1e-05);  _to_copy_2014 = None
    getitem_3520 = native_layer_norm_default_414[0];  native_layer_norm_default_414 = None
    _to_copy_2015 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_42_triangle_attention_pair2b_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_42_triangle_attention_pair2b_weight = None
    _to_copy_2016 = torch.ops.aten._to_copy.default(getitem_3520, dtype = torch.bfloat16)
    t_753 = torch.ops.aten.t.default(_to_copy_2015);  _to_copy_2015 = None
    view_3602 = torch.ops.aten.view.default(_to_copy_2016, [147456, 256]);  _to_copy_2016 = None
    mm_701 = torch.ops.aten.mm.default(view_3602, t_753);  view_3602 = t_753 = None
    view_3603 = torch.ops.aten.view.default(mm_701, [1, 384, 384, 8]);  mm_701 = None
    view_3604 = torch.ops.aten.view.default(view_3603, [1, 384, 384, 2, 4]);  view_3603 = None
    permute_1914 = torch.ops.aten.permute.default(view_3604, [0, 3, 4, 1, 2]);  view_3604 = None
    view_3605 = torch.ops.aten.view.default(permute_1914, [1, 2, 4, 1, 384, 384]);  permute_1914 = None
    view_3606 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 384, 384])
    bitwise_not_216 = torch.ops.aten.bitwise_not.default(view_3606);  view_3606 = None
    masked_fill_216 = torch.ops.aten.masked_fill.Scalar(view_3605, bitwise_not_216, -10000);  view_3605 = bitwise_not_216 = None
    view_3607 = torch.ops.aten.view.default(masked_fill_216, [1, 2, 4, 384, 384]);  masked_fill_216 = None
    permute_1915 = torch.ops.aten.permute.default(view_3607, [1, 0, 2, 3, 4]);  view_3607 = None
    view_3608 = torch.ops.aten.view.default(permute_1915, [2, 4, 1, 384, 384]);  permute_1915 = None
    _to_copy_2017 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_42_triangle_attention_pair2qkvg1_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_42_triangle_attention_pair2qkvg1_weight = None
    _to_copy_2018 = torch.ops.aten._to_copy.default(getitem_3520, dtype = torch.bfloat16)
    t_754 = torch.ops.aten.t.default(_to_copy_2017);  _to_copy_2017 = None
    view_3609 = torch.ops.aten.view.default(_to_copy_2018, [147456, 256]);  _to_copy_2018 = None
    mm_702 = torch.ops.aten.mm.default(view_3609, t_754);  view_3609 = t_754 = None
    view_3610 = torch.ops.aten.view.default(mm_702, [1, 384, 384, 1024]);  mm_702 = None
    select_97 = torch.ops.aten.select.int(view_3608, 0, 0)
    view_3611 = torch.ops.aten.view.default(view_3610, [1, 384, 384, 4, 4, 64]);  view_3610 = None
    permute_1916 = torch.ops.aten.permute.default(view_3611, [4, 0, 3, 1, 2, 5]);  view_3611 = None
    view_3612 = torch.ops.aten.view.default(permute_1916, [4, 4, 384, 384, 64]);  permute_1916 = None
    unbind_int_160 = torch.ops.aten.unbind.int(view_3612);  view_3612 = None
    getitem_3523 = unbind_int_160[0]
    getitem_3524 = unbind_int_160[1]
    getitem_3525 = unbind_int_160[2]
    getitem_3526 = unbind_int_160[3];  unbind_int_160 = None
    expand_237 = torch.ops.aten.expand.default(select_97, [4, 384, 384, 384]);  select_97 = None
    _scaled_dot_product_efficient_attention_default_138 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3523, getitem_3524, getitem_3525, expand_237, False);  getitem_3523 = getitem_3524 = getitem_3525 = expand_237 = None
    getitem_3527 = _scaled_dot_product_efficient_attention_default_138[0];  _scaled_dot_product_efficient_attention_default_138 = None
    sigmoid_291 = torch.ops.aten.sigmoid.default(getitem_3526);  getitem_3526 = None
    mul_481 = torch.ops.aten.mul.Tensor(getitem_3527, sigmoid_291);  getitem_3527 = sigmoid_291 = None
    view_3613 = torch.ops.aten.view.default(mul_481, [1, 4, 384, 384, 64]);  mul_481 = None
    permute_1917 = torch.ops.aten.permute.default(view_3613, [0, 2, 3, 1, 4]);  view_3613 = None
    clone_300 = torch.ops.aten.clone.default(permute_1917, memory_format = torch.contiguous_format);  permute_1917 = None
    _unsafe_view_250 = torch.ops.aten._unsafe_view.default(clone_300, [1, 384, 384, 256]);  clone_300 = None
    transpose_97 = torch.ops.aten.transpose.int(getitem_3520, 1, 2);  getitem_3520 = None
    _to_copy_2019 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_42_triangle_attention_pair2qkvg2_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_42_triangle_attention_pair2qkvg2_weight = None
    _to_copy_2020 = torch.ops.aten._to_copy.default(transpose_97, dtype = torch.bfloat16);  transpose_97 = None
    t_755 = torch.ops.aten.t.default(_to_copy_2019);  _to_copy_2019 = None
    expand_238 = torch.ops.aten.expand.default(_to_copy_2020, [1, 384, 384, 256]);  _to_copy_2020 = None
    view_3614 = torch.ops.aten.view.default(expand_238, [384, 384, 256]);  expand_238 = None
    expand_239 = torch.ops.aten.expand.default(t_755, [1, 384, 256, 1024]);  t_755 = None
    view_3615 = torch.ops.aten.view.default(expand_239, [384, 256, 1024]);  expand_239 = None
    bmm_284 = torch.ops.aten.bmm.default(view_3614, view_3615);  view_3614 = view_3615 = None
    view_3616 = torch.ops.aten.view.default(bmm_284, [1, 384, 384, 1024]);  bmm_284 = None
    select_98 = torch.ops.aten.select.int(view_3608, 0, 1);  view_3608 = None
    view_3617 = torch.ops.aten.view.default(view_3616, [1, 384, 384, 4, 4, 64]);  view_3616 = None
    permute_1918 = torch.ops.aten.permute.default(view_3617, [4, 0, 3, 1, 2, 5]);  view_3617 = None
    view_3618 = torch.ops.aten.view.default(permute_1918, [4, 4, 384, 384, 64]);  permute_1918 = None
    unbind_int_161 = torch.ops.aten.unbind.int(view_3618);  view_3618 = None
    getitem_3531 = unbind_int_161[0]
    getitem_3532 = unbind_int_161[1]
    getitem_3533 = unbind_int_161[2]
    getitem_3534 = unbind_int_161[3];  unbind_int_161 = None
    expand_240 = torch.ops.aten.expand.default(select_98, [4, 384, 384, 384]);  select_98 = None
    _scaled_dot_product_efficient_attention_default_139 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3531, getitem_3532, getitem_3533, expand_240, False);  getitem_3531 = getitem_3532 = getitem_3533 = expand_240 = None
    getitem_3535 = _scaled_dot_product_efficient_attention_default_139[0];  _scaled_dot_product_efficient_attention_default_139 = None
    sigmoid_292 = torch.ops.aten.sigmoid.default(getitem_3534);  getitem_3534 = None
    mul_482 = torch.ops.aten.mul.Tensor(getitem_3535, sigmoid_292);  getitem_3535 = sigmoid_292 = None
    view_3619 = torch.ops.aten.view.default(mul_482, [1, 4, 384, 384, 64]);  mul_482 = None
    permute_1919 = torch.ops.aten.permute.default(view_3619, [0, 2, 3, 1, 4]);  view_3619 = None
    clone_301 = torch.ops.aten.clone.default(permute_1919, memory_format = torch.contiguous_format);  permute_1919 = None
    _unsafe_view_251 = torch.ops.aten._unsafe_view.default(clone_301, [1, 384, 384, 256]);  clone_301 = None
    cat_54 = torch.ops.aten.cat.default([_unsafe_view_250, _unsafe_view_251], dim = -1);  _unsafe_view_250 = _unsafe_view_251 = None
    slice_205 = torch.ops.aten.slice.Tensor(pairformer_stack_blocks_42_triangle_attention_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  pairformer_stack_blocks_42_triangle_attention_out_scalers = None
    unsqueeze_1162 = torch.ops.aten.unsqueeze.default(slice_205, 1);  slice_205 = None
    mul_483 = torch.ops.aten.mul.Tensor(pairformer_stack_blocks_42_triangle_attention_linear_out_weight, unsqueeze_1162);  pairformer_stack_blocks_42_triangle_attention_linear_out_weight = unsqueeze_1162 = None
    _to_copy_2021 = torch.ops.aten._to_copy.default(mul_483, dtype = torch.bfloat16);  mul_483 = None
    t_756 = torch.ops.aten.t.default(_to_copy_2021);  _to_copy_2021 = None
    view_3620 = torch.ops.aten.view.default(cat_54, [147456, 512]);  cat_54 = None
    mm_703 = torch.ops.aten.mm.default(view_3620, t_756);  view_3620 = t_756 = None
    view_3621 = torch.ops.aten.view.default(mm_703, [1, 384, 384, 256]);  mm_703 = None
    add_394 = torch.ops.aten.add.Tensor(add_393, view_3621);  add_393 = view_3621 = None
    split_tensor_388 = torch.ops.aten.split.Tensor(add_387, 384, dim = -2)
    getitem_3539 = split_tensor_388[0];  split_tensor_388 = None
    _to_copy_2022 = torch.ops.aten._to_copy.default(getitem_3539, dtype = torch.float32);  getitem_3539 = None
    native_layer_norm_default_415 = torch.ops.aten.native_layer_norm.default(_to_copy_2022, [256], pairformer_stack_blocks_42_transition_pair_layer_norm_weight, pairformer_stack_blocks_42_transition_pair_layer_norm_bias, 1e-05);  _to_copy_2022 = pairformer_stack_blocks_42_transition_pair_layer_norm_weight = pairformer_stack_blocks_42_transition_pair_layer_norm_bias = None
    getitem_3540 = native_layer_norm_default_415[0];  native_layer_norm_default_415 = None
    _to_copy_2023 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_42_transition_pair_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_42_transition_pair_linear_no_bias_ab_weight = None
    _to_copy_2024 = torch.ops.aten._to_copy.default(getitem_3540, dtype = torch.bfloat16);  getitem_3540 = None
    t_757 = torch.ops.aten.t.default(_to_copy_2023);  _to_copy_2023 = None
    view_3622 = torch.ops.aten.view.default(_to_copy_2024, [147456, 256]);  _to_copy_2024 = None
    mm_704 = torch.ops.aten.mm.default(view_3622, t_757);  view_3622 = t_757 = None
    view_3623 = torch.ops.aten.view.default(mm_704, [1, 384, 384, 1024]);  mm_704 = None
    split_tensor_389 = torch.ops.aten.split.Tensor(view_3623, 512, dim = -1);  view_3623 = None
    getitem_3543 = split_tensor_389[0]
    getitem_3544 = split_tensor_389[1];  split_tensor_389 = None
    silu_99 = torch.ops.aten.silu.default(getitem_3543);  getitem_3543 = None
    mul_484 = torch.ops.aten.mul.Tensor(silu_99, getitem_3544);  silu_99 = getitem_3544 = None
    _to_copy_2025 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_42_transition_pair_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_42_transition_pair_linear_out_weight = None
    t_758 = torch.ops.aten.t.default(_to_copy_2025);  _to_copy_2025 = None
    view_3625 = torch.ops.aten.view.default(mul_484, [147456, 512]);  mul_484 = None
    mm_705 = torch.ops.aten.mm.default(view_3625, t_758);  view_3625 = t_758 = None
    view_3626 = torch.ops.aten.view.default(mm_705, [1, 384, 384, 256]);  mm_705 = None
    add_395 = torch.ops.aten.add.Tensor(add_394, view_3626);  add_394 = view_3626 = None
    _to_copy_2026 = torch.ops.aten._to_copy.default(add_391, dtype = torch.float32)
    native_layer_norm_default_416 = torch.ops.aten.native_layer_norm.default(_to_copy_2026, [384], pairformer_stack_blocks_42_attention_pair_bias_single_layer_norm_weight, pairformer_stack_blocks_42_attention_pair_bias_single_layer_norm_bias, 1e-05);  _to_copy_2026 = pairformer_stack_blocks_42_attention_pair_bias_single_layer_norm_weight = pairformer_stack_blocks_42_attention_pair_bias_single_layer_norm_bias = None
    getitem_3545 = native_layer_norm_default_416[0];  native_layer_norm_default_416 = None
    _to_copy_2027 = torch.ops.aten._to_copy.default(add_387, dtype = torch.float32);  add_387 = None
    native_layer_norm_default_417 = torch.ops.aten.native_layer_norm.default(_to_copy_2027, [256], pairformer_stack_blocks_42_attention_pair_bias_pair_layer_norm_weight, pairformer_stack_blocks_42_attention_pair_bias_pair_layer_norm_bias, 1e-05);  _to_copy_2027 = pairformer_stack_blocks_42_attention_pair_bias_pair_layer_norm_weight = pairformer_stack_blocks_42_attention_pair_bias_pair_layer_norm_bias = None
    getitem_3548 = native_layer_norm_default_417[0];  native_layer_norm_default_417 = None
    _to_copy_2028 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_42_attention_pair_bias_pair_linear_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_42_attention_pair_bias_pair_linear_weight = None
    _to_copy_2029 = torch.ops.aten._to_copy.default(getitem_3548, dtype = torch.bfloat16);  getitem_3548 = None
    t_759 = torch.ops.aten.t.default(_to_copy_2028);  _to_copy_2028 = None
    view_3627 = torch.ops.aten.view.default(_to_copy_2029, [147456, 256]);  _to_copy_2029 = None
    mm_706 = torch.ops.aten.mm.default(view_3627, t_759);  view_3627 = t_759 = None
    view_3628 = torch.ops.aten.view.default(mm_706, [1, 384, 384, 16]);  mm_706 = None
    permute_1920 = torch.ops.aten.permute.default(view_3628, [0, 3, 1, 2]);  view_3628 = None
    view_3629 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384])
    bitwise_not_217 = torch.ops.aten.bitwise_not.default(view_3629);  view_3629 = None
    masked_fill_217 = torch.ops.aten.masked_fill.Scalar(permute_1920, bitwise_not_217, -10000);  permute_1920 = bitwise_not_217 = None
    _to_copy_2030 = torch.ops.aten._to_copy.default(getitem_3545, dtype = torch.bfloat16);  getitem_3545 = None
    _to_copy_2031 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_42_attention_pair_bias_attention_input2qkvg_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_42_attention_pair_bias_attention_input2qkvg_weight = None
    unsqueeze_1163 = torch.ops.aten.unsqueeze.default(_to_copy_2030, 3);  _to_copy_2030 = None
    unsqueeze_1164 = torch.ops.aten.unsqueeze.default(unsqueeze_1163, 4);  unsqueeze_1163 = None
    unsqueeze_1165 = torch.ops.aten.unsqueeze.default(unsqueeze_1164, 5);  unsqueeze_1164 = None
    permute_1921 = torch.ops.aten.permute.default(unsqueeze_1165, [3, 0, 4, 1, 5, 2]);  unsqueeze_1165 = None
    unsqueeze_1166 = torch.ops.aten.unsqueeze.default(_to_copy_2031, 4);  _to_copy_2031 = None
    unsqueeze_1167 = torch.ops.aten.unsqueeze.default(unsqueeze_1166, 5);  unsqueeze_1166 = None
    permute_1922 = torch.ops.aten.permute.default(unsqueeze_1167, [1, 4, 2, 5, 3, 0]);  unsqueeze_1167 = None
    permute_1923 = torch.ops.aten.permute.default(permute_1921, [3, 5, 0, 1, 2, 4]);  permute_1921 = None
    view_3630 = torch.ops.aten.view.default(permute_1923, [1, 384, 384]);  permute_1923 = None
    permute_1924 = torch.ops.aten.permute.default(permute_1922, [5, 0, 1, 2, 4, 3]);  permute_1922 = None
    view_3631 = torch.ops.aten.view.default(permute_1924, [1, 384, 1536]);  permute_1924 = None
    bmm_285 = torch.ops.aten.bmm.default(view_3630, view_3631);  view_3630 = view_3631 = None
    view_3632 = torch.ops.aten.view.default(bmm_285, [384, 1, 4, 1, 16, 24]);  bmm_285 = None
    permute_1925 = torch.ops.aten.permute.default(view_3632, [2, 3, 4, 0, 5, 1]);  view_3632 = None
    view_3633 = torch.ops.aten.view.default(permute_1925, [4, 1, 16, 384, 24]);  permute_1925 = None
    unbind_int_162 = torch.ops.aten.unbind.int(view_3633);  view_3633 = None
    getitem_3551 = unbind_int_162[0]
    getitem_3552 = unbind_int_162[1]
    getitem_3553 = unbind_int_162[2]
    getitem_3554 = unbind_int_162[3];  unbind_int_162 = None
    view_3634 = torch.ops.aten.view.default(pairformer_stack_blocks_42_attention_pair_bias_attention_query_bias, [1, 16, 1, 24]);  pairformer_stack_blocks_42_attention_pair_bias_attention_query_bias = None
    add_396 = torch.ops.aten.add.Tensor(getitem_3551, view_3634);  getitem_3551 = view_3634 = None
    _to_copy_2032 = torch.ops.aten._to_copy.default(add_396, dtype = torch.bfloat16);  add_396 = None
    expand_241 = torch.ops.aten.expand.default(masked_fill_217, [1, 16, 384, 384]);  masked_fill_217 = None
    _scaled_dot_product_efficient_attention_default_140 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_2032, getitem_3552, getitem_3553, expand_241, False);  _to_copy_2032 = getitem_3552 = getitem_3553 = expand_241 = None
    getitem_3555 = _scaled_dot_product_efficient_attention_default_140[0];  _scaled_dot_product_efficient_attention_default_140 = None
    add_397 = torch.ops.aten.add.Tensor(getitem_3554, 1);  getitem_3554 = None
    sigmoid_293 = torch.ops.aten.sigmoid.default(add_397);  add_397 = None
    mul_485 = torch.ops.aten.mul.Tensor(getitem_3555, sigmoid_293);  getitem_3555 = sigmoid_293 = None
    _to_copy_2033 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_42_attention_pair_bias_attention_output_proj_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_42_attention_pair_bias_attention_output_proj_weight = None
    unsqueeze_1168 = torch.ops.aten.unsqueeze.default(mul_485, 4);  mul_485 = None
    permute_1926 = torch.ops.aten.permute.default(unsqueeze_1168, [0, 2, 4, 3, 1]);  unsqueeze_1168 = None
    unsqueeze_1169 = torch.ops.aten.unsqueeze.default(_to_copy_2033, 3);  _to_copy_2033 = None
    unsqueeze_1170 = torch.ops.aten.unsqueeze.default(unsqueeze_1169, 4);  unsqueeze_1169 = None
    permute_1927 = torch.ops.aten.permute.default(unsqueeze_1170, [3, 4, 2, 1, 0]);  unsqueeze_1170 = None
    permute_1928 = torch.ops.aten.permute.default(permute_1926, [1, 3, 4, 0, 2]);  permute_1926 = None
    clone_302 = torch.ops.aten.clone.default(permute_1928, memory_format = torch.contiguous_format);  permute_1928 = None
    _unsafe_view_252 = torch.ops.aten._unsafe_view.default(clone_302, [1, 384, 384]);  clone_302 = None
    permute_1929 = torch.ops.aten.permute.default(permute_1927, [3, 4, 0, 2, 1]);  permute_1927 = None
    clone_303 = torch.ops.aten.clone.default(permute_1929, memory_format = torch.contiguous_format);  permute_1929 = None
    _unsafe_view_253 = torch.ops.aten._unsafe_view.default(clone_303, [1, 384, 384]);  clone_303 = None
    bmm_286 = torch.ops.aten.bmm.default(_unsafe_view_252, _unsafe_view_253);  _unsafe_view_252 = _unsafe_view_253 = None
    view_3635 = torch.ops.aten.view.default(bmm_286, [384, 1, 1, 1, 384]);  bmm_286 = None
    permute_1930 = torch.ops.aten.permute.default(view_3635, [3, 0, 4, 1, 2]);  view_3635 = None
    view_3636 = torch.ops.aten.view.default(permute_1930, [1, 384, 384]);  permute_1930 = None
    unsqueeze_1171 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
    mul_486 = torch.ops.aten.mul.Tensor(view_3636, unsqueeze_1171);  view_3636 = unsqueeze_1171 = None
    add_398 = torch.ops.aten.add.Tensor(add_391, mul_486);  mul_486 = None
    split_tensor_390 = torch.ops.aten.split.Tensor(add_391, 384, dim = -2);  add_391 = None
    getitem_3559 = split_tensor_390[0];  split_tensor_390 = None
    _to_copy_2034 = torch.ops.aten._to_copy.default(getitem_3559, dtype = torch.float32);  getitem_3559 = None
    native_layer_norm_default_418 = torch.ops.aten.native_layer_norm.default(_to_copy_2034, [384], pairformer_stack_blocks_42_transition_single_layer_norm_weight, pairformer_stack_blocks_42_transition_single_layer_norm_bias, 1e-05);  _to_copy_2034 = pairformer_stack_blocks_42_transition_single_layer_norm_weight = pairformer_stack_blocks_42_transition_single_layer_norm_bias = None
    getitem_3560 = native_layer_norm_default_418[0];  native_layer_norm_default_418 = None
    _to_copy_2035 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_42_transition_single_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_42_transition_single_linear_no_bias_ab_weight = None
    _to_copy_2036 = torch.ops.aten._to_copy.default(getitem_3560, dtype = torch.bfloat16);  getitem_3560 = None
    t_760 = torch.ops.aten.t.default(_to_copy_2035);  _to_copy_2035 = None
    view_3637 = torch.ops.aten.view.default(_to_copy_2036, [384, 384]);  _to_copy_2036 = None
    mm_707 = torch.ops.aten.mm.default(view_3637, t_760);  view_3637 = t_760 = None
    view_3638 = torch.ops.aten.view.default(mm_707, [1, 384, 1536]);  mm_707 = None
    split_tensor_391 = torch.ops.aten.split.Tensor(view_3638, 768, dim = -1);  view_3638 = None
    getitem_3563 = split_tensor_391[0]
    getitem_3564 = split_tensor_391[1];  split_tensor_391 = None
    silu_100 = torch.ops.aten.silu.default(getitem_3563);  getitem_3563 = None
    mul_487 = torch.ops.aten.mul.Tensor(silu_100, getitem_3564);  silu_100 = getitem_3564 = None
    _to_copy_2037 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_42_transition_single_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_42_transition_single_linear_out_weight = None
    t_761 = torch.ops.aten.t.default(_to_copy_2037);  _to_copy_2037 = None
    view_3640 = torch.ops.aten.view.default(mul_487, [384, 768]);  mul_487 = None
    mm_708 = torch.ops.aten.mm.default(view_3640, t_761);  view_3640 = t_761 = None
    view_3641 = torch.ops.aten.view.default(mm_708, [1, 384, 384]);  mm_708 = None
    add_399 = torch.ops.aten.add.Tensor(add_398, view_3641);  add_398 = view_3641 = None
    _to_copy_2038 = torch.ops.aten._to_copy.default(add_395, dtype = torch.float32)
    native_layer_norm_default_419 = torch.ops.aten.native_layer_norm.default(_to_copy_2038, [256], pairformer_stack_blocks_43_triangle_multiplication_layernorm_z_in_weight, pairformer_stack_blocks_43_triangle_multiplication_layernorm_z_in_bias, 1e-05);  _to_copy_2038 = pairformer_stack_blocks_43_triangle_multiplication_layernorm_z_in_weight = pairformer_stack_blocks_43_triangle_multiplication_layernorm_z_in_bias = None
    getitem_3565 = native_layer_norm_default_419[0];  native_layer_norm_default_419 = None
    split_with_sizes_default_98 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_43_triangle_multiplication_merged_linear_p_weight, [512, 512]);  pairformer_stack_blocks_43_triangle_multiplication_merged_linear_p_weight = None
    getitem_3568 = split_with_sizes_default_98[0]
    getitem_3569 = split_with_sizes_default_98[1];  split_with_sizes_default_98 = None
    split_with_sizes_default_99 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_43_triangle_multiplication_merged_linear_g_weight, [512, 512, 256]);  pairformer_stack_blocks_43_triangle_multiplication_merged_linear_g_weight = None
    getitem_3570 = split_with_sizes_default_99[0]
    getitem_3571 = split_with_sizes_default_99[1]
    getitem_3572 = split_with_sizes_default_99[2];  split_with_sizes_default_99 = None
    _to_copy_2039 = torch.ops.aten._to_copy.default(getitem_3568, dtype = torch.bfloat16);  getitem_3568 = None
    _to_copy_2040 = torch.ops.aten._to_copy.default(getitem_3565, dtype = torch.bfloat16)
    t_762 = torch.ops.aten.t.default(_to_copy_2039);  _to_copy_2039 = None
    view_3642 = torch.ops.aten.view.default(_to_copy_2040, [147456, 256]);  _to_copy_2040 = None
    mm_709 = torch.ops.aten.mm.default(view_3642, t_762);  view_3642 = t_762 = None
    view_3643 = torch.ops.aten.view.default(mm_709, [1, 384, 384, 512]);  mm_709 = None
    _to_copy_2041 = torch.ops.aten._to_copy.default(getitem_3570, dtype = torch.bfloat16);  getitem_3570 = None
    _to_copy_2042 = torch.ops.aten._to_copy.default(getitem_3565, dtype = torch.bfloat16)
    t_763 = torch.ops.aten.t.default(_to_copy_2041);  _to_copy_2041 = None
    view_3644 = torch.ops.aten.view.default(_to_copy_2042, [147456, 256]);  _to_copy_2042 = None
    mm_710 = torch.ops.aten.mm.default(view_3644, t_763);  view_3644 = t_763 = None
    view_3645 = torch.ops.aten.view.default(mm_710, [1, 384, 384, 512]);  mm_710 = None
    sigmoid_294 = torch.ops.aten.sigmoid.default(view_3645);  view_3645 = None
    mul_488 = torch.ops.aten.mul.Tensor(view_3643, sigmoid_294);  view_3643 = sigmoid_294 = None
    unsqueeze_1172 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
    bitwise_not_218 = torch.ops.aten.bitwise_not.default(unsqueeze_1172);  unsqueeze_1172 = None
    masked_fill_218 = torch.ops.aten.masked_fill.Scalar(mul_488, bitwise_not_218, 0);  mul_488 = bitwise_not_218 = None
    split_tensor_392 = torch.ops.aten.split.Tensor(masked_fill_218, 256, dim = -1)
    getitem_3575 = split_tensor_392[0];  split_tensor_392 = None
    unsqueeze_1175 = torch.ops.aten.unsqueeze.default(getitem_3575, 4);  getitem_3575 = None
    permute_1935 = torch.ops.aten.permute.default(unsqueeze_1175, [0, 1, 4, 3, 2]);  unsqueeze_1175 = None
    permute_1936 = torch.ops.aten.permute.default(permute_1935, [3, 1, 4, 0, 2]);  permute_1935 = None
    view_3648 = torch.ops.aten.view.default(permute_1936, [256, 384, 384]);  permute_1936 = None
    split_tensor_393 = torch.ops.aten.split.Tensor(masked_fill_218, 256, dim = -1);  masked_fill_218 = None
    getitem_3578 = split_tensor_393[1];  split_tensor_393 = None
    unsqueeze_1176 = torch.ops.aten.unsqueeze.default(getitem_3578, 4);  getitem_3578 = None
    permute_1937 = torch.ops.aten.permute.default(unsqueeze_1176, [0, 4, 1, 3, 2]);  unsqueeze_1176 = None
    permute_1938 = torch.ops.aten.permute.default(permute_1937, [3, 4, 0, 2, 1]);  permute_1937 = None
    view_3649 = torch.ops.aten.view.default(permute_1938, [256, 384, 384]);  permute_1938 = None
    bmm_287 = torch.ops.aten.bmm.default(view_3648, view_3649);  view_3648 = view_3649 = None
    view_3650 = torch.ops.aten.view.default(bmm_287, [256, 384, 1, 1, 384]);  bmm_287 = None
    permute_1939 = torch.ops.aten.permute.default(view_3650, [3, 1, 4, 0, 2]);  view_3650 = None
    view_3651 = torch.ops.aten.view.default(permute_1939, [1, 384, 384, 256]);  permute_1939 = None
    _to_copy_2043 = torch.ops.aten._to_copy.default(getitem_3569, dtype = torch.bfloat16);  getitem_3569 = None
    _to_copy_2044 = torch.ops.aten._to_copy.default(getitem_3565, dtype = torch.bfloat16)
    t_764 = torch.ops.aten.t.default(_to_copy_2043);  _to_copy_2043 = None
    view_3652 = torch.ops.aten.view.default(_to_copy_2044, [147456, 256]);  _to_copy_2044 = None
    mm_711 = torch.ops.aten.mm.default(view_3652, t_764);  view_3652 = t_764 = None
    view_3653 = torch.ops.aten.view.default(mm_711, [1, 384, 384, 512]);  mm_711 = None
    _to_copy_2045 = torch.ops.aten._to_copy.default(getitem_3571, dtype = torch.bfloat16);  getitem_3571 = None
    _to_copy_2046 = torch.ops.aten._to_copy.default(getitem_3565, dtype = torch.bfloat16)
    t_765 = torch.ops.aten.t.default(_to_copy_2045);  _to_copy_2045 = None
    view_3654 = torch.ops.aten.view.default(_to_copy_2046, [147456, 256]);  _to_copy_2046 = None
    mm_712 = torch.ops.aten.mm.default(view_3654, t_765);  view_3654 = t_765 = None
    view_3655 = torch.ops.aten.view.default(mm_712, [1, 384, 384, 512]);  mm_712 = None
    sigmoid_295 = torch.ops.aten.sigmoid.default(view_3655);  view_3655 = None
    mul_489 = torch.ops.aten.mul.Tensor(view_3653, sigmoid_295);  view_3653 = sigmoid_295 = None
    view_3656 = torch.ops.aten.view.default(mul_489, [147456, 512]);  mul_489 = None
    view_3657 = torch.ops.aten.view.default(view_3656, [1, 384, 384, 512]);  view_3656 = None
    transpose_98 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
    unsqueeze_1177 = torch.ops.aten.unsqueeze.default(transpose_98, 3);  transpose_98 = None
    clone_304 = torch.ops.aten.clone.default(unsqueeze_1177, memory_format = torch.contiguous_format);  unsqueeze_1177 = None
    bitwise_not_219 = torch.ops.aten.bitwise_not.default(clone_304);  clone_304 = None
    masked_fill_219 = torch.ops.aten.masked_fill.Scalar(view_3657, bitwise_not_219, 0);  view_3657 = bitwise_not_219 = None
    view_3658 = torch.ops.aten.view.default(masked_fill_219, [147456, 512]);  masked_fill_219 = None
    view_3662 = torch.ops.aten.view.default(view_3658, [1, 384, 384, 512])
    split_tensor_394 = torch.ops.aten.split.Tensor(view_3662, 256, dim = -1);  view_3662 = None
    getitem_3581 = split_tensor_394[0];  split_tensor_394 = None
    unsqueeze_1180 = torch.ops.aten.unsqueeze.default(getitem_3581, 4);  getitem_3581 = None
    permute_1944 = torch.ops.aten.permute.default(unsqueeze_1180, [0, 2, 4, 3, 1]);  unsqueeze_1180 = None
    permute_1945 = torch.ops.aten.permute.default(permute_1944, [3, 1, 4, 0, 2]);  permute_1944 = None
    view_3663 = torch.ops.aten.view.default(permute_1945, [256, 384, 384]);  permute_1945 = None
    view_3664 = torch.ops.aten.view.default(view_3658, [1, 384, 384, 512]);  view_3658 = None
    split_tensor_395 = torch.ops.aten.split.Tensor(view_3664, 256, dim = -1);  view_3664 = None
    getitem_3584 = split_tensor_395[1];  split_tensor_395 = None
    unsqueeze_1181 = torch.ops.aten.unsqueeze.default(getitem_3584, 4);  getitem_3584 = None
    permute_1946 = torch.ops.aten.permute.default(unsqueeze_1181, [0, 4, 2, 3, 1]);  unsqueeze_1181 = None
    permute_1947 = torch.ops.aten.permute.default(permute_1946, [3, 4, 0, 2, 1]);  permute_1946 = None
    view_3665 = torch.ops.aten.view.default(permute_1947, [256, 384, 384]);  permute_1947 = None
    bmm_288 = torch.ops.aten.bmm.default(view_3663, view_3665);  view_3663 = view_3665 = None
    view_3666 = torch.ops.aten.view.default(bmm_288, [256, 384, 1, 1, 384]);  bmm_288 = None
    permute_1948 = torch.ops.aten.permute.default(view_3666, [3, 1, 4, 0, 2]);  view_3666 = None
    view_3667 = torch.ops.aten.view.default(permute_1948, [1, 384, 384, 256]);  permute_1948 = None
    _to_copy_2047 = torch.ops.aten._to_copy.default(view_3651, dtype = torch.float32);  view_3651 = None
    native_layer_norm_default_420 = torch.ops.aten.native_layer_norm.default(_to_copy_2047, [256], None, None, 1e-05);  _to_copy_2047 = None
    getitem_3585 = native_layer_norm_default_420[0];  native_layer_norm_default_420 = None
    _to_copy_2048 = torch.ops.aten._to_copy.default(view_3667, dtype = torch.float32);  view_3667 = None
    native_layer_norm_default_421 = torch.ops.aten.native_layer_norm.default(_to_copy_2048, [256], None, None, 1e-05);  _to_copy_2048 = None
    getitem_3588 = native_layer_norm_default_421[0];  native_layer_norm_default_421 = None
    add_400 = torch.ops.aten.add.Tensor(getitem_3585, getitem_3588);  getitem_3585 = getitem_3588 = None
    _to_copy_2049 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_43_triangle_multiplication_linear_z_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_43_triangle_multiplication_linear_z_out_weight = None
    _to_copy_2050 = torch.ops.aten._to_copy.default(add_400, dtype = torch.bfloat16);  add_400 = None
    t_766 = torch.ops.aten.t.default(_to_copy_2049);  _to_copy_2049 = None
    view_3668 = torch.ops.aten.view.default(_to_copy_2050, [147456, 256]);  _to_copy_2050 = None
    mm_713 = torch.ops.aten.mm.default(view_3668, t_766);  view_3668 = t_766 = None
    view_3669 = torch.ops.aten.view.default(mm_713, [1, 384, 384, 256]);  mm_713 = None
    _to_copy_2051 = torch.ops.aten._to_copy.default(getitem_3572, dtype = torch.bfloat16);  getitem_3572 = None
    _to_copy_2052 = torch.ops.aten._to_copy.default(getitem_3565, dtype = torch.bfloat16);  getitem_3565 = None
    t_767 = torch.ops.aten.t.default(_to_copy_2051);  _to_copy_2051 = None
    view_3670 = torch.ops.aten.view.default(_to_copy_2052, [147456, 256]);  _to_copy_2052 = None
    mm_714 = torch.ops.aten.mm.default(view_3670, t_767);  view_3670 = t_767 = None
    view_3671 = torch.ops.aten.view.default(mm_714, [1, 384, 384, 256]);  mm_714 = None
    sigmoid_296 = torch.ops.aten.sigmoid.default(view_3671);  view_3671 = None
    mul_490 = torch.ops.aten.mul.Tensor(view_3669, sigmoid_296);  view_3669 = sigmoid_296 = None
    add_401 = torch.ops.aten.add.Tensor(add_395, mul_490);  mul_490 = None
    _to_copy_2053 = torch.ops.aten._to_copy.default(add_395, dtype = torch.float32)
    native_layer_norm_default_422 = torch.ops.aten.native_layer_norm.default(_to_copy_2053, [256], None, None, 1e-05);  _to_copy_2053 = None
    getitem_3591 = native_layer_norm_default_422[0];  native_layer_norm_default_422 = None
    _to_copy_2054 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_43_triangle_attention_pair2b_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_43_triangle_attention_pair2b_weight = None
    _to_copy_2055 = torch.ops.aten._to_copy.default(getitem_3591, dtype = torch.bfloat16)
    t_768 = torch.ops.aten.t.default(_to_copy_2054);  _to_copy_2054 = None
    view_3672 = torch.ops.aten.view.default(_to_copy_2055, [147456, 256]);  _to_copy_2055 = None
    mm_715 = torch.ops.aten.mm.default(view_3672, t_768);  view_3672 = t_768 = None
    view_3673 = torch.ops.aten.view.default(mm_715, [1, 384, 384, 8]);  mm_715 = None
    view_3674 = torch.ops.aten.view.default(view_3673, [1, 384, 384, 2, 4]);  view_3673 = None
    permute_1949 = torch.ops.aten.permute.default(view_3674, [0, 3, 4, 1, 2]);  view_3674 = None
    view_3675 = torch.ops.aten.view.default(permute_1949, [1, 2, 4, 1, 384, 384]);  permute_1949 = None
    view_3676 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 384, 384])
    bitwise_not_220 = torch.ops.aten.bitwise_not.default(view_3676);  view_3676 = None
    masked_fill_220 = torch.ops.aten.masked_fill.Scalar(view_3675, bitwise_not_220, -10000);  view_3675 = bitwise_not_220 = None
    view_3677 = torch.ops.aten.view.default(masked_fill_220, [1, 2, 4, 384, 384]);  masked_fill_220 = None
    permute_1950 = torch.ops.aten.permute.default(view_3677, [1, 0, 2, 3, 4]);  view_3677 = None
    view_3678 = torch.ops.aten.view.default(permute_1950, [2, 4, 1, 384, 384]);  permute_1950 = None
    _to_copy_2056 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_43_triangle_attention_pair2qkvg1_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_43_triangle_attention_pair2qkvg1_weight = None
    _to_copy_2057 = torch.ops.aten._to_copy.default(getitem_3591, dtype = torch.bfloat16)
    t_769 = torch.ops.aten.t.default(_to_copy_2056);  _to_copy_2056 = None
    view_3679 = torch.ops.aten.view.default(_to_copy_2057, [147456, 256]);  _to_copy_2057 = None
    mm_716 = torch.ops.aten.mm.default(view_3679, t_769);  view_3679 = t_769 = None
    view_3680 = torch.ops.aten.view.default(mm_716, [1, 384, 384, 1024]);  mm_716 = None
    select_99 = torch.ops.aten.select.int(view_3678, 0, 0)
    view_3681 = torch.ops.aten.view.default(view_3680, [1, 384, 384, 4, 4, 64]);  view_3680 = None
    permute_1951 = torch.ops.aten.permute.default(view_3681, [4, 0, 3, 1, 2, 5]);  view_3681 = None
    view_3682 = torch.ops.aten.view.default(permute_1951, [4, 4, 384, 384, 64]);  permute_1951 = None
    unbind_int_163 = torch.ops.aten.unbind.int(view_3682);  view_3682 = None
    getitem_3594 = unbind_int_163[0]
    getitem_3595 = unbind_int_163[1]
    getitem_3596 = unbind_int_163[2]
    getitem_3597 = unbind_int_163[3];  unbind_int_163 = None
    expand_242 = torch.ops.aten.expand.default(select_99, [4, 384, 384, 384]);  select_99 = None
    _scaled_dot_product_efficient_attention_default_141 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3594, getitem_3595, getitem_3596, expand_242, False);  getitem_3594 = getitem_3595 = getitem_3596 = expand_242 = None
    getitem_3598 = _scaled_dot_product_efficient_attention_default_141[0];  _scaled_dot_product_efficient_attention_default_141 = None
    sigmoid_297 = torch.ops.aten.sigmoid.default(getitem_3597);  getitem_3597 = None
    mul_491 = torch.ops.aten.mul.Tensor(getitem_3598, sigmoid_297);  getitem_3598 = sigmoid_297 = None
    view_3683 = torch.ops.aten.view.default(mul_491, [1, 4, 384, 384, 64]);  mul_491 = None
    permute_1952 = torch.ops.aten.permute.default(view_3683, [0, 2, 3, 1, 4]);  view_3683 = None
    clone_305 = torch.ops.aten.clone.default(permute_1952, memory_format = torch.contiguous_format);  permute_1952 = None
    _unsafe_view_254 = torch.ops.aten._unsafe_view.default(clone_305, [1, 384, 384, 256]);  clone_305 = None
    transpose_99 = torch.ops.aten.transpose.int(getitem_3591, 1, 2);  getitem_3591 = None
    _to_copy_2058 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_43_triangle_attention_pair2qkvg2_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_43_triangle_attention_pair2qkvg2_weight = None
    _to_copy_2059 = torch.ops.aten._to_copy.default(transpose_99, dtype = torch.bfloat16);  transpose_99 = None
    t_770 = torch.ops.aten.t.default(_to_copy_2058);  _to_copy_2058 = None
    expand_243 = torch.ops.aten.expand.default(_to_copy_2059, [1, 384, 384, 256]);  _to_copy_2059 = None
    view_3684 = torch.ops.aten.view.default(expand_243, [384, 384, 256]);  expand_243 = None
    expand_244 = torch.ops.aten.expand.default(t_770, [1, 384, 256, 1024]);  t_770 = None
    view_3685 = torch.ops.aten.view.default(expand_244, [384, 256, 1024]);  expand_244 = None
    bmm_289 = torch.ops.aten.bmm.default(view_3684, view_3685);  view_3684 = view_3685 = None
    view_3686 = torch.ops.aten.view.default(bmm_289, [1, 384, 384, 1024]);  bmm_289 = None
    select_100 = torch.ops.aten.select.int(view_3678, 0, 1);  view_3678 = None
    view_3687 = torch.ops.aten.view.default(view_3686, [1, 384, 384, 4, 4, 64]);  view_3686 = None
    permute_1953 = torch.ops.aten.permute.default(view_3687, [4, 0, 3, 1, 2, 5]);  view_3687 = None
    view_3688 = torch.ops.aten.view.default(permute_1953, [4, 4, 384, 384, 64]);  permute_1953 = None
    unbind_int_164 = torch.ops.aten.unbind.int(view_3688);  view_3688 = None
    getitem_3602 = unbind_int_164[0]
    getitem_3603 = unbind_int_164[1]
    getitem_3604 = unbind_int_164[2]
    getitem_3605 = unbind_int_164[3];  unbind_int_164 = None
    expand_245 = torch.ops.aten.expand.default(select_100, [4, 384, 384, 384]);  select_100 = None
    _scaled_dot_product_efficient_attention_default_142 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3602, getitem_3603, getitem_3604, expand_245, False);  getitem_3602 = getitem_3603 = getitem_3604 = expand_245 = None
    getitem_3606 = _scaled_dot_product_efficient_attention_default_142[0];  _scaled_dot_product_efficient_attention_default_142 = None
    sigmoid_298 = torch.ops.aten.sigmoid.default(getitem_3605);  getitem_3605 = None
    mul_492 = torch.ops.aten.mul.Tensor(getitem_3606, sigmoid_298);  getitem_3606 = sigmoid_298 = None
    view_3689 = torch.ops.aten.view.default(mul_492, [1, 4, 384, 384, 64]);  mul_492 = None
    permute_1954 = torch.ops.aten.permute.default(view_3689, [0, 2, 3, 1, 4]);  view_3689 = None
    clone_306 = torch.ops.aten.clone.default(permute_1954, memory_format = torch.contiguous_format);  permute_1954 = None
    _unsafe_view_255 = torch.ops.aten._unsafe_view.default(clone_306, [1, 384, 384, 256]);  clone_306 = None
    cat_55 = torch.ops.aten.cat.default([_unsafe_view_254, _unsafe_view_255], dim = -1);  _unsafe_view_254 = _unsafe_view_255 = None
    slice_206 = torch.ops.aten.slice.Tensor(pairformer_stack_blocks_43_triangle_attention_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  pairformer_stack_blocks_43_triangle_attention_out_scalers = None
    unsqueeze_1182 = torch.ops.aten.unsqueeze.default(slice_206, 1);  slice_206 = None
    mul_493 = torch.ops.aten.mul.Tensor(pairformer_stack_blocks_43_triangle_attention_linear_out_weight, unsqueeze_1182);  pairformer_stack_blocks_43_triangle_attention_linear_out_weight = unsqueeze_1182 = None
    _to_copy_2060 = torch.ops.aten._to_copy.default(mul_493, dtype = torch.bfloat16);  mul_493 = None
    t_771 = torch.ops.aten.t.default(_to_copy_2060);  _to_copy_2060 = None
    view_3690 = torch.ops.aten.view.default(cat_55, [147456, 512]);  cat_55 = None
    mm_717 = torch.ops.aten.mm.default(view_3690, t_771);  view_3690 = t_771 = None
    view_3691 = torch.ops.aten.view.default(mm_717, [1, 384, 384, 256]);  mm_717 = None
    add_402 = torch.ops.aten.add.Tensor(add_401, view_3691);  add_401 = view_3691 = None
    split_tensor_396 = torch.ops.aten.split.Tensor(add_395, 384, dim = -2)
    getitem_3610 = split_tensor_396[0];  split_tensor_396 = None
    _to_copy_2061 = torch.ops.aten._to_copy.default(getitem_3610, dtype = torch.float32);  getitem_3610 = None
    native_layer_norm_default_423 = torch.ops.aten.native_layer_norm.default(_to_copy_2061, [256], pairformer_stack_blocks_43_transition_pair_layer_norm_weight, pairformer_stack_blocks_43_transition_pair_layer_norm_bias, 1e-05);  _to_copy_2061 = pairformer_stack_blocks_43_transition_pair_layer_norm_weight = pairformer_stack_blocks_43_transition_pair_layer_norm_bias = None
    getitem_3611 = native_layer_norm_default_423[0];  native_layer_norm_default_423 = None
    _to_copy_2062 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_43_transition_pair_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_43_transition_pair_linear_no_bias_ab_weight = None
    _to_copy_2063 = torch.ops.aten._to_copy.default(getitem_3611, dtype = torch.bfloat16);  getitem_3611 = None
    t_772 = torch.ops.aten.t.default(_to_copy_2062);  _to_copy_2062 = None
    view_3692 = torch.ops.aten.view.default(_to_copy_2063, [147456, 256]);  _to_copy_2063 = None
    mm_718 = torch.ops.aten.mm.default(view_3692, t_772);  view_3692 = t_772 = None
    view_3693 = torch.ops.aten.view.default(mm_718, [1, 384, 384, 1024]);  mm_718 = None
    split_tensor_397 = torch.ops.aten.split.Tensor(view_3693, 512, dim = -1);  view_3693 = None
    getitem_3614 = split_tensor_397[0]
    getitem_3615 = split_tensor_397[1];  split_tensor_397 = None
    silu_101 = torch.ops.aten.silu.default(getitem_3614);  getitem_3614 = None
    mul_494 = torch.ops.aten.mul.Tensor(silu_101, getitem_3615);  silu_101 = getitem_3615 = None
    _to_copy_2064 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_43_transition_pair_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_43_transition_pair_linear_out_weight = None
    t_773 = torch.ops.aten.t.default(_to_copy_2064);  _to_copy_2064 = None
    view_3695 = torch.ops.aten.view.default(mul_494, [147456, 512]);  mul_494 = None
    mm_719 = torch.ops.aten.mm.default(view_3695, t_773);  view_3695 = t_773 = None
    view_3696 = torch.ops.aten.view.default(mm_719, [1, 384, 384, 256]);  mm_719 = None
    add_403 = torch.ops.aten.add.Tensor(add_402, view_3696);  add_402 = view_3696 = None
    _to_copy_2065 = torch.ops.aten._to_copy.default(add_399, dtype = torch.float32)
    native_layer_norm_default_424 = torch.ops.aten.native_layer_norm.default(_to_copy_2065, [384], pairformer_stack_blocks_43_attention_pair_bias_single_layer_norm_weight, pairformer_stack_blocks_43_attention_pair_bias_single_layer_norm_bias, 1e-05);  _to_copy_2065 = pairformer_stack_blocks_43_attention_pair_bias_single_layer_norm_weight = pairformer_stack_blocks_43_attention_pair_bias_single_layer_norm_bias = None
    getitem_3616 = native_layer_norm_default_424[0];  native_layer_norm_default_424 = None
    _to_copy_2066 = torch.ops.aten._to_copy.default(add_395, dtype = torch.float32);  add_395 = None
    native_layer_norm_default_425 = torch.ops.aten.native_layer_norm.default(_to_copy_2066, [256], pairformer_stack_blocks_43_attention_pair_bias_pair_layer_norm_weight, pairformer_stack_blocks_43_attention_pair_bias_pair_layer_norm_bias, 1e-05);  _to_copy_2066 = pairformer_stack_blocks_43_attention_pair_bias_pair_layer_norm_weight = pairformer_stack_blocks_43_attention_pair_bias_pair_layer_norm_bias = None
    getitem_3619 = native_layer_norm_default_425[0];  native_layer_norm_default_425 = None
    _to_copy_2067 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_43_attention_pair_bias_pair_linear_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_43_attention_pair_bias_pair_linear_weight = None
    _to_copy_2068 = torch.ops.aten._to_copy.default(getitem_3619, dtype = torch.bfloat16);  getitem_3619 = None
    t_774 = torch.ops.aten.t.default(_to_copy_2067);  _to_copy_2067 = None
    view_3697 = torch.ops.aten.view.default(_to_copy_2068, [147456, 256]);  _to_copy_2068 = None
    mm_720 = torch.ops.aten.mm.default(view_3697, t_774);  view_3697 = t_774 = None
    view_3698 = torch.ops.aten.view.default(mm_720, [1, 384, 384, 16]);  mm_720 = None
    permute_1955 = torch.ops.aten.permute.default(view_3698, [0, 3, 1, 2]);  view_3698 = None
    view_3699 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384])
    bitwise_not_221 = torch.ops.aten.bitwise_not.default(view_3699);  view_3699 = None
    masked_fill_221 = torch.ops.aten.masked_fill.Scalar(permute_1955, bitwise_not_221, -10000);  permute_1955 = bitwise_not_221 = None
    _to_copy_2069 = torch.ops.aten._to_copy.default(getitem_3616, dtype = torch.bfloat16);  getitem_3616 = None
    _to_copy_2070 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_43_attention_pair_bias_attention_input2qkvg_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_43_attention_pair_bias_attention_input2qkvg_weight = None
    unsqueeze_1183 = torch.ops.aten.unsqueeze.default(_to_copy_2069, 3);  _to_copy_2069 = None
    unsqueeze_1184 = torch.ops.aten.unsqueeze.default(unsqueeze_1183, 4);  unsqueeze_1183 = None
    unsqueeze_1185 = torch.ops.aten.unsqueeze.default(unsqueeze_1184, 5);  unsqueeze_1184 = None
    permute_1956 = torch.ops.aten.permute.default(unsqueeze_1185, [3, 0, 4, 1, 5, 2]);  unsqueeze_1185 = None
    unsqueeze_1186 = torch.ops.aten.unsqueeze.default(_to_copy_2070, 4);  _to_copy_2070 = None
    unsqueeze_1187 = torch.ops.aten.unsqueeze.default(unsqueeze_1186, 5);  unsqueeze_1186 = None
    permute_1957 = torch.ops.aten.permute.default(unsqueeze_1187, [1, 4, 2, 5, 3, 0]);  unsqueeze_1187 = None
    permute_1958 = torch.ops.aten.permute.default(permute_1956, [3, 5, 0, 1, 2, 4]);  permute_1956 = None
    view_3700 = torch.ops.aten.view.default(permute_1958, [1, 384, 384]);  permute_1958 = None
    permute_1959 = torch.ops.aten.permute.default(permute_1957, [5, 0, 1, 2, 4, 3]);  permute_1957 = None
    view_3701 = torch.ops.aten.view.default(permute_1959, [1, 384, 1536]);  permute_1959 = None
    bmm_290 = torch.ops.aten.bmm.default(view_3700, view_3701);  view_3700 = view_3701 = None
    view_3702 = torch.ops.aten.view.default(bmm_290, [384, 1, 4, 1, 16, 24]);  bmm_290 = None
    permute_1960 = torch.ops.aten.permute.default(view_3702, [2, 3, 4, 0, 5, 1]);  view_3702 = None
    view_3703 = torch.ops.aten.view.default(permute_1960, [4, 1, 16, 384, 24]);  permute_1960 = None
    unbind_int_165 = torch.ops.aten.unbind.int(view_3703);  view_3703 = None
    getitem_3622 = unbind_int_165[0]
    getitem_3623 = unbind_int_165[1]
    getitem_3624 = unbind_int_165[2]
    getitem_3625 = unbind_int_165[3];  unbind_int_165 = None
    view_3704 = torch.ops.aten.view.default(pairformer_stack_blocks_43_attention_pair_bias_attention_query_bias, [1, 16, 1, 24]);  pairformer_stack_blocks_43_attention_pair_bias_attention_query_bias = None
    add_404 = torch.ops.aten.add.Tensor(getitem_3622, view_3704);  getitem_3622 = view_3704 = None
    _to_copy_2071 = torch.ops.aten._to_copy.default(add_404, dtype = torch.bfloat16);  add_404 = None
    expand_246 = torch.ops.aten.expand.default(masked_fill_221, [1, 16, 384, 384]);  masked_fill_221 = None
    _scaled_dot_product_efficient_attention_default_143 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_2071, getitem_3623, getitem_3624, expand_246, False);  _to_copy_2071 = getitem_3623 = getitem_3624 = expand_246 = None
    getitem_3626 = _scaled_dot_product_efficient_attention_default_143[0];  _scaled_dot_product_efficient_attention_default_143 = None
    add_405 = torch.ops.aten.add.Tensor(getitem_3625, 1);  getitem_3625 = None
    sigmoid_299 = torch.ops.aten.sigmoid.default(add_405);  add_405 = None
    mul_495 = torch.ops.aten.mul.Tensor(getitem_3626, sigmoid_299);  getitem_3626 = sigmoid_299 = None
    _to_copy_2072 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_43_attention_pair_bias_attention_output_proj_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_43_attention_pair_bias_attention_output_proj_weight = None
    unsqueeze_1188 = torch.ops.aten.unsqueeze.default(mul_495, 4);  mul_495 = None
    permute_1961 = torch.ops.aten.permute.default(unsqueeze_1188, [0, 2, 4, 3, 1]);  unsqueeze_1188 = None
    unsqueeze_1189 = torch.ops.aten.unsqueeze.default(_to_copy_2072, 3);  _to_copy_2072 = None
    unsqueeze_1190 = torch.ops.aten.unsqueeze.default(unsqueeze_1189, 4);  unsqueeze_1189 = None
    permute_1962 = torch.ops.aten.permute.default(unsqueeze_1190, [3, 4, 2, 1, 0]);  unsqueeze_1190 = None
    permute_1963 = torch.ops.aten.permute.default(permute_1961, [1, 3, 4, 0, 2]);  permute_1961 = None
    clone_307 = torch.ops.aten.clone.default(permute_1963, memory_format = torch.contiguous_format);  permute_1963 = None
    _unsafe_view_256 = torch.ops.aten._unsafe_view.default(clone_307, [1, 384, 384]);  clone_307 = None
    permute_1964 = torch.ops.aten.permute.default(permute_1962, [3, 4, 0, 2, 1]);  permute_1962 = None
    clone_308 = torch.ops.aten.clone.default(permute_1964, memory_format = torch.contiguous_format);  permute_1964 = None
    _unsafe_view_257 = torch.ops.aten._unsafe_view.default(clone_308, [1, 384, 384]);  clone_308 = None
    bmm_291 = torch.ops.aten.bmm.default(_unsafe_view_256, _unsafe_view_257);  _unsafe_view_256 = _unsafe_view_257 = None
    view_3705 = torch.ops.aten.view.default(bmm_291, [384, 1, 1, 1, 384]);  bmm_291 = None
    permute_1965 = torch.ops.aten.permute.default(view_3705, [3, 0, 4, 1, 2]);  view_3705 = None
    view_3706 = torch.ops.aten.view.default(permute_1965, [1, 384, 384]);  permute_1965 = None
    unsqueeze_1191 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
    mul_496 = torch.ops.aten.mul.Tensor(view_3706, unsqueeze_1191);  view_3706 = unsqueeze_1191 = None
    add_406 = torch.ops.aten.add.Tensor(add_399, mul_496);  mul_496 = None
    split_tensor_398 = torch.ops.aten.split.Tensor(add_399, 384, dim = -2);  add_399 = None
    getitem_3630 = split_tensor_398[0];  split_tensor_398 = None
    _to_copy_2073 = torch.ops.aten._to_copy.default(getitem_3630, dtype = torch.float32);  getitem_3630 = None
    native_layer_norm_default_426 = torch.ops.aten.native_layer_norm.default(_to_copy_2073, [384], pairformer_stack_blocks_43_transition_single_layer_norm_weight, pairformer_stack_blocks_43_transition_single_layer_norm_bias, 1e-05);  _to_copy_2073 = pairformer_stack_blocks_43_transition_single_layer_norm_weight = pairformer_stack_blocks_43_transition_single_layer_norm_bias = None
    getitem_3631 = native_layer_norm_default_426[0];  native_layer_norm_default_426 = None
    _to_copy_2074 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_43_transition_single_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_43_transition_single_linear_no_bias_ab_weight = None
    _to_copy_2075 = torch.ops.aten._to_copy.default(getitem_3631, dtype = torch.bfloat16);  getitem_3631 = None
    t_775 = torch.ops.aten.t.default(_to_copy_2074);  _to_copy_2074 = None
    view_3707 = torch.ops.aten.view.default(_to_copy_2075, [384, 384]);  _to_copy_2075 = None
    mm_721 = torch.ops.aten.mm.default(view_3707, t_775);  view_3707 = t_775 = None
    view_3708 = torch.ops.aten.view.default(mm_721, [1, 384, 1536]);  mm_721 = None
    split_tensor_399 = torch.ops.aten.split.Tensor(view_3708, 768, dim = -1);  view_3708 = None
    getitem_3634 = split_tensor_399[0]
    getitem_3635 = split_tensor_399[1];  split_tensor_399 = None
    silu_102 = torch.ops.aten.silu.default(getitem_3634);  getitem_3634 = None
    mul_497 = torch.ops.aten.mul.Tensor(silu_102, getitem_3635);  silu_102 = getitem_3635 = None
    _to_copy_2076 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_43_transition_single_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_43_transition_single_linear_out_weight = None
    t_776 = torch.ops.aten.t.default(_to_copy_2076);  _to_copy_2076 = None
    view_3710 = torch.ops.aten.view.default(mul_497, [384, 768]);  mul_497 = None
    mm_722 = torch.ops.aten.mm.default(view_3710, t_776);  view_3710 = t_776 = None
    view_3711 = torch.ops.aten.view.default(mm_722, [1, 384, 384]);  mm_722 = None
    add_407 = torch.ops.aten.add.Tensor(add_406, view_3711);  add_406 = view_3711 = None
    _to_copy_2077 = torch.ops.aten._to_copy.default(add_403, dtype = torch.float32)
    native_layer_norm_default_427 = torch.ops.aten.native_layer_norm.default(_to_copy_2077, [256], pairformer_stack_blocks_44_triangle_multiplication_layernorm_z_in_weight, pairformer_stack_blocks_44_triangle_multiplication_layernorm_z_in_bias, 1e-05);  _to_copy_2077 = pairformer_stack_blocks_44_triangle_multiplication_layernorm_z_in_weight = pairformer_stack_blocks_44_triangle_multiplication_layernorm_z_in_bias = None
    getitem_3636 = native_layer_norm_default_427[0];  native_layer_norm_default_427 = None
    split_with_sizes_default_100 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_44_triangle_multiplication_merged_linear_p_weight, [512, 512]);  pairformer_stack_blocks_44_triangle_multiplication_merged_linear_p_weight = None
    getitem_3639 = split_with_sizes_default_100[0]
    getitem_3640 = split_with_sizes_default_100[1];  split_with_sizes_default_100 = None
    split_with_sizes_default_101 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_44_triangle_multiplication_merged_linear_g_weight, [512, 512, 256]);  pairformer_stack_blocks_44_triangle_multiplication_merged_linear_g_weight = None
    getitem_3641 = split_with_sizes_default_101[0]
    getitem_3642 = split_with_sizes_default_101[1]
    getitem_3643 = split_with_sizes_default_101[2];  split_with_sizes_default_101 = None
    _to_copy_2078 = torch.ops.aten._to_copy.default(getitem_3639, dtype = torch.bfloat16);  getitem_3639 = None
    _to_copy_2079 = torch.ops.aten._to_copy.default(getitem_3636, dtype = torch.bfloat16)
    t_777 = torch.ops.aten.t.default(_to_copy_2078);  _to_copy_2078 = None
    view_3712 = torch.ops.aten.view.default(_to_copy_2079, [147456, 256]);  _to_copy_2079 = None
    mm_723 = torch.ops.aten.mm.default(view_3712, t_777);  view_3712 = t_777 = None
    view_3713 = torch.ops.aten.view.default(mm_723, [1, 384, 384, 512]);  mm_723 = None
    _to_copy_2080 = torch.ops.aten._to_copy.default(getitem_3641, dtype = torch.bfloat16);  getitem_3641 = None
    _to_copy_2081 = torch.ops.aten._to_copy.default(getitem_3636, dtype = torch.bfloat16)
    t_778 = torch.ops.aten.t.default(_to_copy_2080);  _to_copy_2080 = None
    view_3714 = torch.ops.aten.view.default(_to_copy_2081, [147456, 256]);  _to_copy_2081 = None
    mm_724 = torch.ops.aten.mm.default(view_3714, t_778);  view_3714 = t_778 = None
    view_3715 = torch.ops.aten.view.default(mm_724, [1, 384, 384, 512]);  mm_724 = None
    sigmoid_300 = torch.ops.aten.sigmoid.default(view_3715);  view_3715 = None
    mul_498 = torch.ops.aten.mul.Tensor(view_3713, sigmoid_300);  view_3713 = sigmoid_300 = None
    unsqueeze_1192 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
    bitwise_not_222 = torch.ops.aten.bitwise_not.default(unsqueeze_1192);  unsqueeze_1192 = None
    masked_fill_222 = torch.ops.aten.masked_fill.Scalar(mul_498, bitwise_not_222, 0);  mul_498 = bitwise_not_222 = None
    split_tensor_400 = torch.ops.aten.split.Tensor(masked_fill_222, 256, dim = -1)
    getitem_3646 = split_tensor_400[0];  split_tensor_400 = None
    unsqueeze_1195 = torch.ops.aten.unsqueeze.default(getitem_3646, 4);  getitem_3646 = None
    permute_1970 = torch.ops.aten.permute.default(unsqueeze_1195, [0, 1, 4, 3, 2]);  unsqueeze_1195 = None
    permute_1971 = torch.ops.aten.permute.default(permute_1970, [3, 1, 4, 0, 2]);  permute_1970 = None
    view_3718 = torch.ops.aten.view.default(permute_1971, [256, 384, 384]);  permute_1971 = None
    split_tensor_401 = torch.ops.aten.split.Tensor(masked_fill_222, 256, dim = -1);  masked_fill_222 = None
    getitem_3649 = split_tensor_401[1];  split_tensor_401 = None
    unsqueeze_1196 = torch.ops.aten.unsqueeze.default(getitem_3649, 4);  getitem_3649 = None
    permute_1972 = torch.ops.aten.permute.default(unsqueeze_1196, [0, 4, 1, 3, 2]);  unsqueeze_1196 = None
    permute_1973 = torch.ops.aten.permute.default(permute_1972, [3, 4, 0, 2, 1]);  permute_1972 = None
    view_3719 = torch.ops.aten.view.default(permute_1973, [256, 384, 384]);  permute_1973 = None
    bmm_292 = torch.ops.aten.bmm.default(view_3718, view_3719);  view_3718 = view_3719 = None
    view_3720 = torch.ops.aten.view.default(bmm_292, [256, 384, 1, 1, 384]);  bmm_292 = None
    permute_1974 = torch.ops.aten.permute.default(view_3720, [3, 1, 4, 0, 2]);  view_3720 = None
    view_3721 = torch.ops.aten.view.default(permute_1974, [1, 384, 384, 256]);  permute_1974 = None
    _to_copy_2082 = torch.ops.aten._to_copy.default(getitem_3640, dtype = torch.bfloat16);  getitem_3640 = None
    _to_copy_2083 = torch.ops.aten._to_copy.default(getitem_3636, dtype = torch.bfloat16)
    t_779 = torch.ops.aten.t.default(_to_copy_2082);  _to_copy_2082 = None
    view_3722 = torch.ops.aten.view.default(_to_copy_2083, [147456, 256]);  _to_copy_2083 = None
    mm_725 = torch.ops.aten.mm.default(view_3722, t_779);  view_3722 = t_779 = None
    view_3723 = torch.ops.aten.view.default(mm_725, [1, 384, 384, 512]);  mm_725 = None
    _to_copy_2084 = torch.ops.aten._to_copy.default(getitem_3642, dtype = torch.bfloat16);  getitem_3642 = None
    _to_copy_2085 = torch.ops.aten._to_copy.default(getitem_3636, dtype = torch.bfloat16)
    t_780 = torch.ops.aten.t.default(_to_copy_2084);  _to_copy_2084 = None
    view_3724 = torch.ops.aten.view.default(_to_copy_2085, [147456, 256]);  _to_copy_2085 = None
    mm_726 = torch.ops.aten.mm.default(view_3724, t_780);  view_3724 = t_780 = None
    view_3725 = torch.ops.aten.view.default(mm_726, [1, 384, 384, 512]);  mm_726 = None
    sigmoid_301 = torch.ops.aten.sigmoid.default(view_3725);  view_3725 = None
    mul_499 = torch.ops.aten.mul.Tensor(view_3723, sigmoid_301);  view_3723 = sigmoid_301 = None
    view_3726 = torch.ops.aten.view.default(mul_499, [147456, 512]);  mul_499 = None
    view_3727 = torch.ops.aten.view.default(view_3726, [1, 384, 384, 512]);  view_3726 = None
    transpose_100 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
    unsqueeze_1197 = torch.ops.aten.unsqueeze.default(transpose_100, 3);  transpose_100 = None
    clone_309 = torch.ops.aten.clone.default(unsqueeze_1197, memory_format = torch.contiguous_format);  unsqueeze_1197 = None
    bitwise_not_223 = torch.ops.aten.bitwise_not.default(clone_309);  clone_309 = None
    masked_fill_223 = torch.ops.aten.masked_fill.Scalar(view_3727, bitwise_not_223, 0);  view_3727 = bitwise_not_223 = None
    view_3728 = torch.ops.aten.view.default(masked_fill_223, [147456, 512]);  masked_fill_223 = None
    view_3732 = torch.ops.aten.view.default(view_3728, [1, 384, 384, 512])
    split_tensor_402 = torch.ops.aten.split.Tensor(view_3732, 256, dim = -1);  view_3732 = None
    getitem_3652 = split_tensor_402[0];  split_tensor_402 = None
    unsqueeze_1200 = torch.ops.aten.unsqueeze.default(getitem_3652, 4);  getitem_3652 = None
    permute_1979 = torch.ops.aten.permute.default(unsqueeze_1200, [0, 2, 4, 3, 1]);  unsqueeze_1200 = None
    permute_1980 = torch.ops.aten.permute.default(permute_1979, [3, 1, 4, 0, 2]);  permute_1979 = None
    view_3733 = torch.ops.aten.view.default(permute_1980, [256, 384, 384]);  permute_1980 = None
    view_3734 = torch.ops.aten.view.default(view_3728, [1, 384, 384, 512]);  view_3728 = None
    split_tensor_403 = torch.ops.aten.split.Tensor(view_3734, 256, dim = -1);  view_3734 = None
    getitem_3655 = split_tensor_403[1];  split_tensor_403 = None
    unsqueeze_1201 = torch.ops.aten.unsqueeze.default(getitem_3655, 4);  getitem_3655 = None
    permute_1981 = torch.ops.aten.permute.default(unsqueeze_1201, [0, 4, 2, 3, 1]);  unsqueeze_1201 = None
    permute_1982 = torch.ops.aten.permute.default(permute_1981, [3, 4, 0, 2, 1]);  permute_1981 = None
    view_3735 = torch.ops.aten.view.default(permute_1982, [256, 384, 384]);  permute_1982 = None
    bmm_293 = torch.ops.aten.bmm.default(view_3733, view_3735);  view_3733 = view_3735 = None
    view_3736 = torch.ops.aten.view.default(bmm_293, [256, 384, 1, 1, 384]);  bmm_293 = None
    permute_1983 = torch.ops.aten.permute.default(view_3736, [3, 1, 4, 0, 2]);  view_3736 = None
    view_3737 = torch.ops.aten.view.default(permute_1983, [1, 384, 384, 256]);  permute_1983 = None
    _to_copy_2086 = torch.ops.aten._to_copy.default(view_3721, dtype = torch.float32);  view_3721 = None
    native_layer_norm_default_428 = torch.ops.aten.native_layer_norm.default(_to_copy_2086, [256], None, None, 1e-05);  _to_copy_2086 = None
    getitem_3656 = native_layer_norm_default_428[0];  native_layer_norm_default_428 = None
    _to_copy_2087 = torch.ops.aten._to_copy.default(view_3737, dtype = torch.float32);  view_3737 = None
    native_layer_norm_default_429 = torch.ops.aten.native_layer_norm.default(_to_copy_2087, [256], None, None, 1e-05);  _to_copy_2087 = None
    getitem_3659 = native_layer_norm_default_429[0];  native_layer_norm_default_429 = None
    add_408 = torch.ops.aten.add.Tensor(getitem_3656, getitem_3659);  getitem_3656 = getitem_3659 = None
    _to_copy_2088 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_44_triangle_multiplication_linear_z_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_44_triangle_multiplication_linear_z_out_weight = None
    _to_copy_2089 = torch.ops.aten._to_copy.default(add_408, dtype = torch.bfloat16);  add_408 = None
    t_781 = torch.ops.aten.t.default(_to_copy_2088);  _to_copy_2088 = None
    view_3738 = torch.ops.aten.view.default(_to_copy_2089, [147456, 256]);  _to_copy_2089 = None
    mm_727 = torch.ops.aten.mm.default(view_3738, t_781);  view_3738 = t_781 = None
    view_3739 = torch.ops.aten.view.default(mm_727, [1, 384, 384, 256]);  mm_727 = None
    _to_copy_2090 = torch.ops.aten._to_copy.default(getitem_3643, dtype = torch.bfloat16);  getitem_3643 = None
    _to_copy_2091 = torch.ops.aten._to_copy.default(getitem_3636, dtype = torch.bfloat16);  getitem_3636 = None
    t_782 = torch.ops.aten.t.default(_to_copy_2090);  _to_copy_2090 = None
    view_3740 = torch.ops.aten.view.default(_to_copy_2091, [147456, 256]);  _to_copy_2091 = None
    mm_728 = torch.ops.aten.mm.default(view_3740, t_782);  view_3740 = t_782 = None
    view_3741 = torch.ops.aten.view.default(mm_728, [1, 384, 384, 256]);  mm_728 = None
    sigmoid_302 = torch.ops.aten.sigmoid.default(view_3741);  view_3741 = None
    mul_500 = torch.ops.aten.mul.Tensor(view_3739, sigmoid_302);  view_3739 = sigmoid_302 = None
    add_409 = torch.ops.aten.add.Tensor(add_403, mul_500);  mul_500 = None
    _to_copy_2092 = torch.ops.aten._to_copy.default(add_403, dtype = torch.float32)
    native_layer_norm_default_430 = torch.ops.aten.native_layer_norm.default(_to_copy_2092, [256], None, None, 1e-05);  _to_copy_2092 = None
    getitem_3662 = native_layer_norm_default_430[0];  native_layer_norm_default_430 = None
    _to_copy_2093 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_44_triangle_attention_pair2b_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_44_triangle_attention_pair2b_weight = None
    _to_copy_2094 = torch.ops.aten._to_copy.default(getitem_3662, dtype = torch.bfloat16)
    t_783 = torch.ops.aten.t.default(_to_copy_2093);  _to_copy_2093 = None
    view_3742 = torch.ops.aten.view.default(_to_copy_2094, [147456, 256]);  _to_copy_2094 = None
    mm_729 = torch.ops.aten.mm.default(view_3742, t_783);  view_3742 = t_783 = None
    view_3743 = torch.ops.aten.view.default(mm_729, [1, 384, 384, 8]);  mm_729 = None
    view_3744 = torch.ops.aten.view.default(view_3743, [1, 384, 384, 2, 4]);  view_3743 = None
    permute_1984 = torch.ops.aten.permute.default(view_3744, [0, 3, 4, 1, 2]);  view_3744 = None
    view_3745 = torch.ops.aten.view.default(permute_1984, [1, 2, 4, 1, 384, 384]);  permute_1984 = None
    view_3746 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 384, 384])
    bitwise_not_224 = torch.ops.aten.bitwise_not.default(view_3746);  view_3746 = None
    masked_fill_224 = torch.ops.aten.masked_fill.Scalar(view_3745, bitwise_not_224, -10000);  view_3745 = bitwise_not_224 = None
    view_3747 = torch.ops.aten.view.default(masked_fill_224, [1, 2, 4, 384, 384]);  masked_fill_224 = None
    permute_1985 = torch.ops.aten.permute.default(view_3747, [1, 0, 2, 3, 4]);  view_3747 = None
    view_3748 = torch.ops.aten.view.default(permute_1985, [2, 4, 1, 384, 384]);  permute_1985 = None
    _to_copy_2095 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_44_triangle_attention_pair2qkvg1_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_44_triangle_attention_pair2qkvg1_weight = None
    _to_copy_2096 = torch.ops.aten._to_copy.default(getitem_3662, dtype = torch.bfloat16)
    t_784 = torch.ops.aten.t.default(_to_copy_2095);  _to_copy_2095 = None
    view_3749 = torch.ops.aten.view.default(_to_copy_2096, [147456, 256]);  _to_copy_2096 = None
    mm_730 = torch.ops.aten.mm.default(view_3749, t_784);  view_3749 = t_784 = None
    view_3750 = torch.ops.aten.view.default(mm_730, [1, 384, 384, 1024]);  mm_730 = None
    select_101 = torch.ops.aten.select.int(view_3748, 0, 0)
    view_3751 = torch.ops.aten.view.default(view_3750, [1, 384, 384, 4, 4, 64]);  view_3750 = None
    permute_1986 = torch.ops.aten.permute.default(view_3751, [4, 0, 3, 1, 2, 5]);  view_3751 = None
    view_3752 = torch.ops.aten.view.default(permute_1986, [4, 4, 384, 384, 64]);  permute_1986 = None
    unbind_int_166 = torch.ops.aten.unbind.int(view_3752);  view_3752 = None
    getitem_3665 = unbind_int_166[0]
    getitem_3666 = unbind_int_166[1]
    getitem_3667 = unbind_int_166[2]
    getitem_3668 = unbind_int_166[3];  unbind_int_166 = None
    expand_247 = torch.ops.aten.expand.default(select_101, [4, 384, 384, 384]);  select_101 = None
    _scaled_dot_product_efficient_attention_default_144 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3665, getitem_3666, getitem_3667, expand_247, False);  getitem_3665 = getitem_3666 = getitem_3667 = expand_247 = None
    getitem_3669 = _scaled_dot_product_efficient_attention_default_144[0];  _scaled_dot_product_efficient_attention_default_144 = None
    sigmoid_303 = torch.ops.aten.sigmoid.default(getitem_3668);  getitem_3668 = None
    mul_501 = torch.ops.aten.mul.Tensor(getitem_3669, sigmoid_303);  getitem_3669 = sigmoid_303 = None
    view_3753 = torch.ops.aten.view.default(mul_501, [1, 4, 384, 384, 64]);  mul_501 = None
    permute_1987 = torch.ops.aten.permute.default(view_3753, [0, 2, 3, 1, 4]);  view_3753 = None
    clone_310 = torch.ops.aten.clone.default(permute_1987, memory_format = torch.contiguous_format);  permute_1987 = None
    _unsafe_view_258 = torch.ops.aten._unsafe_view.default(clone_310, [1, 384, 384, 256]);  clone_310 = None
    transpose_101 = torch.ops.aten.transpose.int(getitem_3662, 1, 2);  getitem_3662 = None
    _to_copy_2097 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_44_triangle_attention_pair2qkvg2_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_44_triangle_attention_pair2qkvg2_weight = None
    _to_copy_2098 = torch.ops.aten._to_copy.default(transpose_101, dtype = torch.bfloat16);  transpose_101 = None
    t_785 = torch.ops.aten.t.default(_to_copy_2097);  _to_copy_2097 = None
    expand_248 = torch.ops.aten.expand.default(_to_copy_2098, [1, 384, 384, 256]);  _to_copy_2098 = None
    view_3754 = torch.ops.aten.view.default(expand_248, [384, 384, 256]);  expand_248 = None
    expand_249 = torch.ops.aten.expand.default(t_785, [1, 384, 256, 1024]);  t_785 = None
    view_3755 = torch.ops.aten.view.default(expand_249, [384, 256, 1024]);  expand_249 = None
    bmm_294 = torch.ops.aten.bmm.default(view_3754, view_3755);  view_3754 = view_3755 = None
    view_3756 = torch.ops.aten.view.default(bmm_294, [1, 384, 384, 1024]);  bmm_294 = None
    select_102 = torch.ops.aten.select.int(view_3748, 0, 1);  view_3748 = None
    view_3757 = torch.ops.aten.view.default(view_3756, [1, 384, 384, 4, 4, 64]);  view_3756 = None
    permute_1988 = torch.ops.aten.permute.default(view_3757, [4, 0, 3, 1, 2, 5]);  view_3757 = None
    view_3758 = torch.ops.aten.view.default(permute_1988, [4, 4, 384, 384, 64]);  permute_1988 = None
    unbind_int_167 = torch.ops.aten.unbind.int(view_3758);  view_3758 = None
    getitem_3673 = unbind_int_167[0]
    getitem_3674 = unbind_int_167[1]
    getitem_3675 = unbind_int_167[2]
    getitem_3676 = unbind_int_167[3];  unbind_int_167 = None
    expand_250 = torch.ops.aten.expand.default(select_102, [4, 384, 384, 384]);  select_102 = None
    _scaled_dot_product_efficient_attention_default_145 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3673, getitem_3674, getitem_3675, expand_250, False);  getitem_3673 = getitem_3674 = getitem_3675 = expand_250 = None
    getitem_3677 = _scaled_dot_product_efficient_attention_default_145[0];  _scaled_dot_product_efficient_attention_default_145 = None
    sigmoid_304 = torch.ops.aten.sigmoid.default(getitem_3676);  getitem_3676 = None
    mul_502 = torch.ops.aten.mul.Tensor(getitem_3677, sigmoid_304);  getitem_3677 = sigmoid_304 = None
    view_3759 = torch.ops.aten.view.default(mul_502, [1, 4, 384, 384, 64]);  mul_502 = None
    permute_1989 = torch.ops.aten.permute.default(view_3759, [0, 2, 3, 1, 4]);  view_3759 = None
    clone_311 = torch.ops.aten.clone.default(permute_1989, memory_format = torch.contiguous_format);  permute_1989 = None
    _unsafe_view_259 = torch.ops.aten._unsafe_view.default(clone_311, [1, 384, 384, 256]);  clone_311 = None
    cat_56 = torch.ops.aten.cat.default([_unsafe_view_258, _unsafe_view_259], dim = -1);  _unsafe_view_258 = _unsafe_view_259 = None
    slice_207 = torch.ops.aten.slice.Tensor(pairformer_stack_blocks_44_triangle_attention_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  pairformer_stack_blocks_44_triangle_attention_out_scalers = None
    unsqueeze_1202 = torch.ops.aten.unsqueeze.default(slice_207, 1);  slice_207 = None
    mul_503 = torch.ops.aten.mul.Tensor(pairformer_stack_blocks_44_triangle_attention_linear_out_weight, unsqueeze_1202);  pairformer_stack_blocks_44_triangle_attention_linear_out_weight = unsqueeze_1202 = None
    _to_copy_2099 = torch.ops.aten._to_copy.default(mul_503, dtype = torch.bfloat16);  mul_503 = None
    t_786 = torch.ops.aten.t.default(_to_copy_2099);  _to_copy_2099 = None
    view_3760 = torch.ops.aten.view.default(cat_56, [147456, 512]);  cat_56 = None
    mm_731 = torch.ops.aten.mm.default(view_3760, t_786);  view_3760 = t_786 = None
    view_3761 = torch.ops.aten.view.default(mm_731, [1, 384, 384, 256]);  mm_731 = None
    add_410 = torch.ops.aten.add.Tensor(add_409, view_3761);  add_409 = view_3761 = None
    split_tensor_404 = torch.ops.aten.split.Tensor(add_403, 384, dim = -2)
    getitem_3681 = split_tensor_404[0];  split_tensor_404 = None
    _to_copy_2100 = torch.ops.aten._to_copy.default(getitem_3681, dtype = torch.float32);  getitem_3681 = None
    native_layer_norm_default_431 = torch.ops.aten.native_layer_norm.default(_to_copy_2100, [256], pairformer_stack_blocks_44_transition_pair_layer_norm_weight, pairformer_stack_blocks_44_transition_pair_layer_norm_bias, 1e-05);  _to_copy_2100 = pairformer_stack_blocks_44_transition_pair_layer_norm_weight = pairformer_stack_blocks_44_transition_pair_layer_norm_bias = None
    getitem_3682 = native_layer_norm_default_431[0];  native_layer_norm_default_431 = None
    _to_copy_2101 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_44_transition_pair_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_44_transition_pair_linear_no_bias_ab_weight = None
    _to_copy_2102 = torch.ops.aten._to_copy.default(getitem_3682, dtype = torch.bfloat16);  getitem_3682 = None
    t_787 = torch.ops.aten.t.default(_to_copy_2101);  _to_copy_2101 = None
    view_3762 = torch.ops.aten.view.default(_to_copy_2102, [147456, 256]);  _to_copy_2102 = None
    mm_732 = torch.ops.aten.mm.default(view_3762, t_787);  view_3762 = t_787 = None
    view_3763 = torch.ops.aten.view.default(mm_732, [1, 384, 384, 1024]);  mm_732 = None
    split_tensor_405 = torch.ops.aten.split.Tensor(view_3763, 512, dim = -1);  view_3763 = None
    getitem_3685 = split_tensor_405[0]
    getitem_3686 = split_tensor_405[1];  split_tensor_405 = None
    silu_103 = torch.ops.aten.silu.default(getitem_3685);  getitem_3685 = None
    mul_504 = torch.ops.aten.mul.Tensor(silu_103, getitem_3686);  silu_103 = getitem_3686 = None
    _to_copy_2103 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_44_transition_pair_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_44_transition_pair_linear_out_weight = None
    t_788 = torch.ops.aten.t.default(_to_copy_2103);  _to_copy_2103 = None
    view_3765 = torch.ops.aten.view.default(mul_504, [147456, 512]);  mul_504 = None
    mm_733 = torch.ops.aten.mm.default(view_3765, t_788);  view_3765 = t_788 = None
    view_3766 = torch.ops.aten.view.default(mm_733, [1, 384, 384, 256]);  mm_733 = None
    add_411 = torch.ops.aten.add.Tensor(add_410, view_3766);  add_410 = view_3766 = None
    _to_copy_2104 = torch.ops.aten._to_copy.default(add_407, dtype = torch.float32)
    native_layer_norm_default_432 = torch.ops.aten.native_layer_norm.default(_to_copy_2104, [384], pairformer_stack_blocks_44_attention_pair_bias_single_layer_norm_weight, pairformer_stack_blocks_44_attention_pair_bias_single_layer_norm_bias, 1e-05);  _to_copy_2104 = pairformer_stack_blocks_44_attention_pair_bias_single_layer_norm_weight = pairformer_stack_blocks_44_attention_pair_bias_single_layer_norm_bias = None
    getitem_3687 = native_layer_norm_default_432[0];  native_layer_norm_default_432 = None
    _to_copy_2105 = torch.ops.aten._to_copy.default(add_403, dtype = torch.float32);  add_403 = None
    native_layer_norm_default_433 = torch.ops.aten.native_layer_norm.default(_to_copy_2105, [256], pairformer_stack_blocks_44_attention_pair_bias_pair_layer_norm_weight, pairformer_stack_blocks_44_attention_pair_bias_pair_layer_norm_bias, 1e-05);  _to_copy_2105 = pairformer_stack_blocks_44_attention_pair_bias_pair_layer_norm_weight = pairformer_stack_blocks_44_attention_pair_bias_pair_layer_norm_bias = None
    getitem_3690 = native_layer_norm_default_433[0];  native_layer_norm_default_433 = None
    _to_copy_2106 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_44_attention_pair_bias_pair_linear_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_44_attention_pair_bias_pair_linear_weight = None
    _to_copy_2107 = torch.ops.aten._to_copy.default(getitem_3690, dtype = torch.bfloat16);  getitem_3690 = None
    t_789 = torch.ops.aten.t.default(_to_copy_2106);  _to_copy_2106 = None
    view_3767 = torch.ops.aten.view.default(_to_copy_2107, [147456, 256]);  _to_copy_2107 = None
    mm_734 = torch.ops.aten.mm.default(view_3767, t_789);  view_3767 = t_789 = None
    view_3768 = torch.ops.aten.view.default(mm_734, [1, 384, 384, 16]);  mm_734 = None
    permute_1990 = torch.ops.aten.permute.default(view_3768, [0, 3, 1, 2]);  view_3768 = None
    view_3769 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384])
    bitwise_not_225 = torch.ops.aten.bitwise_not.default(view_3769);  view_3769 = None
    masked_fill_225 = torch.ops.aten.masked_fill.Scalar(permute_1990, bitwise_not_225, -10000);  permute_1990 = bitwise_not_225 = None
    _to_copy_2108 = torch.ops.aten._to_copy.default(getitem_3687, dtype = torch.bfloat16);  getitem_3687 = None
    _to_copy_2109 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_44_attention_pair_bias_attention_input2qkvg_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_44_attention_pair_bias_attention_input2qkvg_weight = None
    unsqueeze_1203 = torch.ops.aten.unsqueeze.default(_to_copy_2108, 3);  _to_copy_2108 = None
    unsqueeze_1204 = torch.ops.aten.unsqueeze.default(unsqueeze_1203, 4);  unsqueeze_1203 = None
    unsqueeze_1205 = torch.ops.aten.unsqueeze.default(unsqueeze_1204, 5);  unsqueeze_1204 = None
    permute_1991 = torch.ops.aten.permute.default(unsqueeze_1205, [3, 0, 4, 1, 5, 2]);  unsqueeze_1205 = None
    unsqueeze_1206 = torch.ops.aten.unsqueeze.default(_to_copy_2109, 4);  _to_copy_2109 = None
    unsqueeze_1207 = torch.ops.aten.unsqueeze.default(unsqueeze_1206, 5);  unsqueeze_1206 = None
    permute_1992 = torch.ops.aten.permute.default(unsqueeze_1207, [1, 4, 2, 5, 3, 0]);  unsqueeze_1207 = None
    permute_1993 = torch.ops.aten.permute.default(permute_1991, [3, 5, 0, 1, 2, 4]);  permute_1991 = None
    view_3770 = torch.ops.aten.view.default(permute_1993, [1, 384, 384]);  permute_1993 = None
    permute_1994 = torch.ops.aten.permute.default(permute_1992, [5, 0, 1, 2, 4, 3]);  permute_1992 = None
    view_3771 = torch.ops.aten.view.default(permute_1994, [1, 384, 1536]);  permute_1994 = None
    bmm_295 = torch.ops.aten.bmm.default(view_3770, view_3771);  view_3770 = view_3771 = None
    view_3772 = torch.ops.aten.view.default(bmm_295, [384, 1, 4, 1, 16, 24]);  bmm_295 = None
    permute_1995 = torch.ops.aten.permute.default(view_3772, [2, 3, 4, 0, 5, 1]);  view_3772 = None
    view_3773 = torch.ops.aten.view.default(permute_1995, [4, 1, 16, 384, 24]);  permute_1995 = None
    unbind_int_168 = torch.ops.aten.unbind.int(view_3773);  view_3773 = None
    getitem_3693 = unbind_int_168[0]
    getitem_3694 = unbind_int_168[1]
    getitem_3695 = unbind_int_168[2]
    getitem_3696 = unbind_int_168[3];  unbind_int_168 = None
    view_3774 = torch.ops.aten.view.default(pairformer_stack_blocks_44_attention_pair_bias_attention_query_bias, [1, 16, 1, 24]);  pairformer_stack_blocks_44_attention_pair_bias_attention_query_bias = None
    add_412 = torch.ops.aten.add.Tensor(getitem_3693, view_3774);  getitem_3693 = view_3774 = None
    _to_copy_2110 = torch.ops.aten._to_copy.default(add_412, dtype = torch.bfloat16);  add_412 = None
    expand_251 = torch.ops.aten.expand.default(masked_fill_225, [1, 16, 384, 384]);  masked_fill_225 = None
    _scaled_dot_product_efficient_attention_default_146 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_2110, getitem_3694, getitem_3695, expand_251, False);  _to_copy_2110 = getitem_3694 = getitem_3695 = expand_251 = None
    getitem_3697 = _scaled_dot_product_efficient_attention_default_146[0];  _scaled_dot_product_efficient_attention_default_146 = None
    add_413 = torch.ops.aten.add.Tensor(getitem_3696, 1);  getitem_3696 = None
    sigmoid_305 = torch.ops.aten.sigmoid.default(add_413);  add_413 = None
    mul_505 = torch.ops.aten.mul.Tensor(getitem_3697, sigmoid_305);  getitem_3697 = sigmoid_305 = None
    _to_copy_2111 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_44_attention_pair_bias_attention_output_proj_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_44_attention_pair_bias_attention_output_proj_weight = None
    unsqueeze_1208 = torch.ops.aten.unsqueeze.default(mul_505, 4);  mul_505 = None
    permute_1996 = torch.ops.aten.permute.default(unsqueeze_1208, [0, 2, 4, 3, 1]);  unsqueeze_1208 = None
    unsqueeze_1209 = torch.ops.aten.unsqueeze.default(_to_copy_2111, 3);  _to_copy_2111 = None
    unsqueeze_1210 = torch.ops.aten.unsqueeze.default(unsqueeze_1209, 4);  unsqueeze_1209 = None
    permute_1997 = torch.ops.aten.permute.default(unsqueeze_1210, [3, 4, 2, 1, 0]);  unsqueeze_1210 = None
    permute_1998 = torch.ops.aten.permute.default(permute_1996, [1, 3, 4, 0, 2]);  permute_1996 = None
    clone_312 = torch.ops.aten.clone.default(permute_1998, memory_format = torch.contiguous_format);  permute_1998 = None
    _unsafe_view_260 = torch.ops.aten._unsafe_view.default(clone_312, [1, 384, 384]);  clone_312 = None
    permute_1999 = torch.ops.aten.permute.default(permute_1997, [3, 4, 0, 2, 1]);  permute_1997 = None
    clone_313 = torch.ops.aten.clone.default(permute_1999, memory_format = torch.contiguous_format);  permute_1999 = None
    _unsafe_view_261 = torch.ops.aten._unsafe_view.default(clone_313, [1, 384, 384]);  clone_313 = None
    bmm_296 = torch.ops.aten.bmm.default(_unsafe_view_260, _unsafe_view_261);  _unsafe_view_260 = _unsafe_view_261 = None
    view_3775 = torch.ops.aten.view.default(bmm_296, [384, 1, 1, 1, 384]);  bmm_296 = None
    permute_2000 = torch.ops.aten.permute.default(view_3775, [3, 0, 4, 1, 2]);  view_3775 = None
    view_3776 = torch.ops.aten.view.default(permute_2000, [1, 384, 384]);  permute_2000 = None
    unsqueeze_1211 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
    mul_506 = torch.ops.aten.mul.Tensor(view_3776, unsqueeze_1211);  view_3776 = unsqueeze_1211 = None
    add_414 = torch.ops.aten.add.Tensor(add_407, mul_506);  mul_506 = None
    split_tensor_406 = torch.ops.aten.split.Tensor(add_407, 384, dim = -2);  add_407 = None
    getitem_3701 = split_tensor_406[0];  split_tensor_406 = None
    _to_copy_2112 = torch.ops.aten._to_copy.default(getitem_3701, dtype = torch.float32);  getitem_3701 = None
    native_layer_norm_default_434 = torch.ops.aten.native_layer_norm.default(_to_copy_2112, [384], pairformer_stack_blocks_44_transition_single_layer_norm_weight, pairformer_stack_blocks_44_transition_single_layer_norm_bias, 1e-05);  _to_copy_2112 = pairformer_stack_blocks_44_transition_single_layer_norm_weight = pairformer_stack_blocks_44_transition_single_layer_norm_bias = None
    getitem_3702 = native_layer_norm_default_434[0];  native_layer_norm_default_434 = None
    _to_copy_2113 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_44_transition_single_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_44_transition_single_linear_no_bias_ab_weight = None
    _to_copy_2114 = torch.ops.aten._to_copy.default(getitem_3702, dtype = torch.bfloat16);  getitem_3702 = None
    t_790 = torch.ops.aten.t.default(_to_copy_2113);  _to_copy_2113 = None
    view_3777 = torch.ops.aten.view.default(_to_copy_2114, [384, 384]);  _to_copy_2114 = None
    mm_735 = torch.ops.aten.mm.default(view_3777, t_790);  view_3777 = t_790 = None
    view_3778 = torch.ops.aten.view.default(mm_735, [1, 384, 1536]);  mm_735 = None
    split_tensor_407 = torch.ops.aten.split.Tensor(view_3778, 768, dim = -1);  view_3778 = None
    getitem_3705 = split_tensor_407[0]
    getitem_3706 = split_tensor_407[1];  split_tensor_407 = None
    silu_104 = torch.ops.aten.silu.default(getitem_3705);  getitem_3705 = None
    mul_507 = torch.ops.aten.mul.Tensor(silu_104, getitem_3706);  silu_104 = getitem_3706 = None
    _to_copy_2115 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_44_transition_single_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_44_transition_single_linear_out_weight = None
    t_791 = torch.ops.aten.t.default(_to_copy_2115);  _to_copy_2115 = None
    view_3780 = torch.ops.aten.view.default(mul_507, [384, 768]);  mul_507 = None
    mm_736 = torch.ops.aten.mm.default(view_3780, t_791);  view_3780 = t_791 = None
    view_3781 = torch.ops.aten.view.default(mm_736, [1, 384, 384]);  mm_736 = None
    add_415 = torch.ops.aten.add.Tensor(add_414, view_3781);  add_414 = view_3781 = None
    _to_copy_2116 = torch.ops.aten._to_copy.default(add_411, dtype = torch.float32)
    native_layer_norm_default_435 = torch.ops.aten.native_layer_norm.default(_to_copy_2116, [256], pairformer_stack_blocks_45_triangle_multiplication_layernorm_z_in_weight, pairformer_stack_blocks_45_triangle_multiplication_layernorm_z_in_bias, 1e-05);  _to_copy_2116 = pairformer_stack_blocks_45_triangle_multiplication_layernorm_z_in_weight = pairformer_stack_blocks_45_triangle_multiplication_layernorm_z_in_bias = None
    getitem_3707 = native_layer_norm_default_435[0];  native_layer_norm_default_435 = None
    split_with_sizes_default_102 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_45_triangle_multiplication_merged_linear_p_weight, [512, 512]);  pairformer_stack_blocks_45_triangle_multiplication_merged_linear_p_weight = None
    getitem_3710 = split_with_sizes_default_102[0]
    getitem_3711 = split_with_sizes_default_102[1];  split_with_sizes_default_102 = None
    split_with_sizes_default_103 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_45_triangle_multiplication_merged_linear_g_weight, [512, 512, 256]);  pairformer_stack_blocks_45_triangle_multiplication_merged_linear_g_weight = None
    getitem_3712 = split_with_sizes_default_103[0]
    getitem_3713 = split_with_sizes_default_103[1]
    getitem_3714 = split_with_sizes_default_103[2];  split_with_sizes_default_103 = None
    _to_copy_2117 = torch.ops.aten._to_copy.default(getitem_3710, dtype = torch.bfloat16);  getitem_3710 = None
    _to_copy_2118 = torch.ops.aten._to_copy.default(getitem_3707, dtype = torch.bfloat16)
    t_792 = torch.ops.aten.t.default(_to_copy_2117);  _to_copy_2117 = None
    view_3782 = torch.ops.aten.view.default(_to_copy_2118, [147456, 256]);  _to_copy_2118 = None
    mm_737 = torch.ops.aten.mm.default(view_3782, t_792);  view_3782 = t_792 = None
    view_3783 = torch.ops.aten.view.default(mm_737, [1, 384, 384, 512]);  mm_737 = None
    _to_copy_2119 = torch.ops.aten._to_copy.default(getitem_3712, dtype = torch.bfloat16);  getitem_3712 = None
    _to_copy_2120 = torch.ops.aten._to_copy.default(getitem_3707, dtype = torch.bfloat16)
    t_793 = torch.ops.aten.t.default(_to_copy_2119);  _to_copy_2119 = None
    view_3784 = torch.ops.aten.view.default(_to_copy_2120, [147456, 256]);  _to_copy_2120 = None
    mm_738 = torch.ops.aten.mm.default(view_3784, t_793);  view_3784 = t_793 = None
    view_3785 = torch.ops.aten.view.default(mm_738, [1, 384, 384, 512]);  mm_738 = None
    sigmoid_306 = torch.ops.aten.sigmoid.default(view_3785);  view_3785 = None
    mul_508 = torch.ops.aten.mul.Tensor(view_3783, sigmoid_306);  view_3783 = sigmoid_306 = None
    unsqueeze_1212 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
    bitwise_not_226 = torch.ops.aten.bitwise_not.default(unsqueeze_1212);  unsqueeze_1212 = None
    masked_fill_226 = torch.ops.aten.masked_fill.Scalar(mul_508, bitwise_not_226, 0);  mul_508 = bitwise_not_226 = None
    split_tensor_408 = torch.ops.aten.split.Tensor(masked_fill_226, 256, dim = -1)
    getitem_3717 = split_tensor_408[0];  split_tensor_408 = None
    unsqueeze_1215 = torch.ops.aten.unsqueeze.default(getitem_3717, 4);  getitem_3717 = None
    permute_2005 = torch.ops.aten.permute.default(unsqueeze_1215, [0, 1, 4, 3, 2]);  unsqueeze_1215 = None
    permute_2006 = torch.ops.aten.permute.default(permute_2005, [3, 1, 4, 0, 2]);  permute_2005 = None
    view_3788 = torch.ops.aten.view.default(permute_2006, [256, 384, 384]);  permute_2006 = None
    split_tensor_409 = torch.ops.aten.split.Tensor(masked_fill_226, 256, dim = -1);  masked_fill_226 = None
    getitem_3720 = split_tensor_409[1];  split_tensor_409 = None
    unsqueeze_1216 = torch.ops.aten.unsqueeze.default(getitem_3720, 4);  getitem_3720 = None
    permute_2007 = torch.ops.aten.permute.default(unsqueeze_1216, [0, 4, 1, 3, 2]);  unsqueeze_1216 = None
    permute_2008 = torch.ops.aten.permute.default(permute_2007, [3, 4, 0, 2, 1]);  permute_2007 = None
    view_3789 = torch.ops.aten.view.default(permute_2008, [256, 384, 384]);  permute_2008 = None
    bmm_297 = torch.ops.aten.bmm.default(view_3788, view_3789);  view_3788 = view_3789 = None
    view_3790 = torch.ops.aten.view.default(bmm_297, [256, 384, 1, 1, 384]);  bmm_297 = None
    permute_2009 = torch.ops.aten.permute.default(view_3790, [3, 1, 4, 0, 2]);  view_3790 = None
    view_3791 = torch.ops.aten.view.default(permute_2009, [1, 384, 384, 256]);  permute_2009 = None
    _to_copy_2121 = torch.ops.aten._to_copy.default(getitem_3711, dtype = torch.bfloat16);  getitem_3711 = None
    _to_copy_2122 = torch.ops.aten._to_copy.default(getitem_3707, dtype = torch.bfloat16)
    t_794 = torch.ops.aten.t.default(_to_copy_2121);  _to_copy_2121 = None
    view_3792 = torch.ops.aten.view.default(_to_copy_2122, [147456, 256]);  _to_copy_2122 = None
    mm_739 = torch.ops.aten.mm.default(view_3792, t_794);  view_3792 = t_794 = None
    view_3793 = torch.ops.aten.view.default(mm_739, [1, 384, 384, 512]);  mm_739 = None
    _to_copy_2123 = torch.ops.aten._to_copy.default(getitem_3713, dtype = torch.bfloat16);  getitem_3713 = None
    _to_copy_2124 = torch.ops.aten._to_copy.default(getitem_3707, dtype = torch.bfloat16)
    t_795 = torch.ops.aten.t.default(_to_copy_2123);  _to_copy_2123 = None
    view_3794 = torch.ops.aten.view.default(_to_copy_2124, [147456, 256]);  _to_copy_2124 = None
    mm_740 = torch.ops.aten.mm.default(view_3794, t_795);  view_3794 = t_795 = None
    view_3795 = torch.ops.aten.view.default(mm_740, [1, 384, 384, 512]);  mm_740 = None
    sigmoid_307 = torch.ops.aten.sigmoid.default(view_3795);  view_3795 = None
    mul_509 = torch.ops.aten.mul.Tensor(view_3793, sigmoid_307);  view_3793 = sigmoid_307 = None
    view_3796 = torch.ops.aten.view.default(mul_509, [147456, 512]);  mul_509 = None
    view_3797 = torch.ops.aten.view.default(view_3796, [1, 384, 384, 512]);  view_3796 = None
    transpose_102 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
    unsqueeze_1217 = torch.ops.aten.unsqueeze.default(transpose_102, 3);  transpose_102 = None
    clone_314 = torch.ops.aten.clone.default(unsqueeze_1217, memory_format = torch.contiguous_format);  unsqueeze_1217 = None
    bitwise_not_227 = torch.ops.aten.bitwise_not.default(clone_314);  clone_314 = None
    masked_fill_227 = torch.ops.aten.masked_fill.Scalar(view_3797, bitwise_not_227, 0);  view_3797 = bitwise_not_227 = None
    view_3798 = torch.ops.aten.view.default(masked_fill_227, [147456, 512]);  masked_fill_227 = None
    view_3802 = torch.ops.aten.view.default(view_3798, [1, 384, 384, 512])
    split_tensor_410 = torch.ops.aten.split.Tensor(view_3802, 256, dim = -1);  view_3802 = None
    getitem_3723 = split_tensor_410[0];  split_tensor_410 = None
    unsqueeze_1220 = torch.ops.aten.unsqueeze.default(getitem_3723, 4);  getitem_3723 = None
    permute_2014 = torch.ops.aten.permute.default(unsqueeze_1220, [0, 2, 4, 3, 1]);  unsqueeze_1220 = None
    permute_2015 = torch.ops.aten.permute.default(permute_2014, [3, 1, 4, 0, 2]);  permute_2014 = None
    view_3803 = torch.ops.aten.view.default(permute_2015, [256, 384, 384]);  permute_2015 = None
    view_3804 = torch.ops.aten.view.default(view_3798, [1, 384, 384, 512]);  view_3798 = None
    split_tensor_411 = torch.ops.aten.split.Tensor(view_3804, 256, dim = -1);  view_3804 = None
    getitem_3726 = split_tensor_411[1];  split_tensor_411 = None
    unsqueeze_1221 = torch.ops.aten.unsqueeze.default(getitem_3726, 4);  getitem_3726 = None
    permute_2016 = torch.ops.aten.permute.default(unsqueeze_1221, [0, 4, 2, 3, 1]);  unsqueeze_1221 = None
    permute_2017 = torch.ops.aten.permute.default(permute_2016, [3, 4, 0, 2, 1]);  permute_2016 = None
    view_3805 = torch.ops.aten.view.default(permute_2017, [256, 384, 384]);  permute_2017 = None
    bmm_298 = torch.ops.aten.bmm.default(view_3803, view_3805);  view_3803 = view_3805 = None
    view_3806 = torch.ops.aten.view.default(bmm_298, [256, 384, 1, 1, 384]);  bmm_298 = None
    permute_2018 = torch.ops.aten.permute.default(view_3806, [3, 1, 4, 0, 2]);  view_3806 = None
    view_3807 = torch.ops.aten.view.default(permute_2018, [1, 384, 384, 256]);  permute_2018 = None
    _to_copy_2125 = torch.ops.aten._to_copy.default(view_3791, dtype = torch.float32);  view_3791 = None
    native_layer_norm_default_436 = torch.ops.aten.native_layer_norm.default(_to_copy_2125, [256], None, None, 1e-05);  _to_copy_2125 = None
    getitem_3727 = native_layer_norm_default_436[0];  native_layer_norm_default_436 = None
    _to_copy_2126 = torch.ops.aten._to_copy.default(view_3807, dtype = torch.float32);  view_3807 = None
    native_layer_norm_default_437 = torch.ops.aten.native_layer_norm.default(_to_copy_2126, [256], None, None, 1e-05);  _to_copy_2126 = None
    getitem_3730 = native_layer_norm_default_437[0];  native_layer_norm_default_437 = None
    add_416 = torch.ops.aten.add.Tensor(getitem_3727, getitem_3730);  getitem_3727 = getitem_3730 = None
    _to_copy_2127 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_45_triangle_multiplication_linear_z_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_45_triangle_multiplication_linear_z_out_weight = None
    _to_copy_2128 = torch.ops.aten._to_copy.default(add_416, dtype = torch.bfloat16);  add_416 = None
    t_796 = torch.ops.aten.t.default(_to_copy_2127);  _to_copy_2127 = None
    view_3808 = torch.ops.aten.view.default(_to_copy_2128, [147456, 256]);  _to_copy_2128 = None
    mm_741 = torch.ops.aten.mm.default(view_3808, t_796);  view_3808 = t_796 = None
    view_3809 = torch.ops.aten.view.default(mm_741, [1, 384, 384, 256]);  mm_741 = None
    _to_copy_2129 = torch.ops.aten._to_copy.default(getitem_3714, dtype = torch.bfloat16);  getitem_3714 = None
    _to_copy_2130 = torch.ops.aten._to_copy.default(getitem_3707, dtype = torch.bfloat16);  getitem_3707 = None
    t_797 = torch.ops.aten.t.default(_to_copy_2129);  _to_copy_2129 = None
    view_3810 = torch.ops.aten.view.default(_to_copy_2130, [147456, 256]);  _to_copy_2130 = None
    mm_742 = torch.ops.aten.mm.default(view_3810, t_797);  view_3810 = t_797 = None
    view_3811 = torch.ops.aten.view.default(mm_742, [1, 384, 384, 256]);  mm_742 = None
    sigmoid_308 = torch.ops.aten.sigmoid.default(view_3811);  view_3811 = None
    mul_510 = torch.ops.aten.mul.Tensor(view_3809, sigmoid_308);  view_3809 = sigmoid_308 = None
    add_417 = torch.ops.aten.add.Tensor(add_411, mul_510);  mul_510 = None
    _to_copy_2131 = torch.ops.aten._to_copy.default(add_411, dtype = torch.float32)
    native_layer_norm_default_438 = torch.ops.aten.native_layer_norm.default(_to_copy_2131, [256], None, None, 1e-05);  _to_copy_2131 = None
    getitem_3733 = native_layer_norm_default_438[0];  native_layer_norm_default_438 = None
    _to_copy_2132 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_45_triangle_attention_pair2b_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_45_triangle_attention_pair2b_weight = None
    _to_copy_2133 = torch.ops.aten._to_copy.default(getitem_3733, dtype = torch.bfloat16)
    t_798 = torch.ops.aten.t.default(_to_copy_2132);  _to_copy_2132 = None
    view_3812 = torch.ops.aten.view.default(_to_copy_2133, [147456, 256]);  _to_copy_2133 = None
    mm_743 = torch.ops.aten.mm.default(view_3812, t_798);  view_3812 = t_798 = None
    view_3813 = torch.ops.aten.view.default(mm_743, [1, 384, 384, 8]);  mm_743 = None
    view_3814 = torch.ops.aten.view.default(view_3813, [1, 384, 384, 2, 4]);  view_3813 = None
    permute_2019 = torch.ops.aten.permute.default(view_3814, [0, 3, 4, 1, 2]);  view_3814 = None
    view_3815 = torch.ops.aten.view.default(permute_2019, [1, 2, 4, 1, 384, 384]);  permute_2019 = None
    view_3816 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 384, 384])
    bitwise_not_228 = torch.ops.aten.bitwise_not.default(view_3816);  view_3816 = None
    masked_fill_228 = torch.ops.aten.masked_fill.Scalar(view_3815, bitwise_not_228, -10000);  view_3815 = bitwise_not_228 = None
    view_3817 = torch.ops.aten.view.default(masked_fill_228, [1, 2, 4, 384, 384]);  masked_fill_228 = None
    permute_2020 = torch.ops.aten.permute.default(view_3817, [1, 0, 2, 3, 4]);  view_3817 = None
    view_3818 = torch.ops.aten.view.default(permute_2020, [2, 4, 1, 384, 384]);  permute_2020 = None
    _to_copy_2134 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_45_triangle_attention_pair2qkvg1_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_45_triangle_attention_pair2qkvg1_weight = None
    _to_copy_2135 = torch.ops.aten._to_copy.default(getitem_3733, dtype = torch.bfloat16)
    t_799 = torch.ops.aten.t.default(_to_copy_2134);  _to_copy_2134 = None
    view_3819 = torch.ops.aten.view.default(_to_copy_2135, [147456, 256]);  _to_copy_2135 = None
    mm_744 = torch.ops.aten.mm.default(view_3819, t_799);  view_3819 = t_799 = None
    view_3820 = torch.ops.aten.view.default(mm_744, [1, 384, 384, 1024]);  mm_744 = None
    select_103 = torch.ops.aten.select.int(view_3818, 0, 0)
    view_3821 = torch.ops.aten.view.default(view_3820, [1, 384, 384, 4, 4, 64]);  view_3820 = None
    permute_2021 = torch.ops.aten.permute.default(view_3821, [4, 0, 3, 1, 2, 5]);  view_3821 = None
    view_3822 = torch.ops.aten.view.default(permute_2021, [4, 4, 384, 384, 64]);  permute_2021 = None
    unbind_int_169 = torch.ops.aten.unbind.int(view_3822);  view_3822 = None
    getitem_3736 = unbind_int_169[0]
    getitem_3737 = unbind_int_169[1]
    getitem_3738 = unbind_int_169[2]
    getitem_3739 = unbind_int_169[3];  unbind_int_169 = None
    expand_252 = torch.ops.aten.expand.default(select_103, [4, 384, 384, 384]);  select_103 = None
    _scaled_dot_product_efficient_attention_default_147 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3736, getitem_3737, getitem_3738, expand_252, False);  getitem_3736 = getitem_3737 = getitem_3738 = expand_252 = None
    getitem_3740 = _scaled_dot_product_efficient_attention_default_147[0];  _scaled_dot_product_efficient_attention_default_147 = None
    sigmoid_309 = torch.ops.aten.sigmoid.default(getitem_3739);  getitem_3739 = None
    mul_511 = torch.ops.aten.mul.Tensor(getitem_3740, sigmoid_309);  getitem_3740 = sigmoid_309 = None
    view_3823 = torch.ops.aten.view.default(mul_511, [1, 4, 384, 384, 64]);  mul_511 = None
    permute_2022 = torch.ops.aten.permute.default(view_3823, [0, 2, 3, 1, 4]);  view_3823 = None
    clone_315 = torch.ops.aten.clone.default(permute_2022, memory_format = torch.contiguous_format);  permute_2022 = None
    _unsafe_view_262 = torch.ops.aten._unsafe_view.default(clone_315, [1, 384, 384, 256]);  clone_315 = None
    transpose_103 = torch.ops.aten.transpose.int(getitem_3733, 1, 2);  getitem_3733 = None
    _to_copy_2136 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_45_triangle_attention_pair2qkvg2_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_45_triangle_attention_pair2qkvg2_weight = None
    _to_copy_2137 = torch.ops.aten._to_copy.default(transpose_103, dtype = torch.bfloat16);  transpose_103 = None
    t_800 = torch.ops.aten.t.default(_to_copy_2136);  _to_copy_2136 = None
    expand_253 = torch.ops.aten.expand.default(_to_copy_2137, [1, 384, 384, 256]);  _to_copy_2137 = None
    view_3824 = torch.ops.aten.view.default(expand_253, [384, 384, 256]);  expand_253 = None
    expand_254 = torch.ops.aten.expand.default(t_800, [1, 384, 256, 1024]);  t_800 = None
    view_3825 = torch.ops.aten.view.default(expand_254, [384, 256, 1024]);  expand_254 = None
    bmm_299 = torch.ops.aten.bmm.default(view_3824, view_3825);  view_3824 = view_3825 = None
    view_3826 = torch.ops.aten.view.default(bmm_299, [1, 384, 384, 1024]);  bmm_299 = None
    select_104 = torch.ops.aten.select.int(view_3818, 0, 1);  view_3818 = None
    view_3827 = torch.ops.aten.view.default(view_3826, [1, 384, 384, 4, 4, 64]);  view_3826 = None
    permute_2023 = torch.ops.aten.permute.default(view_3827, [4, 0, 3, 1, 2, 5]);  view_3827 = None
    view_3828 = torch.ops.aten.view.default(permute_2023, [4, 4, 384, 384, 64]);  permute_2023 = None
    unbind_int_170 = torch.ops.aten.unbind.int(view_3828);  view_3828 = None
    getitem_3744 = unbind_int_170[0]
    getitem_3745 = unbind_int_170[1]
    getitem_3746 = unbind_int_170[2]
    getitem_3747 = unbind_int_170[3];  unbind_int_170 = None
    expand_255 = torch.ops.aten.expand.default(select_104, [4, 384, 384, 384]);  select_104 = None
    _scaled_dot_product_efficient_attention_default_148 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3744, getitem_3745, getitem_3746, expand_255, False);  getitem_3744 = getitem_3745 = getitem_3746 = expand_255 = None
    getitem_3748 = _scaled_dot_product_efficient_attention_default_148[0];  _scaled_dot_product_efficient_attention_default_148 = None
    sigmoid_310 = torch.ops.aten.sigmoid.default(getitem_3747);  getitem_3747 = None
    mul_512 = torch.ops.aten.mul.Tensor(getitem_3748, sigmoid_310);  getitem_3748 = sigmoid_310 = None
    view_3829 = torch.ops.aten.view.default(mul_512, [1, 4, 384, 384, 64]);  mul_512 = None
    permute_2024 = torch.ops.aten.permute.default(view_3829, [0, 2, 3, 1, 4]);  view_3829 = None
    clone_316 = torch.ops.aten.clone.default(permute_2024, memory_format = torch.contiguous_format);  permute_2024 = None
    _unsafe_view_263 = torch.ops.aten._unsafe_view.default(clone_316, [1, 384, 384, 256]);  clone_316 = None
    cat_57 = torch.ops.aten.cat.default([_unsafe_view_262, _unsafe_view_263], dim = -1);  _unsafe_view_262 = _unsafe_view_263 = None
    slice_208 = torch.ops.aten.slice.Tensor(pairformer_stack_blocks_45_triangle_attention_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  pairformer_stack_blocks_45_triangle_attention_out_scalers = None
    unsqueeze_1222 = torch.ops.aten.unsqueeze.default(slice_208, 1);  slice_208 = None
    mul_513 = torch.ops.aten.mul.Tensor(pairformer_stack_blocks_45_triangle_attention_linear_out_weight, unsqueeze_1222);  pairformer_stack_blocks_45_triangle_attention_linear_out_weight = unsqueeze_1222 = None
    _to_copy_2138 = torch.ops.aten._to_copy.default(mul_513, dtype = torch.bfloat16);  mul_513 = None
    t_801 = torch.ops.aten.t.default(_to_copy_2138);  _to_copy_2138 = None
    view_3830 = torch.ops.aten.view.default(cat_57, [147456, 512]);  cat_57 = None
    mm_745 = torch.ops.aten.mm.default(view_3830, t_801);  view_3830 = t_801 = None
    view_3831 = torch.ops.aten.view.default(mm_745, [1, 384, 384, 256]);  mm_745 = None
    add_418 = torch.ops.aten.add.Tensor(add_417, view_3831);  add_417 = view_3831 = None
    split_tensor_412 = torch.ops.aten.split.Tensor(add_411, 384, dim = -2)
    getitem_3752 = split_tensor_412[0];  split_tensor_412 = None
    _to_copy_2139 = torch.ops.aten._to_copy.default(getitem_3752, dtype = torch.float32);  getitem_3752 = None
    native_layer_norm_default_439 = torch.ops.aten.native_layer_norm.default(_to_copy_2139, [256], pairformer_stack_blocks_45_transition_pair_layer_norm_weight, pairformer_stack_blocks_45_transition_pair_layer_norm_bias, 1e-05);  _to_copy_2139 = pairformer_stack_blocks_45_transition_pair_layer_norm_weight = pairformer_stack_blocks_45_transition_pair_layer_norm_bias = None
    getitem_3753 = native_layer_norm_default_439[0];  native_layer_norm_default_439 = None
    _to_copy_2140 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_45_transition_pair_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_45_transition_pair_linear_no_bias_ab_weight = None
    _to_copy_2141 = torch.ops.aten._to_copy.default(getitem_3753, dtype = torch.bfloat16);  getitem_3753 = None
    t_802 = torch.ops.aten.t.default(_to_copy_2140);  _to_copy_2140 = None
    view_3832 = torch.ops.aten.view.default(_to_copy_2141, [147456, 256]);  _to_copy_2141 = None
    mm_746 = torch.ops.aten.mm.default(view_3832, t_802);  view_3832 = t_802 = None
    view_3833 = torch.ops.aten.view.default(mm_746, [1, 384, 384, 1024]);  mm_746 = None
    split_tensor_413 = torch.ops.aten.split.Tensor(view_3833, 512, dim = -1);  view_3833 = None
    getitem_3756 = split_tensor_413[0]
    getitem_3757 = split_tensor_413[1];  split_tensor_413 = None
    silu_105 = torch.ops.aten.silu.default(getitem_3756);  getitem_3756 = None
    mul_514 = torch.ops.aten.mul.Tensor(silu_105, getitem_3757);  silu_105 = getitem_3757 = None
    _to_copy_2142 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_45_transition_pair_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_45_transition_pair_linear_out_weight = None
    t_803 = torch.ops.aten.t.default(_to_copy_2142);  _to_copy_2142 = None
    view_3835 = torch.ops.aten.view.default(mul_514, [147456, 512]);  mul_514 = None
    mm_747 = torch.ops.aten.mm.default(view_3835, t_803);  view_3835 = t_803 = None
    view_3836 = torch.ops.aten.view.default(mm_747, [1, 384, 384, 256]);  mm_747 = None
    add_419 = torch.ops.aten.add.Tensor(add_418, view_3836);  add_418 = view_3836 = None
    _to_copy_2143 = torch.ops.aten._to_copy.default(add_415, dtype = torch.float32)
    native_layer_norm_default_440 = torch.ops.aten.native_layer_norm.default(_to_copy_2143, [384], pairformer_stack_blocks_45_attention_pair_bias_single_layer_norm_weight, pairformer_stack_blocks_45_attention_pair_bias_single_layer_norm_bias, 1e-05);  _to_copy_2143 = pairformer_stack_blocks_45_attention_pair_bias_single_layer_norm_weight = pairformer_stack_blocks_45_attention_pair_bias_single_layer_norm_bias = None
    getitem_3758 = native_layer_norm_default_440[0];  native_layer_norm_default_440 = None
    _to_copy_2144 = torch.ops.aten._to_copy.default(add_411, dtype = torch.float32);  add_411 = None
    native_layer_norm_default_441 = torch.ops.aten.native_layer_norm.default(_to_copy_2144, [256], pairformer_stack_blocks_45_attention_pair_bias_pair_layer_norm_weight, pairformer_stack_blocks_45_attention_pair_bias_pair_layer_norm_bias, 1e-05);  _to_copy_2144 = pairformer_stack_blocks_45_attention_pair_bias_pair_layer_norm_weight = pairformer_stack_blocks_45_attention_pair_bias_pair_layer_norm_bias = None
    getitem_3761 = native_layer_norm_default_441[0];  native_layer_norm_default_441 = None
    _to_copy_2145 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_45_attention_pair_bias_pair_linear_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_45_attention_pair_bias_pair_linear_weight = None
    _to_copy_2146 = torch.ops.aten._to_copy.default(getitem_3761, dtype = torch.bfloat16);  getitem_3761 = None
    t_804 = torch.ops.aten.t.default(_to_copy_2145);  _to_copy_2145 = None
    view_3837 = torch.ops.aten.view.default(_to_copy_2146, [147456, 256]);  _to_copy_2146 = None
    mm_748 = torch.ops.aten.mm.default(view_3837, t_804);  view_3837 = t_804 = None
    view_3838 = torch.ops.aten.view.default(mm_748, [1, 384, 384, 16]);  mm_748 = None
    permute_2025 = torch.ops.aten.permute.default(view_3838, [0, 3, 1, 2]);  view_3838 = None
    view_3839 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384])
    bitwise_not_229 = torch.ops.aten.bitwise_not.default(view_3839);  view_3839 = None
    masked_fill_229 = torch.ops.aten.masked_fill.Scalar(permute_2025, bitwise_not_229, -10000);  permute_2025 = bitwise_not_229 = None
    _to_copy_2147 = torch.ops.aten._to_copy.default(getitem_3758, dtype = torch.bfloat16);  getitem_3758 = None
    _to_copy_2148 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_45_attention_pair_bias_attention_input2qkvg_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_45_attention_pair_bias_attention_input2qkvg_weight = None
    unsqueeze_1223 = torch.ops.aten.unsqueeze.default(_to_copy_2147, 3);  _to_copy_2147 = None
    unsqueeze_1224 = torch.ops.aten.unsqueeze.default(unsqueeze_1223, 4);  unsqueeze_1223 = None
    unsqueeze_1225 = torch.ops.aten.unsqueeze.default(unsqueeze_1224, 5);  unsqueeze_1224 = None
    permute_2026 = torch.ops.aten.permute.default(unsqueeze_1225, [3, 0, 4, 1, 5, 2]);  unsqueeze_1225 = None
    unsqueeze_1226 = torch.ops.aten.unsqueeze.default(_to_copy_2148, 4);  _to_copy_2148 = None
    unsqueeze_1227 = torch.ops.aten.unsqueeze.default(unsqueeze_1226, 5);  unsqueeze_1226 = None
    permute_2027 = torch.ops.aten.permute.default(unsqueeze_1227, [1, 4, 2, 5, 3, 0]);  unsqueeze_1227 = None
    permute_2028 = torch.ops.aten.permute.default(permute_2026, [3, 5, 0, 1, 2, 4]);  permute_2026 = None
    view_3840 = torch.ops.aten.view.default(permute_2028, [1, 384, 384]);  permute_2028 = None
    permute_2029 = torch.ops.aten.permute.default(permute_2027, [5, 0, 1, 2, 4, 3]);  permute_2027 = None
    view_3841 = torch.ops.aten.view.default(permute_2029, [1, 384, 1536]);  permute_2029 = None
    bmm_300 = torch.ops.aten.bmm.default(view_3840, view_3841);  view_3840 = view_3841 = None
    view_3842 = torch.ops.aten.view.default(bmm_300, [384, 1, 4, 1, 16, 24]);  bmm_300 = None
    permute_2030 = torch.ops.aten.permute.default(view_3842, [2, 3, 4, 0, 5, 1]);  view_3842 = None
    view_3843 = torch.ops.aten.view.default(permute_2030, [4, 1, 16, 384, 24]);  permute_2030 = None
    unbind_int_171 = torch.ops.aten.unbind.int(view_3843);  view_3843 = None
    getitem_3764 = unbind_int_171[0]
    getitem_3765 = unbind_int_171[1]
    getitem_3766 = unbind_int_171[2]
    getitem_3767 = unbind_int_171[3];  unbind_int_171 = None
    view_3844 = torch.ops.aten.view.default(pairformer_stack_blocks_45_attention_pair_bias_attention_query_bias, [1, 16, 1, 24]);  pairformer_stack_blocks_45_attention_pair_bias_attention_query_bias = None
    add_420 = torch.ops.aten.add.Tensor(getitem_3764, view_3844);  getitem_3764 = view_3844 = None
    _to_copy_2149 = torch.ops.aten._to_copy.default(add_420, dtype = torch.bfloat16);  add_420 = None
    expand_256 = torch.ops.aten.expand.default(masked_fill_229, [1, 16, 384, 384]);  masked_fill_229 = None
    _scaled_dot_product_efficient_attention_default_149 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_2149, getitem_3765, getitem_3766, expand_256, False);  _to_copy_2149 = getitem_3765 = getitem_3766 = expand_256 = None
    getitem_3768 = _scaled_dot_product_efficient_attention_default_149[0];  _scaled_dot_product_efficient_attention_default_149 = None
    add_421 = torch.ops.aten.add.Tensor(getitem_3767, 1);  getitem_3767 = None
    sigmoid_311 = torch.ops.aten.sigmoid.default(add_421);  add_421 = None
    mul_515 = torch.ops.aten.mul.Tensor(getitem_3768, sigmoid_311);  getitem_3768 = sigmoid_311 = None
    _to_copy_2150 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_45_attention_pair_bias_attention_output_proj_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_45_attention_pair_bias_attention_output_proj_weight = None
    unsqueeze_1228 = torch.ops.aten.unsqueeze.default(mul_515, 4);  mul_515 = None
    permute_2031 = torch.ops.aten.permute.default(unsqueeze_1228, [0, 2, 4, 3, 1]);  unsqueeze_1228 = None
    unsqueeze_1229 = torch.ops.aten.unsqueeze.default(_to_copy_2150, 3);  _to_copy_2150 = None
    unsqueeze_1230 = torch.ops.aten.unsqueeze.default(unsqueeze_1229, 4);  unsqueeze_1229 = None
    permute_2032 = torch.ops.aten.permute.default(unsqueeze_1230, [3, 4, 2, 1, 0]);  unsqueeze_1230 = None
    permute_2033 = torch.ops.aten.permute.default(permute_2031, [1, 3, 4, 0, 2]);  permute_2031 = None
    clone_317 = torch.ops.aten.clone.default(permute_2033, memory_format = torch.contiguous_format);  permute_2033 = None
    _unsafe_view_264 = torch.ops.aten._unsafe_view.default(clone_317, [1, 384, 384]);  clone_317 = None
    permute_2034 = torch.ops.aten.permute.default(permute_2032, [3, 4, 0, 2, 1]);  permute_2032 = None
    clone_318 = torch.ops.aten.clone.default(permute_2034, memory_format = torch.contiguous_format);  permute_2034 = None
    _unsafe_view_265 = torch.ops.aten._unsafe_view.default(clone_318, [1, 384, 384]);  clone_318 = None
    bmm_301 = torch.ops.aten.bmm.default(_unsafe_view_264, _unsafe_view_265);  _unsafe_view_264 = _unsafe_view_265 = None
    view_3845 = torch.ops.aten.view.default(bmm_301, [384, 1, 1, 1, 384]);  bmm_301 = None
    permute_2035 = torch.ops.aten.permute.default(view_3845, [3, 0, 4, 1, 2]);  view_3845 = None
    view_3846 = torch.ops.aten.view.default(permute_2035, [1, 384, 384]);  permute_2035 = None
    unsqueeze_1231 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
    mul_516 = torch.ops.aten.mul.Tensor(view_3846, unsqueeze_1231);  view_3846 = unsqueeze_1231 = None
    add_422 = torch.ops.aten.add.Tensor(add_415, mul_516);  mul_516 = None
    split_tensor_414 = torch.ops.aten.split.Tensor(add_415, 384, dim = -2);  add_415 = None
    getitem_3772 = split_tensor_414[0];  split_tensor_414 = None
    _to_copy_2151 = torch.ops.aten._to_copy.default(getitem_3772, dtype = torch.float32);  getitem_3772 = None
    native_layer_norm_default_442 = torch.ops.aten.native_layer_norm.default(_to_copy_2151, [384], pairformer_stack_blocks_45_transition_single_layer_norm_weight, pairformer_stack_blocks_45_transition_single_layer_norm_bias, 1e-05);  _to_copy_2151 = pairformer_stack_blocks_45_transition_single_layer_norm_weight = pairformer_stack_blocks_45_transition_single_layer_norm_bias = None
    getitem_3773 = native_layer_norm_default_442[0];  native_layer_norm_default_442 = None
    _to_copy_2152 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_45_transition_single_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_45_transition_single_linear_no_bias_ab_weight = None
    _to_copy_2153 = torch.ops.aten._to_copy.default(getitem_3773, dtype = torch.bfloat16);  getitem_3773 = None
    t_805 = torch.ops.aten.t.default(_to_copy_2152);  _to_copy_2152 = None
    view_3847 = torch.ops.aten.view.default(_to_copy_2153, [384, 384]);  _to_copy_2153 = None
    mm_749 = torch.ops.aten.mm.default(view_3847, t_805);  view_3847 = t_805 = None
    view_3848 = torch.ops.aten.view.default(mm_749, [1, 384, 1536]);  mm_749 = None
    split_tensor_415 = torch.ops.aten.split.Tensor(view_3848, 768, dim = -1);  view_3848 = None
    getitem_3776 = split_tensor_415[0]
    getitem_3777 = split_tensor_415[1];  split_tensor_415 = None
    silu_106 = torch.ops.aten.silu.default(getitem_3776);  getitem_3776 = None
    mul_517 = torch.ops.aten.mul.Tensor(silu_106, getitem_3777);  silu_106 = getitem_3777 = None
    _to_copy_2154 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_45_transition_single_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_45_transition_single_linear_out_weight = None
    t_806 = torch.ops.aten.t.default(_to_copy_2154);  _to_copy_2154 = None
    view_3850 = torch.ops.aten.view.default(mul_517, [384, 768]);  mul_517 = None
    mm_750 = torch.ops.aten.mm.default(view_3850, t_806);  view_3850 = t_806 = None
    view_3851 = torch.ops.aten.view.default(mm_750, [1, 384, 384]);  mm_750 = None
    add_423 = torch.ops.aten.add.Tensor(add_422, view_3851);  add_422 = view_3851 = None
    _to_copy_2155 = torch.ops.aten._to_copy.default(add_419, dtype = torch.float32)
    native_layer_norm_default_443 = torch.ops.aten.native_layer_norm.default(_to_copy_2155, [256], pairformer_stack_blocks_46_triangle_multiplication_layernorm_z_in_weight, pairformer_stack_blocks_46_triangle_multiplication_layernorm_z_in_bias, 1e-05);  _to_copy_2155 = pairformer_stack_blocks_46_triangle_multiplication_layernorm_z_in_weight = pairformer_stack_blocks_46_triangle_multiplication_layernorm_z_in_bias = None
    getitem_3778 = native_layer_norm_default_443[0];  native_layer_norm_default_443 = None
    split_with_sizes_default_104 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_46_triangle_multiplication_merged_linear_p_weight, [512, 512]);  pairformer_stack_blocks_46_triangle_multiplication_merged_linear_p_weight = None
    getitem_3781 = split_with_sizes_default_104[0]
    getitem_3782 = split_with_sizes_default_104[1];  split_with_sizes_default_104 = None
    split_with_sizes_default_105 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_46_triangle_multiplication_merged_linear_g_weight, [512, 512, 256]);  pairformer_stack_blocks_46_triangle_multiplication_merged_linear_g_weight = None
    getitem_3783 = split_with_sizes_default_105[0]
    getitem_3784 = split_with_sizes_default_105[1]
    getitem_3785 = split_with_sizes_default_105[2];  split_with_sizes_default_105 = None
    _to_copy_2156 = torch.ops.aten._to_copy.default(getitem_3781, dtype = torch.bfloat16);  getitem_3781 = None
    _to_copy_2157 = torch.ops.aten._to_copy.default(getitem_3778, dtype = torch.bfloat16)
    t_807 = torch.ops.aten.t.default(_to_copy_2156);  _to_copy_2156 = None
    view_3852 = torch.ops.aten.view.default(_to_copy_2157, [147456, 256]);  _to_copy_2157 = None
    mm_751 = torch.ops.aten.mm.default(view_3852, t_807);  view_3852 = t_807 = None
    view_3853 = torch.ops.aten.view.default(mm_751, [1, 384, 384, 512]);  mm_751 = None
    _to_copy_2158 = torch.ops.aten._to_copy.default(getitem_3783, dtype = torch.bfloat16);  getitem_3783 = None
    _to_copy_2159 = torch.ops.aten._to_copy.default(getitem_3778, dtype = torch.bfloat16)
    t_808 = torch.ops.aten.t.default(_to_copy_2158);  _to_copy_2158 = None
    view_3854 = torch.ops.aten.view.default(_to_copy_2159, [147456, 256]);  _to_copy_2159 = None
    mm_752 = torch.ops.aten.mm.default(view_3854, t_808);  view_3854 = t_808 = None
    view_3855 = torch.ops.aten.view.default(mm_752, [1, 384, 384, 512]);  mm_752 = None
    sigmoid_312 = torch.ops.aten.sigmoid.default(view_3855);  view_3855 = None
    mul_518 = torch.ops.aten.mul.Tensor(view_3853, sigmoid_312);  view_3853 = sigmoid_312 = None
    unsqueeze_1232 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
    bitwise_not_230 = torch.ops.aten.bitwise_not.default(unsqueeze_1232);  unsqueeze_1232 = None
    masked_fill_230 = torch.ops.aten.masked_fill.Scalar(mul_518, bitwise_not_230, 0);  mul_518 = bitwise_not_230 = None
    split_tensor_416 = torch.ops.aten.split.Tensor(masked_fill_230, 256, dim = -1)
    getitem_3788 = split_tensor_416[0];  split_tensor_416 = None
    unsqueeze_1235 = torch.ops.aten.unsqueeze.default(getitem_3788, 4);  getitem_3788 = None
    permute_2040 = torch.ops.aten.permute.default(unsqueeze_1235, [0, 1, 4, 3, 2]);  unsqueeze_1235 = None
    permute_2041 = torch.ops.aten.permute.default(permute_2040, [3, 1, 4, 0, 2]);  permute_2040 = None
    view_3858 = torch.ops.aten.view.default(permute_2041, [256, 384, 384]);  permute_2041 = None
    split_tensor_417 = torch.ops.aten.split.Tensor(masked_fill_230, 256, dim = -1);  masked_fill_230 = None
    getitem_3791 = split_tensor_417[1];  split_tensor_417 = None
    unsqueeze_1236 = torch.ops.aten.unsqueeze.default(getitem_3791, 4);  getitem_3791 = None
    permute_2042 = torch.ops.aten.permute.default(unsqueeze_1236, [0, 4, 1, 3, 2]);  unsqueeze_1236 = None
    permute_2043 = torch.ops.aten.permute.default(permute_2042, [3, 4, 0, 2, 1]);  permute_2042 = None
    view_3859 = torch.ops.aten.view.default(permute_2043, [256, 384, 384]);  permute_2043 = None
    bmm_302 = torch.ops.aten.bmm.default(view_3858, view_3859);  view_3858 = view_3859 = None
    view_3860 = torch.ops.aten.view.default(bmm_302, [256, 384, 1, 1, 384]);  bmm_302 = None
    permute_2044 = torch.ops.aten.permute.default(view_3860, [3, 1, 4, 0, 2]);  view_3860 = None
    view_3861 = torch.ops.aten.view.default(permute_2044, [1, 384, 384, 256]);  permute_2044 = None
    _to_copy_2160 = torch.ops.aten._to_copy.default(getitem_3782, dtype = torch.bfloat16);  getitem_3782 = None
    _to_copy_2161 = torch.ops.aten._to_copy.default(getitem_3778, dtype = torch.bfloat16)
    t_809 = torch.ops.aten.t.default(_to_copy_2160);  _to_copy_2160 = None
    view_3862 = torch.ops.aten.view.default(_to_copy_2161, [147456, 256]);  _to_copy_2161 = None
    mm_753 = torch.ops.aten.mm.default(view_3862, t_809);  view_3862 = t_809 = None
    view_3863 = torch.ops.aten.view.default(mm_753, [1, 384, 384, 512]);  mm_753 = None
    _to_copy_2162 = torch.ops.aten._to_copy.default(getitem_3784, dtype = torch.bfloat16);  getitem_3784 = None
    _to_copy_2163 = torch.ops.aten._to_copy.default(getitem_3778, dtype = torch.bfloat16)
    t_810 = torch.ops.aten.t.default(_to_copy_2162);  _to_copy_2162 = None
    view_3864 = torch.ops.aten.view.default(_to_copy_2163, [147456, 256]);  _to_copy_2163 = None
    mm_754 = torch.ops.aten.mm.default(view_3864, t_810);  view_3864 = t_810 = None
    view_3865 = torch.ops.aten.view.default(mm_754, [1, 384, 384, 512]);  mm_754 = None
    sigmoid_313 = torch.ops.aten.sigmoid.default(view_3865);  view_3865 = None
    mul_519 = torch.ops.aten.mul.Tensor(view_3863, sigmoid_313);  view_3863 = sigmoid_313 = None
    view_3866 = torch.ops.aten.view.default(mul_519, [147456, 512]);  mul_519 = None
    view_3867 = torch.ops.aten.view.default(view_3866, [1, 384, 384, 512]);  view_3866 = None
    transpose_104 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
    unsqueeze_1237 = torch.ops.aten.unsqueeze.default(transpose_104, 3);  transpose_104 = None
    clone_319 = torch.ops.aten.clone.default(unsqueeze_1237, memory_format = torch.contiguous_format);  unsqueeze_1237 = None
    bitwise_not_231 = torch.ops.aten.bitwise_not.default(clone_319);  clone_319 = None
    masked_fill_231 = torch.ops.aten.masked_fill.Scalar(view_3867, bitwise_not_231, 0);  view_3867 = bitwise_not_231 = None
    view_3868 = torch.ops.aten.view.default(masked_fill_231, [147456, 512]);  masked_fill_231 = None
    view_3872 = torch.ops.aten.view.default(view_3868, [1, 384, 384, 512])
    split_tensor_418 = torch.ops.aten.split.Tensor(view_3872, 256, dim = -1);  view_3872 = None
    getitem_3794 = split_tensor_418[0];  split_tensor_418 = None
    unsqueeze_1240 = torch.ops.aten.unsqueeze.default(getitem_3794, 4);  getitem_3794 = None
    permute_2049 = torch.ops.aten.permute.default(unsqueeze_1240, [0, 2, 4, 3, 1]);  unsqueeze_1240 = None
    permute_2050 = torch.ops.aten.permute.default(permute_2049, [3, 1, 4, 0, 2]);  permute_2049 = None
    view_3873 = torch.ops.aten.view.default(permute_2050, [256, 384, 384]);  permute_2050 = None
    view_3874 = torch.ops.aten.view.default(view_3868, [1, 384, 384, 512]);  view_3868 = None
    split_tensor_419 = torch.ops.aten.split.Tensor(view_3874, 256, dim = -1);  view_3874 = None
    getitem_3797 = split_tensor_419[1];  split_tensor_419 = None
    unsqueeze_1241 = torch.ops.aten.unsqueeze.default(getitem_3797, 4);  getitem_3797 = None
    permute_2051 = torch.ops.aten.permute.default(unsqueeze_1241, [0, 4, 2, 3, 1]);  unsqueeze_1241 = None
    permute_2052 = torch.ops.aten.permute.default(permute_2051, [3, 4, 0, 2, 1]);  permute_2051 = None
    view_3875 = torch.ops.aten.view.default(permute_2052, [256, 384, 384]);  permute_2052 = None
    bmm_303 = torch.ops.aten.bmm.default(view_3873, view_3875);  view_3873 = view_3875 = None
    view_3876 = torch.ops.aten.view.default(bmm_303, [256, 384, 1, 1, 384]);  bmm_303 = None
    permute_2053 = torch.ops.aten.permute.default(view_3876, [3, 1, 4, 0, 2]);  view_3876 = None
    view_3877 = torch.ops.aten.view.default(permute_2053, [1, 384, 384, 256]);  permute_2053 = None
    _to_copy_2164 = torch.ops.aten._to_copy.default(view_3861, dtype = torch.float32);  view_3861 = None
    native_layer_norm_default_444 = torch.ops.aten.native_layer_norm.default(_to_copy_2164, [256], None, None, 1e-05);  _to_copy_2164 = None
    getitem_3798 = native_layer_norm_default_444[0];  native_layer_norm_default_444 = None
    _to_copy_2165 = torch.ops.aten._to_copy.default(view_3877, dtype = torch.float32);  view_3877 = None
    native_layer_norm_default_445 = torch.ops.aten.native_layer_norm.default(_to_copy_2165, [256], None, None, 1e-05);  _to_copy_2165 = None
    getitem_3801 = native_layer_norm_default_445[0];  native_layer_norm_default_445 = None
    add_424 = torch.ops.aten.add.Tensor(getitem_3798, getitem_3801);  getitem_3798 = getitem_3801 = None
    _to_copy_2166 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_46_triangle_multiplication_linear_z_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_46_triangle_multiplication_linear_z_out_weight = None
    _to_copy_2167 = torch.ops.aten._to_copy.default(add_424, dtype = torch.bfloat16);  add_424 = None
    t_811 = torch.ops.aten.t.default(_to_copy_2166);  _to_copy_2166 = None
    view_3878 = torch.ops.aten.view.default(_to_copy_2167, [147456, 256]);  _to_copy_2167 = None
    mm_755 = torch.ops.aten.mm.default(view_3878, t_811);  view_3878 = t_811 = None
    view_3879 = torch.ops.aten.view.default(mm_755, [1, 384, 384, 256]);  mm_755 = None
    _to_copy_2168 = torch.ops.aten._to_copy.default(getitem_3785, dtype = torch.bfloat16);  getitem_3785 = None
    _to_copy_2169 = torch.ops.aten._to_copy.default(getitem_3778, dtype = torch.bfloat16);  getitem_3778 = None
    t_812 = torch.ops.aten.t.default(_to_copy_2168);  _to_copy_2168 = None
    view_3880 = torch.ops.aten.view.default(_to_copy_2169, [147456, 256]);  _to_copy_2169 = None
    mm_756 = torch.ops.aten.mm.default(view_3880, t_812);  view_3880 = t_812 = None
    view_3881 = torch.ops.aten.view.default(mm_756, [1, 384, 384, 256]);  mm_756 = None
    sigmoid_314 = torch.ops.aten.sigmoid.default(view_3881);  view_3881 = None
    mul_520 = torch.ops.aten.mul.Tensor(view_3879, sigmoid_314);  view_3879 = sigmoid_314 = None
    add_425 = torch.ops.aten.add.Tensor(add_419, mul_520);  mul_520 = None
    _to_copy_2170 = torch.ops.aten._to_copy.default(add_419, dtype = torch.float32)
    native_layer_norm_default_446 = torch.ops.aten.native_layer_norm.default(_to_copy_2170, [256], None, None, 1e-05);  _to_copy_2170 = None
    getitem_3804 = native_layer_norm_default_446[0];  native_layer_norm_default_446 = None
    _to_copy_2171 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_46_triangle_attention_pair2b_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_46_triangle_attention_pair2b_weight = None
    _to_copy_2172 = torch.ops.aten._to_copy.default(getitem_3804, dtype = torch.bfloat16)
    t_813 = torch.ops.aten.t.default(_to_copy_2171);  _to_copy_2171 = None
    view_3882 = torch.ops.aten.view.default(_to_copy_2172, [147456, 256]);  _to_copy_2172 = None
    mm_757 = torch.ops.aten.mm.default(view_3882, t_813);  view_3882 = t_813 = None
    view_3883 = torch.ops.aten.view.default(mm_757, [1, 384, 384, 8]);  mm_757 = None
    view_3884 = torch.ops.aten.view.default(view_3883, [1, 384, 384, 2, 4]);  view_3883 = None
    permute_2054 = torch.ops.aten.permute.default(view_3884, [0, 3, 4, 1, 2]);  view_3884 = None
    view_3885 = torch.ops.aten.view.default(permute_2054, [1, 2, 4, 1, 384, 384]);  permute_2054 = None
    view_3886 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 384, 384])
    bitwise_not_232 = torch.ops.aten.bitwise_not.default(view_3886);  view_3886 = None
    masked_fill_232 = torch.ops.aten.masked_fill.Scalar(view_3885, bitwise_not_232, -10000);  view_3885 = bitwise_not_232 = None
    view_3887 = torch.ops.aten.view.default(masked_fill_232, [1, 2, 4, 384, 384]);  masked_fill_232 = None
    permute_2055 = torch.ops.aten.permute.default(view_3887, [1, 0, 2, 3, 4]);  view_3887 = None
    view_3888 = torch.ops.aten.view.default(permute_2055, [2, 4, 1, 384, 384]);  permute_2055 = None
    _to_copy_2173 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_46_triangle_attention_pair2qkvg1_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_46_triangle_attention_pair2qkvg1_weight = None
    _to_copy_2174 = torch.ops.aten._to_copy.default(getitem_3804, dtype = torch.bfloat16)
    t_814 = torch.ops.aten.t.default(_to_copy_2173);  _to_copy_2173 = None
    view_3889 = torch.ops.aten.view.default(_to_copy_2174, [147456, 256]);  _to_copy_2174 = None
    mm_758 = torch.ops.aten.mm.default(view_3889, t_814);  view_3889 = t_814 = None
    view_3890 = torch.ops.aten.view.default(mm_758, [1, 384, 384, 1024]);  mm_758 = None
    select_105 = torch.ops.aten.select.int(view_3888, 0, 0)
    view_3891 = torch.ops.aten.view.default(view_3890, [1, 384, 384, 4, 4, 64]);  view_3890 = None
    permute_2056 = torch.ops.aten.permute.default(view_3891, [4, 0, 3, 1, 2, 5]);  view_3891 = None
    view_3892 = torch.ops.aten.view.default(permute_2056, [4, 4, 384, 384, 64]);  permute_2056 = None
    unbind_int_172 = torch.ops.aten.unbind.int(view_3892);  view_3892 = None
    getitem_3807 = unbind_int_172[0]
    getitem_3808 = unbind_int_172[1]
    getitem_3809 = unbind_int_172[2]
    getitem_3810 = unbind_int_172[3];  unbind_int_172 = None
    expand_257 = torch.ops.aten.expand.default(select_105, [4, 384, 384, 384]);  select_105 = None
    _scaled_dot_product_efficient_attention_default_150 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3807, getitem_3808, getitem_3809, expand_257, False);  getitem_3807 = getitem_3808 = getitem_3809 = expand_257 = None
    getitem_3811 = _scaled_dot_product_efficient_attention_default_150[0];  _scaled_dot_product_efficient_attention_default_150 = None
    sigmoid_315 = torch.ops.aten.sigmoid.default(getitem_3810);  getitem_3810 = None
    mul_521 = torch.ops.aten.mul.Tensor(getitem_3811, sigmoid_315);  getitem_3811 = sigmoid_315 = None
    view_3893 = torch.ops.aten.view.default(mul_521, [1, 4, 384, 384, 64]);  mul_521 = None
    permute_2057 = torch.ops.aten.permute.default(view_3893, [0, 2, 3, 1, 4]);  view_3893 = None
    clone_320 = torch.ops.aten.clone.default(permute_2057, memory_format = torch.contiguous_format);  permute_2057 = None
    _unsafe_view_266 = torch.ops.aten._unsafe_view.default(clone_320, [1, 384, 384, 256]);  clone_320 = None
    transpose_105 = torch.ops.aten.transpose.int(getitem_3804, 1, 2);  getitem_3804 = None
    _to_copy_2175 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_46_triangle_attention_pair2qkvg2_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_46_triangle_attention_pair2qkvg2_weight = None
    _to_copy_2176 = torch.ops.aten._to_copy.default(transpose_105, dtype = torch.bfloat16);  transpose_105 = None
    t_815 = torch.ops.aten.t.default(_to_copy_2175);  _to_copy_2175 = None
    expand_258 = torch.ops.aten.expand.default(_to_copy_2176, [1, 384, 384, 256]);  _to_copy_2176 = None
    view_3894 = torch.ops.aten.view.default(expand_258, [384, 384, 256]);  expand_258 = None
    expand_259 = torch.ops.aten.expand.default(t_815, [1, 384, 256, 1024]);  t_815 = None
    view_3895 = torch.ops.aten.view.default(expand_259, [384, 256, 1024]);  expand_259 = None
    bmm_304 = torch.ops.aten.bmm.default(view_3894, view_3895);  view_3894 = view_3895 = None
    view_3896 = torch.ops.aten.view.default(bmm_304, [1, 384, 384, 1024]);  bmm_304 = None
    select_106 = torch.ops.aten.select.int(view_3888, 0, 1);  view_3888 = None
    view_3897 = torch.ops.aten.view.default(view_3896, [1, 384, 384, 4, 4, 64]);  view_3896 = None
    permute_2058 = torch.ops.aten.permute.default(view_3897, [4, 0, 3, 1, 2, 5]);  view_3897 = None
    view_3898 = torch.ops.aten.view.default(permute_2058, [4, 4, 384, 384, 64]);  permute_2058 = None
    unbind_int_173 = torch.ops.aten.unbind.int(view_3898);  view_3898 = None
    getitem_3815 = unbind_int_173[0]
    getitem_3816 = unbind_int_173[1]
    getitem_3817 = unbind_int_173[2]
    getitem_3818 = unbind_int_173[3];  unbind_int_173 = None
    expand_260 = torch.ops.aten.expand.default(select_106, [4, 384, 384, 384]);  select_106 = None
    _scaled_dot_product_efficient_attention_default_151 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3815, getitem_3816, getitem_3817, expand_260, False);  getitem_3815 = getitem_3816 = getitem_3817 = expand_260 = None
    getitem_3819 = _scaled_dot_product_efficient_attention_default_151[0];  _scaled_dot_product_efficient_attention_default_151 = None
    sigmoid_316 = torch.ops.aten.sigmoid.default(getitem_3818);  getitem_3818 = None
    mul_522 = torch.ops.aten.mul.Tensor(getitem_3819, sigmoid_316);  getitem_3819 = sigmoid_316 = None
    view_3899 = torch.ops.aten.view.default(mul_522, [1, 4, 384, 384, 64]);  mul_522 = None
    permute_2059 = torch.ops.aten.permute.default(view_3899, [0, 2, 3, 1, 4]);  view_3899 = None
    clone_321 = torch.ops.aten.clone.default(permute_2059, memory_format = torch.contiguous_format);  permute_2059 = None
    _unsafe_view_267 = torch.ops.aten._unsafe_view.default(clone_321, [1, 384, 384, 256]);  clone_321 = None
    cat_58 = torch.ops.aten.cat.default([_unsafe_view_266, _unsafe_view_267], dim = -1);  _unsafe_view_266 = _unsafe_view_267 = None
    slice_209 = torch.ops.aten.slice.Tensor(pairformer_stack_blocks_46_triangle_attention_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  pairformer_stack_blocks_46_triangle_attention_out_scalers = None
    unsqueeze_1242 = torch.ops.aten.unsqueeze.default(slice_209, 1);  slice_209 = None
    mul_523 = torch.ops.aten.mul.Tensor(pairformer_stack_blocks_46_triangle_attention_linear_out_weight, unsqueeze_1242);  pairformer_stack_blocks_46_triangle_attention_linear_out_weight = unsqueeze_1242 = None
    _to_copy_2177 = torch.ops.aten._to_copy.default(mul_523, dtype = torch.bfloat16);  mul_523 = None
    t_816 = torch.ops.aten.t.default(_to_copy_2177);  _to_copy_2177 = None
    view_3900 = torch.ops.aten.view.default(cat_58, [147456, 512]);  cat_58 = None
    mm_759 = torch.ops.aten.mm.default(view_3900, t_816);  view_3900 = t_816 = None
    view_3901 = torch.ops.aten.view.default(mm_759, [1, 384, 384, 256]);  mm_759 = None
    add_426 = torch.ops.aten.add.Tensor(add_425, view_3901);  add_425 = view_3901 = None
    split_tensor_420 = torch.ops.aten.split.Tensor(add_419, 384, dim = -2)
    getitem_3823 = split_tensor_420[0];  split_tensor_420 = None
    _to_copy_2178 = torch.ops.aten._to_copy.default(getitem_3823, dtype = torch.float32);  getitem_3823 = None
    native_layer_norm_default_447 = torch.ops.aten.native_layer_norm.default(_to_copy_2178, [256], pairformer_stack_blocks_46_transition_pair_layer_norm_weight, pairformer_stack_blocks_46_transition_pair_layer_norm_bias, 1e-05);  _to_copy_2178 = pairformer_stack_blocks_46_transition_pair_layer_norm_weight = pairformer_stack_blocks_46_transition_pair_layer_norm_bias = None
    getitem_3824 = native_layer_norm_default_447[0];  native_layer_norm_default_447 = None
    _to_copy_2179 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_46_transition_pair_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_46_transition_pair_linear_no_bias_ab_weight = None
    _to_copy_2180 = torch.ops.aten._to_copy.default(getitem_3824, dtype = torch.bfloat16);  getitem_3824 = None
    t_817 = torch.ops.aten.t.default(_to_copy_2179);  _to_copy_2179 = None
    view_3902 = torch.ops.aten.view.default(_to_copy_2180, [147456, 256]);  _to_copy_2180 = None
    mm_760 = torch.ops.aten.mm.default(view_3902, t_817);  view_3902 = t_817 = None
    view_3903 = torch.ops.aten.view.default(mm_760, [1, 384, 384, 1024]);  mm_760 = None
    split_tensor_421 = torch.ops.aten.split.Tensor(view_3903, 512, dim = -1);  view_3903 = None
    getitem_3827 = split_tensor_421[0]
    getitem_3828 = split_tensor_421[1];  split_tensor_421 = None
    silu_107 = torch.ops.aten.silu.default(getitem_3827);  getitem_3827 = None
    mul_524 = torch.ops.aten.mul.Tensor(silu_107, getitem_3828);  silu_107 = getitem_3828 = None
    _to_copy_2181 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_46_transition_pair_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_46_transition_pair_linear_out_weight = None
    t_818 = torch.ops.aten.t.default(_to_copy_2181);  _to_copy_2181 = None
    view_3905 = torch.ops.aten.view.default(mul_524, [147456, 512]);  mul_524 = None
    mm_761 = torch.ops.aten.mm.default(view_3905, t_818);  view_3905 = t_818 = None
    view_3906 = torch.ops.aten.view.default(mm_761, [1, 384, 384, 256]);  mm_761 = None
    add_427 = torch.ops.aten.add.Tensor(add_426, view_3906);  add_426 = view_3906 = None
    _to_copy_2182 = torch.ops.aten._to_copy.default(add_423, dtype = torch.float32)
    native_layer_norm_default_448 = torch.ops.aten.native_layer_norm.default(_to_copy_2182, [384], pairformer_stack_blocks_46_attention_pair_bias_single_layer_norm_weight, pairformer_stack_blocks_46_attention_pair_bias_single_layer_norm_bias, 1e-05);  _to_copy_2182 = pairformer_stack_blocks_46_attention_pair_bias_single_layer_norm_weight = pairformer_stack_blocks_46_attention_pair_bias_single_layer_norm_bias = None
    getitem_3829 = native_layer_norm_default_448[0];  native_layer_norm_default_448 = None
    _to_copy_2183 = torch.ops.aten._to_copy.default(add_419, dtype = torch.float32);  add_419 = None
    native_layer_norm_default_449 = torch.ops.aten.native_layer_norm.default(_to_copy_2183, [256], pairformer_stack_blocks_46_attention_pair_bias_pair_layer_norm_weight, pairformer_stack_blocks_46_attention_pair_bias_pair_layer_norm_bias, 1e-05);  _to_copy_2183 = pairformer_stack_blocks_46_attention_pair_bias_pair_layer_norm_weight = pairformer_stack_blocks_46_attention_pair_bias_pair_layer_norm_bias = None
    getitem_3832 = native_layer_norm_default_449[0];  native_layer_norm_default_449 = None
    _to_copy_2184 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_46_attention_pair_bias_pair_linear_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_46_attention_pair_bias_pair_linear_weight = None
    _to_copy_2185 = torch.ops.aten._to_copy.default(getitem_3832, dtype = torch.bfloat16);  getitem_3832 = None
    t_819 = torch.ops.aten.t.default(_to_copy_2184);  _to_copy_2184 = None
    view_3907 = torch.ops.aten.view.default(_to_copy_2185, [147456, 256]);  _to_copy_2185 = None
    mm_762 = torch.ops.aten.mm.default(view_3907, t_819);  view_3907 = t_819 = None
    view_3908 = torch.ops.aten.view.default(mm_762, [1, 384, 384, 16]);  mm_762 = None
    permute_2060 = torch.ops.aten.permute.default(view_3908, [0, 3, 1, 2]);  view_3908 = None
    view_3909 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384])
    bitwise_not_233 = torch.ops.aten.bitwise_not.default(view_3909);  view_3909 = None
    masked_fill_233 = torch.ops.aten.masked_fill.Scalar(permute_2060, bitwise_not_233, -10000);  permute_2060 = bitwise_not_233 = None
    _to_copy_2186 = torch.ops.aten._to_copy.default(getitem_3829, dtype = torch.bfloat16);  getitem_3829 = None
    _to_copy_2187 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_46_attention_pair_bias_attention_input2qkvg_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_46_attention_pair_bias_attention_input2qkvg_weight = None
    unsqueeze_1243 = torch.ops.aten.unsqueeze.default(_to_copy_2186, 3);  _to_copy_2186 = None
    unsqueeze_1244 = torch.ops.aten.unsqueeze.default(unsqueeze_1243, 4);  unsqueeze_1243 = None
    unsqueeze_1245 = torch.ops.aten.unsqueeze.default(unsqueeze_1244, 5);  unsqueeze_1244 = None
    permute_2061 = torch.ops.aten.permute.default(unsqueeze_1245, [3, 0, 4, 1, 5, 2]);  unsqueeze_1245 = None
    unsqueeze_1246 = torch.ops.aten.unsqueeze.default(_to_copy_2187, 4);  _to_copy_2187 = None
    unsqueeze_1247 = torch.ops.aten.unsqueeze.default(unsqueeze_1246, 5);  unsqueeze_1246 = None
    permute_2062 = torch.ops.aten.permute.default(unsqueeze_1247, [1, 4, 2, 5, 3, 0]);  unsqueeze_1247 = None
    permute_2063 = torch.ops.aten.permute.default(permute_2061, [3, 5, 0, 1, 2, 4]);  permute_2061 = None
    view_3910 = torch.ops.aten.view.default(permute_2063, [1, 384, 384]);  permute_2063 = None
    permute_2064 = torch.ops.aten.permute.default(permute_2062, [5, 0, 1, 2, 4, 3]);  permute_2062 = None
    view_3911 = torch.ops.aten.view.default(permute_2064, [1, 384, 1536]);  permute_2064 = None
    bmm_305 = torch.ops.aten.bmm.default(view_3910, view_3911);  view_3910 = view_3911 = None
    view_3912 = torch.ops.aten.view.default(bmm_305, [384, 1, 4, 1, 16, 24]);  bmm_305 = None
    permute_2065 = torch.ops.aten.permute.default(view_3912, [2, 3, 4, 0, 5, 1]);  view_3912 = None
    view_3913 = torch.ops.aten.view.default(permute_2065, [4, 1, 16, 384, 24]);  permute_2065 = None
    unbind_int_174 = torch.ops.aten.unbind.int(view_3913);  view_3913 = None
    getitem_3835 = unbind_int_174[0]
    getitem_3836 = unbind_int_174[1]
    getitem_3837 = unbind_int_174[2]
    getitem_3838 = unbind_int_174[3];  unbind_int_174 = None
    view_3914 = torch.ops.aten.view.default(pairformer_stack_blocks_46_attention_pair_bias_attention_query_bias, [1, 16, 1, 24]);  pairformer_stack_blocks_46_attention_pair_bias_attention_query_bias = None
    add_428 = torch.ops.aten.add.Tensor(getitem_3835, view_3914);  getitem_3835 = view_3914 = None
    _to_copy_2188 = torch.ops.aten._to_copy.default(add_428, dtype = torch.bfloat16);  add_428 = None
    expand_261 = torch.ops.aten.expand.default(masked_fill_233, [1, 16, 384, 384]);  masked_fill_233 = None
    _scaled_dot_product_efficient_attention_default_152 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_2188, getitem_3836, getitem_3837, expand_261, False);  _to_copy_2188 = getitem_3836 = getitem_3837 = expand_261 = None
    getitem_3839 = _scaled_dot_product_efficient_attention_default_152[0];  _scaled_dot_product_efficient_attention_default_152 = None
    add_429 = torch.ops.aten.add.Tensor(getitem_3838, 1);  getitem_3838 = None
    sigmoid_317 = torch.ops.aten.sigmoid.default(add_429);  add_429 = None
    mul_525 = torch.ops.aten.mul.Tensor(getitem_3839, sigmoid_317);  getitem_3839 = sigmoid_317 = None
    _to_copy_2189 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_46_attention_pair_bias_attention_output_proj_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_46_attention_pair_bias_attention_output_proj_weight = None
    unsqueeze_1248 = torch.ops.aten.unsqueeze.default(mul_525, 4);  mul_525 = None
    permute_2066 = torch.ops.aten.permute.default(unsqueeze_1248, [0, 2, 4, 3, 1]);  unsqueeze_1248 = None
    unsqueeze_1249 = torch.ops.aten.unsqueeze.default(_to_copy_2189, 3);  _to_copy_2189 = None
    unsqueeze_1250 = torch.ops.aten.unsqueeze.default(unsqueeze_1249, 4);  unsqueeze_1249 = None
    permute_2067 = torch.ops.aten.permute.default(unsqueeze_1250, [3, 4, 2, 1, 0]);  unsqueeze_1250 = None
    permute_2068 = torch.ops.aten.permute.default(permute_2066, [1, 3, 4, 0, 2]);  permute_2066 = None
    clone_322 = torch.ops.aten.clone.default(permute_2068, memory_format = torch.contiguous_format);  permute_2068 = None
    _unsafe_view_268 = torch.ops.aten._unsafe_view.default(clone_322, [1, 384, 384]);  clone_322 = None
    permute_2069 = torch.ops.aten.permute.default(permute_2067, [3, 4, 0, 2, 1]);  permute_2067 = None
    clone_323 = torch.ops.aten.clone.default(permute_2069, memory_format = torch.contiguous_format);  permute_2069 = None
    _unsafe_view_269 = torch.ops.aten._unsafe_view.default(clone_323, [1, 384, 384]);  clone_323 = None
    bmm_306 = torch.ops.aten.bmm.default(_unsafe_view_268, _unsafe_view_269);  _unsafe_view_268 = _unsafe_view_269 = None
    view_3915 = torch.ops.aten.view.default(bmm_306, [384, 1, 1, 1, 384]);  bmm_306 = None
    permute_2070 = torch.ops.aten.permute.default(view_3915, [3, 0, 4, 1, 2]);  view_3915 = None
    view_3916 = torch.ops.aten.view.default(permute_2070, [1, 384, 384]);  permute_2070 = None
    unsqueeze_1251 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
    mul_526 = torch.ops.aten.mul.Tensor(view_3916, unsqueeze_1251);  view_3916 = unsqueeze_1251 = None
    add_430 = torch.ops.aten.add.Tensor(add_423, mul_526);  mul_526 = None
    split_tensor_422 = torch.ops.aten.split.Tensor(add_423, 384, dim = -2);  add_423 = None
    getitem_3843 = split_tensor_422[0];  split_tensor_422 = None
    _to_copy_2190 = torch.ops.aten._to_copy.default(getitem_3843, dtype = torch.float32);  getitem_3843 = None
    native_layer_norm_default_450 = torch.ops.aten.native_layer_norm.default(_to_copy_2190, [384], pairformer_stack_blocks_46_transition_single_layer_norm_weight, pairformer_stack_blocks_46_transition_single_layer_norm_bias, 1e-05);  _to_copy_2190 = pairformer_stack_blocks_46_transition_single_layer_norm_weight = pairformer_stack_blocks_46_transition_single_layer_norm_bias = None
    getitem_3844 = native_layer_norm_default_450[0];  native_layer_norm_default_450 = None
    _to_copy_2191 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_46_transition_single_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_46_transition_single_linear_no_bias_ab_weight = None
    _to_copy_2192 = torch.ops.aten._to_copy.default(getitem_3844, dtype = torch.bfloat16);  getitem_3844 = None
    t_820 = torch.ops.aten.t.default(_to_copy_2191);  _to_copy_2191 = None
    view_3917 = torch.ops.aten.view.default(_to_copy_2192, [384, 384]);  _to_copy_2192 = None
    mm_763 = torch.ops.aten.mm.default(view_3917, t_820);  view_3917 = t_820 = None
    view_3918 = torch.ops.aten.view.default(mm_763, [1, 384, 1536]);  mm_763 = None
    split_tensor_423 = torch.ops.aten.split.Tensor(view_3918, 768, dim = -1);  view_3918 = None
    getitem_3847 = split_tensor_423[0]
    getitem_3848 = split_tensor_423[1];  split_tensor_423 = None
    silu_108 = torch.ops.aten.silu.default(getitem_3847);  getitem_3847 = None
    mul_527 = torch.ops.aten.mul.Tensor(silu_108, getitem_3848);  silu_108 = getitem_3848 = None
    _to_copy_2193 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_46_transition_single_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_46_transition_single_linear_out_weight = None
    t_821 = torch.ops.aten.t.default(_to_copy_2193);  _to_copy_2193 = None
    view_3920 = torch.ops.aten.view.default(mul_527, [384, 768]);  mul_527 = None
    mm_764 = torch.ops.aten.mm.default(view_3920, t_821);  view_3920 = t_821 = None
    view_3921 = torch.ops.aten.view.default(mm_764, [1, 384, 384]);  mm_764 = None
    add_431 = torch.ops.aten.add.Tensor(add_430, view_3921);  add_430 = view_3921 = None
    _to_copy_2194 = torch.ops.aten._to_copy.default(add_427, dtype = torch.float32)
    native_layer_norm_default_451 = torch.ops.aten.native_layer_norm.default(_to_copy_2194, [256], pairformer_stack_blocks_47_triangle_multiplication_layernorm_z_in_weight, pairformer_stack_blocks_47_triangle_multiplication_layernorm_z_in_bias, 1e-05);  _to_copy_2194 = pairformer_stack_blocks_47_triangle_multiplication_layernorm_z_in_weight = pairformer_stack_blocks_47_triangle_multiplication_layernorm_z_in_bias = None
    getitem_3849 = native_layer_norm_default_451[0];  native_layer_norm_default_451 = None
    split_with_sizes_default_106 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_47_triangle_multiplication_merged_linear_p_weight, [512, 512]);  pairformer_stack_blocks_47_triangle_multiplication_merged_linear_p_weight = None
    getitem_3852 = split_with_sizes_default_106[0]
    getitem_3853 = split_with_sizes_default_106[1];  split_with_sizes_default_106 = None
    split_with_sizes_default_107 = torch.ops.aten.split_with_sizes.default(pairformer_stack_blocks_47_triangle_multiplication_merged_linear_g_weight, [512, 512, 256]);  pairformer_stack_blocks_47_triangle_multiplication_merged_linear_g_weight = None
    getitem_3854 = split_with_sizes_default_107[0]
    getitem_3855 = split_with_sizes_default_107[1]
    getitem_3856 = split_with_sizes_default_107[2];  split_with_sizes_default_107 = None
    _to_copy_2195 = torch.ops.aten._to_copy.default(getitem_3852, dtype = torch.bfloat16);  getitem_3852 = None
    _to_copy_2196 = torch.ops.aten._to_copy.default(getitem_3849, dtype = torch.bfloat16)
    t_822 = torch.ops.aten.t.default(_to_copy_2195);  _to_copy_2195 = None
    view_3922 = torch.ops.aten.view.default(_to_copy_2196, [147456, 256]);  _to_copy_2196 = None
    mm_765 = torch.ops.aten.mm.default(view_3922, t_822);  view_3922 = t_822 = None
    view_3923 = torch.ops.aten.view.default(mm_765, [1, 384, 384, 512]);  mm_765 = None
    _to_copy_2197 = torch.ops.aten._to_copy.default(getitem_3854, dtype = torch.bfloat16);  getitem_3854 = None
    _to_copy_2198 = torch.ops.aten._to_copy.default(getitem_3849, dtype = torch.bfloat16)
    t_823 = torch.ops.aten.t.default(_to_copy_2197);  _to_copy_2197 = None
    view_3924 = torch.ops.aten.view.default(_to_copy_2198, [147456, 256]);  _to_copy_2198 = None
    mm_766 = torch.ops.aten.mm.default(view_3924, t_823);  view_3924 = t_823 = None
    view_3925 = torch.ops.aten.view.default(mm_766, [1, 384, 384, 512]);  mm_766 = None
    sigmoid_318 = torch.ops.aten.sigmoid.default(view_3925);  view_3925 = None
    mul_528 = torch.ops.aten.mul.Tensor(view_3923, sigmoid_318);  view_3923 = sigmoid_318 = None
    unsqueeze_1252 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
    bitwise_not_234 = torch.ops.aten.bitwise_not.default(unsqueeze_1252);  unsqueeze_1252 = None
    masked_fill_234 = torch.ops.aten.masked_fill.Scalar(mul_528, bitwise_not_234, 0);  mul_528 = bitwise_not_234 = None
    split_tensor_424 = torch.ops.aten.split.Tensor(masked_fill_234, 256, dim = -1)
    getitem_3859 = split_tensor_424[0];  split_tensor_424 = None
    unsqueeze_1255 = torch.ops.aten.unsqueeze.default(getitem_3859, 4);  getitem_3859 = None
    permute_2075 = torch.ops.aten.permute.default(unsqueeze_1255, [0, 1, 4, 3, 2]);  unsqueeze_1255 = None
    permute_2076 = torch.ops.aten.permute.default(permute_2075, [3, 1, 4, 0, 2]);  permute_2075 = None
    view_3928 = torch.ops.aten.view.default(permute_2076, [256, 384, 384]);  permute_2076 = None
    split_tensor_425 = torch.ops.aten.split.Tensor(masked_fill_234, 256, dim = -1);  masked_fill_234 = None
    getitem_3862 = split_tensor_425[1];  split_tensor_425 = None
    unsqueeze_1256 = torch.ops.aten.unsqueeze.default(getitem_3862, 4);  getitem_3862 = None
    permute_2077 = torch.ops.aten.permute.default(unsqueeze_1256, [0, 4, 1, 3, 2]);  unsqueeze_1256 = None
    permute_2078 = torch.ops.aten.permute.default(permute_2077, [3, 4, 0, 2, 1]);  permute_2077 = None
    view_3929 = torch.ops.aten.view.default(permute_2078, [256, 384, 384]);  permute_2078 = None
    bmm_307 = torch.ops.aten.bmm.default(view_3928, view_3929);  view_3928 = view_3929 = None
    view_3930 = torch.ops.aten.view.default(bmm_307, [256, 384, 1, 1, 384]);  bmm_307 = None
    permute_2079 = torch.ops.aten.permute.default(view_3930, [3, 1, 4, 0, 2]);  view_3930 = None
    view_3931 = torch.ops.aten.view.default(permute_2079, [1, 384, 384, 256]);  permute_2079 = None
    _to_copy_2199 = torch.ops.aten._to_copy.default(getitem_3853, dtype = torch.bfloat16);  getitem_3853 = None
    _to_copy_2200 = torch.ops.aten._to_copy.default(getitem_3849, dtype = torch.bfloat16)
    t_824 = torch.ops.aten.t.default(_to_copy_2199);  _to_copy_2199 = None
    view_3932 = torch.ops.aten.view.default(_to_copy_2200, [147456, 256]);  _to_copy_2200 = None
    mm_767 = torch.ops.aten.mm.default(view_3932, t_824);  view_3932 = t_824 = None
    view_3933 = torch.ops.aten.view.default(mm_767, [1, 384, 384, 512]);  mm_767 = None
    _to_copy_2201 = torch.ops.aten._to_copy.default(getitem_3855, dtype = torch.bfloat16);  getitem_3855 = None
    _to_copy_2202 = torch.ops.aten._to_copy.default(getitem_3849, dtype = torch.bfloat16)
    t_825 = torch.ops.aten.t.default(_to_copy_2201);  _to_copy_2201 = None
    view_3934 = torch.ops.aten.view.default(_to_copy_2202, [147456, 256]);  _to_copy_2202 = None
    mm_768 = torch.ops.aten.mm.default(view_3934, t_825);  view_3934 = t_825 = None
    view_3935 = torch.ops.aten.view.default(mm_768, [1, 384, 384, 512]);  mm_768 = None
    sigmoid_319 = torch.ops.aten.sigmoid.default(view_3935);  view_3935 = None
    mul_529 = torch.ops.aten.mul.Tensor(view_3933, sigmoid_319);  view_3933 = sigmoid_319 = None
    view_3936 = torch.ops.aten.view.default(mul_529, [147456, 512]);  mul_529 = None
    view_3937 = torch.ops.aten.view.default(view_3936, [1, 384, 384, 512]);  view_3936 = None
    transpose_106 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
    unsqueeze_1257 = torch.ops.aten.unsqueeze.default(transpose_106, 3);  transpose_106 = None
    clone_324 = torch.ops.aten.clone.default(unsqueeze_1257, memory_format = torch.contiguous_format);  unsqueeze_1257 = None
    bitwise_not_235 = torch.ops.aten.bitwise_not.default(clone_324);  clone_324 = None
    masked_fill_235 = torch.ops.aten.masked_fill.Scalar(view_3937, bitwise_not_235, 0);  view_3937 = bitwise_not_235 = None
    view_3938 = torch.ops.aten.view.default(masked_fill_235, [147456, 512]);  masked_fill_235 = None
    view_3942 = torch.ops.aten.view.default(view_3938, [1, 384, 384, 512])
    split_tensor_426 = torch.ops.aten.split.Tensor(view_3942, 256, dim = -1);  view_3942 = None
    getitem_3865 = split_tensor_426[0];  split_tensor_426 = None
    unsqueeze_1260 = torch.ops.aten.unsqueeze.default(getitem_3865, 4);  getitem_3865 = None
    permute_2084 = torch.ops.aten.permute.default(unsqueeze_1260, [0, 2, 4, 3, 1]);  unsqueeze_1260 = None
    permute_2085 = torch.ops.aten.permute.default(permute_2084, [3, 1, 4, 0, 2]);  permute_2084 = None
    view_3943 = torch.ops.aten.view.default(permute_2085, [256, 384, 384]);  permute_2085 = None
    view_3944 = torch.ops.aten.view.default(view_3938, [1, 384, 384, 512]);  view_3938 = None
    split_tensor_427 = torch.ops.aten.split.Tensor(view_3944, 256, dim = -1);  view_3944 = None
    getitem_3868 = split_tensor_427[1];  split_tensor_427 = None
    unsqueeze_1261 = torch.ops.aten.unsqueeze.default(getitem_3868, 4);  getitem_3868 = None
    permute_2086 = torch.ops.aten.permute.default(unsqueeze_1261, [0, 4, 2, 3, 1]);  unsqueeze_1261 = None
    permute_2087 = torch.ops.aten.permute.default(permute_2086, [3, 4, 0, 2, 1]);  permute_2086 = None
    view_3945 = torch.ops.aten.view.default(permute_2087, [256, 384, 384]);  permute_2087 = None
    bmm_308 = torch.ops.aten.bmm.default(view_3943, view_3945);  view_3943 = view_3945 = None
    view_3946 = torch.ops.aten.view.default(bmm_308, [256, 384, 1, 1, 384]);  bmm_308 = None
    permute_2088 = torch.ops.aten.permute.default(view_3946, [3, 1, 4, 0, 2]);  view_3946 = None
    view_3947 = torch.ops.aten.view.default(permute_2088, [1, 384, 384, 256]);  permute_2088 = None
    _to_copy_2203 = torch.ops.aten._to_copy.default(view_3931, dtype = torch.float32);  view_3931 = None
    native_layer_norm_default_452 = torch.ops.aten.native_layer_norm.default(_to_copy_2203, [256], None, None, 1e-05);  _to_copy_2203 = None
    getitem_3869 = native_layer_norm_default_452[0];  native_layer_norm_default_452 = None
    _to_copy_2204 = torch.ops.aten._to_copy.default(view_3947, dtype = torch.float32);  view_3947 = None
    native_layer_norm_default_453 = torch.ops.aten.native_layer_norm.default(_to_copy_2204, [256], None, None, 1e-05);  _to_copy_2204 = None
    getitem_3872 = native_layer_norm_default_453[0];  native_layer_norm_default_453 = None
    add_432 = torch.ops.aten.add.Tensor(getitem_3869, getitem_3872);  getitem_3869 = getitem_3872 = None
    _to_copy_2205 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_47_triangle_multiplication_linear_z_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_47_triangle_multiplication_linear_z_out_weight = None
    _to_copy_2206 = torch.ops.aten._to_copy.default(add_432, dtype = torch.bfloat16);  add_432 = None
    t_826 = torch.ops.aten.t.default(_to_copy_2205);  _to_copy_2205 = None
    view_3948 = torch.ops.aten.view.default(_to_copy_2206, [147456, 256]);  _to_copy_2206 = None
    mm_769 = torch.ops.aten.mm.default(view_3948, t_826);  view_3948 = t_826 = None
    view_3949 = torch.ops.aten.view.default(mm_769, [1, 384, 384, 256]);  mm_769 = None
    _to_copy_2207 = torch.ops.aten._to_copy.default(getitem_3856, dtype = torch.bfloat16);  getitem_3856 = None
    _to_copy_2208 = torch.ops.aten._to_copy.default(getitem_3849, dtype = torch.bfloat16);  getitem_3849 = None
    t_827 = torch.ops.aten.t.default(_to_copy_2207);  _to_copy_2207 = None
    view_3950 = torch.ops.aten.view.default(_to_copy_2208, [147456, 256]);  _to_copy_2208 = None
    mm_770 = torch.ops.aten.mm.default(view_3950, t_827);  view_3950 = t_827 = None
    view_3951 = torch.ops.aten.view.default(mm_770, [1, 384, 384, 256]);  mm_770 = None
    sigmoid_320 = torch.ops.aten.sigmoid.default(view_3951);  view_3951 = None
    mul_530 = torch.ops.aten.mul.Tensor(view_3949, sigmoid_320);  view_3949 = sigmoid_320 = None
    add_433 = torch.ops.aten.add.Tensor(add_427, mul_530);  mul_530 = None
    _to_copy_2209 = torch.ops.aten._to_copy.default(add_427, dtype = torch.float32)
    native_layer_norm_default_454 = torch.ops.aten.native_layer_norm.default(_to_copy_2209, [256], None, None, 1e-05);  _to_copy_2209 = None
    getitem_3875 = native_layer_norm_default_454[0];  native_layer_norm_default_454 = None
    _to_copy_2210 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_47_triangle_attention_pair2b_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_47_triangle_attention_pair2b_weight = None
    _to_copy_2211 = torch.ops.aten._to_copy.default(getitem_3875, dtype = torch.bfloat16)
    t_828 = torch.ops.aten.t.default(_to_copy_2210);  _to_copy_2210 = None
    view_3952 = torch.ops.aten.view.default(_to_copy_2211, [147456, 256]);  _to_copy_2211 = None
    mm_771 = torch.ops.aten.mm.default(view_3952, t_828);  view_3952 = t_828 = None
    view_3953 = torch.ops.aten.view.default(mm_771, [1, 384, 384, 8]);  mm_771 = None
    view_3954 = torch.ops.aten.view.default(view_3953, [1, 384, 384, 2, 4]);  view_3953 = None
    permute_2089 = torch.ops.aten.permute.default(view_3954, [0, 3, 4, 1, 2]);  view_3954 = None
    view_3955 = torch.ops.aten.view.default(permute_2089, [1, 2, 4, 1, 384, 384]);  permute_2089 = None
    view_3956 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 384, 384])
    bitwise_not_236 = torch.ops.aten.bitwise_not.default(view_3956);  view_3956 = None
    masked_fill_236 = torch.ops.aten.masked_fill.Scalar(view_3955, bitwise_not_236, -10000);  view_3955 = bitwise_not_236 = None
    view_3957 = torch.ops.aten.view.default(masked_fill_236, [1, 2, 4, 384, 384]);  masked_fill_236 = None
    permute_2090 = torch.ops.aten.permute.default(view_3957, [1, 0, 2, 3, 4]);  view_3957 = None
    view_3958 = torch.ops.aten.view.default(permute_2090, [2, 4, 1, 384, 384]);  permute_2090 = None
    _to_copy_2212 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_47_triangle_attention_pair2qkvg1_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_47_triangle_attention_pair2qkvg1_weight = None
    _to_copy_2213 = torch.ops.aten._to_copy.default(getitem_3875, dtype = torch.bfloat16)
    t_829 = torch.ops.aten.t.default(_to_copy_2212);  _to_copy_2212 = None
    view_3959 = torch.ops.aten.view.default(_to_copy_2213, [147456, 256]);  _to_copy_2213 = None
    mm_772 = torch.ops.aten.mm.default(view_3959, t_829);  view_3959 = t_829 = None
    view_3960 = torch.ops.aten.view.default(mm_772, [1, 384, 384, 1024]);  mm_772 = None
    select_107 = torch.ops.aten.select.int(view_3958, 0, 0)
    view_3961 = torch.ops.aten.view.default(view_3960, [1, 384, 384, 4, 4, 64]);  view_3960 = None
    permute_2091 = torch.ops.aten.permute.default(view_3961, [4, 0, 3, 1, 2, 5]);  view_3961 = None
    view_3962 = torch.ops.aten.view.default(permute_2091, [4, 4, 384, 384, 64]);  permute_2091 = None
    unbind_int_175 = torch.ops.aten.unbind.int(view_3962);  view_3962 = None
    getitem_3878 = unbind_int_175[0]
    getitem_3879 = unbind_int_175[1]
    getitem_3880 = unbind_int_175[2]
    getitem_3881 = unbind_int_175[3];  unbind_int_175 = None
    expand_262 = torch.ops.aten.expand.default(select_107, [4, 384, 384, 384]);  select_107 = None
    _scaled_dot_product_efficient_attention_default_153 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3878, getitem_3879, getitem_3880, expand_262, False);  getitem_3878 = getitem_3879 = getitem_3880 = expand_262 = None
    getitem_3882 = _scaled_dot_product_efficient_attention_default_153[0];  _scaled_dot_product_efficient_attention_default_153 = None
    sigmoid_321 = torch.ops.aten.sigmoid.default(getitem_3881);  getitem_3881 = None
    mul_531 = torch.ops.aten.mul.Tensor(getitem_3882, sigmoid_321);  getitem_3882 = sigmoid_321 = None
    view_3963 = torch.ops.aten.view.default(mul_531, [1, 4, 384, 384, 64]);  mul_531 = None
    permute_2092 = torch.ops.aten.permute.default(view_3963, [0, 2, 3, 1, 4]);  view_3963 = None
    clone_325 = torch.ops.aten.clone.default(permute_2092, memory_format = torch.contiguous_format);  permute_2092 = None
    _unsafe_view_270 = torch.ops.aten._unsafe_view.default(clone_325, [1, 384, 384, 256]);  clone_325 = None
    transpose_107 = torch.ops.aten.transpose.int(getitem_3875, 1, 2);  getitem_3875 = None
    _to_copy_2214 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_47_triangle_attention_pair2qkvg2_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_47_triangle_attention_pair2qkvg2_weight = None
    _to_copy_2215 = torch.ops.aten._to_copy.default(transpose_107, dtype = torch.bfloat16);  transpose_107 = None
    t_830 = torch.ops.aten.t.default(_to_copy_2214);  _to_copy_2214 = None
    expand_263 = torch.ops.aten.expand.default(_to_copy_2215, [1, 384, 384, 256]);  _to_copy_2215 = None
    view_3964 = torch.ops.aten.view.default(expand_263, [384, 384, 256]);  expand_263 = None
    expand_264 = torch.ops.aten.expand.default(t_830, [1, 384, 256, 1024]);  t_830 = None
    view_3965 = torch.ops.aten.view.default(expand_264, [384, 256, 1024]);  expand_264 = None
    bmm_309 = torch.ops.aten.bmm.default(view_3964, view_3965);  view_3964 = view_3965 = None
    view_3966 = torch.ops.aten.view.default(bmm_309, [1, 384, 384, 1024]);  bmm_309 = None
    select_108 = torch.ops.aten.select.int(view_3958, 0, 1);  view_3958 = None
    view_3967 = torch.ops.aten.view.default(view_3966, [1, 384, 384, 4, 4, 64]);  view_3966 = None
    permute_2093 = torch.ops.aten.permute.default(view_3967, [4, 0, 3, 1, 2, 5]);  view_3967 = None
    view_3968 = torch.ops.aten.view.default(permute_2093, [4, 4, 384, 384, 64]);  permute_2093 = None
    unbind_int_176 = torch.ops.aten.unbind.int(view_3968);  view_3968 = None
    getitem_3886 = unbind_int_176[0]
    getitem_3887 = unbind_int_176[1]
    getitem_3888 = unbind_int_176[2]
    getitem_3889 = unbind_int_176[3];  unbind_int_176 = None
    expand_265 = torch.ops.aten.expand.default(select_108, [4, 384, 384, 384]);  select_108 = None
    _scaled_dot_product_efficient_attention_default_154 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3886, getitem_3887, getitem_3888, expand_265, False);  getitem_3886 = getitem_3887 = getitem_3888 = expand_265 = None
    getitem_3890 = _scaled_dot_product_efficient_attention_default_154[0];  _scaled_dot_product_efficient_attention_default_154 = None
    sigmoid_322 = torch.ops.aten.sigmoid.default(getitem_3889);  getitem_3889 = None
    mul_532 = torch.ops.aten.mul.Tensor(getitem_3890, sigmoid_322);  getitem_3890 = sigmoid_322 = None
    view_3969 = torch.ops.aten.view.default(mul_532, [1, 4, 384, 384, 64]);  mul_532 = None
    permute_2094 = torch.ops.aten.permute.default(view_3969, [0, 2, 3, 1, 4]);  view_3969 = None
    clone_326 = torch.ops.aten.clone.default(permute_2094, memory_format = torch.contiguous_format);  permute_2094 = None
    _unsafe_view_271 = torch.ops.aten._unsafe_view.default(clone_326, [1, 384, 384, 256]);  clone_326 = None
    cat_59 = torch.ops.aten.cat.default([_unsafe_view_270, _unsafe_view_271], dim = -1);  _unsafe_view_270 = _unsafe_view_271 = None
    slice_210 = torch.ops.aten.slice.Tensor(pairformer_stack_blocks_47_triangle_attention_out_scalers, dim = 0, start = 0, end = 9223372036854775807);  pairformer_stack_blocks_47_triangle_attention_out_scalers = None
    unsqueeze_1262 = torch.ops.aten.unsqueeze.default(slice_210, 1);  slice_210 = None
    mul_533 = torch.ops.aten.mul.Tensor(pairformer_stack_blocks_47_triangle_attention_linear_out_weight, unsqueeze_1262);  pairformer_stack_blocks_47_triangle_attention_linear_out_weight = unsqueeze_1262 = None
    _to_copy_2216 = torch.ops.aten._to_copy.default(mul_533, dtype = torch.bfloat16);  mul_533 = None
    t_831 = torch.ops.aten.t.default(_to_copy_2216);  _to_copy_2216 = None
    view_3970 = torch.ops.aten.view.default(cat_59, [147456, 512]);  cat_59 = None
    mm_773 = torch.ops.aten.mm.default(view_3970, t_831);  view_3970 = t_831 = None
    view_3971 = torch.ops.aten.view.default(mm_773, [1, 384, 384, 256]);  mm_773 = None
    add_434 = torch.ops.aten.add.Tensor(add_433, view_3971);  add_433 = view_3971 = None
    split_tensor_428 = torch.ops.aten.split.Tensor(add_427, 384, dim = -2)
    getitem_3894 = split_tensor_428[0];  split_tensor_428 = None
    _to_copy_2217 = torch.ops.aten._to_copy.default(getitem_3894, dtype = torch.float32);  getitem_3894 = None
    native_layer_norm_default_455 = torch.ops.aten.native_layer_norm.default(_to_copy_2217, [256], pairformer_stack_blocks_47_transition_pair_layer_norm_weight, pairformer_stack_blocks_47_transition_pair_layer_norm_bias, 1e-05);  _to_copy_2217 = pairformer_stack_blocks_47_transition_pair_layer_norm_weight = pairformer_stack_blocks_47_transition_pair_layer_norm_bias = None
    getitem_3895 = native_layer_norm_default_455[0];  native_layer_norm_default_455 = None
    _to_copy_2218 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_47_transition_pair_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_47_transition_pair_linear_no_bias_ab_weight = None
    _to_copy_2219 = torch.ops.aten._to_copy.default(getitem_3895, dtype = torch.bfloat16);  getitem_3895 = None
    t_832 = torch.ops.aten.t.default(_to_copy_2218);  _to_copy_2218 = None
    view_3972 = torch.ops.aten.view.default(_to_copy_2219, [147456, 256]);  _to_copy_2219 = None
    mm_774 = torch.ops.aten.mm.default(view_3972, t_832);  view_3972 = t_832 = None
    view_3973 = torch.ops.aten.view.default(mm_774, [1, 384, 384, 1024]);  mm_774 = None
    split_tensor_429 = torch.ops.aten.split.Tensor(view_3973, 512, dim = -1);  view_3973 = None
    getitem_3898 = split_tensor_429[0]
    getitem_3899 = split_tensor_429[1];  split_tensor_429 = None
    silu_109 = torch.ops.aten.silu.default(getitem_3898);  getitem_3898 = None
    mul_534 = torch.ops.aten.mul.Tensor(silu_109, getitem_3899);  silu_109 = getitem_3899 = None
    _to_copy_2220 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_47_transition_pair_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_47_transition_pair_linear_out_weight = None
    t_833 = torch.ops.aten.t.default(_to_copy_2220);  _to_copy_2220 = None
    view_3975 = torch.ops.aten.view.default(mul_534, [147456, 512]);  mul_534 = None
    mm_775 = torch.ops.aten.mm.default(view_3975, t_833);  view_3975 = t_833 = None
    view_3976 = torch.ops.aten.view.default(mm_775, [1, 384, 384, 256]);  mm_775 = None
    add_435 = torch.ops.aten.add.Tensor(add_434, view_3976);  add_434 = view_3976 = None
    _to_copy_2221 = torch.ops.aten._to_copy.default(add_431, dtype = torch.float32)
    native_layer_norm_default_456 = torch.ops.aten.native_layer_norm.default(_to_copy_2221, [384], pairformer_stack_blocks_47_attention_pair_bias_single_layer_norm_weight, pairformer_stack_blocks_47_attention_pair_bias_single_layer_norm_bias, 1e-05);  _to_copy_2221 = pairformer_stack_blocks_47_attention_pair_bias_single_layer_norm_weight = pairformer_stack_blocks_47_attention_pair_bias_single_layer_norm_bias = None
    getitem_3900 = native_layer_norm_default_456[0];  native_layer_norm_default_456 = None
    _to_copy_2222 = torch.ops.aten._to_copy.default(add_427, dtype = torch.float32);  add_427 = None
    native_layer_norm_default_457 = torch.ops.aten.native_layer_norm.default(_to_copy_2222, [256], pairformer_stack_blocks_47_attention_pair_bias_pair_layer_norm_weight, pairformer_stack_blocks_47_attention_pair_bias_pair_layer_norm_bias, 1e-05);  _to_copy_2222 = pairformer_stack_blocks_47_attention_pair_bias_pair_layer_norm_weight = pairformer_stack_blocks_47_attention_pair_bias_pair_layer_norm_bias = None
    getitem_3903 = native_layer_norm_default_457[0];  native_layer_norm_default_457 = None
    _to_copy_2223 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_47_attention_pair_bias_pair_linear_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_47_attention_pair_bias_pair_linear_weight = None
    _to_copy_2224 = torch.ops.aten._to_copy.default(getitem_3903, dtype = torch.bfloat16);  getitem_3903 = None
    t_834 = torch.ops.aten.t.default(_to_copy_2223);  _to_copy_2223 = None
    view_3977 = torch.ops.aten.view.default(_to_copy_2224, [147456, 256]);  _to_copy_2224 = None
    mm_776 = torch.ops.aten.mm.default(view_3977, t_834);  view_3977 = t_834 = None
    view_3978 = torch.ops.aten.view.default(mm_776, [1, 384, 384, 16]);  mm_776 = None
    permute_2095 = torch.ops.aten.permute.default(view_3978, [0, 3, 1, 2]);  view_3978 = None
    view_3979 = torch.ops.aten.view.default(arg1407_1, [1, 1, 384, 384]);  arg1407_1 = None
    bitwise_not_237 = torch.ops.aten.bitwise_not.default(view_3979);  view_3979 = None
    masked_fill_237 = torch.ops.aten.masked_fill.Scalar(permute_2095, bitwise_not_237, -10000);  permute_2095 = bitwise_not_237 = None
    _to_copy_2225 = torch.ops.aten._to_copy.default(getitem_3900, dtype = torch.bfloat16);  getitem_3900 = None
    _to_copy_2226 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_47_attention_pair_bias_attention_input2qkvg_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_47_attention_pair_bias_attention_input2qkvg_weight = None
    unsqueeze_1263 = torch.ops.aten.unsqueeze.default(_to_copy_2225, 3);  _to_copy_2225 = None
    unsqueeze_1264 = torch.ops.aten.unsqueeze.default(unsqueeze_1263, 4);  unsqueeze_1263 = None
    unsqueeze_1265 = torch.ops.aten.unsqueeze.default(unsqueeze_1264, 5);  unsqueeze_1264 = None
    permute_2096 = torch.ops.aten.permute.default(unsqueeze_1265, [3, 0, 4, 1, 5, 2]);  unsqueeze_1265 = None
    unsqueeze_1266 = torch.ops.aten.unsqueeze.default(_to_copy_2226, 4);  _to_copy_2226 = None
    unsqueeze_1267 = torch.ops.aten.unsqueeze.default(unsqueeze_1266, 5);  unsqueeze_1266 = None
    permute_2097 = torch.ops.aten.permute.default(unsqueeze_1267, [1, 4, 2, 5, 3, 0]);  unsqueeze_1267 = None
    permute_2098 = torch.ops.aten.permute.default(permute_2096, [3, 5, 0, 1, 2, 4]);  permute_2096 = None
    view_3980 = torch.ops.aten.view.default(permute_2098, [1, 384, 384]);  permute_2098 = None
    permute_2099 = torch.ops.aten.permute.default(permute_2097, [5, 0, 1, 2, 4, 3]);  permute_2097 = None
    view_3981 = torch.ops.aten.view.default(permute_2099, [1, 384, 1536]);  permute_2099 = None
    bmm_310 = torch.ops.aten.bmm.default(view_3980, view_3981);  view_3980 = view_3981 = None
    view_3982 = torch.ops.aten.view.default(bmm_310, [384, 1, 4, 1, 16, 24]);  bmm_310 = None
    permute_2100 = torch.ops.aten.permute.default(view_3982, [2, 3, 4, 0, 5, 1]);  view_3982 = None
    view_3983 = torch.ops.aten.view.default(permute_2100, [4, 1, 16, 384, 24]);  permute_2100 = None
    unbind_int_177 = torch.ops.aten.unbind.int(view_3983);  view_3983 = None
    getitem_3906 = unbind_int_177[0]
    getitem_3907 = unbind_int_177[1]
    getitem_3908 = unbind_int_177[2]
    getitem_3909 = unbind_int_177[3];  unbind_int_177 = None
    view_3984 = torch.ops.aten.view.default(pairformer_stack_blocks_47_attention_pair_bias_attention_query_bias, [1, 16, 1, 24]);  pairformer_stack_blocks_47_attention_pair_bias_attention_query_bias = None
    add_436 = torch.ops.aten.add.Tensor(getitem_3906, view_3984);  getitem_3906 = view_3984 = None
    _to_copy_2227 = torch.ops.aten._to_copy.default(add_436, dtype = torch.bfloat16);  add_436 = None
    expand_266 = torch.ops.aten.expand.default(masked_fill_237, [1, 16, 384, 384]);  masked_fill_237 = None
    _scaled_dot_product_efficient_attention_default_155 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_2227, getitem_3907, getitem_3908, expand_266, False);  _to_copy_2227 = getitem_3907 = getitem_3908 = expand_266 = None
    getitem_3910 = _scaled_dot_product_efficient_attention_default_155[0];  _scaled_dot_product_efficient_attention_default_155 = None
    add_437 = torch.ops.aten.add.Tensor(getitem_3909, 1);  getitem_3909 = None
    sigmoid_323 = torch.ops.aten.sigmoid.default(add_437);  add_437 = None
    mul_535 = torch.ops.aten.mul.Tensor(getitem_3910, sigmoid_323);  getitem_3910 = sigmoid_323 = None
    _to_copy_2228 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_47_attention_pair_bias_attention_output_proj_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_47_attention_pair_bias_attention_output_proj_weight = None
    unsqueeze_1268 = torch.ops.aten.unsqueeze.default(mul_535, 4);  mul_535 = None
    permute_2101 = torch.ops.aten.permute.default(unsqueeze_1268, [0, 2, 4, 3, 1]);  unsqueeze_1268 = None
    unsqueeze_1269 = torch.ops.aten.unsqueeze.default(_to_copy_2228, 3);  _to_copy_2228 = None
    unsqueeze_1270 = torch.ops.aten.unsqueeze.default(unsqueeze_1269, 4);  unsqueeze_1269 = None
    permute_2102 = torch.ops.aten.permute.default(unsqueeze_1270, [3, 4, 2, 1, 0]);  unsqueeze_1270 = None
    permute_2103 = torch.ops.aten.permute.default(permute_2101, [1, 3, 4, 0, 2]);  permute_2101 = None
    clone_327 = torch.ops.aten.clone.default(permute_2103, memory_format = torch.contiguous_format);  permute_2103 = None
    _unsafe_view_272 = torch.ops.aten._unsafe_view.default(clone_327, [1, 384, 384]);  clone_327 = None
    permute_2104 = torch.ops.aten.permute.default(permute_2102, [3, 4, 0, 2, 1]);  permute_2102 = None
    clone_328 = torch.ops.aten.clone.default(permute_2104, memory_format = torch.contiguous_format);  permute_2104 = None
    _unsafe_view_273 = torch.ops.aten._unsafe_view.default(clone_328, [1, 384, 384]);  clone_328 = None
    bmm_311 = torch.ops.aten.bmm.default(_unsafe_view_272, _unsafe_view_273);  _unsafe_view_272 = _unsafe_view_273 = None
    view_3985 = torch.ops.aten.view.default(bmm_311, [384, 1, 1, 1, 384]);  bmm_311 = None
    permute_2105 = torch.ops.aten.permute.default(view_3985, [3, 0, 4, 1, 2]);  view_3985 = None
    view_3986 = torch.ops.aten.view.default(permute_2105, [1, 384, 384]);  permute_2105 = None
    unsqueeze_1271 = torch.ops.aten.unsqueeze.default(arg1406_1, -1);  arg1406_1 = None
    mul_536 = torch.ops.aten.mul.Tensor(view_3986, unsqueeze_1271);  view_3986 = unsqueeze_1271 = None
    add_438 = torch.ops.aten.add.Tensor(add_431, mul_536);  mul_536 = None
    split_tensor_430 = torch.ops.aten.split.Tensor(add_431, 384, dim = -2);  add_431 = None
    getitem_3914 = split_tensor_430[0];  split_tensor_430 = None
    _to_copy_2229 = torch.ops.aten._to_copy.default(getitem_3914, dtype = torch.float32);  getitem_3914 = None
    native_layer_norm_default_458 = torch.ops.aten.native_layer_norm.default(_to_copy_2229, [384], pairformer_stack_blocks_47_transition_single_layer_norm_weight, pairformer_stack_blocks_47_transition_single_layer_norm_bias, 1e-05);  _to_copy_2229 = pairformer_stack_blocks_47_transition_single_layer_norm_weight = pairformer_stack_blocks_47_transition_single_layer_norm_bias = None
    getitem_3915 = native_layer_norm_default_458[0];  native_layer_norm_default_458 = None
    _to_copy_2230 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_47_transition_single_linear_no_bias_ab_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_47_transition_single_linear_no_bias_ab_weight = None
    _to_copy_2231 = torch.ops.aten._to_copy.default(getitem_3915, dtype = torch.bfloat16);  getitem_3915 = None
    t_835 = torch.ops.aten.t.default(_to_copy_2230);  _to_copy_2230 = None
    view_3987 = torch.ops.aten.view.default(_to_copy_2231, [384, 384]);  _to_copy_2231 = None
    mm_777 = torch.ops.aten.mm.default(view_3987, t_835);  view_3987 = t_835 = None
    view_3988 = torch.ops.aten.view.default(mm_777, [1, 384, 1536]);  mm_777 = None
    split_tensor_431 = torch.ops.aten.split.Tensor(view_3988, 768, dim = -1);  view_3988 = None
    getitem_3918 = split_tensor_431[0]
    getitem_3919 = split_tensor_431[1];  split_tensor_431 = None
    silu_110 = torch.ops.aten.silu.default(getitem_3918);  getitem_3918 = None
    mul_537 = torch.ops.aten.mul.Tensor(silu_110, getitem_3919);  silu_110 = getitem_3919 = None
    _to_copy_2232 = torch.ops.aten._to_copy.default(pairformer_stack_blocks_47_transition_single_linear_out_weight, dtype = torch.bfloat16);  pairformer_stack_blocks_47_transition_single_linear_out_weight = None
    t_836 = torch.ops.aten.t.default(_to_copy_2232);  _to_copy_2232 = None
    view_3990 = torch.ops.aten.view.default(mul_537, [384, 768]);  mul_537 = None
    mm_778 = torch.ops.aten.mm.default(view_3990, t_836);  view_3990 = t_836 = None
    view_3991 = torch.ops.aten.view.default(mm_778, [1, 384, 384]);  mm_778 = None
    add_439 = torch.ops.aten.add.Tensor(add_438, view_3991);  add_438 = view_3991 = None
    return pytree.tree_unflatten((add_439, add_435), self._out_spec)
